{
    "paperId": [
        "3b9f6bcd462e7bcfe33ed9d152abd3a88e8f52cc",
        "73f402be8b3e5d57295ab6a3918f4a73626ef362",
        "4066c7764c021daff18cfb3413477537a80712d8",
        "ff955d5890388e0d30038db39b7b979071b62ce9",
        "2cc739dd0b42d2463e6413d2ab8ed0bf578f0f8f",
        "2b4df34c178fa53893ca325afc4e0f4c6128a1f0",
        "c6f1821a464273fb9c52b132327a798d97b87212",
        "d8df315a84133b11c05309f173b0eb7ebde647ff",
        "ce7a910fa5056d8b632ac55a0ad383aa673575a5",
        "c02f1687def234b46c80890b617790d3fcf3d266",
        "61ca1b51dad6e2d3a3a991d8bf7b7034471c4e9c",
        "ab0c5e05dc2be760c6bea2412cdd4787ea4ea740",
        "d8108cc291537782a336987a540ff55f94e6ffd8",
        "cda3d900c2436faee53ca1082fcd8b544a6f2623",
        "5ee5ef7440c54ab3d6337616c201a848a55ef12e",
        "1fefecf4d5174910b46d9497a7dae3cc70e4ba69",
        "dd2bce906f0d4c21df32864c22e53b6574b8ee57",
        "4634ceff81333cea40b007bbd1c8dd480db35cf3",
        "e8e6e678dcf77766945f94a91013ac86b7fb74ba",
        "d8cadaabc17880ee4ee769ce4f4c398f471d9f6b",
        "5b6f15a62639fb9bfed2cd1bb23239ec52d27474",
        "805474b35bc739baa6cc9900a22789b76ea54338",
        "bb0fd99cf3d782b9a73e8504ad38d5088597725b",
        "62704ad1354aaa70b1feb779938825915efb3e48",
        "6131c9a3a28d1dae68f4755090eda9952ecc59f8",
        "78c491fb642b67717da4c64617869100388162b3",
        "338aaa39d6c3f769f671a6570365bf93d2e12f9d",
        "37610489569fc0c8b324270f1e651e0c8a2cddb3",
        "4dd6cb975a96a96755177c4ced5f62df975fd22a",
        "2f71ff8ef4334ceb0c8524b767ba9926102b9a45",
        "b124b49b57887a26c1ec254a249d7c901a473bef",
        "33325291f2666984648998e87b0511c2a847ad9a",
        "c5a2fa9f863352994100461f2a9d766db0cc9a0c",
        "6da4c856383161431554828ee4d6d5df6220cbd6",
        "9d3134ff7eee32f829dcbafb4a02710269fbc5bf",
        "cc1041769f0ef883d3d866ff0a49165f397b65a4",
        "e8c2704cd19fc2e41067f763e8b60bf41d1d1dc4",
        "902d2a8340e926e31edf71967c6852f2f73a1142",
        "ff9f969dcac14d534db21b309eb37c940272ec39",
        "6164a904794e7ebecd4ff738a4f6b29c38dbf955",
        "4634489cd4bc446b2b5cd2cc82b9d4a0d8055430",
        "39d54375dd9187bc8db711daa028f7de7f675f6b",
        "0c6579f22404a58ff07f27822b1d6decb0a2ef8a",
        "78e8e6f5214f9771dacfce4e5717b17b0f2b7f66",
        "95cf294ca51db1b88b3a5777f684550b85411f02",
        "615e0c7a4c96ebcd8fa9b0477085d11e0a390e7a",
        "10b380c8bf3e08d9de0ef8066dbcd543f39f9ce6",
        "ea47aa32b4c5bb52dec038e918730072a96aeb98",
        "68df071f3f18ee9e2aa69057c522dedc863f3493",
        "fbc9e7523f62e335a485c2b85742eb82dd377ab2"
    ],
    "title": [
        "What impact do chronic disease self-management support interventions have on health inequity gaps related to socioeconomic status: a systematic review",
        "Interactions between Climate Change and Infrastructure Projects in Changing Water Resources: An Ethnobiological Perspective from the Daasanach, Kenya",
        "Recommendations for the Development of Socioeconomically-Situated and Clinically-Relevant Neuroimaging Models of Pain",
        "How do pregnant and lactating women, and young children, experience religious food restriction at the community level? A qualitative study of fasting traditions and feeding behaviors in four regions of Ethiopia",
        "Data as Entanglement: New Definitions and Uses of Data in Qualitative Research, Policy, and Neoliberal Governance",
        "From older to younger: intergenerational promotion of health behaviours in Portuguese families affected by familial amyloid polyneuropathy",
        "Psychological distress, fear and coping among Malaysians during the COVID-19 pandemic",
        "Alcohol-Related Disparities Among Women: Evidence and Potential Explanations",
        "Discrimination against and Associated Stigma Experienced by Transgender Women with Intersectional Identities in Thailand",
        "Implementing computer-mediated intercultural communication in English education: A critical reflection on its pedagogical challenges",
        "Pregnant women, their male partners and health care providers\u2019 perceptions of HIV self-testing in Kampala, Uganda: Implications for integration in prevention of mother-to-child transmission programs and scale-up",
        "Consent to Specimen Storage and Continuing Studies by Race and Ethnicity: A Large Dataset Analysis Using the 2011-2012 National Health and Nutrition Examination Survey",
        "Contraceptive Methods Women Have Ever Used:United States, 2015-2019.",
        "Social Innovation and Sustainable Development: An Analysis of its Impact Areas and its Relationship with the SDGs.",
        "Enhancing health outcomes for M\u0101ori elders through an intergenerational cultural exchange and physical activity programme: a cross-sectional baseline study",
        "Viral misinformation and echo chambers: the diffusion of rumors about genetically modified organisms on social media",
        "The Impact on Two Practising Social Workers Who Taught Social Work Students in a University Setting",
        "Changes in Islamic Society and Culture in Customary Marriage Within the Uluan Musi Community",
        "Social Complex Contagion in Music Listenership: A Natural Experiment with 1.3 Million Participants",
        "Sibling Violence and Position in Sibling Dyad in a Sample of Adolescents: How Does It Relate to Self-Esteem?",
        "The Magnitude and Impact of Bullying among School Pupils in Muscat, Oman: A Cross-Sectional Study",
        "Ethnography: principles, practice and potential.",
        "Exploring the \"Time Bank\" Mutual Support Model for the Elderly in Linyi City",
        "Male involvement in prevention of mother to child transmission of human immunodeficiency virus and associated factors in Enebsiesarmider District, north West Ethiopia, 2018: a cross-sectional study",
        "\u201cThe Grind Never Stops\u201d Mental Health and Expectations of Productivity in the North American University",
        "Child abuse in the West Bank of the occupied Palestinian territory (WB/oPt): social and political determinants",
        "Meeting the Multifaceted Needs of Expectant and Parenting Young Families Through the Pregnancy Assistance Fund",
        "Association between socioeconomic status and net survival after primary lung cancer surgery: a tertiary university hospital retrospective observational study in Japan.",
        "Social resilience in online communities: the autopsy of friendster",
        "Patients\u2019 socioeconomic status and their evaluations of primary care in Hong Kong",
        "What contributes to the long-term implementation of an evidence-based early childhood intervention: a qualitative study from Germany",
        "Caregiving in a Pandemic: Health-Related Socioeconomic Vulnerabilities Among Women Caregivers Early in the COVID-19 Pandemic",
        "The Effect of Intergenerational Parenting Mode on Children\u2019s Outdoor Activities: A Case Study of Downtown Shanghai Communities",
        "Sense of coherence and intentions to retire early among Finnish women and men",
        "Measuring the Parenting Practices of Custodial Grandmothers",
        "Qualitative description of outreach and engagement in perinatal substance treatment in Finland",
        "Factors associated with elder abuse and neglect in rural Uganda: A cross-sectional study of community older adults attending an outpatient clinic",
        "Getting from A to IRB: Developing an Institutional Review Board at a Historically Black University",
        "A Way Forward: Transparency at American Law Schools",
        "The Art and Science of Networking Extension",
        "Analysing global professional gender gaps using LinkedIn advertising data",
        "Barriers to therapeutic clinical trials enrollment: Differences between African-American and white cancer patients identified at the time of eligibility assessment",
        "Why Don\u2019t Employers Hire and Retain Workers with Disabilities?",
        "Male Hegemony through Education: Construction of Gendered Identities",
        "The History of Mental Health Services in Modern England: Practitioner Memories and the Direction of Future Research",
        "A Systematic Literature Review of Predictors of Social Media Popularity",
        "Role of Social and Informational Support while Deciding on Pregnancy Termination for Medical Reasons",
        "The role of different social contexts in shaping influenza transmission during the 2009 pandemic",
        "Traditions for Future Cross-National Food Security\u2014Food and Foraging Practices among Different Native Communities in the Western Himalayas",
        "Modeling framework unifying contact and social networks."
    ],
    "year": [
        2019,
        2021,
        2021,
        2018,
        2018,
        2017,
        2021,
        2020,
        2022,
        2018,
        2021,
        2014,
        2023,
        2023,
        2023,
        2020,
        2023,
        2023,
        2017,
        2023,
        2014,
        2015,
        2023,
        2020,
        2023,
        2020,
        2020,
        2023,
        2013,
        2013,
        2024,
        2022,
        2023,
        2010,
        2015,
        2023,
        2023,
        2010,
        2012,
        2020,
        2021,
        2012,
        2011,
        2012,
        2015,
        2023,
        2018,
        2014,
        2022,
        2023
    ],
    "publicationTypes": [
        [
            "JournalArticle",
            "Review"
        ],
        [
            "JournalArticle"
        ],
        [
            "JournalArticle",
            "Review"
        ],
        [
            "JournalArticle"
        ],
        [
            "JournalArticle"
        ],
        [
            "JournalArticle"
        ],
        [
            "JournalArticle",
            "Review"
        ],
        [
            "Review",
            "JournalArticle"
        ],
        [
            "JournalArticle"
        ],
        [
            "JournalArticle"
        ],
        [
            "JournalArticle"
        ],
        [
            "JournalArticle",
            "Review"
        ],
        [
            "JournalArticle",
            "CaseReport",
            "Review"
        ],
        [
            "JournalArticle",
            "Review"
        ],
        [
            "JournalArticle",
            "Review"
        ],
        [
            "JournalArticle"
        ],
        [
            "JournalArticle"
        ],
        [
            "JournalArticle"
        ],
        [
            "JournalArticle"
        ],
        [
            "JournalArticle"
        ],
        [
            "Study",
            "JournalArticle",
            "Review"
        ],
        [
            "JournalArticle"
        ],
        [
            "JournalArticle"
        ],
        [
            "JournalArticle",
            "Review"
        ],
        [
            "JournalArticle"
        ],
        [
            "JournalArticle"
        ],
        [
            "JournalArticle",
            "Review"
        ],
        [
            "JournalArticle"
        ],
        [
            "JournalArticle"
        ],
        [
            "JournalArticle",
            "Review"
        ],
        [
            "JournalArticle"
        ],
        [
            "JournalArticle",
            "Review"
        ],
        [
            "JournalArticle"
        ],
        [
            "JournalArticle",
            "Review"
        ],
        [
            "JournalArticle"
        ],
        [
            "JournalArticle"
        ],
        [
            "JournalArticle"
        ],
        [
            "JournalArticle",
            "Review"
        ],
        [
            "JournalArticle",
            "Review"
        ],
        [
            "JournalArticle"
        ],
        [
            "JournalArticle"
        ],
        [
            "JournalArticle"
        ],
        [
            "JournalArticle",
            "Review"
        ],
        [
            "JournalArticle"
        ],
        [
            "JournalArticle"
        ],
        [
            "JournalArticle",
            "Review"
        ],
        [
            "JournalArticle",
            "Review"
        ],
        [
            "JournalArticle"
        ],
        [
            "JournalArticle"
        ],
        [
            "JournalArticle"
        ]
    ],
    "Abstract": [
        "Background: The social gradient in chronic disease (CD) is well-documented, and the ability to effectively selfmanage is crucial to reducing morbidity and mortality from CD. This systematic review aimed to assess the moderating effect of socioeconomic status on self-management support (SMS) interventions in relation to participation, retention and post-intervention outcomes. Methods: Six databases were searched for studies of any design published until December 2018. Eligible studies reported on outcomes from SMS interventions for adults with chronic disease, where socioeconomic status was recorded and a between-groups comparison on SES was made. Possible outcomes were participation rates, retention rates and clinical or behavioural post-intervention results. Results: Nineteen studies were retrieved, including five studies on participation, five on attrition and nine studies reporting on outcomes following SMS intervention. All participation studies reported reduced engagement in low SES cohorts. Studies assessing retention and post-intervention outcomes had variable results, related to the diversity of interventions. A reduction in health disparity was seen in longer interventions that were individually tailored. Most studies did not provide a theoretical justification for the intervention being investigated, although four studies referred to Bandura's concept of self-efficacy.The limited research suggests that socioeconomic status does moderate the efficacy of SMS interventions, such that without careful tailoring and direct targeting of barriers to self-management, SMS may exacerbate the social gradient in chronic disease outcomes. Screening for patient disadvantage or workload, rather than simply recording SES, may increase the chances of tailored interventions being directed to those most likely to benefit from them. Future interventions for low SES populations should consider focussing more on treatment burden and patient capacity.",
        "The fast and widespread environmental changes that have intensified in the last decades are bringing disproportionate impacts to Indigenous Peoples and Local Communities. Changes that affect water resources are particularly relevant for subsistence-based peoples, many of whom already suffer from constraints regarding reliable access to safe water. Particularly in areas where water is scarce, climate change is expected to amplify existing stresses in water availability, which are also exacerbated by multiple socioeconomic drivers. In this paper, we look into the local perceptions of environmental change expressed by the Daasanach people of northern Kenya, where the impacts of climate change overlap with those brought by large infrastructure projects recently established in the Omo River. We show that the Daasanach have rich and detailed understanding of changes in their environment, especially in relation to water resources. Daasanach understand observations of change in different elements of the social-ecological system as an outcome of complex interactions between climatic and non-climatic drivers of change. Our findings highlight the perceived synergistic effects of climate change and infrastructure projects in water resources, driving multiple and cascading impacts on biophysical elements and local livelihoods. Our results also demonstrate the potential of Local Ecological Knowledge in enhancing the understanding of complex social-ecological issues, such as the impacts of environmental change in local communities. To minimize and mitigate the social-ecological impacts of development projects, it is essential to consider potential synergies between climatic and socioeconomic factors and to ensure inclusive governance rooted in local understandings of environmental change.",
        "Pain is a complex, multidimensional experience that emerges from interactions among sensory, affective, and cognitive processes in the brain. Neuroimaging allows us to identify these component processes and model how they combine to instantiate the pain experience. However, the clinical impact of pain neuroimaging models has been limited by inadequate population sampling -young healthy college students are not representative of chronic pain patients. The biopsychosocial approach to pain management situates a person's pain within the diverse socioeconomic environments they live in. To increase the clinical relevance of pain neuroimaging models, a three-fold biopsychosocial approach to neuroimaging biomarker development is recommended. The first level calls for the development of diagnostic biomarkers via the standard population-based (nomothetic) approach with an emphasis on diverse sampling. The second level calls for the development of treatment-relevant models via a constrained person-based (idiographic) approach tailored to unique individuals. The third level calls for the development of prevention-relevant models via a novel society-based (social epidemiologic) approach that combines survey and neuroimaging data to predict chronic pain risk based on one's socioeconomic conditions. The recommendations in this article address how we can leverage pain's complexity in service of the patient and society by modeling not just individuals and populations, but also the socioeconomic structures that shape any individual's expectations of threat, safety, and resource availability.",
        "Maternal and child feeding behaviors are often rooted in family and sociocultural context, making these an important point of inquiry for improving nutrition and health over the life course. The present study explored the practice of fasting during religious periods in relation to eating patterns of pregnant and lactating women and young children in four regions of Ethiopia, a nation which has experienced rapid economic growth and marked improvement in health and nutrition outcomes over the last two decades. Qualitative data collection and analysis at community level illustrated conflicting areas of understanding and practice related to diets of children and pregnant and lactating women during fasting times, potentially leading to gaps in nutrition. Community participants described different understandings of fasting requirements for these vulnerable populations and associated social norms and doxa, not always in accordance with religious texts or published guidance. Useful behavior change strategies may be developed through these results to address the potential barriers to appropriate feeding patterns for pregnant and lactating women and young children in Ethiopia. This will include continuing to work with communities and religious leaders to clarify that religious doctrine promotes improved nutrition outcomes.",
        "Data is an increasingly contested term and concept in qualitative research, but its definition and use is also changing in social policy development and public service management. The paper will explore these parallel and apparently independent developments and argue that, while deriving from different fields and aspirations, these developments have elements in common and data is a term now as much applied to and used in political governance, as it is in (what used to be seen as) disinterested science.",
        "The role of older generations in families with hereditary diseases has been recognised and associated to their function as guardians of the family's medical history. However, research is scarce in examining the roles that older generations play in terms of health promotion and risk management towards younger generations, which is particularly evident with incurable genetically inherited disorders such as familial amyloid polyneuropathy (FAP) ATTR Val30Met. This qualitative exploratory study examines the roles that older generations play towards younger generations, in terms of health promotion and risk management, in families with FAP. It also explores the intergenerational flow by analysing who from the older generation plays what role(s) towards whom from the younger generation. This study adopts the critical incidents technique. The sample comprises 18 participants that reported 76 critical incidents. The interviews were audio-taped and submitted for content analysis with the main findings suggesting four roles performed by the older family members towards the younger ones: modelling, encouraging, informing and supporting. The intergenerational flow takes place mostly between women, from mother to daughter, and from older affected individuals to young pre-symptomatic carriers. The older generations can be involved in the clinical practice as partners in supporting younger relatives in families with FAP. Clinical genetic services and the health-care system more broadly might want to consider these roles and the intergenerational flow of support so that this information can be used to maximise health promotion behaviours in at-risk families.",
        "The COVID-19 pandemic has enormously affected the psychological well-being, social and working life of millions of people across the world. This study aimed to investigate the psychological distress, fear and coping strategies as a result of the COVID-19 pandemic and its associated factors among Malaysian residents.Participants were invited to an online cross-sectional survey from Aug-Sep 2020. The study assessed psychological distress using the Kessler Psychological Distress Scale, level of fear using the Fear of COVID-19 Scale, and coping strategies using the Brief Resilient Coping Scale. Univariate and multivariate logistic regression analyses were conducted to adjust for potential confounders.The mean age (\u00b1SD) of the participants (N = 720) was 31.7 (\u00b111.5) years, and most of them were females (67.1%). Half of the participants had an income source, while 216 (30%) identified themselves as frontline health or essential service workers. People whose financial situation was impacted due to COVID-19 (AOR 2.16, 95% CIs 1.54-3.03), people who drank alcohol in the last four weeks (3.43, 1.45-8.10), people who were a patient (2.02, 1.39-2.93), and had higher levels of fear of ) were more likely to have higher levels of psychological distress. Participants who self-isolated due to exposure to ) and who had moderate to very high levels of psychological",
        "Although research on alcohol-related disparities among women is a highly understudied area, evidence shows that racial/ethnic minority women, sexual minority women, and women of low socioeconomic status (based on education, income, or residence in disadvantaged neighborhoods) are more likely to experience alcohol-related problems. These problems include alcohol use disorder, particularly after young adulthood, and certain alcohol-related health, morbidity, and mortality outcomes. In some cases, disparities may reflect differences in alcohol consumption, but in other cases such disparities appear to occur despite similar and possibly lower levels of consumption among the affected groups. To understand alcoholrelated disparities among women, several factors should be considered. These include age; the duration of heavy drinking over the life course; the widening disparity in cumulative socioeconomic disadvantage and health in middle adulthood; social status; sociocultural context; genetic factors that affect alcohol metabolism; and access to and quality of alcohol treatment services and health care. To inform the development of interventions that might mitigate disparities among women, research is needed to identify the factors and mechanisms that contribute most to a group's elevated risk for a given alcohol-related problem.",
        "Although Thailand is overtly open to diversity and promotes equality, discrimination of minorities based on gender, ethnicity, and/or certain occupations is unfortunately still prevalent. Society either obstructs their inclusion or accepts them but only under certain conditions. The objective of this study is to examine the discrimination of TGWs with intersectional identities within Thai society. A total of 19 TGW participants were recruited and underwent in-depth thematic interviews about their experiences of discrimination. Rechecking of the extracted information from the interview transcripts and the subsequent encoding process were conducted using the NVivo program. The results show that the median age was 30 years old, and the majority of the individuals with intersectional identities were ethnic minority TGWs (47%). The in-depth interviews were divided into four main themes, including discrimination at an educational institution, discrimination in the workplace, discrimination in daily life, and discrimination at a healthcare facility. Our findings reflect problems associated with multiple sources of discrimination aimed at transgender women with an intersectional identity in Thailand in every aspect, including harsh speech or physical abuse; occupational, social, and legal inequality; and healthcare provision disparity. Raising awareness about gender diversity and intersectionality, as well as enforcing anti-bullying legislation and antidiscrimination laws, should be continually pursued in order to protect the rights and improve the quality of life of transgender individuals with an intersectional identity.",
        "Integrating Computer-Mediated Intercultural Communication (CMIC) activities into language curricula has been discussed as an innovative approach to supporting students in developing intercultural communicative competence, an essential skill in today's internationalised society. Despite the increasing number of successful examples reporting positive outcomes of adopting CMIC activities in language learning environments, it has also been noted that there are a number of significant pedagogical challenges observed when the CMIC activities are introduced into classrooms. However, previous studies tend only to briefly mention some of those challenges and to list general solutions to them, rather than deeply engaging with the issues and their educational and social consequences upon students learning and language practices. Thus, this qualitative case study aims to investigate specifically such issues: it documents diverse pedagogical challenges faced by Korean students and a teacher in their EFL classroom, in which a series of CMIC activities were set up and conducted. The challenges analysed in this paper include: the unrealistically high implementation cost (which is not merely financial); diverse educational inequality issues created by irreducible gaps in students' socio-economic backgrounds and perpetuated within CMIC activities; and the unresolved struggles to develop positive student subjectivities and linguistic identities. In conclusion, the article proposes three teaching and instructional principles, drawn in turn from the two closely interrelated theories: Multiliteracies Pedagogy and Critical Pedagogy. Fundamentally, the author argues that effective adoption of the CMIC activities needs a more pedagogically, socially, and culturally sensitive approach.",
        "HIV status awareness is critical for HIV prevention and care but HIV testing rates remain low in Uganda, especially among men. One suggested approach to increase access and utilisation of HIV testing services is HIV self-testing. We explored perceptions of pregnant and lactating women and their male partners who attended antenatal care, and health care providers in a government hospital in Kampala, Uganda, about HIV self-testing for initial or repeat testing for women and their partners during pregnancy and postpartum We draw implications for scaling-up this new testing approach in Uganda.This was a qualitative study conducted at Mulago National Referral Hospital, Kampala, Uganda, between April and December 2017. We conducted in-depth interviews with five pregnant or lactating women and their five male partners; five focus group discussions (two with women, two with health workers and one with male partners of women attending antenatal care) and five key informant interviews with health workers providing prevention of mother-to-child HIV transmission (PMTCT) services. Data were analysed using content thematic approach.There was limited awareness about HIV self-testing especially among pregnant or lactating women and their male partners. Study participants mentioned that HIV self-testing would",
        "Purpose. To determine if significant differences exist in consent rates for biospecimen storage and continuing studies between non-Hispanic Whites and minority ethnic groups in the National Health and Nutrition Examination Survey (NHANES). Methods. Using logistic regression, we analyzed 2011-2012 NHANES data to determine whether race/ethnicity, age, gender, and education level influence consent to specimen storage or future testing. Results. Compared to non-Hispanic Whites, some minorities were less willing to donate a specimen for storage and continuing studies, including other Hispanics (non-Mexican) (OR 0.236, 95% CI: 0.079, 0.706), non-Hispanic Asians (OR 0.212, 95% CI: 0.074, 0.602), and other/multiracial ethnic groups (OR 0.189, 95% CI: 0.037, 0.957). Within race and ethnic groups, those aged 20-39 years (OR 2.215, 95% CI: 1.006-4.879) and 40-59 years (OR 9.375,) are more willing than those over 60 years to provide consent. Conclusion. Lower consent rates by other Hispanics, non-Hispanic Asians, and other/multiracial individuals in this study represent the first published comparison of consent rates among these groups to our knowledge. To best meet the health care needs of this segment of the population and to aid in designing future genetic studies, reassessment of ethnic minority groups concerning these issues is important.",
        "Objective-This report describes methods of contraception ever used by U.S. women ages 15-49 who had ever had sexual intercourse with a male partner. Estimates are shown overall and by Hispanic origin and race, education, religious affiliation and importance, and urban-rural residence. Discontinuation of selected contraceptive methods is also described. Methods-This report focuses on information collected from the 11,695 women ages 15-49 interviewed in the 2015-2019 National Survey of Family Growth, a nationally representative survey conducted by the Centers for Disease Control and Prevention's National Center for Health Statistics. Most estimates shown are based on data on contraceptive methods ever used by the 10,122 interviewed women who had ever had sexual intercourse with a male partner. Results-Based on 2015-2019 data, virtually all women of reproductive age who had ever had sexual intercourse with a male partner used at least one contraceptive method at some point in their life up to the time of interview (99.2%, or 63.2 million women ages 15-49), including 87.8% who had ever used a \"most or moderately effective reversible method\": the pill, an injectable, contraceptive patch, contraceptive ring, contraceptive implant, or intrauterine device. Most women had used the male condom with a partner (94.5%), the pill (79.8%), or withdrawal (65.7%). About one in four women reported ever using long-acting reversible contraception (intrauterine device or contraceptive implant) (24.9%) or emergency contraception (23.5%). The type of methods ever used varied by Hispanic origin and race, nativity among Hispanic women, education, religious affiliation and importance, and urban-rural residence. Among women who had ever discontinued use of the pill or intrauterine devices due to dissatisfaction (and not for seeking a pregnancy), side effects were the most common reason.",
        "Resumen: En la \u00faltima d\u00e9cada, la innovaci\u00f3n social ha surgido como soluci\u00f3n multidisciplinar para abordar diversos problemas sociales. Este estudio emplea una metodolog\u00eda que combina la revisi\u00f3n de la literatura y entrevistas con expertos en innovaci\u00f3n social. Como resultado, se identificaron 17 \u00e1reas de impacto principales, clasificadas en tres bloques: econ\u00f3mico, global y social. Lo m\u00e1s notable es que estas \u00e1reas est\u00e1n significativamente relacionadas con todos los Objetivos de Desarrollo Sostenible (ODS). Esto sugiere que la innovaci\u00f3n social tiene un notable potencial como herramienta facilitadora para la consecuci\u00f3n de los ODS, destacando su importancia en la b\u00fasqueda de soluciones eficaces a los retos globales del desarrollo sostenible.",
        "Enhancing health outcomes for M \u0101ori elders through an intergenerational cultural exchange and physical activity programme: a cross-sectional baseline study.",
        "Purpose -The spread of rumors on social media has caused increasing concerns about an under-informed or even misinformed public when it comes to scientific issues. However, researchers have rarely investigated their diffusion in non-western contexts. This study aims to systematically examine the content and network structure of rumor-related discussions around genetically modified organisms (GMOs) on Chinese social media. Design/methodology/approach -This study identified 21,837 rumor-related posts of GMOs on Weibo, one of China's most popular social media platforms. An approach combining social network analysis and content analysis was employed to classify user attitudes toward rumors, measure the level of homophily of their attitudes and examine the nature of their interactions. Findings -Though a certain level of homophily existed in the interaction networks, referring to the observed echo chamber effect, Weibo also served as a public forum for GMO discussions in which cross-cutting ties between communities existed. A considerable amount of interactions emerged between the pro-and anti-GMO camps, and most of them involved providing or requesting information, which could mitigate the likelihood of opinion polarization. Moreover, this study revealed the declining role of traditional opinion leaders and pointed toward the need for alternative strategies for efficient fact-checking. Originality/value -In general, the findings of this study suggested that microblogging platforms such as Weibo can function as public forums for discussing GMOs that expose users to ideologically cross-cutting viewpoints. This study stands to provide important insights into the viral processes of scientific rumors on social media. Keywords Rumor, Genetically modified organism, Echo chamber, Chinese social media, Comments Paper type Research paper The spread of rumors about scientific topics has posed a persistent threat amid the rise of social media (De Domenico et al., 2013). As people increasingly rely upon social media for science information to inform their decisions, the rapid propagation of rumors across social networking sites have considerably increased the chances of citizens to be misinformed about science (Bessi et al., 2015). Although researchers have often investigated the diffusion of scientific rumors in the US and European settings, very few have examined its diffusion in non-western contexts. Such research is necessary, however, not only because rumors are diffused and assessed within communities in distinct cultural contexts, but also because information and its judgment are socially located and not merely the result of individual decisions (Fine, 2007). Researchers to date have focused on the characteristics of audiences and their critical ability to assess rumors and tend to downplay how communal judgments shape individual responses within particular cultural contexts. To narrow those gaps in the literature, we set out to examine, from a social network perspective, how scientific rumors spread on Chinese social media, namely, on Weibo, the Chinese equivalent of Twitter. In particular, we scrutinized how social media users responded to rumorsdisseminated them, Viral misinformation and echo chambers",
        "The impact of two practising social workers who taught social work students in a university setting.",
        "Marriage is a culture that grows based on three values: religiosity, customary, and national. In Indonesian culture, the marriage of the Uluan Musi community per these three values has changed the implementation procedure. These changes occurred in the structure and culture, which impacted the customary law system. Therefore, this research aims to observe how changes in Islamic law culture in customary practices occurred in the Uluan Musi community. This qualitative study collects data from traditional and religious leaders through in-depth interviews, observations, and documentation. The results indicated that the changes in the Islamic legal culture in the marriage practice were caused by the fading of the meaning of Islamic legal values (i.e., profanization) of marriage customs due to cultural coexistence and globalization. These changes cause social leeway with the potential to lose cultural identity. Therefore, legal remodification must be carried out to maintain the continuity of Islamic law.",
        "Can live music events generate complex contagion in music streaming? This paper finds evidence in the affirmative-but only for the most popular artists. We generate a novel dataset from Last.fm, a music tracking website, to analyse the listenership history of 1.3 million users over a two-month time horizon. We use daily play counts along with event attendance data to run a regression discontinuity analysis in order to show the causal impact of concert attendance on music listenership among attendees and their friends network. First, we show that attending a music artist's live concert increases that artist's listenership among the attendees of the concert by approximately 1 song per day per attendee (p-value<0.001). Moreover, we show that this effect is contagious and can spread to users who did not attend the event. However, the extent of contagion depends on the type of artist. We only observe contagious increases in listenership for well-established, popular artists (.06 more daily plays per friend of an attendee [p<0.001]), while the effect is absent for emerging stars. We also show that the contagion effect size increases monotonically with the number of friends who have attended the live event.",
        "1) Background: Research regarding sibling violence is still scarce, although it is the most common type of intrafamily violence. Every sibling's position in the sibling dyad seems to influence this type of violent conduct since every status has its characteristics. Siblings involved in aggressive behavior seem to be described as having low self-esteem. This study intends to test the predictive effect of self-esteem, sibling position and sex on sibling violence development. (2) Method: The sample consists of 286 students, aged between 12 and 17 years, from both sexes. A social demographic questionnaire and the Revised Conflict Tactics Scales-the Portuguese Sibling Version (CTS2-SP) and the Rosenberg Self-Esteem Scale were used for data collection. (3) Results: The results show an association between self-esteem in sibling violence, as well as an association between sibling position on negotiation and sexual coercion's perpetration and victimization. Sex also predicts the negotiation of psychological aggression's perpetration and psychological and physical aggression's victimization. (4) Discussion: the results will be discussed according to the attachment theory, considering the importance of affective bonds with siblings as adaptive development facilitators.",
        "Research about bullying among school pupils in the Arab/Muslim population is scarce. This study evaluates the characteristics of bullying and its impact among school pupils in Oman via cross-sectional survey among eighth grade school pupils (n = 1,229) during the academic year 2006-2007. The participants were selected using stratified random selection among 6 administrative divisions of one the governorates in the country. Data were collected using self-completed structured questionnaires. This study found similar percentages of males and females (76%) have experienced one form of bullying, and the majority of the incidents (80%) occurred in the vicinity of the school. In almost half of the cases, the bullying was initiated by a student of the same age or older than the victim. The most common type of bullying encountered in this study was verbal (47.7%), followed by misuse (45.9%), physical (43.9%), and, finally, social isolation/exclusion (22.5%). Although the failure of an academic year was uncommon among victims of bullying, the number of pupils who missed 4-6 and \u22657 school days was higher among bullied pupils. If this study will withstand further research, educational initiatives are needed to mitigate the rate of bullying in Oman.",
        "For guidance on citations see FAQs.",
        "At present, China is facing increasingly severe pension problems, the weakening of the family pension function, the pension security system is not perfect, the number of elderly and incapacitated elderly people is rising year by year, to explore the diversified pension model is the new era of China's characteristics of the positive response to the aging of the population road requirements. Based on this, this paper focuses on the willingness of the rural elderly to participate in the \"time bank\" mutual care model, and analyses their willingness to participate and the factors affecting them through the research on the rural elderly in Linyi City, in order to provide policy suggestions for the improvement of China's pension service system. This paper uses a combination of qualitative and quantitative research methods, combined with the current situation of the elderly in Linyi City, and analyses the reliability, validity and descriptive statistics of the collected primary data, focusing on analysing the willingness of rural elderly to participate in the \"time bank\" mutual support for the elderly and the influencing factors, and finally this paper proposes a deepening of the \"time bank\" model of elderly care, based on the problems found in the study. Finally, based on the problems found in the study, this paper proposes to deepen the awareness of \"Time Bank\", strengthen the standardised training for the elderly who participate in \"Time Bank\", as well as to realise the combination of public welfare and incentive service willingness to enhance the idea of gradually improving the enthusiasm of the participation of the elderly in rural areas, and to promote the \"Time Bank\". the sustainability of the \"time bank\" model of mutual support for the elderly.",
        "Background: Globally, male involvement has been identified as a priority target area to be strengthened in the prevention of mother to child transmission (PMTCT) of HIV. However, there are limited studies on husband involvement in the prevention of mother to child transmission of HIV in Ethiopia. Therefore, this study aimed to assess male involvement in the prevention of mother to child transmission of HIV and associated factors among males whose wives gave birth in the last six months before the survey in Enebsiesarmider district, Northwest Ethiopia. Methods: A Community-based cross-sectional study was employed to assess male involvement in the prevention of mother to child transmission of human immunodeficiency virus and associated factors in Enebsiesarmider District, Northwest Ethiopia. The study was conducted from February 10-30, 2018. A total of 525 participants were involved in the study. A stratified cluster sampling method was used to recruit study participants. Data were collected using a structured interviewer-administered questionnaire. Data were entered using the epi Data software and exported to SPPS for analysis. Descriptive statistics including mean, a proportion were used to describe study variables. Multivariable logistic regression was employed to describe variables with the outcome variable. Result: Overall male involvement in PMTCT was found to be 26. 1% [95%CI,]. Respondents who have attended secondary education and above were more likely get involved in PMTCT than who have no formal education [AOR 2.45, 95%CI,, Respondents who have good knowledge on PMTCT [AOR 2.57, 95%CI,, good knowledge on ANC [AOR 2.10, 95%CI,, low cultural barriers [AOR 2.20, 95%CI,] low health system barriers [AOR 2.40, 95%CI, were variables that significantly increase male involvement in PMTCT practices.",
        "Using a critical phenomenology approach, I explore how the neoliberal social context of the North American university produces normative expectations which both interact with and pattern student experiences and understandings of mental health struggles in this environment. The data I analyze comes from semi-structured interviews with 24 university students between 18 and 24 years of age who self-identify as experiencing mental health struggles, as well as participant observation at university wellness events. In this context, both students and the university understand wellness as the ability to maintain constant academic productivity. While university wellness programming promotes goal-oriented individualized \"self \"-care as the gold standard for attaining and maintaining wellness, students often view self-care activities as unproductive, instead prioritizing academic productivity over subjective well-being in striving to maintain an image as the \"good\" student. I argue that understanding mental health in this way both causes and exacerbates harm, introducing the conceptual contrast between \"Student Wellness\"-academic success-and \"Human Wellness\"-subjective wellbeing-as a means of understanding how university attempts to increase wellness often support neoliberal agendas to the detriment of their students' well-being.",
        "Background: Child maltreatment is a global epidemic. It affects morbidity, mortality, social behavior, wellbeing, and quality of life of children. This study aims to assess prevalence of child abuse in the West Bank (WB) of the occupied Palestinian territory (oPt) and to determine some of its social and political associated factors. Methods: We analyzed secondary data obtained from a cross sectional study conducted on a sample representing Palestinian children on the West Bank and using the International Society for the Prevention of Child Abuse and Neglect (ISPCAN) tool. The ISPCAN Child Abuse Screening Tool for parents (ICAST-P) questionnaire was completed by 1107 Palestinian mothers to estimate physical and emotional child abusive practices at home for children aged 0-12 years. Univariate, bivariate, and multivariate binary logistic regression analyses were performed using the SPSS\u00ae version 20 to assess prevalence and predictors of child abuse. Results: Overall, around 34% of the West Bank-children were abused by their mothers. Results of the logistic regression analysis indicated that male children, children of younger mothers, children whose fathers were with low levels of education, children whose mothers reported low levels of parental warmth, and children whose parents were exposed to political violence were at greater risk of being abused. Conclusions: Child abuse is highly prevalent among children of the Palestinian society in the West Bank. Policy makers need to pay more attention to this epidemic. The association between child abuse and political violence found in this study makes a just solution for Palestinians essential for improving the welfare of children and families.",
        "The Pregnancy Assistance Fund (PAF) program funds states and tribes to provide a wide range of services to improve health, social, educational, and economic outcomes for expectant and parenting teens and young adults, their children, and their families. This introductory article to the Maternal and Child Health Journal supplement Supporting Expectant and Parenting Teens: The Pregnancy Assistance Fund provides a description of the PAF program, including the program goals and structure, participants and communities served, and services provided; presents data on the reach and success of the program; and describes lessons learned from PAF grantees on how to enhance programs and services to have the best outcomes for expectant and parenting young families. Methods Performance measure data are used to describe the reach and success of the PAF program, and implementation experiences and lessons learned from PAF grantees were gathered through a standardized review of grantee applications and from interviews with grant administrators. Results Since its establishment in 2010, the PAF program has served 109,661 expectant and parenting teens, young adults, and their families across 32 states, including the District of Columbia, and seven tribal organizations; established more than 3400 partnerships; and trained more than 7500 professionals. Expectant and parenting teens and young adults who participated in the PAF program stay in high school, make plans to attend college, and have low rates of repeat pregnancy within a year. Conclusions Expectant and parenting teens and young adults in the PAF program demonstrated success in meeting their educational goals and preventing repeat unintended pregnancies. In addition, the staff who implemented the PAF programs learned many lessons for how to enhance programs and services to have the best outcomes for expectant and parenting young families, including creating partnerships to meet the multifaceted needs of teen parents and using evidence-based programs to promote program sustainability.Expectant and parenting teens \u2022 Young families \u2022 High school graduation \u2022 Repeat pregnancy \u2022 Lessons learned \u2022 Teen parentsThis article presents an overview of the Pregnancy Assistance Fund (PAF) program, a unique program designed to address the multifaceted needs of expectant and parenting teens and young adults, their children, and their families. The article presents data and lessons from the PAF program that suggest that providing expectant and parenting young families with a wide range of coordinated services and supports can result in improved outcomes for the young parents and their children.",
        "Background: Inequalities in opportunities for primary lung cancer surgery due to socioeconomic status exist. We investigated whether socioeconomic inequalities exist in net survival after curative intent surgery at a tertiary university hospital, in Japan. Methods: Data from the hospital-based cancer registry on primary lung cancer patients who received lung resection between 2010 and 2018 were linked to the surgical dataset. An area deprivation index, calculated from small area statistics and ranked into tertiles based on Japanwide distribution, was linked with the patient's address as a proxy measure for individual socioeconomic status. We estimated net survival of up to 5 years by deprivation tertiles. Socioeconomic inequalities in cancer survival were analyzed using an excess hazard model. Results: Of the 1039 patient-sample, advanced stage (Stage IIIA+) was more prevalent in the most deprived group (28.1%) than the least deprived group (18.0%). The 5-year net survival rates (95% confidence interval) from the least to the most deprived tertiles were 82.1% (76.2-86.6), 77.6% (70.8-83.0) and 71.4% (62.7-78.4), respectively. The sex-and age-adjusted excess hazard ratio of 5-year death was significantly higher in the most deprived group than the least deprived (excess hazard ratio = 1.64, 95% confidence interval: 1.09-2.47). The hazard ratio reduced toward null after additionally accounting for disease stage, suggesting that the advanced stage may explain the poor prognosis among the deprived group. Conclusion: There was socioeconomic inequality in the net survival of patients who received curative intent surgery for primary lung cancer. The lower socioeconomic status group might be less likely to receive early curative surgery.",
        "We empirically analyze five online communities: Friendster, Livejournal, Facebook, Orkut, Myspace, to identify causes for the decline of social networks. We define social resilience as the ability of a community to withstand changes. We do not argue about the cause of such changes, but concentrate on their impact. Changes may cause users to leave, which may trigger further leaves of others who lost connection to their friends. This may lead to cascades of users leaving. A social network is said to be resilient if the size of such cascades can be limited. To quantify resilience, we use the k-core analysis, to identify subsets of the network in which all users have at least k friends. These connections generate benefits (b) for each user, which have to outweigh the costs (c) of being a member of the network. If this difference is not positive, users leave. After all cascades, the remaining network is the k-core of the original network determined by the cost-to-benefit (c/b) ratio. By analysing the cumulative distribution of k-cores we are able to calculate the number of users remaining in each community. This allows us to infer the impact of the c/b ratio on the resilience of these online communities. We find that the different online communities have different k-core distributions. Consequently, similar changes in the c/b ratio have a different impact on the amount of active users. As a case study, we focus on the evolution of Friendster. We identify time periods when new users entering the network observed an insufficient c/b ratio. This measure can be seen as a precursor of the later collapse of the community. Our analysis can be applied to estimate the impact of changes in the user interface, which may temporarily increase the c/b ratio, thus posing a threat for the community to shrink, or even to collapse.",
        "Background: Strengthening primary care is key to Hong Kong's ongoing health system reform. Primary care remains unregulated, private sector dominated and financed mainly out-of-pocket. This study sought to examine the association between patients' socioeconomic status (SES), source of health payments and the quality of primary care they accessed to inform policy discussions. Methods: Data was collected from 1,994 respondents in a stratified random telephone survey with a 68% response rate, using the validated primary care assessment tool (PCAT). Education, household-income and type of housing were selected as indicators of SES. Multivariable ordinal logistic regression models were created to examine associations between indicators of SES and scores of quality. Results: Higher household-income was most significantly associated with better experiences of quality. Respondents with HK$ 15000-39999 (USD1934-5158) and HK$ 40000 (USD5159) and above were 47% (OR 1.47, 95% CI 1.10-1.96) and 2 times (OR 2.07, 95% CI 1.38-3.09) more likely to experience better quality than the lowest-income group respectively. Income group HK$ 40000 (USD5159) and above was 84% more likely to have better utilization (OR 1.84, 95% CI (1.21-2.78), and 2 times more likely to receive better comprehensiveness (OR 1.90, 95% CI 1.26-2.87). Patients who used only private insurance were 80% (OR 1.80, 95% CI 1.20-2.68) more likely to experience better quality than those who paid out-of-pocket. Conclusions: Our results show that the quality of primary care experienced in HK tended to be higher for those who had higher income and private insurance, and were able to pay out-of-pocket for the care. This indicated that the inequality in primary care is likely to be related with the private dominated primary care system in Hong Kong. More public responsibility on primary health care should be sought for in HK and similar contexts to reduce the inequality in primary care.",
        "Background: Rigorous research trials have demonstrated that early childhood interventions can reach socially disadvantaged families and can have a lasting impact on the healthy development of their children. However, little is known about the internal and contextual factors that contribute to the long-term implementation of such interventions. In this study, we investigated the development of the home visiting program Pro Kind. The program was adapted from the evidence-based US-American Nurse-Family Partnership program and was implemented in Germany in 2006. Using an exploratory approach, we examined factors contributing to the long-term implementation of this program. Methods: Qualitative interviews with program implementers (midwives, social workers, program managers) of the Pro Kind program and key stakeholders in two cities in Germany were conducted. Interview guides were developed to assess participants' perceptions and experiences on how the program had developed over time internally and in the interaction with its environment. Data were collected between March and September 2021. Drawing on the Consolidated Framework for Implementation Research (CFIR), data was coded according to the principles of thematic analysis. Results: A total of 25 individuals (11 program implementers, 14 key stakeholders) were interviewed. The identified factors related to three out of five domains of the CFIR model in our analysis. First, regarding the intervention characteristics, the evidence of effectiveness and the relative advantage of the implementation of the program compared to similar interventions were viewed as contributors to long-term implementation. However, the program's adaptability was discussed as a constraining factor for reaching the target group. Second, concerning the inner setting, stakeholders and program implementers perceived the implementation climate, the leadership engagement and the program's size as relevant factors for networking strategies and program visibility. Third, as part of the outer setting, the degree Abbreviations CFIR, consolidated framework for implementation research; COREQ, consolidated criteria for reporting qualitative research; ECI, early childhood intervention program; NFP, nurse-family partnership; RCT, randomized controlled trial.",
        "PURPOSE Family and friends who provide regular care for a sick or dependent individual (\"caregivers\") are at increased risk of health-related socioeconomic vulnerabilities (HRSVs). This study examined pre-pandemic prevalence of and early pandemic changes in HRSVs among women caregivers compared with non-caregivers. METHODS A cross-sectional survey was conducted in April 2020 (early pandemic) with 3,200 English-speaking US women aged 18 years or older, 30% of whom identified as caregivers. We modeled adjusted odds of self-reported HRSVs (financial strain, food/ housing insecurity, interpersonal violence, transportation/utilities difficulties) before and changes during the early pandemic by caregiving status. Models were adjusted for age, race/ethnicity, marital status, education, income, number of people in household, number of children in household, physical and mental health, and number of comorbidities.Pre-pandemic, 63% of caregivers and 47% of non-caregivers reported 1 or more vulnerability (P <.01); food insecurity was most prevalent (48% of caregivers vs 33% of non-caregivers, P <.01). In the early pandemic, caregivers had higher odds than non-caregivers of financial strain, both incident (adjusted odds ratio [AOR] = 2.1; 95% CI, 1.6-2.7) and worsening (AOR = 2.0; 95% CI, 1.4-2.8); incident interpersonal violence (AOR = 2.0; 95% CI, 1.5 -2.7); incident food insecurity (AOR = 1.6; 95% CI, 1.2-2.1); incident transportation difficulties (AOR = 1.9; 95% CI, 1.3-2.6); and incident housing insecurity (AOR = 1.6; 95% CI, 1.1-2.3).The coronavirus disease 2019 (COVID-19) pandemic increased risk of incident and worsening HRSVs for caregivers more than for non-caregivers. COVID-19 response and recovery efforts should target caregivers to reduce modifiable HRSVs and promote the health of caregivers and those who depend on them.",
        "The miniaturization of Chinese urban families has led to an increasing trend of \"intergenerational parenting\" among urban families. However, the influence of different types of caregivers on children's outdoor activities remains unclear. This study aimed to investigate the impact of various accompanying modes used by different caregivers on the outdoor activities of preschool children. The research employed mixed methods, including questionnaires and interviews, and focused on the intergenerational relationship in communities in Shanghai, China. The findings of this research reveal that caregivers from different generations adopt distinct accompanying modes when engaging in children's outdoor activities. Grandparents tend to prioritize protection, while parents prefer to spend more time playing with their children. These preferences can be attributed to the division of labor within families and the social and physical environment of the communities in urban central areas. By shedding light on the intergenerational dynamics and caregiving approaches, this study provides a unique Chinese perspective on understanding the factors influencing children's outdoor activities and experiences.",
        "Background: Previous studies have shown that age, physical and mental health status and working circumstances, along with different socio-economic and psychosocial factors affect the retirement process. However, the role of psychological resources, such as sense of coherence (SOC), on the retirement process is still poorly understood. This study investigated the associations between SOC and intentions to retire early and whether these associations were explained by socio-economic, psychosocial and work and health related factors. Methods: The data were derived from the Finnish Health and Social Support (HeSSup) Study. The information was gathered from postal surveys in 1998 (baseline) and in 2003 (follow-up). The analyzed data consisted of 7409 women and 4866 men aged 30-54 at baseline. SOC and background factors including childhood circumstances, language, education, working circumstances, social support, health behaviour and somatic and mental health status were assessed at baseline. The intentions to retire early were assessed at follow-up using logistic regression analysis. Results: SOC was associated with intentions to retire early among both genders. Socio-economic, psychosocial and work and health behaviour related factors did not influence the association between SOC and intentions to retire early among women and men reporting somatic or mental illness. Further, the association between SOC and intentions to retire early remained among (somatically and mentally) healthy men. Among healthy women the association was weaker and statistically non-significant. Among unhealthy women, the odds ratios of SOC was 0.97 (CI 95% 0.96-0.98) and 0.97 among ill men (CI 95% 0.96-0.98), i.e., each additional SOC score reduced the risk of intentions by 3% among both genders. Conclusion: Unhealthy employees with low SOC and low education were in the greatest risk to have reported intentions to retire early. SOC had an independent effect on intentions to retire early, and a strong SOC may have a potential to prevent early retirement in groups otherwise at risk. An important challenge would be to target the resources of SOC to the most vulnerable and design appropriate interventions in order to strengthen the level of SOC and hence prolong working years of the aging employees.",
        "Despite increased interest in parenting among custodial grandmothers (CGM), there is scant research on assessing their parenting practices. With CGMs as informants we examined the factor structure for five self-report scales developed as measures of parental nurturance and discipline with birth parents, and then tested for measurement invariance by grandchildren's age (4 -<7 versus \u22657 -12). We also examined concurrent validity for these scales according to the Family Stress Model. Data were from 343 CGMs (M = 58.45, SD = 8.22) enrolled in a randomized clinical trial caring for grandchildren (GC) aged 4 to 12 (M = 7.81, SD = 2.56). Discipline was assessed by three scales from the Parental Behavior Inventory (Consistency, Effective, and Punitive). Nurturance was assessed by the Positive Affect Index and the Supportive Engaged Behavior scale of the Parenting Practice Interview. Confirmatory factor analysis (CFA) revealed that these scales were best represented as five distinct yet covarying factors (RMSEA = .055; SRMR = .07). Follow-up CFAs within each GC age group supported this model, with only few changes suggested by the corresponding diagnostic tests. A model with these changes was then examined for measurement invariance by CG age group, with complete measurement invariance found and all items loading onto their respective factors significantly. The five scales also correlated as expected with indices of CGM psychological distress and GC adjustment. We conclude that the scales examined here can be used meaningfully with CGM as respondents. An earlier version of this paper was presented at that 67 th Annual Scientific Meeting of the Gerontological Society of America (November, 2014).",
        "Background Women with perinatal substance problems experience a multitude of barriers to care. They have specific early intervention needs, they endure societal stigma, and both substances and mental health issues influence the way they navigate within support and treatment systems. Early interventions for women with perinatal substance problems are underresearched contexts. The aim of the study is to describe building relationships and engagement within an outreach and low threshold service encounter tailored for pregnant women with SUD (substance use disorder).The data consist of online written narratives from 11 workers involved in the program and feedback from 504 families in the recovery process comprising 228 open-ended answers. The data were analyzed with a thematic analysis.The programs are characterized by flexibility and the implementation of inclusive ways to approach families. The themes for enhancing relationships and engagement within outreach and low threshold programs are Acceptance and attitude: a sensitive approach of approval; flexibility within strictness to allow for diversity and individuality; availability and space to ensure a trustful atmosphere; negotiating via doing to build connections; and everyday life changes: imagining recovery. The themes represent the need of being available, focusing on the worker's attitudes and building connections by doing together, and visioning recovery together.The study results can add to the understanding of SUD outreach and low threshold work during pregnancy. The elements described in this study need further theoretical development, research and critical assessment. Building relationships during pregnancy were characterized by connecting within everyday life situations and supporting the development of an attachment relationship between the baby and the parents. To promote recovery, a comprehensive approach in which substance-related issues and mental health conditions are interconnected can be favored. Engaging early on during pregnancy might enhance success during future rehabilitation.",
        "Elderlies are vulnerable to abuse, and evidence suggests that one in three elderlies experience abuse. Abuse can impact the well-being of older persons, decreasing their quality of life, leading to mental health challenges, and increasing morbidity and mortality rates. Evidence on older person/elder abuse and neglect is vital to facilitate initiatives, but there are fewer studies on elder abuse and neglect in Africa, particularly in Uganda. Thus, this study aimed to determine the prevalence of different types of abuse and neglect, and their associated factors among older persons (aged 60 years and above) attending an outpatient clinic.In this cross-sectional study, information on sociodemographic characteristics, functional impairment using the Barthel Index, and elder abuse severity using the Hwalek-Sengstock Elder Abuse Screening Test were collected. In addition, types of abuse were assessed using questions adapted from the US National Research Council on elder mistreatment monograph. Linear and logistic regression analyses were used to determine the factors associated with elder abuse severity and the different types of abuse, respectively.Overall, the prevalence of elder abuse was 89.0%. Neglect was the most common type of elder abuse (86%), followed by emotional abuse (49%), financial abuse (46.8%), physical mistreatment (25%), and sexual abuse (6.8%). About 30.4% of the abused elders experienced at least two forms of abuse. Factors associated with elder abuse severity were having a secondary level of education and physical impairment. Moderate to severe functional dependence was associated with almost all forms of abuse. Individuals who reported the",
        "Shaw University, the oldest historically black college or university in the southern USA, recently partnered with the University of North Carolina at Chapel Hill, a major research institution in North Carolina, to further develop Shaw's research infrastructure. One aim of the partnership involved establishing a human research ethics committee and an accompanying administrative structure and research ethics education program. This paper describes the process of developing an entire human research protection program de novo through collaboration with and mentoring by the members of the human research protection program at a nearby major research institution. This paper provides a detailed description of the aims, procedures, accomplishments, and challenges involved in such a project, which may serve as a useful model for other primarily teaching institutions wishing to develop research infrastructure and ethical capacity.historically black colleges and universities; research infrastructure; partnering; research ethics committee; REC; Institutional Review Board; IRB Historically black colleges and Universities (HBCUs) have traditionally been known for their supportive educational and teaching experiences. Many HBCUs have focused little on research in deference to their primary educational missions (Carey et al., 2005). Faculties at HBCUs typically have large teaching loads and little time for research.",
        "The decision to attend law school in the 21 st century requires an increasingly significant financial investment, yet very little information about the value of a legal education is available for prospective law students. Prospectives use various tools provided by schools and third parties while seeking to make an informed decision about which law school to attend. This Article surveys the available information with respect to one important segment of the value analysis: post-graduation employment outcomes. One of the most pressing issues with current access to information is the ability to hide outcomes in aggregate statistical forms. Just about every tool enables this behavior, which, while misleading, often complies with the current ABA and U.S. News reporting standards. In this Article, we propose a new standard for employment reporting grounded in compromise. Our hope is that this standard enables prospectives to take a detailed, holistic look at the diverse employment options from different law schools. In time, improved transparency at American law institutions can produce generations of lawyers who were better informed about the range of jobs obtainable with a law degree. * Co-Founder, Law School Transparency, JD expected 2011, Vanderbilt University Law School. Also known as jenesaislaw. Law School Transparency is a Tennessee non-profit dedicated to improving the employment reporting standard at American law schools. As board members, we have a vested interest in the analytical successes of this Article. We hope that this Article stimulates discussion among law schools, law students, and the legal profession to review and debate the merits of what we argue. \u2020 Co-Founder, Law School Transparency, JD expected 2010, Vanderbilt University Law School. Also known as observationalist. We wish to thank the large number of prospective and current law students who have helped motivate this project over the last two years, primarily through their discussions on two law school forums, www.top-law-schools.com and www.lawschooldiscussion.org. We would also like to thank our friends, as well as numerous law school administrators and faculty, for their invaluable help. All errors in this paper are our own.",
        "As Extension professionals are increasingly tasked with moving beyond program delivery into the murky realm of systems change, networks represent an essential organizing framework for this transition. In this article, we examine the ways in which networks are becoming a modern mode for social change. By providing examples from our work with food networks, we demonstrate how these collaborative approaches can produce a greater impact for Extension and the communities we serve. Lastly, we discuss the critical characteristics of successful networks and the role Extension can play in their optimization.",
        "Although women's participation in tertiary education and the labour force has expanded over the past decades, women continue to be underrepresented in technical and managerial occupations. We analyse if gender inequalities also manifest themselves in online populations of professionals by leveraging audience estimates from LinkedIn's advertisement platform to explore gender gaps among LinkedIn users across countries, ages, industries and seniorities. We further validate LinkedIn gender gaps against ground truth professional gender gap indicators derived from the International Labour Organization's (ILO) Statistical Database, and examine the feasibility and biases of predicting global professional gender gap indicators using gender gaps computed from LinkedIn's online population. We find that women are significantly underrepresented relative to men on LinkedIn in countries in Africa, the Middle East and South Asia, among older individuals, in Science, Technology, Engineering and Mathematics (STEM) fields and higher-level managerial positions. Furthermore, a simple, aggregate indicator of the female-to-male ratio of LinkedIn users, which we term the LinkedIn Gender Gap Index (GGI), shows strong positive correlations with ILO ground truth professional gender gaps. A parsimonious regression model using the LinkedIn GGI to predict ILO professional gender gaps enables us to expand country coverage of different ILO indicators, albeit with better performance for general professional gender gaps than managerial gender gaps. Nevertheless, predictions generated using the LinkedIn population show some distinctive biases. Notably, we find that in countries where there is greater gender inequality in internet access, LinkedIn data predict greater gender equality than the ground truth, indicating an overrepresentation of high status women online in these settings. Our work contributes to a growing literature seeking to harness the 'data revolution' for global sustainable development by evaluating the potential of a novel data source for filling gender data gaps and monitoring key indicators linked to women's economic empowerment.",
        "Background-Clinical trials (CTs) are the mechanism by which research is translated into standards of care. Low recruitment among underserved and minority populations may result in inequity in access to the latest technology and treatments, compromise the generalizability, and lead to failure in identification of important positive or negative treatment effects among underrepresented populations. Methods-Data were collected over a 39-month period on patient eligibility for available therapeutic cancer CTs. Reasons for ineligibility and refusal were collected. The data were captured using an automated software tool for tracking eligibility pre-enrollment. We examined characteristics associated with being evaluated for a trial, and reasons for ineligibility and refusal, overall and by patient race. Results-African-Americans (AAs) were more likely than Whites to be ineligible (odds ratio, (OR) = 1.26, 95% confidence interval (CI) = 1.0-1.58) and if eligible, to refuse participation (OR = 1.79, 95% CI = 1.27-2.52), even after adjusting for insurance, age, gender, study phase, and cancer type. White patients were more likely to be ineligible due to study-specific or cancer characteristics. AAs were more likely to be ineligible due to mental status or perceived noncompliance. Whites were more likely to refuse due to extra burden, due to concerns with randomization and toxicity, or because they express a positive treatment preference. AAs were more likely to refuse because they were not interested in CTs, because of family pressures, or they felt overwhelmed (NS)).",
        "Introduction Despite persistently low employment rates among working-age adults with disabilities, prior research on employer practices and attitudes toward workers with disabilities paints a generally rosy picture of successfully accommodated workers in a welcoming environment. Findings from previous studies might have been biased because of either employer self-selection or social desirability, yielding non-representative or artificially positive conclusions. Methods In this study, a novel approach was used to survey human resource professionals and supervisors working for employers known or reputed to be resistant to complying with the ADA's employment provisions. Attendees of employer-requested ADA training sessions were asked to assess various possible reasons that employers in general might not hire, retain, or accommodate workers with disabilities and to rate strategies and policy changes that might make it more likely for employers to do so. Results As cited by respondents, the principal barriers to employing workers with disabilities are lack of awareness of disability and accommodation issues, concern over costs, and fear of legal liability. With regard to strategies employers might use to increase hiring and retention, respondents identified increased training and centralized disability and accommodation expertise and mechanisms. Public policy approaches preferred by respondents include no-cost external problem-solving, subsidized accommodations, tax breaks, and mediation in lieu of formal complaints or lawsuits. Conclusions Findings suggest straightforward approaches that employers might use to facilitate hiring and retention of workers with disabilities, as well as new public programs or policy changes that could increase labor force participation among working-age adults who have disabilities.",
        "The fundamental presupposition of this paper is that 'gender' is a social category, hence a social construction, which can be negotiated and left fluid instead of something fixed and eternal. To examine the gendered social order, this study focuses on how hegemonic masculinity and feminine subordination are naturalized by positioning men as physically strong and women as weak on the ground of biological differences between the sexes. The study is informed by social constructionist understandings of gender. The main focus of the paper is to highlight how gendered discourses in Pakistan inform textbooks as objective and true knowledge. The data for the study comes from 28 educationists (11 females and 17 males). The study's findings revealed that, despite prevailing claims to establishing gender equality and equity in education, educationists are active in the production of gender/sexual identities and hierarchies in a ways that reinforces hegemonic 'masculinity' and a fixed notion of 'femininity'. The paper concludes that what ends up as school knowledge arises from gendered power/knowledge relations.",
        "Writing the recent history of mental health services requires a conscious departure from the historiographical tropes of the nineteenth and twentieth centuries which have emphasised the experience of those identified (and legally defined) as lunatics and the social, cultural, political, medical and institutional context of their treatment. A historical narrative structured around rights (to health and liberty) is now complicated by the rise of new organising categories such as 'costs', 'risks', 'needs' and 'values'. This paper, drawing on insights from a series of witness seminars attended by historians, clinicians and policymakers, proposes a programme of research to place modern mental health services in England and Wales in a richer historical context. Historians should recognise the fragmentation of the concepts of mental illness and mental health need, acknowledge the relationship between critiques of psychiatry and developments in other intellectual spheres, place the experience of the service user in the context of wider socio-economic and political change, understand the impacts of the social perception of 'risk' and of moral panic on mental health policy, relate the politics of mental health policy and resources to the general determinants of institutional change in British central and local government, and explore the sociological and institutional complexity of the evolving mental health professions and their relationships with each other and with their clients. While this is no small challenge, it is perhaps the only way to avoid the perpetuation of 'single-issue mythologies' in describing and accounting for change.",
        "A growing area of research has examined the individual behaviors and social antecedents that enable and constrain the popularity of social media users. This systematic review gathers and summarizes 68 naturalistic studies that measure popularity based on users' reach (e.g., followers, fans and subscribers) or engagement (e.g., likes, comments and shares) on multiple platforms. It draws on Barnlund's (2008) transactional model of communication to organize the literature and provides a roadmap for future research by identifying areas of the research that are characterized by consensus and disagreement. It also reveals a gap in the literature. Previous research focuses on communication strategies that maximize reach and engagement and provides less evidence of social structural influences on popularity. More research is needed to understand how the social, economic, and cultural characteristics of users affect their success.",
        "Background: Poland is a country with restrictive laws concerning abortion, which is only allowed if the mother's life and health are in danger, in case of rape, and severe defects in the fetus. This paper specifies the forms of support expected by women considering termination from their family, people in their surroundings and professional medical personnel. Methods: Between June 2014 and May 2016 patients eligible to terminate a pregnancy for medical reasons were asked to complete an anonymous survey consisting of sixty questions to determine patient profile and forms of support expected from the society, family and professional medical personnel as well as to assess informational support provided. Results: Women do not take into consideration society's opinion on pregnancy termination (95%). The majority of the respondents think that financial support from the state is not sufficient to provide for sick children (81%). Despite claiming to have a medium standard of life (75%), nearly half of the respondents (45%) say that they do not have the financial resources to take care of a sick child. The women have informed their partner (97%) and closest family members (82%) and a low percentage have informed friends (32%). Nearly one third (31%) have not talked to the attending gynecologist about their decision. Conclusions: The decision to terminate a pregnancy is made by mature women with a stable life situation-supported by their partner and close family. They do not expect systemic support, as they believe it is marginal, and only seek emotional support from their closest family. They appreciate support provided by professional medical personnel if it is personal.",
        "Evaluating the relative importance of different social contexts in which infection transmission occurs is critical for identifying optimal intervention strategies. Nonetheless, an overall picture of influenza transmission in different social contexts has yet to emerge. Here we provide estimates of the fraction of infections generated in different social contexts during the 2009 H1N1 pandemic in Italy by making use of a highly detailed individual-based model accounting for time use data and parametrized on the basis of observed age-specific seroprevalence. We found that 41.6% (95%CI: 39-43.7%) of infections occurred in households, 26.7% (95%CI: 21-33.2) in schools, 3.3% (95%CI: 1.7-5%) in workplaces, and 28.4% (95%CI: 24.6-31.9%) in the general community. The above estimates strongly depend on the lower susceptibility to infection of individuals 191 years old compared to younger ones, estimated to be 0.2 (95%CI 0.12-0.28). We also found that school closure over the weekends contributed to decrease the effective reproduction number of about 8% and significantly affected the pattern of transmission. These results highlight the pivotal role played by schools in the transmission of the 2009 H1N1 influenza. They may be relevant in the evaluation of intervention options and, hence, for informing policy decisions.",
        "Biological and cultural history is linked with neglected and underused species (NUS). Wild food plants and animals have been the focus of important investigations carried out around the world in recent decades, and it is seen as a key issue in the fight against hunger and malnutrition. In this study, we assessed the effect of geographical, political, social and economic scenarios of neighboring countries that have led to different food and foraging practices in the erstwhile princely state of Jammu and Kashmir. We recorded 209 species, broadly classified into plants (n = 152) and animals (n = 57), used by the local people from four biogeographic regions (Kashmir, Jammu, Azad Kashmir and Ladakh) of erstwhile princely state of Jammu and Kashmir. Among wild flora, 139 vascular plants and 13 mushrooms were used by the indigenous communities, whereas in the case of wild fauna, 14 mammals, 22 birds and 21 fishes were used. The Jammu and Kashmir regions showed greater similarity, whereas the least overlap was observed between the Jammu and Ladakh regions. A cross-cultural comparison of wild food resources showed that maximum overlapping was observed in plant resources, and minimum in animals between the four regions of the study area. The usage of different wild foods is also dependent on seasonality. Wild animals and birds are preferred as food in the winter season. However, in the warm months of the year, plants and fish were preferred during the summer season due to their easy accessibility, and they replace the requirement of animals during the warm months of the year. The results of the current study show that the geopolitical scenario in this region has also affected the usage of different animal and plant species for food in the historically unified region of Jammu and Kashmir. This is one of the first comprehensive studies that document the wild food heritage essential for future projects aimed at fostering conservation, environmental sustainability, food security and climate change.",
        "Temporal networks of face-to-face interactions between individuals are useful proxies of the dynamics of social systems on fast time scales. Several empirical statistical properties of these networks have been shown to be robust across a large variety of contexts. In order to better grasp the role of various mechanisms of social interactions in the emergence of these properties, models in which schematic implementations of such mechanisms can be carried out have proven useful. Here, we put forward a new framework to model temporal networks of human interactions, based on the idea of a co-evolution and feedback between (i) an observed network of instantaneous interactions and (ii) an underlying unobserved social bond network: social bonds partially drive interaction opportunities, and in turn are reinforced by interactions and weakened or even removed by the lack of interactions. Through this co-evolution, we also integrate in the model well-known mechanisms such as triadic closure, but also the impact of shared social context and non-intentional (casual) interactions, with several tunable parameters. We then propose a method to compare the statistical properties of each version of the model with empirical face-to-face interaction data sets, to determine which sets of mechanisms lead to realistic social temporal networks within this modeling framework."
    ],
    "All_Text_with_Titles": [
        "Background\n\nChronic health conditions are increasingly common, with some population groups, such as those of lower socioeconomic status (SES) having both a greater incidence of chronic disease and a poorer prognosis [1][2][3]. The long-term nature of these conditions means that the patient is largely responsible for day-to-day disease management [4,5] and since many chronic conditions are lifestyle-related [6], the quality of patient selfmanagement is important. Self-management support (SMS) approaches have been developed to give people the skills to more effectively manage their health. These interventions involve both education and behaviour change strategies to address the medical, physical, emotional and social challenges associated with CD, aiming to help the person adapt to their changed circumstances whilst still leading a meaningful life [4,5,7].\nAlthough SMS interventions are now widespread, outcomes have been mixed, with the benefits being limited to short-term improvements in psychological variables such as self-efficacy, rather than sustained clinical or behavioural changes [4,6,8,9]. Most SMS interventions are theoretically grounded in Bandura's concept of selfefficacy [5] and utilise specific techniques to enhance self-efficacy [5-7, 10, 11]. Self-efficacy theory refers to an individual's belief or confidence in their capacity to undertake tasks or achieve goals, which can translate into health behaviour change and by implication, improved health status [4,5].\nPersisting questions remain, however, about the effectiveness of SMS in low SES and other disadvantaged groups. The original SMS trials were conducted in selfselected, higher SES populations [4,6,10] and studies in disadvantaged populations have reported poorer outcomes and lower levels of adherence [12,13]. Several writers have theorised that the individual patient focus of SMS limits its effectiveness in these groups. By prioritising individual self-efficacy and activation, the potential barriers to self-management within the patient's wider social context (e.g. literacy, resources, social supports) are ignored [6,10,11,14,15]. Although the dominant role of the social determinants of health is acknowledged in CD epidemiology, their influence on treatment engagement is rarely addressed [15].\nEffective chronic disease (CD) management should include both an improvement in overall population health and a reduction in health inequities [16][17][18]. An intervention that appears more effective in a better-off population may widen the disparity gap, and there are strong suggestions that individually-focussed 'downstream' interventions, such as SMS, can increase disparity [17,19,20]. Specific targeting of disadvantaged groups is one way to deal with inequity, and tailored SMS approaches for these groups have been trialled, but systematic reviews have shown inconsistent and dose-dependent benefits [13,21].\nIn addition, such interventions may have positive outcomes, but still not address the disparity gap [18].\nAlthough there are suggestions that SMS interventions may be less effective in low SES groups, this can only be determined by comparing SMS outcomes between more and less advantaged groups. There have been no previous reviews on this topic, despite many researchers stressing the importance of addressing and quantifying the equity gap in CD [18-20, 22, 23]. This is partly due to statistical challenges, since the evidence will emerge from subgroup analyses [19,24,25]. However, given the strong connection between the social determinants of health and health outcomes, subgroup analyses need not be post-hoc data dredging but can be planned and valid approaches to answering these questions [26][27][28].\nThis review aims to examine studies that have looked at differences between socioeconomic groups undergoing SMS interventions, in order to answer the following questions: 1. Is there evidence that SES influences participation rates in SMS interventions? 2. Is there evidence that SES influences rates of retention or dropout from SMS interventions? 3: Is there evidence that SES affects clinical, behavioural or other specified outcomes following SMS interventions?\n---\nMethods\n\n\n---\nSearch strategy and data abstraction\n\nWe conducted a systematic review of the literature using the PRISMA reporting guidelines [29] to structure the report. We searched for full-text articles in English to December 2018 in the following databases: Cochrane database; PubMed; Cinahl; Embase; Proquest and Psychinfo. The search terms covered the following areas, using MeSH terms and synonyms: [1] Chronic condition, including diabetes, cardiovascular disease, musculoskeletal conditions and chronic pulmonary disease [2]; Self-management [3]; Socio-economic status, including associated terms such as inequity, disparity, 'vulnerable groups'; and [4] Terms related to outcomes, efficacy, retention or participation. The PubMed search strategy is available in Additional file 1. No date filter was employed in order to obtain the widest possible search. In the course of the search thirteen related systematic reviews were located and their references were screened resulting in seven additional papers.\n---\nInclusion criteria\n\nInclusion and exclusion criteria are outlined in Table 1. We looked for four main chronic conditions: cardiovascular disease (CVD), musculoskeletal conditions (MSK), pulmonary disease (COPD) and diabetes. All these conditions contribute significantly to the burden of disease and share many common risk factors. We included studies of co/multimorbidity since this is representative of the CD population. A decision was made to focus only on socio-economic status (SES), which has welldocumented and consistent effects on chronic disease, rather than on other WHO PROGRESS+ factors such as gender and ethnicity, which can vary between countries [19]. All studies needed to provide a comparison between a less and more advantaged group, based on income, education or socioeconomic area. Comparisons based on literacy or ethnicity were only included if there was a quantifiable relationship between these variables and other SES measures. As well as post-intervention outcomes such as behavioural or clinical changes, outcomes related to participation and dropout were included to fully capture potential areas of disparity. Study designs could include randomised controlled trials with subgroup analyses, pre-post designs, cross-sectional or longitudinal data analyses. SES described in terms of education, income, area or occupation. 'Disadvantaged' (e.g. ethnic minority) population without quantifiable reference to SES.\nIntervention Includes a self-management support intervention incorporating at least 3 recognised elements of SM [7] Single-component SMS intervention (e.g. education, medication adherence only).\nComparison Includes analysis of whether the response to the intervention differs according to SES.\nNo measurement of SES disparity in reporting of outcomes.\n---\nOutcome\n\nReporting of outcomes which may be clinical, behavioural, psychosocial or related to participation/attrition.\n---\nFig. 1 PRISMA diagram\n\n\n---\nSearch outcomes\n\nTitle and abstract screening reduced the number of papers to 310. Articles were excluded according to the criteria outlined in Table 1. Common reasons for exclusion were no SMS intervention (e.g. studies of self-care or adherence behaviours); SES not quantified, and no measurement of SES disparity. A full list of reasons for exclusion of the 291 full-text articles is available in Additional file 2. Figure 1 illustrates the search process undertaken. One reviewer (RH) completed the initial search and a second reviewer (ES) independently assessed the final papers to ensure agreement on inclusion criteria. Nineteen studies were included in the review.\n---\nData abstraction\n\nThe data was summarised on the setting, study design, type of CD, sample size, description of intervention and control, outcomes or variables measured, follow-up time, results and study quality (Table 2 and Table 3). Table 4 and Table 5 summarises data related specifically to SES and disparity, including the theory behind the SM intervention (or study question for participation/attrition studies), intervention description, SES adaptations made, SES status of population, results in relation to SES, dropout rates and overall impact on SES disparity. Related papers were retrieved to provide additional data about the population or intervention as needed [31][32][33][34][35][36][37][38]. Quality analysis was undertaken using the Joanna Briggs Institute checklists [30] for randomised controlled trials (RCTs) and observational studies, and the Sun/ Oxman criteria [27,28] for subgroup analyses.\n---\nData synthesis\n\nNo meta-analysis was possible due to the diversity of study designs, interventions and outcome variables.\n---\nResults\n\n\n---\nKey study characteristics\n\nNineteen studies were identified, all published in English. Five studies looked at participation in SMS; five studied attrition from SMS programmes and nine assessed outcomes from SMS interventions. Interventions were very diverse, ranging from studies of the group-based Stanford Chronic Disease Self-Management Programme (CDSMP -4 studies) to highly tailored 1-1 interventions. Table 2 and Table 3 details the main features of all studies.\n---\nMethodological quality\n\nMost studies were of moderate to good quality although two RCTs [39,40] and three subgroup analyses [39][40][41] rated poorly. A summary of quality ratings is included in Four cross-sectional studies and one cohort study looked at initial participation in SMS programmes. All were large population surveys ranging from 2600 to 80, 000 people. There were three reports on diabetes SMS education programmes [42][43][44], one on the Stanford CDSMP [45] and the final study examined recruitment to an internet diabetes SMS programme [46]. In all studies, low SES (as measured by education, income or location) was significantly and consistently associated with lower levels of participation, suggesting that disparity in CDSM starts here. Some studies [43,45] suggested that this imbalance was related to course availability, cost or marketing strategies. However, the studies which did match attendance to course availability and cost [42,44] found that this did not influence participation in the low SES population. Glasgow [46] also compared participation rates in a self-selected (via media advertising) population to a referred population and found even greater disparity. As well as being of higher SES, the selfselected participants were those at lowest risk and least in need of the intervention.\nThere is consistent evidence that low SES is associated with lower levels of participation in SMS interventions, and some evidence that this is unrelated to access to SMS interventions.\n---\nIs there evidence that SES influences rates of retention or dropout from SMS interventions?\n\nFive studies examined attrition: two cross-sectional studies and three RCTs with subgroup analysis, with sample sizes from 100 to 300. Two RCTs [41,47] were of more advantaged populations. Of these, one reported low (22.8%) completion rates of the Stanford CDSMP [41], but predictors were related to poor physical health rather than SES. Since this was a high-risk multimorbid rather than a low SES population, dropout likely reflects increased treatment burden, as noted in other multimorbid populations [48]. The second study [47], of a diverse urban population, reported no difference in use of a supported internet programme in terms of SES (education). This intervention had been carefully tailored to maximise engagement across population groups and included extensive community involvement in the design process. Three studies [49][50][51] focussed on low SES populations. Two cross-sectional studies [49,50] reported that dropout rates correlated to social stressors and lack of job   Studies listed in order of quality as measured by Johanna Briggs Institute (JBI) criteria [30] and Sun/Oxman (S/O) subgroup analysis (for RCTs) criteria [27,28]. RCTs listed first, followed by cohort and cross-sectional studies.  Literacy was used as an SES measure where it was clearly correlated with education and income.\nflexibility, suggesting that attrition within a low SES population may be influenced by socioeconomic factors that are not captured by education or income alone. Finally, a small RCT [51] of a tailored group programme found that high levels of dropout were significantly associated with low income and education. By contrast, Horrell et al. [45] noted that although SES area predicted enrolment in the Stanford CDSMP, it did not affect rates of completion. SES is not consistently associated with dropout from SMS interventions. SES may be one of a number of factors associated with programme attrition, as suggested by qualitative studies on this topic [52].\n---\nIs there evidence that SES affects clinical, behavioural or other specified outcomes following SMS interventions?\n\nNine studies looked at outcomes following SMS interventions, with four describing group interventions (including 2 of the Stanford CDSMP) and five individual [1] interventions. Only two of the RCTs [53,54] were sufficiently powered for subgroup analysis and most had follow-up periods of 6 months or less.\nThree of the nine studies featured outlier populations (in terms of age, sex and/or level of disadvantage), including the two lower-quality studies [39,40] and the cohort study [55]. The findings from these studies may not be reliable or relevant to the wider low SES population.\nThe remaining six studies, of moderate to high quality, described broadly similar populations in terms of age, sex, education and income. Of these studies, one reported increased disparity following the intervention; two reported no change; and three studies reported a reduction in SES disparity.\nThree of the studies, all individual interventions, described programmes specifically tailored for low SES groups, including extra supports and literacy adaptations. These included a 6-month peer support programme [56] and two 12-month phone support programmes [53,57] (conducted by the same research group, but with different chronic diseases and interventions). All studies reported clinically and statistically significant changes in either hospitalisation rates [53] or HbA1c [56,57] in favour of the intervention. Two of the studies also reported a reduction in SES disparity from the intervention, with low-literacy patients experiencing greater benefit from the intervention than their higher literacy counterparts. In an already low-SES population, this was found to be a stronger predictor than income or education. The third study (the peer support programme) reported no change in disparity, with benefits across all education levels and the greatest benefit experienced by those with poorer medication adherence and self-management ability.\nThe remaining studiescomprising one individual and two group interventionsdid not provide specific tailoring for low SES participants. The individual intervention [58], a 6-week CBT programme designed to increase self-efficacy, found clinically significant improvements in depression only in the higher educated, with no change and higher rates of dropout in the lower educated. The group interventions, which were both for people with heart failure, included the 6-week CDSMP and a year-long SMS group programme. The CDSMP study did show short-term benefits as compared to usual care, but no overall gains at 6 or 12 months. The lower educated patients did better than their higher educated counterparts in terms of cardiac quality of life (QOL) (p = 0.018) over 12 months, suggesting a reduction in SES disparity, although it was not clear whether this was clinically significant. The second group programme [54] used an active education control and found no additional benefit from an SMS group. Low-income participants receiving the intervention did have a longer time to cardiac event (death or hospitalisation), but this was not statistically significant. Overall there was no change in SES disparity, nor any added benefit from the intervention.\nThere is limited evidence to suggest that SES does affect outcomes following SMS interventions. Interventions that were tailored for low SES participants reported significant improvements in clinical outcomes, which in some cases also included a reduction in SES disparity following the intervention.\n---\nDiscussion\n\n\n---\nMain findings\n\nThis systematic review of disparities related to SMS interventions has reinforced observations [18-20, 22, 25] that there is a lack of research in this area. Although many studies of low SES groups have been undertaken, very few have focused on whether the outcomes compare favourably to those in higher SES groups. There are practical and statistical challenges in comparing population subgroups. Many studies had SES groupings that were fairly homogenous, limiting the ability to compare outcomes within the analysis, and almost all subgroup analyses were insufficiently powered. Larger studies and co-operation between different study populations are needed so that there is a more distinct contrast between SES levels across groups.\nResponses to study questions.\n---\nIs there evidence that SES influences participation rates in SMS interventions?\n\nThis review confirms that low SES groups are significantly less likely to participate in SMS interventions [42][43][44][45][46]. Thus, healthcare disparity is increasing before an intervention even commences. In order to reach those who need the intervention, targeted recruitment and retention strategies will be needed. Self-selection runs the risk of spending limited resources on those who need them least [46].\n2. Is there evidence that SES influences rates of retention or dropout from SMS interventions?\nThe findings in relation to retention and dropout are less clear-cut, with few studies and small sample sizes. Social factors do appear to be important [49][50][51], although a simple measure of SES may not capture the barriers to engagement.\n---\nIs there evidence that SES affects clinical, behavioural or other specified outcomes following SMS interventions?\n\nWith the limited number of high-quality studies available, there was some evidence that SES does affect outcomes following SMS interventions, depending on the type of intervention on offer. No trends were observed in terms of the SM components, which varied little between studies, or the type of service providers involved.\nProgramme structure (group or individual) did seem to affect both dropout rates and outcomes, with fewer benefits observed in the group interventions. In the few programmes that recorded dropout by SES, it appeared that attrition was also greater from group programmes (see Table 4 andTable 5). High rates of dropout from group programmes have been reported in several reviews of CD interventions in low SES and other vulnerable groups [21,59], while other reviews [13,60,61] have noted that individually tailored interventions appear to reduce disparity. Other authors have noted that although group programmes provide beneficial social support and peer modelling [5], they can also present many barriers to a low SES population who may have less flexibility in terms of work, transport or caring demands [21,59]. In the current review, interventions over longer time periods (6-12 months) also seemed to be more effective at reducing disparity [53,56,57], consistent with a CD review on similar populations [13].\n---\nInterpretation of findings\n\n\n---\n'Low SES' is a heterogenous group\n\nThis review suggests that SMS interventions may impact differently on low SES populations, and that more individualised treatment over longer time periods may be needed. Some writers have suggested that SES could be used as a 'high risk' predictor to identify those needing an earlier or more intensive intervention [23,62], although this encompasses a large population group and has significant resource implications, emphasising the need for appropriate targeting of interventions.\nData from the current review indicates that low SES groups are heterogeneous, with additional factors such as literacy, social stressors and social capital influencing SM ability, engagement, health outcomes [49,50,53,57] and thus disparity. Therefore, some low SES groups may benefit simply from better marketing of and access to generic SM courses [45] and lower-level interventions, while others will require a more intensive, tailored approach. The ability to accurately identify these groups, perhaps by using a triage instrument, could lead to more effective resource allocation, increased participation and better outcomes in terms of both efficacy and equity.\n---\nAre self-management mechanisms different in low SES populations?\n\nFew studies reviewed described the theory behind the proposed SMS intervention, as noted in other reviews of SMS [12,63], although several referred to the role of self-efficacy [40,54,58,64], as described in Bandura's social-cognitive theory [4,5]. The studies which targeted a low SES or otherwise diverse population did note particular challenges for disadvantaged groups in terms of knowledge or literacy [47,53,56,57], and those which adapted to these challenges often had better outcomes. In contrast, 'one size fits all' programmes [45,46,54,58] had fewer benefits, and in some cases increased disparity.\nSMS approaches informed only by self-efficacy have been criticised as overly individualistic [10,11,15] and it has been observed that the relationship between selfefficacy and self-management ability is weaker in vulnerable groups [65], indicating that other barriers play an important part. Furthermore, since the development of self-efficacy depends both on one's behaviour and on social/environmental feedback [66], several authors [11,58] have suggested that increasing self-efficacy may be harder if environmental feedback (e.g. job or housing insecurity) negates a belief in control over one's circumstances.\n---\nWhat other factors are important for selfmanagement in low SES groups?\n\nThis suggests that for SMS interventions to be effective in low SES populations, attention should be paid to other factors that influence self-management ability. Health provider/system issues [67,68]; resources (literacy, financial, job/carer demands) [67,[69][70][71]; and condition demands (multimorbidity, treatment burden) [48,71,72] have been consistently identified in qualitative reviews as barriers to self-management. Each of these factors will impact disproportionately on a low SES population. Health providers/ systems can be less accessible due to cost, literacy levels and a limited understanding of the social determinants of health by providers [67,68]. Although few studies of SM in disadvantaged populations look at interventions at the health provider/system level [18,21], it would seem a potentially effective way to reduce disparity without increasing the patient's treatment burden.\nBarriers related to resources and condition demands are far greater for the low SES population [73][74][75], who have fewer financial and social resources; higher levels of overall social complexity (job/housing insecurity, family demands, trauma history [3]); and higher rates of multimorbidity at earlier ages [76]. They experience both more disease-related workload (treatment burden) and non-disease workload (life burden) [73,77]. Unfortunately, many SMS interventions, especially those requiring regular attendances or homework, will increase workload. Approaches that reduce patient workload or increase access to resources are rarely tried, but are likely to be important in low SES groups [73]. Phone consultations, problem-solving of specific barriers, integrating healthcare with social services and directing interventions toward healthcare practitioners rather than individual patients can all reduce treatment burden and maximise resources. Coventry [76], in a qualitative study of SM and multimorbidity, identifies three factors required for engagement in SM: capacity (resources, knowledge and energy); responsibility (shared understanding between the patient and provider about how to manage the treatment workload) and motivation. All three are negatively impacted by low SES, yet many SMS interventions [10] aim to increase motivation without recognising responsibility or capacity, and thus may contribute to increasing disparity in low SES groups.\n---\nStrengths and limitations\n\nThis review identifies important gaps in knowledge and potential directions for future research. It reveals the assumptions informing SMS approaches and the inadequacy of using 'low SES' to define a population group. The study limitations include the lack of published research on disparity in SM interventions. It was difficult to conduct a comprehensive literature search of this topic because many subgroup analyses were a relatively small part of the overall paper. It is possible that some studies were missed that may have provided useful data. Meta-analysis was not possible due to the variety of studies available; therefore, no strong conclusions can be formed. In addition, the methodology of many of the studies prohibited causal inference: several studies were cross-sectional and most subgroup analyses were underpowered or did not formulate a priori hypotheses.\n---\nConclusion\n\nThis review has identified several important themes in relation to self-management and socioeconomic disparity. First and most obviously, there is a great need for equity considerations to be included in CD studies, as advocated by Cochrane reviewers [22,25]. Given the strength of evidence available about social determinants of health, it should be possible to establish a priori hypotheses and sample sizes sufficient for subgroup analysis (including the availability of relevant comparator groups) for many interventions.\nSecondly, any intervention in a low SES or otherwise disadvantaged group should consider its theoretical basis. Social-contextual approaches, rather than selfefficacy approaches, may be more effective. Paying greater attention to the large and consistent body of qualitative studies on barriers to SM can provide both theoretical and practical guidance as to interventions that can address disparity. Approaches such as the Cumulative Complexity Model [77], which is founded on patient burden-capacity balance, have much to offer.\nFinally, levels of disadvantage vary, and there is a need for risk identification within the low SES population. For many people, improving access to simple SM interventions (e.g. assistance with childcare or transport, free programmes at community locations) may be all that is needed. For othersespecially those with multimorbidity, poor literacy or social complexityan individually tailored approach will be needed to be effective. Research to develop a risk assessment system may ensure that those most in need receive the greatest support as opposed to the current situation.\n---\nAvailability of data and materials\n\nData sharing is not applicable to this article as no datasets were generated or analysed during the current study.\n---\n\n\nEthics approval and consent to participate Not applicable.\n---\nConsent for publication\n\nNot applicable.\n---\nCompeting interests\n\nThe authors declare that they have no competing interests.\n---\nPublisher's Note\n\nSpringer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.",
        "Introduction\n\nEnvironmental changes are driven by the interaction between multiple biophysical, ecological, and socioeconomic factors operating at different scales. When over-lapping spatially and temporally, these interactions may have antagonistic-but most often synergistic-effects, with significant impacts on ecological processes and human health and wellbeing (D\u00edaz et historical interactions between people and waterscapes (Prestes-Carneiro et al. 2021) and how water defines our food systems (Young et al. 2021). Not surprisingly, the study of Indigenous and local knowledge around water is gaining prominence in the discipline (e.g., Goodall 2008;McLean 2017).\nChanges in water driven by direct human interventions, like those brought by large-scale infrastructure projects that drastically impact water quality, availability, and dynamics, can be, however, severe and immediate (e.g., Veldkamp et al. 2017). The challenges of a rapidly changing global environment further compound, and are aggravated by, colonial settlement, land dispossession, resource extraction, and the imposition of large-scale infrastructures, all of which continue to deny IPLC access to a safe and clean environment, including water (Armstrong and Brown 2019;Estes and Dhillon 2019). These interlinked changes endanger IPLC livelihoods and the very foundations of their ways of life (e.g., Gebreyes et al. 2020;Wolverton et al. 2014).\nThe complexity of changes in water and their consequences represents a challenge for ethnobiologists aspiring to understand the myriad of impacts upon biophysical systems and IPLC livelihoods. In this sense, addressing the interwoven nature of drivers of environmental changes through Local Ecological Knowledge (LEK) has been gaining traction in ethnobiology and other related fields (e.g., Pyh\u00e4l\u00e4 et al. 2016;Reyes-Garc\u00eda et al. 2016). Changes in water affecting IPLC are expressed and observed locally, but driven by intricate political and socioeconomic pressures that happen simultaneously at local, regional, and global scales (see G\u00f3mez-Baggethun et al. 2013). Although some IPLC might not always be fully aware of socioeconomic or political changes at regional or global scales, they deal with the expressions of these changes in their daily activities and their surround-al. 2019). Indigenous Peoples and Local Communities (IPLC) across the world are largely dependent on natural resources for subsistence (FPP 2020). Thus, the fast and widespread environmental changes that have intensified in the last decades disproportionately impact their livelihoods, wellbeing, and social-ecological resilience (Ford et al. 2020;Savo et al. 2016).\nChanges related to water are driven by complex interactions of socioeconomic and ecological factors, notably climate change and direct human interventions (Droogers et al. 2012;Graham et al. 2020;Haddeland et al. 2014). Changing rainfall patterns, for example, result from alterations in the global climatic system but also from land-use changes, with impacts on water evaporation, infiltration, and discharge (Gerten et al. 2008;Sterling et al. 2013). In some coastal areas, water quality is being compromised by increased salinity, driven by sea-level rise and intensive farming (Vineis et al. 2011). Beyond the direct impacts, these changes in hydrological patterns and processes may also lead to shifts in multiple biotic components of ecosystems, such as in species distribution or phenology (e.g., Comte et al. 2013).\nChanges in water and associated impacts are particularly relevant for IPLC, many of whom already suffer from constraints regarding water security and sovereignty (e.g., Rosinger et al. 2020;Wilson 2014), and especially for those who inhabit regions where water is scarce and/or whose livelihoods are largely dependent on aquatic resources (Alessa et al. 2008;Dow et al. 2007). These changes may also negatively impact IPLC's multifunctional and holistic systems of water management, sometimes referred to as \"Indigenous water cultures\" (McLean 2017). Many ethnobiological studies have taken water as a starting point to explore the complex inter-connections between people and their biological worlds (e.g., Reyes-Garc\u00eda et al. 2011;Singh 2006). Ethnobiologists have long examined the ing environments (Fern\u00e1ndez-Llamazares et al. 2015;Savo et al. 2016). Also, through their long-term and intimate empirical interactions with nature (Eira et al. 2018;Orlove et al. 2010), IPLC have developed holistic understandings and rationales about observed changes and their interconnections and drivers, on which they base their decisions and adjust their livelihoods (e.g., Mustonen and Huusari 2020;Reid et al. 2014). Understanding IPLC perceptions of environmental change, therefore, offers a window of opportunity to bring into focus the complex interrelations between climatic and other drivers of change.\nIn this paper, we study local perceptions of environmental change among the Daasanach people, an agropastoralist society inhabiting one of the driest regions of Kenya, where access to clean water is limited and unreliable (Bethancourt et al. 2020). Besides being affected by climate change (Kogo et al. 2020;Niang et al. 2014;Ouma et al. 2018), the region is also witnessing a surge of large-scale infrastructure projects, such as dams and irrigation projects (Carr 2017;Hodbod et al. 2019), which are bringing substantial hydrological, biological, and socio-ecological changes. By tapping into the Daasanach perceptions of change, the goals of this study are (1) to identify the changes that the Daasanach perceive in the climatic and biophysical environment, their drivers and interconnections, focusing on water-related changes;\n(2) to assess the salience of water-related changes within the Daasanach's perceptions of environmental change; and (3) to evaluate locally reported interactions between climate change and infrastructure projects in driving water-related changes.\n---\nThe Daasanach People and Their Environment\n\n\n---\nBiophysical CHairacteristics\n\nOur study site is located at the northeastern shore of Lake Turkana, in northern Kenya, bordering with Ethiopia (Figure 1A). The region's environment is cHairacterized by a semi-desert with an arid climate, with mean annual temperatures around 32 \u00b0C (Mbaluka and Brown 2016). Mean annual rainfall is around 150 mm distributed along two rainy seasons: from March to May and from October to December (Mbaluka and Brown 2016). The rainy and dry seasons are modulated by the El Ni\u00f1o Southern Oscillation and the Indian Ocean Dipole, which sometimes lead to the occurrence of pronounced rains and floods (Owiti et al. 2008). Historical climatic data for the region indicates a warming trend since the early or mid-twentieth century (Niang et al. 2014;Ouma et al. 2018). Total annual precipitation has shown no clear trends (Niang et al. 2014;Ouma et al. 2018), but the distribution of rainfall throughout the year has been changing, with the first rainy season becoming drier and the second becoming wetter (Liebmann et al. 2014).\nA fundamental component of the landscape is Lake Turkana, which covers about 7560 km 2 , known as the world's largest permanent desert lake and the largest alkaline water body (Ojwang et al. 2016). The lake hosts a rich aquatic ecosystem that provides food to several local Indigenous communities (e.g., Turkana, Daasanach, El Molo) and sustains small commercial fisheries (Gownaris et al. 2017). The level and chemical cHairacteristics of the lake water are highly variable and largely dependent on the Omo River, which contributes 90% of the water inflow (Avery 2010). River and lake levels fluctuate naturally on an annual basis, flooding extensive areas along the margins of Lake Turkana and the Omo Delta (Avery 2012;Gownaris et al. 2017;Ojwang et al. 2016).\nThe landscape is dominated by dwarf shrubland and grassland vegetation, with some riparian forests along ephemeral streams that carry underground water (Mbaluka and Brown 2016). The region contains unique flora and fauna and hosts a world-renowned paleontological record of human evolution (e.g., Leakey et al. 2012), all of which contributed to the establishment of Sibiloi National Park in 1973, declared as a UNESCO World Heritage site in 1993 (IUCN 2020;Mbaluka and Brown 2016;NMK and KWS 2019). The region has suffered a process of large-scale defaunation, attributed to overhunting and livestock overgrazing associated with a rapidly growing population of Daasanach pastoralists (Cabeza et al. 2016;NMK and KWS 2019;Torrents-Tic\u00f3 et al. 2021). These threats, together with those brought about by large-scale infrastructure projects, led to the Sibiloi National Park being inscribed in 2018 in the UNESCO List of World Heritage in Danger (UNESCO 2018).\n---\nThe Daasanach\n\nThe Daasanach are a semi-nomadic agropastoralist group extending between southeast of South Sudan, southern Ethiopia, and northern Kenya, occupying the northern shores of Lake Turkana, the lower stretch of the Omo River valley and its delta (Mwamidi et al. 2018; Figure 1A). They are Cushites of the Omo-Tana branch, who speak the Daasanach language (Tosco 2001). They are primarily considered an agropastoral society (Almagor 1978), mostly herding cattle, sheep, and goats (as well as donkeys and camels, to a lesser extent), and opportunistically growing maize, sorghum, and beans in flooded plains. Agriculture is practiced regularly by Daasanach living close to the Omo Delta in Ethiopia (Gebre 2012) and, more sporadically, among Kenyan Daasanach (Bethancourt et al. 2020), although some people occasionally migrate temporarily to practice cultivation in the Omo Delta. Some Daasanach have also recently started fishing in Lake Turkana, as other resources (e.g., cattle) become increasingly scarce or unavailable (Figure 1D). In this study, we focus on the Daasanach of northern Kenya, who number about 17,000 people (KNBS 2019).\nThe main drinking water sources for the Daasanach in northern Kenya are traditional wells, boreholes, and waterholes dug in dry riverbeds (Bethancourt et al. 2020; Figure 1C). Due to its salinity, the lake is rarely used as a source of drinking water (Bethancourt et al. 2020), but wildlife and livestock drink water directly from the lake (Kaijage and Nyagah 2009) (Figure 1B). Herding is strongly dependent on the availability of pastures, maintained by seasonal rainfall. During the dry season, conflicts over suitable grazing sites often arise with other ethnic groups (Gebre 2012) and with Kenyan authorities due to the use of pastures inside the Sibiloi National Park (IUCN 2020). Their long histories of place-based living and time-honored traditions have generated intricate and complex knowledge systems (e.g., Daasanach Community 2019; Mwamidi et al. 2018), including sophisticated sets of knowledge and practice about both climate and water management.\n---\nInfrastructure Projects along the Omo River\n\nThe Omo River and Lake Turkana are under increasing stress from hydropower development and large-scale waterintensive agricultural schemes in Ethiopia (Avery and Tebbs 2018;Tebbs et al. 2020).\nThe construction of the 1870-megawatt Gibe III dam, one of Africa's largest hydropower projects, is predicted to recede the water flow to Lake Turkana by up to 70% (Ojwang et al. 2016;UNEP 2013), most likely opening up a Pandora's box of interrelated social and environmental problems. The dam, inaugurated in 2016, has already allowed the diversion of Omo River water for the large-scale irrigation of sugar and cotton plantations (Avery and Tebbs 2018). Such water abstractions could considerably reduce the depth of Lake Turkana, affecting its aquatic resources and fish communities (Avery 2012;Gownaris et al. 2017). Although the actual biophysical impacts of the dam are still to be thoroughly understood, some impacts, such as alterations in the seasonal water fluctuation of Lake Turkana (Spruill 2019) and decline in primary production (Tebbs et al. 2020), have already been reported.\nIt should be noted that the expansion of water-intensive plantations in the Omo River was not mentioned in the dam's environmental impact assessment (Asnake et al. 2009;Avery 2012), which neglected both international standards and Ethiopian domestic legislation (Schapper and Urban 2021). On the Ethiopian side, many communities were forcibly evicted from their ancestral lands (Carr 2017), and Kenyan Daasanach communities claim that they were never consulted about the project (Hathaway 2009). The absence of social, cultural, or environmental safeguards in this infrastructure project has triggered widespread condemnation in both the international media and the scholarly literature (Schapper and Urban 2021).\n---\nMethods\n\nWe conducted our study in permanent and temporary Daasanach settlements surrounding the town of Ileret (4.314\u00b0 N, 36.227\u00b0 E and ~380 m.a.s.l.; Figure 1A). We used qualitative methods to obtain and interpret data on Daasanach percep-tions of change. Data collection took place during 2019 and 2020 and involved semi-structured interviews and a focus group discussion, following a framework to document local observations of change developed by Reyes-Garc\u00eda et al. (2020). This research was conducted as part of a long-term participatory research project to document local knowledge and build social-ecological resilience, developed by the University of Helsinki in partnership with the Daasanach community (e.g., Cabeza et al. 2016;Daasanach Community 2019;Mwamidi et al. 2018;Torrents-Tic\u00f3 et al. 2021). We employed a participatory approach, engaging two members of the Daasanach community (P. Lokono Haira and J. Guol Nasak.) in most phases of the research process and making sure that local residents adequately understood essential concepts addressed in the interviews (e.g., \"change\" or \"environment\").\nPrior to data collection, we conducted a meeting with local leaders and elders from all the communities involved in this research to present the project and obtain authorization to conduct research. We obtained oral free, prior, and informed consent from each person we interviewed, as well as permission from the relevant administrative organization in the area (Ileret County Ward). After finishing data collection, we conducted a meeting with the local communities to present and discuss research results obtained. This research adheres to the Code of Ethics of the International Society of Ethnobiology (ISE 2006), and has been conducted with the permission of the National Commission for Science, Technology, and Innovation of Kenya (NACOSTI/P/18/21446/20296), and the Ethics Committee of the Universitat Aut\u00f2noma de Barcelona (CEEAH 4781).\n---\nData Collection\n\n\n---\nSemi-structured Interviews\n\nTo capture the widest possible spectrum of perceptions on changes taking place in the environment, we targeted local residents recognized as knowledgeable by members of the community (using snowball sampling), trying to cover a wide age range and diversified expertise in the local livelihood activities: livestock herding, farming, and fishing. In total, we interviewed 45 local residents (32 men and 13 women, with ages varying from 18 to ~75 years old) in eight villages and two temporary settlements. All interviews were conducted in the Daasanach language by P. Lokono Haira and J. Guol Nasak.\nThe semi-structured interviews had two main objectives: (1) to identify the changes that the Daasanach perceive in the climatic and biophysical environment and (2) to identify the local perceptions on the drivers of these changes and their interconnections. To do so, we asked each of the 45 interviewees about which changes they had noticed in their environment since they were young (i.e., approximately when they got married for the first time, usually corresponding to 13-15 years old). For every change mentioned, we also asked why they thought these changes were happening and what was causing them. Although our analyses are mostly focused on water-related changes, our interviews were designed to capture more broadly the local perceptions of environmental change. Given the wide age range of the interviewees, we were able to reconstruct changes happening in different time scales.\n---\nFocus Group Discussion\n\nAfter compiling the information from the semi-structured interviews, we conducted a focus group discussion with ,20 residents (,14 men and ,6 women) from the same villages and with the same age range. About half of the participants of the focus group discussion also participated in the semi-structured interviews. The focus group discussion aimed to assess the level of group consensus regarding the changes mentioned, clarify some incomplete information, and-potentially-document new observations of change. We presented to the group every change mentioned during the semi-structured interviews, asking if they had noticed these changes, what was causing them, and what their consequences were. Therefore, all the information on the observations of change and their connections reported here are based on collectively validated information.\n---\nData Analysis\n\nEach individual change reported (hereafter \"citation\") was coded using a four-step procedure. First, we grouped similar citations into \"observations of change\" (hereafter OC) and calculated their frequency of citations in semi-structured interviews. Second, we coded all OC following the hierarchical classification of \"Local Indicators of Climate Change Impacts\" proposed by Reyes-Garc\u00eda et al. (2019), in which changes are classified according to the \"system\" in which they are observed: climatic (e.g., rainfall), physical (e.g., soil), biological (e.g., plants), and human (e.g., agriculture, livestock). Although this categorization was originally designed to classify climate-related changes, we used it as a framework to classify all observations of change, irrespective of the driver. Third, we categorized the perceived drivers of change according to the Daasanach's own accounts and used three categories: \"climatic\" (changing climatic conditions), \"sociodemographic\" (changing sociodemographic conditions, e.g., growing population), and \"infrastructure\" (changes driven by infrastructure projects in the Omo River). A given OC may be simultaneously associated with different drivers. Lastly, we identified OC perceived to be related with water (hereafter \"water-related\"), categorizing them between those directly observed in hydrological processes (e.g., precipitation, river flow; \"now there is less rain\") and those caused by changes in hydrological processes (e.g., \"now there are fewer pastures because there is less rain\").\nTo identify the changes that the Daasanach perceive in the climatic and bio-physical environment, their drivers and interconnections, we selected the most salient OC and associated drivers based on the frequency in which these were reported in the semi-structured interviews, focusing particularly on water-related changes. We then discussed these changes based on local rationales expressed during the interviews and the focus group discussion. To evaluate the perceived interactions between climate change and infrastructure projects leading to changes in water, we quantified the proportion of water-related changes that were caused by each of the three categories of drivers, and we represented these proportions using Euler diagrams (Figure 2). As part of the analysis, we illustrate the Daasanach's integrated perceptions of change using a visual representation of water-related changes, their drivers, and connections, based on the information obtained in the semi-structured interviews and collectively validated in the focus group discussion.\n---\nResults\n\n\n---\nLocal Observations of Environmental Change and Their Drivers\n\nIn total, the 45 Daasanach interviewees mentioned environmental changes 194 times (citations), which were grouped in 73 observations of change (Supplementary Table 1). Most observations of change (25; 34.2 % of the total) referred to changes in the biological system, followed by changes in the human (23; 31.5 %), climatic (13; 17.8 %), and physical systems (12; 16.4 %) (Table 1; Supplementary Table 1).\nMost changes in the climatic system referred to increases in the frequency, duration, and intensity of drought, to an overall lower and more variable precipitation, and to increases in temperature and wind strength (Supplementary Table 1). These changes, particularly drought, were frequently mentioned as major or indirect drivers of changes in other systems. For example, several respondents mentioned that \"now there is less grass for the animals because the droughts are longer.\" Regarding the physical system, most of the reported changes referred to changes in water quality and dynamics of Lake Turkana and to changing soil conditions (Supplementary Table 1). Respondents reported, for example, that \"now the water of the lake is more salty,\" or that \"the lake floods do not happen as before.\" Soils were mentioned to have become drier and less fertile, owing to drought and to the increase in wind-induced erosion (Supplementary Table 1).\nDaasanach interviewees also mentioned several changes in the biological system, including changes in the abundance and species composition of freshwater animals and plants, the abundance and behavior of wild fauna, and the mortality, productivity, and phenology of terrestrial plants (Table 1; Supplementary Table 1). People said, for example, that \"now there is less grass and less fish in the lake,\" that \"drought and hunting have killed all the big animals,\" and that \"there are fewer trees, [because] they are dying from drought and people are cutting them down.\" Changes in the human system were among the most frequently mentioned and largely focused on livestock and pastures (Table 1). The Daasanach agreed that there has been an increase in the amount of livestock (owing to population growth) which, together with the increased droughts, are reducing the availability and productivity of pastures. This overall reduced availability of pastures, they say, is leading to lower productivity of livestock and more livestock diseases. Changes in agriculture were less frequently reported and mostly related to the reduction of suitable cultivation areas along the Omo Delta and along small rivers (Supplementary Table 1).\nMost OC (64; 87.7%) were associated with \"climatic\" drivers, followed by \"socio-demographic\" (28.8%) and \"infrastructure\" drivers (26%), with 35% of the changes associated with more than one driver (Figure 2A).\n---\nThe Salience of Water-Related Local Observations of Change\n\nWater-related changes are salient in local perceptions of environmental change.\nA total of 17 OC (23.3%) referred to changes directly observed in hydrological processes (Table 1). Most of these (9 OC; 12.3%) reported changes in precipitation, namely the reduction in rain events during the dry season, changes in the temporal distribution and predictability of rains, the increase in intensity and frequency of droughts, and a prolonged dry season. The other eight OC (11%) referred to changes in water quality, availability, and dynamics, such as an increase in the salinity and changes in the flood dynamics of Lake Turkana, and the increased depth of the water table (Supplementary Table 1). Another 39 OC (53.4%) were mentioned as, directly or indirectly, resulting from changes in hydrological processes (Table 1). For example, respondents reported that \"now the livestock are producing less milk because the lake water is more salty.\"\nThe Daasanach reported climate and infrastructure changes as the most important drivers of water-related changes. Within Table 1. Observations of environmental change reported by the Daasanach. Each change mentioned in the interviews (\"citations\") was grouped into \"observations of change\" (OC) and categorized based on the \"system\" (e.g., climatic) and \"subsystem\" (e.g., air masses) in which they were observed. Based on the local understanding of changes and their connections, drivers of change are categorized into \"climatic\" (\"Clim\"), \"demographic\" (\"Dem\"), and \"infrastructure\" (\"Infr\"), and OC are identified as \"water-related\" when directly observed in hydrological processes (\"Observed\") or when caused by changes in hydrological processes (\"Caused\"). Cells show the number of citations or OC, and numbers in parentheses indicate percentages.  18) 13 ( 18) 9 ( 12)\n---\nSystem\n\non other elements of the biophysical and human systems. In particular, the overall lower rainfall and higher frequency and intensity of drought are directly linked with changes in water, but are also reported as major causes for changes in wild and cultivated plants, wild fauna, and the availability of pastures for livestock. Changes reported in the fluctuation of the Omo River and Lake Turkana as a result of infrastructure projects are also associated with multiple impacts in freshwater fauna and the availability of areas for cultivation (Figure 3).\n---\nDiscussion\n\nThe ongoing global social-ecological crisis poses important threats and challenges to IPLC, which are likely to intensify and multiply in the near future (Reo and Parker 2013;Savo et al. 2016). Based on their LEK and close interaction with nature, IPLC are developing their own understandings of these changes and drivers. Here, we show the detailed understanding of shifts in the environment by the Daasanach in northern Kenya, with changes in water playing a central role, and where observations of change in different elements of the social-ecological system are understood as the outcome of imbricate interactions between climatic and socioeconomic drivers.\nOur results show how changes in water are central in local perceptions of change, driving multiple changes in other elements of the biocultural system. Many IPLC across the world face important constraints to their water security and sovereignty, driven by both geographic and climatic conditions, but also by socioeconomic and political arrangements (Alessa et al. 2008;Fern\u00e1ndez-Llamazares et al. 2020;Rosinger et al. 2020). Ongoing and predicted changes in climate, such as changes in rainfall and in river fluctuation regimes (Jim\u00e9nez Cisneros et al. 2014), will affect the dynamics, availability, and quality of water resources. Although the Daasanach have historically adapted to dry and markedly seasonal landscapes the 17 OC directly observed in hydrological processes, nearly all (16 OC; 94.1%) were attributed to climatic drivers, while four OC (23.5%) were attributed to the dam construction, and three OC (17.6%) were attributed to more than one driver (Figure 2B). Within the 39 OC understood as being caused by changes in water processes and reservoirs, most (36 OC; 92.3%) were linked with climate drivers, followed by demographic drivers (15 OC, 38.5%) and infrastructure (15 OC; 38.5%). Twenty-two OC (56.4%) were linked with more than one driver (Table 1; Figure 2C). For example, the Daasanach reported that \"now the fish in the lake are smaller and scarcer,\" which they attribute to changes in the lake water driven by the dam construction but also to overfishing, as \"now there are more people fishing.\" Similarly, the observation that \"now there are fewer people farming, it is much more difficult to farm\" is associated with changes in water fluctuations in the Omo River (reducing the floodplains in the delta, where they used to cultivate in the past) as well as with increased droughts, both of which have led to an overall decrease in seed circulation and in the availability of suitable cultivation areas.\n---\nDaasanach Holistic Understanding of Changes\n\nThe Daasanach holistic understanding of environmental change involves a complex network of interacting changes, drivers, and cascading effects. Based on the interconnections between changes mentioned during the interviews and the focus group discussion, we were able to reconstruct some of these networks, which evidence the perception that socioeconomic, demographic, and climatic changes simultaneously impact local livelihoods (Figure 3).\nGiven their relevance in local perceptions of environmental change, water-related changes also tend to be important elements in these networks, having cascading impacts (Mwamidi et al. 2018), the perception that drought patterns and river-lake water dynamics are changing are both undisputed, and this is understood to have several cascading consequences.\nOverall, the salience of changes in water amid the local perceptions of environmental change is largely due to the fact that the Daasanach's main livelihood activities are all finely tuned with seasonal fluctuations in water availability. Thus, consequences of changes in these fluctuations are perceived in many elements of local livelihoods. Hence, considering only direct impacts of environmental change on biocultural heritage or over-simplifying the richness of IPLC environmental relations can underestimate the severity of rapid social-ecological changes on IPLC cultures and livelihoods (see Lyver et al. 2019). While water-related changes affect (and are perceived by) IPLC all over the world, these can be particularly salient and harmful to subsistence-based groups, like the Daasanach, who inhabit dry or very seasonal regions, where certain livelihood activities may become untenable and water security may worsen, as societies face more extreme or unpredictable climatic conditions (see Dong et al. 2011).\nWe also show how climatic and socioeconomic/infrastructure changes interact synergistically, threatening local livelihoods through their impacts on water resources. The Daasanach unanimously report to be facing increasingly drier and unpredictable climatic conditions, perceptions that align closely with historical and predicted climatic data (Funk 2020;Liebmann et al. 2014). Current and planned large infrastructure projects in the Omo River, including dams and the establishment of sugar estates, are predicted to bring significant hydrological and social-ecological impacts to the Omo-Turkana basin (Avery and Tebbs 2018;Carr 2017;Hodbod et al. 2019). Indeed, the Daasanach refer to these infrastructure projects (particularly the construction of dams) as a major cause for changes in the hydrological dynamics in the Omo-Turkana basin, with multiple reported impacts on their livelihood activities. According to them, infrastructure projects have changed the Lake Turkana aquatic fauna and flora, including reductions in fish stocks and size, and increased water salinity. Besides the impacts on fishing, these changes also affect livestock herding, as saline water is considered to affect milk quality negatively (with potential dietary implications for the Daasanach people), and the changes in the annual flood pulse prevent the regeneration of pastures in the lake margins. Importantly, the Daasanach argue that hydrological changes brought up by the dam construction have significantly reduced the floodplains in the Omo Delta, further constraining their opportunities for practicing agriculture. While the actual infrastructure projects along the Omo River are mostly outside the territories traditionally occupied by the Daasanach, and in spite of the fact that their perceptions might be to some degree influenced by external information sources, they clearly associate these projects to several of the perceived changes in their environment.\nOverall, together with changes in the Omo-Turkana system brought by large-scale infrastructure projects, climatic changes are driving important shifts in how the Daasanach are managing their biocultural landscapes and adjusting their livelihoods. Agriculture has become increasingly difficult, restricted both by the scarcer and more unpredictable rains (preventing rainfed agriculture which, although minor, was practiced along small temporary rivers) and by changes in the flood dynamics of the Omo River (preventing flood-recession agriculture in the Omo Delta). Fishing, which used to be a marginal activity associated with times of strong droughts and food shortage, is reported to have become increasingly common (in spite of the perceived diminishing stocks), owing to increased marked opportunities but also to growing challenges associated with livestock herding and plant cultivation. Besides highlighting the threats brought by environmental changes to the different livelihood activities on which the Daasanach have historically relied, understanding changes in livelihoods from the integrative perspective of the Daasanach can be used to explore viable adaptations to environmental change.\nOur findings suggest that many biocultural impacts were undervalued during the planning phases of the Gibe III dam and its associated large-scale irrigation schemes (Carr 2017;Schapper and Urban 2021). Many top-down, state-sanctioned development interventions undervalue, discount, and overlook impacts on biocultural heritage (e.g., Estes and Dhillon 2019; Hinzo 2018). Although much of this relates to power dynamics and macroeconomic forces, these omissions might also stem from the difficulties in grasping the intertwined nature of many of these changes, and their cascading biocultural impacts (see also Lyver et al. 2019). Many biocultural losses continue to be largely invisible to development planners and policy-makers, particularly those that affect intangible biocultural heritage (see Turner et al. 2008). The fact that the biocultural ramifications of the dam construction were largely missing in the environmental impact assessment of the Gibe III dam (Schapper and Urban 2021) is particularly illustrative in this regard.\nFinally, our results call attention to the potential of LEK in enhancing the understanding of complex social-ecological issues, such as the impacts of environmental change in local communities. A growing number of studies highlight the potential of LEK for the understanding of environmental changes (Fern\u00e1ndez-Llamazares et al. 2017;Nabhan 2010;Postigo 2014) and the potential synergies between LEK and scientific knowledge to enhance our capacity for dealing with environmental change (Shaffer 2014;Teng\u00f6 et al. 2014). By tapping into Daasanach observations of environmental change, we show that they recognize numerous indicators of change that are deeply interwoven and manifested in many different elements of their environment. We argue that ethnobiological studies around climate change should pay more attention to IPLC holistic understandings of changes, with an explicit focus on how these communities understand the drivers of changes and the myriad of interrelations between them (see Wyndham 2009). This would allow identifying impacts that may be largely overlooked by government and development agencies and foster more refined understandings of how different environmental and socioeconomic pressures interact and how they impact local biocultural systems.\n---\nConclusions\n\nOur findings highlight how climatic and socioeconomic/infrastructure drivers interact synergistically and lead to substantial impacts in water resources, with cascading effects in multiple components of the biocultural system and with important consequences for the livelihoods of the Daasanach people of northern Kenya. Besides jeopardizing water security, shifts in water affect all livelihood activities on which Daasanach have historically relied to manage their arid landscapes and to deal with social-ecological changes. In dry regions of Africa and elsewhere, climate change is expected to amplify existing stresses in water availability, which are also exacerbated by multiple socioeconomic drivers (Droogers et al. 2012;Niang et al. 2014). Synergistic negative effects of climate change and socioeconomic drivers on water, as illustrated here by the case of the Daasanach, are likely to occur in many other regions. It is, therefore, essential that these potential synergies are taken into account in the design of development and infrastructure projects that impact water, aiming to minimize their social-ecological impacts and to promote more equitable and ethical sharing of their benefits. We argue that inclusive governance which explicitly incorporates IPLC understanding of envi-ronmental change is a promising pathway towards achieving these goals.\n---\n\n\nSoil and Land 9 (5) 4 ( 5) 4 ( 5) 2 (3)\nBiological system 64 ( 33) 25 ( 34) 20 ( 27) 10 ( 14) 11 ( 15) 22 (30)\nFreshwater 31 ( 16) 11 ( 15) 9 ( 12) 4 ( 5) 9 ( 12) 10 ( 14)\nTerrestrial Fauna 17 ( 9) 7 ( 10) 5 ( 7) 4 ( 5 ",
        "INTRODUCTION\n\nNeuroimaging models have significantly expanded our understanding of the neural processes that instantiate a person's subjective pain experience [for reviews see (1)(2)(3)]. Through neuroimaging, we have learned that the brain representation of pain is highly distributed and multidimensional involving sensory, cognitive, and affective components (4)(5)(6)(7). Neuroimaging models employing multivariate [i.e., multivoxel pattern analysis or MVPA; (8)], predictive (i.e., machine learning), and network analysis techniques can, respectively, delineate multiple component processes that contribute to both acute and chronic pain (7,(9)(10)(11), predict a person's self-reported evoked pain intensity (12,13), and localize sites of functional connectivity disruption across chronic pain phenotypes (14,15).\nDespite these important advances, neuroimaging research has yet to significantly impact the clinic. Anatomical and resting state markers lack specificity-it remains unknown whether changes are due to chronic pain or to co-morbidities like anxiety and depression [for reviews see (16,17)]. Furthermore, most models are developed on experimental data of evoked phasic pain where participants experience a brief (under 12 s) noxious stimulus such as prick or a hot plate against the skin. This does not translate well to chronic pain which must persist 3 or more months. Acute or phasic pain is typically appraised as temporary and separate from the self, while chronic pain is typically appraised as unending and apart of one's life (18). Chronic pain is also highly personalized and embedded in spontaneous and tonic, rather than evoked and phasic, activity in the brain (19)(20)(21)(22). Finally, population samples are not wellstratified across economic class, race, or ethnicity (23). In most cases, participant socioeconomic status (SES) is not reported nor well-measured [for a review see (24)]. Because chronic pain disproportionately affects the poor and working class across the globe (25)(26)(27)(28)(29)(30)(31)(32)(33), neuroimaging models of pain must take socioeconomic information into account.\nThe biopsychosocial approach to pain management attempts to encapsulate the broader societal issues which situate interactions among the biological, psychological, and social components of the pain experience (34). This conceptual framework states that understanding pain requires an understanding of the whole patient, their relationships, and society (35,36). However, the biopsychosocial approach is largely theoretical and has yet to be well-integrated into pain neuroimaging research. To resolve this translational gap, this perspective formulizes the biopsychosocial approach into testable neuroimaging models intended for the diagnosis, treatment, and prevention of chronic pain. These models endeavor to predict and understand chronic pain from three levels, that of the individual, of the population, and of society.\nFirst, recommendations are made to increase the diagnostic relevance of the population-based, or nomothetic, approach to the development of pain neuroimaging models. These recommendations include a shift in focus from evoked phasic pain to evoked tonic pain paradigms and the recruitment of larger and more diverse population samples. Second, a person-based or idiographic approach to the development of treatment-relevant models is discussed. Recommendations are made for the training and implementation of these models so that they can be used to track disease progress and treatment efficacy within individual patients. Finally, a novel society-based, or social epidemiological approach to the development of prevention-relevant models is proposed. This approach situates an individual's disease state within the socioeconomic conditions they live in. Lastly, implications for both the clinic and public policy are outlined.\n---\nNOMOTHETIC (POPULATION-BASED) APPROACH TO DIAGNOSTIC MODELS\n\nHuman subjects research is largely nomothetic, that is, the goal is to generate an explanation of brain activity that is \"universal\" and generalizable to entire populations (Figure 1A). Such models are trained on many different people sampled from the same population. Individual differences are treated as noise and intentionally minimized through careful inclusion/exclusion criteria, outlier removal, and the inclusion of confound regressors controlling for demographic variables such as age and gender identity [for a review see (37)]. The nomothetic approach is appropriate for the development of diagnostic biomarkers because inferences must be drawn from the wider population to identify pain pathologies in new patients presenting symptoms for the first time.\nNomothetic neuroimaging model weights are estimates of population-level associations between brain activity and pain outcomes (i.e., self-reported pain intensity). Models are crossvalidated via an iterative \"leave-N-subjects-out\" procedure to assess performance on out-of-sample participants [for recommendations see (38)]. Next, they are validated on held out \"validation sets\"; though this external validation process is not common in single neuroimaging studies due to the demand on sample size. More often, this validation process occurs over a series of papers across unique data sets collected on different scanners in varied locations [for a review see (2)]. This a slower validation process, but it is a more thorough and robust one. Once validated, the model's predictions are deemed suitable for application to a new individual drawn from the same population.\nA strength of this approach is its ability to identify separable component processes of pain (7). For example, the neurologic pain signature (NPS) is a well-validated model for acute pain evoked by noxious events (13). It captures a component process that contributes to the perceived intensity of an acute painful stimulus. It includes patterns of activity in the anterior cingulate, somatosensory cortex, and periaqueductal gray. Woo et al. (7) developed a separate multivariate predictive model of pain called the stimulus intensity independent pain signature-1 (SIIPS1). SIIPS1 captures fluctuations in pain independent of noxious stimulus intensity. It includes activity in the nucleus accumbens, lateral prefrontal cortex (PFC), and parahippocampal cortex. When combined with the NPS, the two explain more variance in brain activity than either model alone. However, the combined variance explained is 30%, indicating that there are more component processes relevant to evoked pain experiencing that have yet to be discovered (Figure 1C).\nThough the NPS and SIIPS1 can predict different aspects of acute pain experiencing, they cannot distinguish between chronic pain patients and controls. It is unclear whether models trained on evoked phasic pain are informative for the diagnosis of chronic pain. To distinguish between fibromyalgia patients and healthy controls, the NPS was subdivided into its positive activations and then combined with a multisensory model similar to SIIPS1 and a separate model trained to predict evoked pain in fibromyalgia patients (9). The combinatorial model FIGURE 1 | Three-level biopsychosocial approach to neuroimaging biomarker development. (A) Population-based (nomothetic) approach to diagnostic model development. Neuroimaging model weights are estimates of population-level associations between brain activity and pain outcomes (i.e., diagnostic category vs. healthy control). Population samples should be large and diversely sampled across gender identity, race, and socioeconomic identities. Models should be validated on external clinical data sets. Models can then be applied to the brain activity of a new patient to diagnosis their pain condition. (B) Person-based (idiographic) approach to treatment-relevant models. Neuroimaging model weights are estimates of person-level associations between brain activity and pain outcomes (i.e., pain severity) for the same person through time. Models weights can be regulated by nomothetic models to lessen demands on data collection from one patient. Models can be applied in the same patient at later time points to assess their disease progression or to assess treatment efficacy. Such models can be used to tailor treatment selection on a case-by-case basis. (C) Society-based (social epidemiologic) approach to prevention-relevant models. This approach requires two steps. First, participants complete a multidimensional survey that assesses both their environment (i.e., socioeconomic status) and their personal internalization of these conditions (Table 1). Then, a risk model is trained on these survey data to predict pain severity. The weights of this risk model are estimates of population-level associations between a person's socioeconomic conditions and pain outcomes. This model can be applied to the survey data of a new patient to assess their risk of pain chronification. Person-level survey data can be related to person-level pain-related brain activity, and then a neuroimaging model of the SES component of pain processing can be developed. Neuroimaging model weights are estimates of group-level associations between the socioeconomic conditions a person lives in and their pain-related brain activity. Such a model could be combined with other neuroimaging component process models of pain, such as the NPS and SIIPS1, to predict clinical outcomes in new patients.\nperformed with high accuracy within the study it was developed, however, it is unknown how it performs in external data sets.\nCombining models like this may be prone to overfitting, so the preregistration of model combinations is recommended.\nThe translational limitations of evoked phasic pain models may be due to the phasic, rather than the evoked, nature of the noxious stimuli. Recently, a tonic pain neuroimaging biomarker with clinical relevance was developed. This biomarker, called TOPS, was trained on evoked tonic pain trails in healthy controls (39). In this experiment, capsaicin was placed on the tongue to evoke pain for 1-2 min. TOPS can predict clinical pain severity and distinguish between patients and controls in two independent studies of chronic low back pain. It is possible that tonic stimulations hold greater clinical utility than phasic because longer stimulations allow for rumination and the activation of resting state networks that may play a role in the chronification of pain (22,40,41).\nTOPS was able to track within-individual variations in pain avoidance ratings with an average correlation of r = 0.51. Though this holds promise for the clinic, there is still much variance left to be explained. Pain is an idiosyncratic experience with many dimensions; therefore, the nomothetic approach may never be able to explain the entirely of an individual's pain experience, however, a \"good enough\" approximation might be achieved through the development of a suite of component process models that can be combined on a person-by-person basis. As we build more models of pain components, such as social context, interoception, affect, and expectations for pain relief, we may begin to chip away at this complex neural representation.\nTo this end, I make the following recommendations: First, a concerted effort must be made to recruit larger, more representative samples of the population. Nomothetic models are only suitable for application on new individuals drawn from the same population in which they were trained. The NPS was trained on only 20 participants, eight of which are women and 79% are White. Sampling procedures which primarily recruit from the student pool of the universities where the research is conducted unintentionally select for young high income and high education level White participants not of Hispanic origin (23). This is not representative of the world at large, nor is it representative of populations suffering from chronic pain. In the United States, most chronic pain patients are low-education and low-income women of color over the age of 45 (26,42,43).\nFunding agencies must provide sufficient support so that researchers can expand their recruitment, possibly by employing companies that specialize in representative sampling to stratify samples across age, gender identity, race, ethnicity, wealth and income, education level, and personality traits. Second, pain models and pain data sets should be made open and shareable to increase collective clinical impact. Patient data sets, especially those involving spontaneous pain paradigms, are difficult to collect, but are the most clinically-relevant. With increased data sharing, new pain components developed in easier to collect (i.e., evoked pain in healthy controls) diverse populations can be validated in clinically-relevant samples to improve translation and impact.\n---\nIDIOGRAPHIC (PERSON-BASED) APPROACH TO TREATMENT-RELEVANT MODELS\n\nPain is heterogeneous. The nomothetic assumption that \"onesize-fits-all\" ignores diversity in economic class, cultural background, gender identity, ethnicity, and personality, and limits applicability in real-world pain treatment. For example, emotional pain is positively correlated with physical pain at the group level, but this relationship is inconsistent across time within unique individuals (44). Indeed, neither SIIPS1 nor TOPS positively predicts pain in each individual the model was trained on; approximately 2-3% of the training data show effects in the opposite direction. It is possible that one's unique experiences with pain can influence the magnitude or direction of the relationship a pain component process has on their individual pain response. The idiographic approach accounts for variance across individuals by allowing for personalized predictions. Individual differences in pain expression have made it difficult for biomarkers to be developed on lower dimensional data like facial expressions, skin conductance responses, and heart rate, however, recent idiographic approaches to modeling these types of data have significantly improved their predictive power (45)(46)(47). In the clinic, such models may provide objective assessments of disease progression and treatment progress.\nIn the person-based approach, models are trained on many different samples from the same individual (Figure 1B). This commonly involves estimating pain-related brain activity from single trials within one experimental session. Predictive brain maps developed on one participant should be internally crossvalidated to test the model's ability to predict pain outcomes on out-of-sample trials from the same participant. While it might be useful to validate the model on later timepoints, current evidence suggests that there is stability in a single individual's network-level representation of the same stimulus through time (48).\nAdvantages of these models include improved accuracy and the ability to capture representations at finer spatial scales [e.g., (49)(50)(51)(52)(53)]. Because idiographic models require hours of data acquisition from a single participant, it can be difficult to collect from patients. One way to reduce the demands on scan time is to constrain the idiographic model with nomothetic priors. For example, Lindquist et al. ( 52) regularized an idiographic model of acute pain in healthy controls with the NPS. The regularized model performed better than both the NPS and a purely idiographic model trained on that subject's data alone. This method of regularization is known as group-regularized individual prediction (GRIP). It combines population-based and idiographic models in proportion to their variances. It does this by applying a shrinkage factor to the model weights. The shrinkage factor penalizes idiographic activity that appears unlikely (i.e., noise) relative to group activity.\nNon-regularized idiographic models are still likely to be useful if sufficient data are collected from the patient. The recommendation here is to compare the performance of regularized and non-regularized idiographic models within patients and select the best model on a patient-by-patient basis. This patient-tailored model can later be applied to their own brain activity in longitudinal follow-ups and intervention paradigms to track disease progress and treatment efficacy. It could also be deployed in real-time neurofeedback paradigms where participants can test multiple interventions and empirically validate which works best for them [see (54)]. Within this framework, a diversity of treatments (e.g., drugs, expectancy manipulations, placebo interventions, self-regulation, or mindfulness) can be tested with reduced bias.\n---\nSOCIAL EPIDEMIOLOGIC (SOCIETY-BASED) APPROACH TO PREVENTION-RELEVANT MODELS\n\nStudies of global chronic pain prevalence suggest that societal stressors may contribute to the chronification of pain (32,(55)(56)(57)(58). This is not surprising-the relationship between one's economic class and chronic illness has been observed as early as 1848, when Rudolph Virchow determined that treating the Typhus epidemic in Upper Silesia would require more than medicine. Virchow prescribed changes to the material conditions of the people whom the epidemic most severely impacted-the poor and working class (59). He concluded that though all illness has a biological origin, where it spreads and who is most susceptible is determined by structural factors such as housing, working conditions, diet, and sanitation (60). Similar observations have been made about chronic pain today. When controlling for age, race, and education level, a study conducted in an urban trauma center found that homelessness and low income were strongly associated with chronic pain (27).\nRelationships between low economic class and chronic pain prevalence have been found across the United States (26,61,62) as well as across different cultures and countries including South Africa (63), Brazil (31), Iran (64), Germany (65), Austria (56), Sweden (66), Finland (67), the United Kingdom (25,68), Japan (28), Nepal (33), and South Korea (69). Despite the long history and geographic spread of these associations, SES has largely been ignored by pain neuroimaging research. There are several reasons for this: First, there is little communication between epidemiologists and neuroimagers [an effort to correct this has begun, see (70)]. Second, the lack of socioeconomic diversity in research samples obfuscates this connection. Finally, it is difficult to mathematically relate complex social structures to functional brain activity. To the author's knowledge, only one neuroimaging study has done this to date (10). Here I propose to resolve this gap with a social epidemiologic approach to neuroimaging models of chronic pain.\nSocial epidemiologists study how socioeconomic structures, institutions (i.e., law, education), and social relationships influence health outcomes. A social epidemiologic approach to neuroimaging models of pain relates the structure of society to brain health and function. The primary goal of this approach is chronic pain prevention. The first step is to collect survey data assessing an individual's socioeconomic conditions and subjective experience of social status. This multidimensional assay can then be applied to pain-related brain activity to develop a neuroimaging model of socioeconomic contributions to chronic pain (Figure 1C). The resulting SES neuroimaging model may be a component process of pain useful for combinatorial models described earlier. This approach may allow us to identify patients most at risk for pain chronification because one's internalization of their socioeconomic conditions may play a role in the onset and maintenance of chronic pain (58,61,71).\nThe transition from acute to chronic pain is marked by a shift in processing from nociceptive components to socioemotional components of pain-specifically, PFC-limbic circuitry, including the NAc/striatum, amygdala, and hippocampus (72,73), and the default mode network [DMN; (41)]. Changes to PFC-limbic circuitry may indicate a change in the valuation of pain (11,74). Changes to DMN connectivity may change how the pain experience is construed in relation to the self (75,76). Both of these networks are altered by poverty and socioeconomic stress (77). Activity in the PFC (78,79) and ventral striatum (80) differs as a function of SES during both valuation and the processing of self-related information (81)(82)(83). Childhood poverty is correlated with aberrant functional connectivity within the DMN (84,85). Interestingly, these aberrations can be reversed in people who have high income later in life (86). Relatedly, (10) found a threshold in annual income (>$25,000) that delineated vulnerability from protection in chronic pain patients. In the United States, the poverty line for a family of four is $26,200; meaning families that make less than this cannot afford food, rent, and other basic needs (87). It is unknown whether changes in income can reverse chronic pain status, however, chronic pain patients of high SES tend to have better clinical outcomes (88).\nThe impact of socioeconomic stress on chronic pain may not be reducible to income alone. The experience of social strain or subordination itself may contribute to chronic illness above and beyond income-level (89,90). In non-human primates low social status is associated with immune system deficits that increase risk of infection and slow wound healing (91,92). Chronic social stress may underlie immunosuppression in humans and animals [for a review see (93)]. People in lower social classes have a lower sense of personal control which is associated with higher levels of stress and pain (94). However, a high sense of self-efficacy is protective against chronic pain and pain severity (95). The protective effect of self-efficacy may be independent of class. For example, a large study in South Korea (N = 28,532) demonstrated that when controlling for monthly income, the presence of labor unions reduced low back pain prevalence (69). Another study in the United States found that unionized workers experience less severe pain for work-related musculoskeletal disorders (96). One interpretation of these effects is that labor unions change perceptions of selfefficacy, pain controllability, and expectations for care and safety by giving worker's the ability to advocate for themselves through collective bargaining (97).\nA major barrier to the study of socioeconomic factors in chronic pain is the lack of a standardized assessment of  A multidimensional assay of socioeconomic conditions, their internalization, and pain-related appraisals and personality traits.\nSES. Here I propose the creation of a \"Pain-Predispositions Profile Survey\" (Table 1), a multidimensional assay of debt, income, property ownership, investments/savings, family wealth, education, perceived social status, environment (urban or rural), housing situation, childhood attachment, SESrelated personality/evaluative traits (i.e., pain catastrophizing, controllability perceptions), as well as measures of income inequality within the city and country the patient resides in. A predisposition model of chronic pain can then be developed on these survey data that predicts patient pain status or severity. A cross-validated procedure similar to that employed by Vachon-Presseau et al. (10) can then be used to relate the survey data to functional networks in chronic pain patients (or healthy participants in evoked pain paradigms) to uncover a socioeconomic-related component process contributing to the pain experience (Figure 1C). Neuroimaging may not always be an available tool for the diagnosis and treatment of chronic pain-the survey-based model, however, is scalable and can be leveraged for treatment selection by matching people on survey similarity. Treatment programs that are validated on patients in neuroimaging studies can then be recommended to new patients with greater confidence.\n---\nDISCUSSION\n\nAn individual's valuation of a painful event (113)(114)(115), their expectations for support and health care (116)(117)(118), their beliefs about pain permanence (119,120), personality traits (10,121), and the socioeconomic conditions they exist in (10,122) influence their brains' representation of pain. Pain, therefore, is a personal experience instantiated by biological processes and situated within one's socioeconomic conditions. Neuroimaging models situated within the socioeconomic structures of the population being studied are necessary for the development of a more complete understanding of the complexities of human pain. In this perspective, I discuss how three approaches to the development of pain neuroimaging models-nomothetic (population-based), idiographic (personbased), and social epidemiologic (society-based)-can be applied to the diagnosis, treatment, and prevention of chronic pain. These three approaches taken together serve to operationalize the biopsychosocial model of pain within a neuroimaging context. It is estimated that 1% of the world's population controlled 44.8% of the world's wealth in 2018 (123). Economists from varied and opposing points on the political spectrum agree that an increasingly globalized and automated economy will heighten existing barriers to economic mobility and make income inequality more stark, widespread, and permanent (124). Therefore, it is my final recommendation that scientists and clinicians advocate for chronic pain patients at the level of public policy. In the words of Virchow, \"Disease is only a manifestation of life under pathological conditions. . . Medicine is a social science and politics is nothing else but medicine on a large scale.\"\n---\nDATA AVAILABILITY STATEMENT\n\nThe original contributions presented in the study are included in the article/supplementary material, further inquiries can be directed to the corresponding author/s.\n---\nAUTHOR CONTRIBUTIONS\n\nThe author confirms being the sole contributor of this work and has approved it for publication.\n---\nFUNDING\n\nNIMH (R01 MH112560), Computational and brain predictors of emotion cue integration (PI: Zaki).\n---\nConflict of Interest:\n\nThe author declares that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.\nPublisher's Note: All claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers. Any product that may be evaluated in this article, or claim that may be made by its manufacturer, is not guaranteed or endorsed by the publisher.",
        "Introduction\n\nEthiopia has made great strides in improving health and nutrition outcomes in recent years with a nearly 20-percentage-point decrease in stunting among 0-5 year olds from 2000 to 2016 [1]. However, the overall situation of child undernutrition in the country requires further reduction, with 38% of children under-5 stunted, 10% wasted and 24% underweight in 2016 [1]. Drought-related food emergencies have also highlighted the tenuous state of nutrition and food-security for vulnerable communities [2,3].\nInfant and young child feeding (IYCF) is closely linked with child nutritional outcomes and child development generally [4][5][6][7], and evidence-based child-feeding practices require scaling up throughout Ethiopia. In 2016, only 58% of children under-6 months were exclusively breastfed [1]. The percentage of children receiving a diet at the recommended level of diversity, defined as foods from four or more distinct groups in the previous 24 hours [8], remains a challenge for most families in Ethiopia, with only 14% children meeting this standard in 2016 [1].\nReligious adherence is an important factor in health and nutrition. Qualitative research reveals that the health dictates of strict faiths are often negotiated [9]. Level of religiosity is also an important factor in how religious doctrine interacts with health or diet-related practices [10], wherein the study of strict adherents facilitates understanding of how doctrine may directly impact health outcomes [11].\nThe two most common religions in Ethiopia are the Ethiopian Orthodox Tewahedo Church (EOTC) and Islam with 43.5% and 33.9% adherents respectively in the population [12]. Both of these religions include fasting at prescribed times during the yearly calendar of religious events and holidays, and restrictions on food, as well as taboos related to consumption of animal source foods [13,14]. However, the fasting traditions of these two religious groups are markedly different.\nFor adult members of the EOTC, there are approximately 250 fasting days in the year although not all are compulsory for everyone, with the average person fasting for around 180 days per year [14]. Most Wednesdays and Fridays are fasting days in addition to several periods of continuous fasting such as 55 days for Lent and a 43-day fast of the prophets. On fasting days in the EOTC all food and drink is abstained from until noon at the earliest when a small meal may be taken. Meat and animal products, including milk and eggs, are avoided entirely during fasting times in the EOTC. Church doctrine exempts children under-seven and pregnant women from fasting. There is extremely scant information related to EOTC fasting in relation to health, though some information is available on Eastern or Greek Orthodox fasting [15], which has similar tenets, though not the same schedule and fewer days of dietary or caloric restriction than EOTC fasting traditions.\nBy contrast, fasting in the Muslim tradition comprises one period throughout the year, the month long fast of Ramadan, when no food or drink is taken from sunrise to sunset [16]. A major distinction from that of EOTC fasting is that meats and animal source foods are allowed during the post-sunset evening meal of Iftar, which is meant to be substantial, festive, and shared with family and friends [17]. Exemption of children from fasting during Ramadan is the norm, but advice for pregnant and breastfeeding women is less clear and leaves the decision to the woman with the option to fast at a later time as an acceptable way to make up missed fasting [16]. Ramadan fasting is prohibited for menstruating women and translates to a prohibition for recently delivered women who are experiencing postpartum bleeding. Detailed material available through S1 File provides additional information on specific fasting doctrines. Personal level of religiosity and individual adherence to doctrine is an important factor which can influence actual practice of fasting [13].\nPositive health-related markers have been found in relation to certain types of fasting among adults [18][19][20][21][22]. Fasting practices of adults who are parents may influence the amount and type of complementary foods that children are offered, particularly as a child transitions to family foods. The impact of fasting on health is not well described in the public health literature from low income countries in relation to child growth and anthropometry, though it has been studied in relation to many other topics related to adult health [15,[23][24][25][26][27][28][29][30][31].\nLittle research has focused on the impact of fasting on the diets of children in households where adults are fasting. The practice may influence breastfeeding, impacting the practice of exclusive breastfeeding of babies younger than 6 months or potentially altering the quantity or constituents of breastmilk received. Evidence on the impact of fasting on breastmilk composition and production is mixed [32,33], and impacts will vary by fasting practices such as duration and types of dietary and caloric restriction.\nIf fasting requires dietary restriction of animal source foods, as in EOTC religious fasting, these foods may not be available in the household or the local community supply may decrease during periods of extended fasting (such as the 40+ day fasts), and therefore may not be available for child consumption. Animal source foods are an important source of macro-and micronutrients for children in low-income settings [34][35][36] and promoting provision of ironrich animal source foods has been linked with improved dietary diversity and other important indicators of appropriate feeding [37].\nThe goal of this research was to explore fasting from the perspectives of mothers and other community members, and to utilize their descriptions of practices (and individual or social influences on those) from the study regions to understand and improve nutrition counseling and services. Given the complex nature of nutrition and fasting behaviors in relation to religious and sociocultural context, qualitative methods were chosen as the most appropriate to explore how participants perceive and understand these practices as part of their day to day lives [38].\n---\nMaterials and methods\n\n\n---\nSampling\n\nA multi-method study of infant and young child feeding practices (IYCF) took place in selected four regions of Ethiopia from October through December 2015. The regions included Afar, Amhara, Benishangul-Gumuz, and Tigray. Geographic remoteness and resource limitations necessitated the survey include 2 zones per region. The zones were purposively selected based on IYCF factors to capture a range of experiences within region. The selected zones included Zones 1 and 4 in Afar, Eastern and North Western Zones in Tigray, South Wollo and West Gojjam in Amhara, and Assosa and Metekel in Benishangul-Gumuz. Within the selected zones, a household survey, cluster survey, food market survey, in-depth-interviews (IDI) and focus group discussions (FGD) were used to gather data on children less than 36 months of age and their mothers or caretakers living in rural areas. This paper utilizes results from the qualitative data collected during IDI and FGD.\n---\nData collection methods and tools\n\nQualitative data was collected from each cluster in the form of semi-structured IDIs and FGDs with health extension workers (HEWs) and caregivers. Interviews and focus group discussions were selected on the basis of being both the most appropriate for gathering qualitative data and the most feasible to undertake given the geographic distribution and distance [39]. The IDIs were conducted in person in the homes of the interviewed caregiver to facilitate opportunistic observations to corroborate findings. FGDs took place at common community meeting places. Having a child age 6-36 months living in the household was the inclusion criteria for caregiver participants. Participants were asked to refer to the youngest child when responding to study questions if multiple children met this age range. Caregivers and HEWs were selected for IDI and FGD participation based on maximum variation purposive sampling [40] in order to identify a range of participants to represent the array of experiences, characteristics, and perceptions that contribute to the behaviors under investigation. Characteristics for investigation included religion, parity, age, and livelihood. Identification of interview and FGD participants was aided by community leaders and could include those who also completed the quantitative survey. All caregiver IDI participants and most FGD participants were female. Some FGDs were conducted with male caregivers but these were not conducted in mixed company with female caregivers.\nSemi-structured interview guides and FGD topic guides were developed in English, translated to Amharic, back translated to English, and pre-tested. Interviewers and FGD leaders all had previous experience conducting qualitative research and participated in a four-day training session prior to data collection. The training session covered protocol, interview and FGD techniques, and use of the guides. Conversations from FGDs and IDIs were audio recorded in the field and then transcribed by the interviewers and FGD leaders. The IDIs and FGDs from Benishangul-Gumuz and Amhara, and the FGDs in Tigray were transcribed verbatim.\nThe research team obtained approval from the Tulane University Health Sciences Institutional Review Board for secondary data analysis of data collected previously by a local nongovernmental organization (NGO). The qualitative research staff of the NGO ensured that informed consent was obtained from all participants prior to data collection. Before commencing IDIs and FGDs, the interviewers and FGD leaders introduced themselves and indicated the study purpose. The interviewers/FGD leaders then read aloud an informed consent document. Interviewees were asked to sign the informed consent document prior to beginning data collection and verbal agreement was requested at FGDs. All data was de-identified upon importation for analysis to ensure confidentiality.\nThe data derive from 16 FGDs and 40 IDIs with caregivers and 32 IDIs with HEWs. These were performed across the eight zones sampled with 5 caregiver IDIs, 4 HEW IDIs, and 2 FGDs taking place in each zone by Ethiopian qualitative researchers who were fluent in local languages.\nA naturalistic approach [41] to interpretive analysis guided this qualitative descriptive [42] exploration of data. Initially, a sample of transcripts was coded to develop a preliminary coding scheme for discussion, and this coding scheme was revised iteratively over the course of weekly discussions and memo-making by the analytic team of three experienced, female qualitative researchers. Content analysis was employed to understand experiences and behaviors related to optimal infant and young child feeding in the surveyed zones, and the sociocultural factors underlying these. Themes were derived from the data and not identified prior to analysis. NVivo software (Version 11) was used for analysis of the qualitative data. The COREQ guidance was utilized during to guide the presentation of this research [43].\n---\nResults and discussion\n\n\n---\nDescription of participants\n\nFemale caregiver IDI participants were on average 29 years of age with fewer than two years of formal education. Half were Muslim, 48% were Orthodox and the remaining few were protestant. This ratio was similar among FGD participants. The mean age of the youngest child in the household of caregiver IDI participants was 18 months and this was slightly younger (17 months) among FGD participants. Of 127 FGD participants, 15% were male caregivers (from two FGDs conducted in Tigray). The majority of FGD participants were farmers (74%), while 21% were housewives. The 32 HEW IDI participants were 25 years old on average, all but two were female, 53% were Orthodox and 44% were Muslim. Additional descriptive characteristics of IDI and FGD participants are given in Table 1 below.\nThroughout the data, participants expressed varied understanding of fasting requirements for different life stages and for each religion. \n---\nFasting for pregnant and/or lactating women (PLW)\n\nSocial norms and views on fasting. Orthodox and Muslim participants alike reported that all adults are expected to fast during fasting times and that it is the norm for pregnant and breastfeeding women to fast, except for potentially a short period immediately postpartum.\nMuslim participants described that pregnant women should fast and breastfeeding women are exempt from fasting for ten to forty days after birth. A few participants indicated that fasting might impact perinatal health.\n---\nIf pregnant mother try to fast like this, it harms (FGD, Muslim Mother, age 19, Benishangul-Gumuz).\n\nFor pregnant woman, when she is fasting, amount of blood decreases. The whole day she do not take water. Also, she do not take foods. When she take food/water after this delay, the pregnancy affected (FGD, Muslim Mother, age 25, Benishangul-Gumuz).\nOne participant contrasted with others on the requirements to fast, because of this issue:\nAccording to our religion, pregnant and lactating mothers are not expected to fast. Because during this time mothers are not only accountable for themselves but also for their children (FGD, Muslim Mother, age 37, Afar).\nHowever, participants described that they felt an obligation to follow the fasting traditions of their religion, as another mother recalled.\nIn this locality, mothers are expected to fast after 40 days of child birth. We give priority for the obligations and commitments expected from our religion. It is normal to see women who are fasting when pregnant (FGD, Muslim Mother, age 30, Afar). Specific fasting practices. Orthodox participants also indicated pregnant and breastfeeding women are expected to fast except for approximately 7-12 days postpartum, when mothers are allowed to eat non-fasting (animal source) foods. Some participants related the exemption from fasting immediately following birth to postpartum bleeding, similar to the fasting exemption for menstruating women involved with perceived impurity.\nDuring Ramadan, pregnant women are expected to fast during fasting days. Because they must fast and pray until they get birth. But they will be exempted from fasting during birth and 40 days right after birth. This is because according to our religion delivered women can't be clean until 40 days since they flow a lot of blood (FGD, Muslim Mother, age unknown, Amhara).\n---\nDuring Ramadan lactating women should fast during fasting days, because according to our religion women should be exempted if and only when they flow blood-during their menstrual cycle and 40 days right after their birth (FGD, Muslim Mother, age unknown, Amhara).\n\n\n---\nIf the women have bleeding (like menstruation), their fasting is not accepted (FGD, Muslim Mother, age unknown, Amhara).\n\nIt is allowed for women to eat non-fasting food only for 40 days after they give birth until they will not have bleeding (FGD, Muslim Mother, age unknown, Amhara).\nOne mother from Afar mentioned that the post-partum exemption from fasting was related to physical recovery.\nA newly delivered mother and sick mothers are not expected to fast but any pregnant and lactating mother should fast. Immediately after delivery the mother is thought to be weak and depleted so that she will be exempted from fasting (FGD, Muslim Mother, age 35, Afar).\nOne mention was made on a modification to fasting for pregnant or breastfeeding mothers by not fasting the entire time during the day but abstaining from non-fasting foods.\n---\nThough it is different among [other] lactating mothers, I eat fasting foods at times of fasting and never wait until 3:00PM. How I fast is abstaining from animal source foods (IDI, Orthodox Mother, age 32, Tigray).\n\nSome mothers reported not fasting at all while breastfeeding infants. This participant from Afar explained that she did not fast for entire fasting periods while her child was young.\n---\nI've had experience of breastfeeding my youngest baby during fasting. In most cases, I don't fast the whole fasting periods allotted for a given year until the baby's age turns two years. Since I'm expected to deliver enough amount of breastmilk for my child (IDI, Muslim Mother, age 27, Afar).\n\nSeveral participants mentioned the need to compensate for missed fasting days by fasting at a later time.\nPregnant and lactating women are expected to fast but if they can't do that nothing will happen to them except priests will give them directions to fast on another time so as to be cleansed from sin (FGD, Orthodox Fathers, Tigray).\nNo problem if she eats, she will compensate it some other time and no stigma or rejection happens to her (FGD, Muslim Mother, age 28, Afar).\nA mother who delivered recently does not fast and she will compensate and fast on another time (FGD, Muslim Mother, age 17, Afar).\n. . . there is no punishments for not fasting all seasons of fasting. But they are expected to replace (i.e. if she missed five regular fasting days she has to replace the missed number of days another time) when it is appropriate (when they are not breast feeding) for them (FGD, Muslim Mother, age 27, Afar).\nThis participant explained the different options for fasting, indicating pregnant and breastfeeding mothers in her community fast specifically by abstaining from animal source foods.\nThere are two types of fasting. Fasting that staying without any foods and drinking till mid afternoon and eating enjera and fasting that avoids eating animal products. Here, lactating/ pregnant mothers not use animal products and eat enjera. They do not use milk and meat (FGD, Orthodox Mother, age unknown, Benishalgul-Gumuz).\nRationale for fasting of PLW. Mothers of both religions reported perception of negative consequences if a person fails to fast. These were often related to individual or community understandings of religious doctrine.\nIf someone from the family members fails to fast in fasting day, bad things will be happened to them. They will be punished by 'akera' [punishment after death] (IDI, Muslim Mother, age 27, Afar).\nIn our religion, an adult person who fails to fast on a fasting day without any problem and illness will be asked by Allah and he will encounter a bad thing on earth (IDI, Muslim, Mother age 25, Benishangul-Gumuz).\n---\nNo, we do not eat on fasting day. We respect our religion. As our soul father told us from the books [holy book and other books used in the church], person who fail from fasting would face many problems. It is written in the books (FGD, Orthodox Mother, age unknown, Amhara).\n\n\n---\nThere is no question someone [must] fast but it is believed that if she declines to fast, she commits sin and becomes sinful (FGD, Orthodox Mother, age 29, Tigray).\n\nOne father also indicated spiritual rationale as the most important:\nPregnant and lactating women also were expected to fast. . . . Nothing would not happen to this people apart from they should fast for their souls and spiritual benefits (FGD, Orthodox Father, age 38, Tigray).\nOthers mentioned possible stigmatization within the community and negative social consequences from family and neighbors for not fasting.\n---\nIf a pregnant/lactating woman does not fast, she would face shame from the community. She could not be able to go to church (FGD, Orthodox Mother, age unknown, Amhara).\n\nIf a pregnant and lactating woman is not fasting during fasting days, they may face some social problems from their families or neighbors (FGD, Orthodox Mother, age unknown, Amhara).\nSince we all follow the same religion here, someone cannot deviate from what the religion requires. .\n---\n. anyone not doing what is required won't be accepted by the community (FGD, Orthodox Father, age 35, Tigray).\n\nParticipants also mentioned religious leaders as an influence on the rationale for fasting.\nIf lactating or pregnant mother does not fast, the religious leaders pressure them and ask them to fast. . . (FGD, Muslim Mother, age 29, Afar).\n---\nIf [PLW] don't fast. . . it is condemned by religious fathers. So it is impossible (FGD, Orthodox Mother, age 21, Benishangul-Gumuz).\n\nIf we break fasting we regret and must get blessing from the spiritual fathers. Then we will be made to fast another time for the missed fasting (FGD, Orthodox Mother, age 22, Tigray).\n---\nIf they break fasting, [PLW] will report to their spiritual fathers and will be ordered to compensate after getting repentance or regret (FGD, Orthodox Father, age 43, Tigray).\n\nSome mothers noted that fasting is related to their personal relationship with the divine.\n---\nIf pregnant and lactating mothers are not fasting during 'Ramadan', there are no consequences of this. People are fasting for the sake of making peace with Allah. Allah will pay back to their activities either in this world or in the eternal life (FGD, Muslim Mother age 35, Afar).\n\nPregnant or lactating women would not face any problem from the society if she does not fast. Nobody from the society would ignore/stigma her. Only God would ask her (FGD, Muslim Mother, age unknown, Amhara).\n---\nOne participant emphasized personal choice:\n\n\n---\nIf they are not fasting, nothing will happen by the community. But they may be told to fast. But if the lactating women don't want to fast, the decision will up to her (FGD, Muslim Mother, age unknown, Amhara).\n\nPerceptions of fasting in relation to lactation and breastfeeding. All participants agreed that infants and children were exempt from fasting in order to breastfeed during fasting days (as they normally would during non-fasting times).\nIn our culture a newly delivered woman can be exempted from fasting only for ten days of after birth. After ten days she must fast. But she can breast feed her child (FGD, Orthodox Mother, age unknown, Amhara).\n---\nIn our culture pregnant and lactating women should fast on fasting days. They must fast at least until mid-day [instead of up to 3 pm]. But she must feed her child even if she is fasting (FGD, Orthodox Mother, age unknown, Amhara).\n\nHowever, some reported difficulties breastfeeding while fasting. One participant described it this way:\nThe child will breast feed during fast. 'Anaaf cimaadha'means breast feeding during fasting is difficult for me. How it is difficult is, I am fasting, [the baby] is breastfeeding. But I do not eat and drink all day. It is difficult. Difficult. Breast does not produce 'hin mirgisuuf' (IDI, Muslim Mother, age 34, Benishangul-Gumuz).\n---\nOther participants described difficulties:\n\nThe baby is allowed to breastfeed during fasting days. But during the day time [mothers] are not eating, even not allowed to drink water but breastfeeding, and this is a difficult situation and [mothers] suffer a lot (IDI, Muslim Mother, age 26, Afar).\n---\nWe can breastfeed baby during fasting period but it affects health since we could not eat the whole day but feed breast to the baby (IDI, Muslim Mother, age 25, Afar).\n\nSome participants explicitly described not having enough breast milk while fasting.\n---\nDuring fasting days, I was breastfeeding my child. And it was one of my responsibilities and duties no matter how I felt hungry and uncomfortable. . . [Baby] was eating food items as much as she can. . . Because at that time I might not have enough breast milk to feed her since I was fasting (IDI, Muslim Mother, age 40, Amhara).\n\n\n---\nIt is so difficult to breastfeed while fasting for the reason that breast won't produce enough milk and as a result [mother] feels pain as baby suckles and the baby cries a lot since he cannot get enough (IDI, Orthodox Mother, age 32, Tigray).\n\nOn fasting and non-fasting days. . . the amount of breast milk become less in fasting times since [mothers] fast. We add more [solid] foods on fasting days to compensate that (IDI, Orthodox Mother, age 28, Tigray).\n---\nComplementary feeding and fasting\n\nParticipants also explained that it is necessary to give additional supplements to babies who receive complementary food while fasting, due to perceived decrease in milk production.\nDuring fasting days we can breast feed our children . . . however, our breasts may not produce enough milk since we don't feed well. So in order to supplement our children with food, we should prepare them some food items such as soup, macaroni, and pasta (FGD, Muslim Mother, age unknown, Amhara, South Wollo).\n---\nThe amount of breast milk may be reduced since [mother] fasts, but [they] can eat fasting foods like lettuce (IDI, Orthodox HEW, age 23, Tigray).\n\nReligious doctrine for children. Participants generally reported that young children are excused from fasting rules, as exemplified by this mother from Afar:\n---\nThere is no problem of feeding a baby food like milk, cheese, butter, eggs, meat, during fasting time. No restriction on food type. (IDI, Muslim Mother, age 25, Afar).\n\nThe age when children are expected to begin fasting reportedly differed between the two religious groups, but participants unanimously stated that children less than 5 are excused. Most Muslim mothers noted 15 was about the age children begin fasting, though some mentioned age 8, and the Orthodox participants stated 7 to 10 years was the age children begin fasting.\nWe are all Muslims and in our religion a child starts fasting when the child is between twelve and fifteen years depending on the parent's preference and decision. No fasting before this age of the child (FGD, Muslim Mother, age 28,Afar) According to the culture of Muslims children less than 15 years will be exempted from fasting since they are unable to fast the whole day as the adults. There are no obligations related to the religion which force them to fast (FGD, Muslim Mother, age 30, Afar).\n---\nIn our religion, Muslims must begin to fast when completed 7-years age and begin 8 years of age. Yes, [younger] children are exempt from fasting. Children less than eight years are exempted from fasting (FGD, Muslim Mother, age unknown, Amhara).\n\n\n---\nChildren are exempted from fasting during fasting days. They are not expected to fast until they get ten years old. They can even eat all non-fasting food items during fasting days (FGD, Muslim Mother, age unknown, Amhara).\n\nUnintended consequences for children. Participants discussed the issue that if a mother herself is fasting, she would potentially refrain from preparing non-fasting (animal source) foods for her children, even if she believed they were not prohibited for the child, due to concern over contamination of the family's utensils and dishes for those who are fasting.\n---\nI am afraid that [cooking] utensils/materials may touch each other. I don't give [child] butter, meat. I don't eat it, so how do I give her? It is difficult. How do I prepare for them separately?\n\nThe spoon, dish may touch each other for us. So [we are] frightened for contaminating the materials (IDI, Orthodox Mother, age 28, Benishangul-Gumuz).\nOne Health Extension Worker (HEW) from Amhara explained the concern mothers in her community have about preparing non-fasting foods.\n---\nNo, mostly mothers do not feed the same thing on fasting and non-fasting days. They have concern of contamination of their fasting food with the children non-fasting food. Mostly they prefer to give [animal] milk if they do have milk at home in the fasting days (IDI, Orthodox, HEW, age 28, Amhara).\n\nAnother HEW mentioned that mothers have asked them about how they should feed their children during fasting days, indicating it is a concern for some mothers.\n---\nSome mothers ask us how to feed their children during fasting days. Because they don't want to mix non fasting foods with that of fasting ones. Whenever mothers ask us such kind of question, we advise them to feed their children from separate pots to avoid confusion and fear of mixing non fasting foods with that of fasting ones (IDI, Orthodox, HEW, age 27, Amhara).\n\nSome Orthodox mothers may feel comfortable preparing non-fasting foods for their children by using separate bowls and cooking utensils. This HEW mentioned these mothers share their methods with other mothers.\nActually, some Christian mothers had the concern of contamination of fasting food with nonfasting food. But other Christians said we have separate cooking utensil for fasting and nonfasting food. They shared this experience for those mothers who had the concern of contamination (IDI, Muslim, HEW, age 24, Amhara).\nOne participant from Tigray mentioned mothers have concerns over preparing non-fasting foods for children on fasting days since they cannot taste the foods.\n---\nThe mother's concern was how can she taste (for salt or flavor) a non-fasting/animal product while she is fasting and cooking or [worried for] the feeding tool to be mixed (IDI, Orthodox, HEW, age 23, Tigray).\n\nThe concept that women may not be allowed to prepare non-fasting foods for their children may come from others within the religious community. An Orthodox HEW from Tigray mentioned that these perceptions may be changing.\nPeople from the religious community (church) were say that a fasting person shouldn't prepare a non-fasting food for anyone else. But this is changing and these church people are now teaching it is possible to wash hands with soap after preparing non fasting food (IDI, Orthodox, HEW, age 28, Tigray).\n---\nAvailability of animal source foods/nutrient dense foods during fasting\n\nAccording to participants, meat was reported to be scarcer and more expensive during fasting times, particularly during long periods of fasting like Lent in Orthodox communities, due to the lack of butchers willing to slaughter animals (fasting adults would be unlikely to do this). This decreased availability also increased the price of available meat. In Fig 1 below, a photo illustrates a butcher shop staffed by a woman in the study area (permission to be photographed was given, and the person in the photo was not a study participant) which is selling meat during non-fasting days.\nSeveral participants confirmed the lack of availability of meat during fasting times:\nIn the long fasting periods, meat is not available and he (youngest baby) does not eat meat (IDI, OrthodoxMother age 35, Amhara).\nExpenditure will be higher during non-fasting days because animal source foods are expensive (IDI, Orthodox, Mother, age 25, Tigray).\nNot all mothers give all the recommended food items especially meat due to shortages to fulfill that for children and problems of perception slaughtering/ buying meat exclusively for children in fasting seasons (IDI, Orthodox HEW age 28, Tigray).\nChurch teachings don't prohibit giving animal source foods (ASF) to children, except for meat, that meat will not be prepared for children during fasting [due to availability] (IDI, Orthodox, HEW, age 20, Tigray).\nHowever, participants noted that eggs and milk may be more readily available ASF during fasting times in Orthodox communities. \n---\nIn the Christian families [they] can get especially egg and milk otherwise it is difficult to get meat because they do not slaughter animals during the fasting periods (IDI, Muslim, HEW, age 28, Amhara).\n\nThere is no difference for children [diet during fasting] especially feeding milk and eggs remain allowed. But meat is not easily available in fasting days and is less probable to be fed to children. (IDI, Orthodox HEW age 28, Tigray) For Muslims, meat and animal source foods are allowed during the month of Ramadan and frequently eaten at the meal at the end of the fasting day. An HEW from Afar explained that diets may actually increase in variety during the month of Ramadan due to the celebratory nature of the evening meals.\nIn Muslims religion the diet and variety during fasting times is better than the other days. Children under two years are not supposed to fast in the first place and during these fasting time more meat, soup, and other animal source foods will be available at home and the children will be fed during the fast times. The religious teaching has no influence on children feeding practice and no fasting for children under fifteen years. (IDI, Muslim HEW age 20, Afar)\nA Muslim mother indicates this phenomenon as well, by saying her food costs increase during Ramadan due to buying more things to eat than normal.\nDuring Ramadan our expense becomes higher than the normal circumstance. Because during which, we buy a lot of things to eat. (IDI, Muslim Mother age 40, Amhara)\nOne HEW from Amhara noted the restrictive fasting diets of adults may benefit children's diets due to lack of competition for meat in the household, and the decreased demand for meat in the community during fasting times.\n---\nIn fasting days, children of Christian family members may benefit a lot due to the fact that no one will [need to] share with children if they eat meat during fasting days. Animal source of food is available adequately because [it]can't be used by adult during fasting days-especially in the Christian family members. (IDI, Muslim HEW age 29, Amhara)\n\nThe data analysis identified diverse patterns of feeding for infants, young children and pregnant women. In addition, the participants expressed divergent understandings of sociocultural and theological norms around fasting rules for these groups. There are also distinctions between fasting traditions between the two main religious groups. In relation to EOTC fasting, one study is available on the impact on child nutrition, in which fasting was observed to be associated with decreased dietary diversity in one region [44]. However, there is little information in the peer-reviewed literature related to how fasting practices impact the diets of pregnant and lactating women or young children.\nThe effect of fasting on breastfeeding mothers is not fully understood. Some evidence of metabolic changes including increased metabolic stress in fasting breastfeeding mothers may place them at risk of fasting hypoglycemia [45,46], potentially making it difficult for mothers to carry out daily activities including caring for and continuing to breastfeed their children. However, there is no direct evidence from human studies indicating that intermittent fasting results in decreased breast milk production. Observational studies on humans have given mixed results regarding changes in breastmilk composition from religious fasting of mothers [32,47,48]. Some animal studies have shown a potential decrease in milk production associated with decreased maternal energy intake, though there may be a minimum threshold at which this effect occurs [33]. More research is needed to understand the impact on milk production and composition of frequent intermittent fasting, which is the predominant pattern among the largest religious group in Ethiopia [14].\nOur results indicate that some mothers felt their milk supply decreased during fasting. Mothers' confidence about milk supply is closely associated with breastfeeding practices, so it is important that this does not negatively impact exclusive breastfeeding. Potentially using social and behavioral communication to support community members understanding of the mechanics of breastmilk production, especially the changes in volume and constitution to specifically meet the child's unique need, could be important. Lactating women could also be encouraged to abstain from fasting, which is acceptable according to religious doctrine. In 2016, the EOTC developed a nutrition sermon guide in collaboration with USAID/ENGINE (Empowering New Generations to Improve Nutrition and Economic opportunities) which has been endorsed by the patriarch of the EOTC [49]. It encourages pregnant and lactating women and children under-seven years of age to eat nutritious foods, including animal source foods, during its official fasting periods. This is encouraging but its use and implementation of the actions it advises requires engagement with everyone in the community, especially religious leaders.\nFasting may inadvertently impact dietary diversity of young children, as some caregivers indicated they are not able to prepare ASFs for children on fasting days due to fear of contaminating family foods. A recent cross-sectional study found Ethiopian children of Orthodox Christian households during a fasting season whose mothers did not feed ASF to their child due to this fear, were 1.5 times less likely to have met the dietary diversity recommendation as compared to those who did not feed ASF for economic reasons [44]. In secondary analysis of data from the 2005 and 2011 Ethiopian DHS, Alive and Thrive found children from Orthodox families to be less likely to have consumed ASFs and meet the dietary diversity recommendations compared to children from other religions [50]. Furthermore, certain foods like meats, which are not allowed during fasting, become more expensive and more difficult to acquire during fasting times thereby limiting the ability to provide them to young children despite their exemption from fasting. One study found decreased calcium and vitamin B 2 , and decreased protein intakes in 6 to 36 month olds during an extended fasting time (Lent) compared to their intakes during non-fasting times [51].\nThere is limited research on the direct impact of fasting on nutritional markers in Ethiopian Orthodox children, and it may not necessarily exhibit detrimental impact. There is some evidence of improvement in health indicators of religiously fasting adults, mediated by intermittent vegetarianism or reduced consumption throughout the year [18,52]. Rather than focusing on potential harms from religious fasting, this research highlights the importance of pinpointing the unique aspects of fasting for different groups that influence the diets of children.\nWhile Ethiopia as a nation has made important population-level health and nutrition gains over the last decade, there may be social factors that could result in unaddressed disparities among vulnerable groups [53,54]. Given the findings from this study, health and nutrition programming may helpfully be explored that addresses fasting practices in a supportive way for varied populations. Padela and colleagues present a framework for tailoring behavior change to religious affiliation [55]. Such an approach may be used in Ethiopia and other settings where religion is an important factor in social and behavioral context for health and nutrition. Table 3 below provides a summary of areas where formative research on nutrition and health interventions would be beneficial, based on the results of the study.\n---\nLimitations\n\nAn important limitation of this paper is that participants were not asked to provide information on their personal level of practice of religion or strictness of adherence to the doctrines which may provide greater insight [56]; given the varied responses from participants, it is difficult to ascertain whether feeding behaviors are associated with adherence to religious guidelines and religiosity of individuals, or localized cultural or traditional practices. The qualitative portions of the study included caretakers of children ages 6 to 36 months, excluding children at younger ages. Although only participants with young children (under 36 months) were included, recall bias may result in inaccuracies in descriptions of past experiences. It is not expected that this would be differential by important participant characteristics. The study was conducted in purposively selected zones within 4 regions of the country, and practices may vary by region and zone based on factors that were not captured by this qualitative study.\n---\nConclusion\n\nFasting is an important practice for adherents to the two most common religions in Ethiopia and deeply rooted in sociocultural norms around feeding behaviors. Considering this alongside participants' understandings and experiences may allow for useful behavior change strategies to be developed through formative research or human centered design to address potential barriers to recommended feeding patterns for pregnant and lactating women and young children. \n---\n\n\nAll data used for this study, which are strictly qualitative in nature, are within the manuscript.\n---\nSupporting information\n\n",
        "Introduction\n\nData is an increasingly contested term and concept in qualitative research. Many recent contributions argue that data can no longer be treated as discrete, inert and interpretable, but rather must be understood as an emergent and relational manifestation of research activity, mutually constituted by researchers and participants acting in particular material circumstances (e.g. St. Pierre, Jackson and Mazzei 2016;Koro-Ljungberg, MacLure and Ulmer 2018). Equally however, the definition and use of data is also changing in social policy development and public service management. Data is now as much associated with the processes and procedures of systemic accountability and 'governing at a distance' (Lemke 2012, Rose andMiller 2008) as it is with the pursuit of supposedly objective evidence to inform the development and evaluation of policy. Data now actively manages and drives policy and practice by recursively impacting on the behaviours of social actors in situ. This move transcends policy fields but can be particularly observed in health, social care and education as governments seek to manage public services by setting targets for service delivery and render the individuals within them responsible for meeting the targets. This paper will explore these parallel and apparently independent developments and argue that, while deriving from different fields and aspirations, these developments have elements in common and data is a term now as much applied to and used in political governance, as it is in (what used to be seen as) disinterested science.\nDefinitions and conceptualisations of data in the natural and social sciences\nThe term 'data' derives from Latin -'something given' or 'having been given' (from dare, to give)implying that it can indeed be given, that it is external to the observer or knower, tangible and transferable. It is associated with observations and experiments in the natural sciences and continues to carry the implications and resonance of science for activity in the social sciences, including qualitative research. The Oxford English Dictionary (OED) defines data as \"Related items of (chiefly numerical) information considered collectively, typically obtained by scientific work and used for reference, analysis, or calculation\". (It further notes, for pedants, that in this definition it is \"a mass noun\" and can take a singular verb.) The OED goes on to elucidate various compound words and uses including data analysis, data handling, data mining, databank, and so on. Thus, classically, data is inert, passive, 'out there', waiting to be discovered and collected, pre-existing and separate from the scientist who collects it. Moreover, data is not just collected, but categorised in various ways, so that analysis can aggregate and compare 'like-with-like'. Similarly, when variables are manipulated in experimental situations, data are, in effect, created, but are still regarded as being a property of the interaction of variables, external to the observer. The experimenter changes the independent variable to produce data pertaining to the dependent or outcome variable in question.\nA similar set of assumptions seem to operate in much social scientific and qualitative research.\nQualitative methods such as structured observation and even participant observation attempt to collect what we might term 'naturally occurring' data in situ. The implication and the assumption seems to be that the researcher can directly observe events without significantly interfering or intervening in them. Interviewing, focus groups and so forth try to elicit (create) data more specifically by directly interrogating participants. Clearly this involves intervening in social situations and setting up particular encounters, but still the assumption seems to be that this can be done without distorting the data collected in important ways. The texts produced -observational field notes and interview transcripts -are then regarded as the 'raw data' for conceptual categorisation (coding), aggregation and analysis. There is of course extensive discussion in qualitative research about the extent to which this can be done without interfering with and/or biasing the 'findings' of the research. However, with appropriate practices and protocols (immersion in the field, interview schedules, triangulation of data sources and methods, member checks, etc.) it has long been argued that qualitative data can be collected, and that findings which are relatively independent of the researcher can be produced (Denzin 1970, Hammersley and Atkinson 1983, Miles and Huberman 1994). Furthermore with developments in digital technology and pressure for research to deliver better value-for-money and build knowledge across individual studies, archiving data, including qualitative data, is now becoming commonplace. This implies that qualitative data can be treated as an object, removed from the circumstances of its production, and aggregated and analysed across contexts and over time. We might wonder why so much qualitative research has adopted the language and underlying philosophy of positivist natural science, given that its starting point and rationale is, ostensibly, very different, i.e. to identify, describe, analyse and report the perceptions, interpretations and understandings of social actors from their own perspectives. But it can be argued that the very act of research implies the existence of some sort of additional external vantage point, some sort of privileged position from which to conduct the endeavour. Deriving from anthropology, and the very obvious powerful positioning of the (however benign) colonial observer over the 'native' or the 'other', qualitative research still assumes the position of observer, external to the culture, institutions and practices that are being investigated and reported on (of work, school, health care, youth, poverty, etc.). Moreover, given the increasing pressure for the findings of research to be immediately 'useful' in the context of (so-called) evidence-based policy making, the 'what works' movement, and 'scientifically-based research' (Torrance 2018), then it is perhaps not surprising that issues of sampling, coding, validity, reliability and generalisability have come to dominate many discussions of both the quality and the teaching of qualitative methods (cf. AERA 2006, Brown 2010, Cresswell et. al. 2011, Ragin et. al. 2004, and  Documents and artefacts are found, explored, compared and contrasted, but are also recognised as social products in and of themselves, to be evaluated for warrant and veracity, rather than treated as objective 'data' per se. A high status profession such as law, with very high stakes consequences\nriding on what counts as admissible 'evidence', grounds decisions in the examination of cases and the interrogation of individual witnesses. These processes are then deployed in the exercise of deliberation and judgement, and conclusions reached in the context of relevant precedents. The individual investigating police officer does not also act as judge and jury in the way that at least some social science researchers seem to do. Such observations about other models of inquiry have been made before (e.g. Stenhouse 1978, Stake 1995), yet seem to be regularly eclipsed in the recurring 'paradigm wars' of social policy development. Social science has first and foremost appealed to the processes and practices of 'science' for its legitimacy, rather than those of history or the criminal justice system.\nA static and linear model of research, policy and practice A further problem with 'conventional humanist qualitative methodology' is its acceptance, along with social science research more generally, of a linear model of research and the implication that the production of knowledge (research) can and should precede action (i.e. policy and practice).\nMany philosophical issues are begged by whether or not we can observe data, isolate variables, identify cause and effect in social action, and so forth. They have been reviewed extensively elsewhere and I will not cover similar ground now (e.g. Howe 2004, Maxwell 2012, Morgan 2014).\nHowever a key empirical problem with assuming that research must precede the improvement of policy and practice, and in particular with the 'what works' call for scientifically-based evidence, is that the linear model which it invokes, of problem identification, intervention, evaluation and application/dissemination, takes too long and, ironically, just doesn't work. The 'what works' movement seems to believe that the social world is essentially static, that it can be treated as somehow 'standing still', waiting for a solution to a problem to be found and implemented. The assumption seems to be that a particular issue can be identified as a topic of policy concern and solutions pursued in a relatively straightforward manner.\nTake the issue of raising educational standards, for example, which is then broken down into ostensibly interrelated constituent parts, with a series of causal links or 'mechanisms' being posited and pursued: the underachievement of poor inner city children, the importance of early reading, the development of intervention programs to promote early reading in target groups. Curriculum materials and teaching strategies are developed, interventions are designed and evaluated. Thus a multitude of intervening and interacting variables are identified and addressed. If appropriately developed and effectively taught such interventions may make a positive difference for some children. Often, of course, they do not; often there is 'no significant difference' found between intervention and control groups (Viadero 2009). But even successful interventions do not and cannot make a difference to all children in the target population -even positive results are only reported at the level of statistical probability, not individual certainty.\nMeanwhile large scale replication and dissemination is difficult, demands additional and/or redirection of existing resources, and often creates as many problems as it solves. California's attempt to implement smaller class sizes off the back of the apparent success of the Tennessee \"STAR\" evaluation illustrates many of these problems. The Tennessee experiment worked with a sample of schools, whereas California attempted statewide implementation, creating more problems than they solved by creating teacher shortages, especially in poorer neighborhoods in the state. There simply weren't enough well-qualified teachers available to reduce class size statewide, and those that were tended to move to schools in richer neighborhoods when more jobs in such schools became available (see Grissmer, Subotnik, & Orland, 2009).\nFurthermore, to return to the question of addressing underachievement and raising educational standards, the nature and the context of the problem changes over time, such that the relevance of the disseminated 'solution' diminishes. Large numbers of poor inner city children remain as underachievers but the social and educational milieu, and conditions of production of their A much more open, dynamic and iterative model of social action is needed in order to explore the ways in which research might make a positive but not necessarily predictable or pre-determined difference to social problems . Such a model would investigate and explore options in action, without assuming that a particular or single best solution must exist, or that such a solution will not interact with the changing nature of the problem in unpredictable ways. On the face of it, qualitative approaches to research ought to be able to encompass and generate such a model, since social interaction is the core of a qualitative approach to research and the basis for the development of qualitative methods. But the issue of a linear and chronological approach to the relationship of research, policy and practice is not simply a product of the 'what works' movement. Social research more generally has experienced similar disappointments. Successive generations of social and educational researchers, including qualitative researchers, too often discover and rediscover social issues and problems rather than contribute to solving them. A significant illustration might be the cumulative work of researchers such as Hargreaves (1967), P. Jackson (1971), Willis (1979), McNeil (1986), McLaren (1989), Delpit (1995), and Lipman (2004), on the social organization of schooling and its impact on disadvantaged groups of students. These studies are exemplars of the very best of their kind, and constitute a formidable body of knowledge about the ways in which schooling privileges particular manifestations of middle class culture and behaviour. The studies demonstrate how schooling contributes to the reproduction of social inequality, often despite the best intentions of teachers, administrators, and, sometimes, the researchers themselves as they have sought to feedback findings to promote change. However, while this and similar research has produced Additional to this discussion, but clearly related to it, are new approaches to understanding number and quantification, and the nature and implications of the calculations and analyses that underpin and derive from new computational technologies (de Freitas, Dixon-Roman and Lather 2016). De\nFreitas and colleagues note that number and quantification no longer simply count and analyse what 'is' (i.e. externally observable, static, data), but create and bring into being what is 'yet to come' (i.e.\nentangled data) through \"computational reconfigurations of subjectivity and the social\" (p. 431).\nThey argue that \"algorithms are making high-stakes decisions\" (p. 432) but that the ontology and epistemology of number which underpin algorithms must encompass indeterminancy rather than certainty if new forms of quantification are to remain open to new possibilities.\nI will return to these arguments below. The point which I want to make for the moment however, is that these critiques of the concept and use of the term 'data' in social research relate to philosophical and methodological arguments within the research community. They are largely internal debates prompted, perhaps, by some of the engagements of qualitative inquiry with the demands of policy, and disillusionment with lack of educational and social change, but they are largely internal to the field none-the-less. The argument is about the philosophical basis and direction of social research, particularly qualitative research, and what theories and activities qualitative inquiry might encompass in the future. However, similar or, at least, parallel and somewhat comparable changes can also be identified in the field of policy and governance and it is to this that I now turn.\nThe conceptualisation and use of data in neo-liberal governance\nThe definition and utility of 'data' is also beginning to morph and develop in the field of social policy and public service management. Data is now as much associated with the processes and procedures of accountability and 'governing at a distance' (Foucault 2009, Lemke 2012, Rose and Miller 2008) as it is with the pursuit of 'research' or 'science'. This move transcends policy fields but can be particularly observed in health, social care and education as governments seek to manage public services by setting targets for service delivery and render the individuals within them responsible for meeting the targets (Crawshaw 2012, Ozga 2009, Lingard, Thompson and Sellar 2016, Torrance 2015). In education the move is perhaps exemplified and amplified, globally, by international At first sight the setting and measuring of public service targets, and the construction of league tables and international rankings of countries' achievements in Maths, Science and Literacy, would appear to invoke the external and inert model of the natural sciences -data about educational achievement as somehow 'out there', waiting to be discovered -\"Related items of (chiefly numerical) information considered collectively\" (OED). But these \"items\" are very specifically created and produced, manufactured, fabricated, in order to support policy and, in turn, political  Thus 'data' used dynamically in the policy sphere, renders actual definitions and manifestations of quality redundant. Rather the 'data' is taken as an absolute indicator of quality, a substitute for it, becoming as important if not more so in driving changes in behaviour as static scientific data about 'what works'. There are parallels again with Sellar and Thomson's (2016) observations about the impact of real-time monitoring and feedback loops, here embedded in the technology of national testing and league tables. Why wait for the (possibly unhelpful) results of the (scientific) evaluation when you can build policy implementation and behaviour change into the instrument of evaluation itself. The implications of Sellar and Thomson's analysis are similarly pertinent -that such feedback loops can only reinforce the policy status quo, the already-assumed-to-be-perfect solution which is built into the evaluation and feedback procedure of a closed system.\nBe careful what you wish for -entangled data\nThe debate about what counts as data in qualitative research might seem a little parochial then, even narrow and esoteric, in comparison to the pressure on teachers, students and others involved in the provision of public services. Yet it may be that there are some resonances or echoes of the one in the other. Arguments about entanglement, immanence and the emergent position of the researcher in the research process seems to have some parallels with the way in which the production of data to inform the management of public services has morphed into constant realtime data awareness, data vigilance and even data servitude. Just as the 'research act' emerges, so too do the effects of data management on those in public services that construct the data.\nKoro-Ljungberg, MacLure and Ulmer (2018), reviewing the debate over data in qualitative research, argue that:\nIt is no longer possible to imagine the researcher positioned at arm's length from the data, exercising interpretive dominion over it...Equally however data cannot be thought of as mere social construction with no material footing in the world. However, to link back to the production and role of data in neo-liberal governance, might not the individual responsibilisation of teachers, students, health care workers and the like, in their quest to \"get the data looking good\" be one example of such a \"network of mutual determinations\"? Isn't this exactly how neo-liberalism insinuates itself into every aspect of our professional and personal lives? It seems as if neo-liberalism already operates with a much more sophisticated theory of change than empirical social science. Change has occurred in public institutions (and indeed in commercial organisations as well) in precisely this \"immanent\" incremental fashion -with \"practicebased accretions\" slowly 'bringing the frog to the boil' so to speak, so that almost without noticing it, everything has changed. Lather (2016a) actually makes almost exactly the same interpretive point but draws different conclusions, seeing the slow accretions of neo-liberal accountability as evidence of their fragility:\nHow everyday material practices assemble and align with objects, ideas and behaviours involved in \"new governing behaviours\" particularly the over-reliance on \"flows of data\" as \"calculating devices\"...illustrate the precarity of what looks so solid and immutable (p.4).\nWell, certainly, neo-liberal \"governing behaviours\" are assembled and invoked in and through everyday practice, they are indeed neither \"solid\" nor \"immutable\", but they feel as if they are. This surely exemplifies the power of neo-liberalism and the paradox of current theoretical thinking. Data are certainly not 'out there', waiting to be discovered. They are not inert, passive, manipulable .   (Hargreaves 1999). Each of these various approaches to research understands that research is an iterative and cyclical process, but perhaps still interprets that cyclicality in terms of forward movement and pragmatic, interactive engagement with the objects of 'the real', rather than emergence, indeterminancy and the intra-active production of 'the new' in situ. To return to Lather's (2016a) reflections on the development of a more relational ontology, it is not that we necessarily will produce something new, but that the prospect at least exists if we conceptualise research as a process of constant possibility rather than something which we can drive in particular directions. Equally however it becomes clear that any claims for research must become more modest, and take their place in the larger assemblage. Data, once released from the Pandora's Box of inertia and passivity, will not necessarily prove benign in its effects. The \"incalculable subject\" is currently very busy trying to calibrate itself. Even governments, pursuing better national positions in international league tables, and coming under pressure if a nation appears to perform worse than previously, end up being as much subject to data as in control of it. Having said this however, it is apparent that entanglement and emergence are indeed \"not containable, in excess of meaning\" and as such at least provide the possibility of producing something new and, as yet, unforeseen.",
        "INTRODUCTION\n\nTransthyretin-related amyloid familial amyloid polyneuropathy (FAP), also known as Paramyloidosis, is an autosomal-dominant neurodegenerative disorder; Val30Met is the most common disease-related variant. 1 Symptoms are highly incapacitating including progressive sensory impairment, with neuropathic pain or numbness in the hands and feet, and a reduction in fine motor skills. FAP has a high lifelong penetrance and its age of onset occurs mainly between the ages of 25 and 35 years, but this can vary greatly. Non-curative therapeutic measures that slow down the progress of symptoms in affected carriers are available, such as liver transplantation and oral medication. 1 FAP has been initially described by Andrade 2 in Portugal, where it has the largest cluster of patients worldwide. The disease occurs worldwide, but it is endemic in some regions, most notably northern Portugal (P\u00f3voa de Varzim and Vila do Conde), where FAP was initially found and where its incidence 3 may reach 163 in 100 000.\nIn Portugal, pre-symptomatic genetic tests (PST) for at-risk individuals has been possible through biochemical testing since 1984 (later through a molecular genetic test); genetic counselling and psychosocial support are provided at some genetic services, and community-based genetic counselling has been offered 4 since 1986. Preimplantation genetic diagnosis is available in Portugal 5 since 2000. Carriers of the pathogenic variant are offered regular clinical monitoring to detect early signs of the disease and, therefore, maximise the effectiveness of the available non-curative treatments. 1 The disclosure of disease risk information to at-risk relatives is key to obtaining the full benefits of genetic health care. 6 The sharing of such information can be crucial for their health management, but also in terms of lifeplanning issues, including employment, lifestyle and reproduction choices. Therefore, the promotion of risk management behaviours is key in the health of members of FAP families.\nPrevious studies have evidenced the importance of the supportive and informational role played by family members in facilitating the engagement in health promotion strategies and at-risk management behaviours among other relatives, and in influencing decisions about genetic screening and testing. [7][8][9][10] In addition, research has suggested that at-risk individuals who do not discuss risk factors with family members are less likely to engage in recommended cancer surveillance. 9,11 Regarding older family members, literature has portrayed them as a critical source of family health information and important suppliers of emotional support and affection to younger generations. 12,13 The role of older generations in families with hereditary diseases has been recognised and is predominantly attached to their function of guardians or gatekeepers of the family's medical history; 14 this is so given their presumed privileged knowledge about illness-related information of ancestor members of the family to whom access is often not available (due to death or incapacity). However, literature is rather scarce in reporting other roles of older generations regarding health promotion and risk management towards younger generations. In later life, older persons often wish to provide for future generations (generativity), which makes their involvement, in the health management of younger generations, foreseeable. 15,16 Indeed, recent research has examined what roles older generations play in terms of health promotion and risk management towards younger generations, and has mainly focused on conditions to which susceptibility genetic tests and preventative treatment options are available, such as hereditary cancers, diabetes or heart disease. These studies have shown that members from older generations are likely to act as screening encouragers and facilitators in the transmission of self-care knowledge for younger generations. 17,18 Yet, to the best of our knowledge, there is no research exploring these roles in incurable and highly incapacitating genetically inherited conditions where PST are available, such as FAP. Therefore, the aim of this exploratory study is to examine the roles that members of older generations play towards younger generations, in terms of health promotion and risk management, in families with FAP; and to explore the intergenerational flow by analysing who from the older generation (gender, kinship and disease status) plays what role(s) towards whom from the younger generation (gender, kinship and disease status). The results of this study contribute to better understand the role of the older generation in families affected by genetically inherited disorders for which no curative treatments are available; contribute to the awareness of how the intergenerational promotion of health flows within these families; and to understand how the older family members can be further involved as partners at the clinical practice level.\n---\nMETHODS\n\nThis exploratory qualitative study is based on the critical incidents technique, which is a method that allows the collection of events that reveal life meaningful experiences by using the perspectives of those involved in the event, thus having access to the individual perspective of significant roles. 19 An incident is an observable human activity and incidents typically include a description of the key players and the outcomes. A critical incident may be a commonplace, everyday event or interaction, but it is 'critical' in that it stands for the one who lives it. 20 critical incidents technique has been widely used to examine the relations between health professionals and users. 21,22 And more recently to analyse the intergenerational transmission of gynaecologic information. 23 \n---\nProcedures\n\nThis study was approved by the IBMC Human Ethics Committee. The sample was intentional and non-probabilistic. The recruitment was mediated by the national FAP patient's association (APP), that was contacted directly, and a meeting was scheduled to present the study and explain the needed collaboration. Moreover, APP accepted to collaborate by making the study known to their associates and by mediating the contact between potential participants and the researchers. The inclusion (members of families with a history of FAP, \u2265 18 years, oriented in time and space) and exclusion criteria (having a mental or psychological disease, substance abuse, in critical clinical situation that might limit consent) were indicated. To those potential participants that showed interest in participating, APP was authorised to provide the contact details (e-mail address or mobile telephone number) of the researchers. The other possibility was to have APP send the contact information of the potential participants by e-mail to the researchers who then established direct contact with the subjects. This procedure was complemented by snow-ball sampling, that is, after the interview the researcher asks the participant to identify other potential participant(s) and to mediate that contact. So, the first contact with potential participants was done via a telephone call by the first author who explained the objectives of the study, and its methods, in detail and requested the individuals' collaboration. After this initial contact, all of the identified persons agreed to participate and the interviews were scheduled (date and time) taking into account the participants' convenience. The verbal consent was obtained and audio-recorded.\n---\nInstrument\n\nThe collection of data was performed by the first author and the critical incidents technique was administered based on a semi-structured schedule and executed via telephone interviews. The interview was introduced with the following invitation:\n'Please think about one or more episodes that have occurred in your family, and that you have observed or participated in, involving a person from the older generation and a person from the younger generation. Think about episodes that have been significant and meaningful for you and/or for members of the younger generation in your family in terms of how FAP is dealt with and managed by individuals and by the family as a whole. Think about events that you remember clearly. Please indicate your role in the event: observer, member of the younger generation or member of the older generation.' The interview was completed by asking questions that went into more detail in terms of the initial invitation: detailed description of the event, who was present, and what was more relevant, significant and/or meaningful. At the end, the following data were collected from the participants/narrators, for both older and young protagonists: sociodemographic (age, gender and academic status), disease status (non-carrier, pre-symptomatic carrier, affected carrier) and treatment modalities for the affected participants (no treatment, liver transplantation and oral medication), and kinship. The recruitment stopped at 18 participants (76 critical incidents; 3 participants related 1 incident-minimum; and 1 recounted 10-maximum; mean of 4.3 incidents per participant) (Table 1), which was when the authors agreed that theoretical saturation (situation where the data became redundant) had been reached. 24 The saturation point was determined by using the agreement of the inter-judges in the following manner: the first author, who conducted and transcribed the interviews, announced that saturation had been reached, then the other two authors read the transcripts of the interviews independently and indicated their agreement. The average duration of the interviews was 25 min, ranging from 10 to 49 min.\n---\nSample\n\nThe sample comprises 18 participants (narrators of the incidents), 10 of which are women (Table 1). The ages range from 18 to 65 years (mean = 42 years). In relation to the academic status, the following was observed: 1-4 years of schooling, n = 1; 5-9 years of schooling, n = 10; 10-12 years, n = 4; university degree, n = 2 (1 missing value).\nThe 18 participants are members of 11 families; 7 participants do not have family relationships within the sample, while 11 participants are members of 4 families. Regarding the disease status, the data show that 11 participants are affected (8 with liver transplantation and 3 with oral medication), 2 are presymptomatic carriers, 4 are non-carriers and 1 has an unknown illness status (has not undergone PST). The participants tend to be the younger protagonist (64 incidents; 84.2%), less frequently are the older protagonist (11 incidents; 14.5%), and in one event is a witness.\n---\nData analysis\n\nInterviews were audio-taped, transcribed in full and the critical incidents submitted for content analysis by the first and third authors. The two coders started by reading all of the incidents. After the process of content analysis had been conducted comprising two stages: the definition of the categories of the older family members' roles and the classification of incidents into these categories. The initial stage of analysis aimed to define categories, and the creation and testing of the categorisation system was an iterative process of successive refinement that involved two independent coders. Each coder read all of the incidents and developed a list of categories and subcategories. Both coders then met in order to compare and discuss their proposals until agreement was reached. Lastly, each coder randomly categorised five incidents in order to confirm that the categorisation system fitted the episodes. The list of categories and subcategories were organised along with a definition and criteria for assignment plus an example of incidents that would fit the category. In a second phase, the two coders independently categorised the incidents. After each coder had independently analysed all of the incidents, they met and registered their agreements as well as their disagreements. The inter-judgment agreement (this score was reached by dividing the number of agreements by the total number of agreements plus disagreements) was 81.6%, indicating a quite high reliability. 25 Finally, the two coders discussed the incidents on which they disagreed, and this discussion led to the total agreement on all of the incidents. After this, the analysis focused on the intergenerational flow. Hence, for each critical incident the older and younger protagonists were identified, and kinship, sociodemographic data (gender, age) and disease status were obtained. Then cross-tabulations were calculated to depict the number of times each of the possible variables combinations occurred in the incidents.\n---\nRESULTS\n\nThe obtained results indicate four categories of roles played by the older family members regarding their younger relatives (Table 2): 'Modelling health-related behaviour' (52.6%), '(Not) Encouraging to carry out the PST' (18.4%), 'Supporting' (15.8%) and '(Not) Informing of the risk of disease' (13.2%).\nThe modelling category comprises events in which the older persons influence the younger ones by their own example and, at least apparently, without intention. It contains three subcategories: 'Normalising the illness experience' (25%) that describes situations where the older protagonists that are affected continue to care for themselves and still support and worry about others in the family; 'Dramatising the illness experience' (5.3%) that includes episodes where the older protagonists that are affected show despair and the desire to die; 'Living transmitters of the disease experience' (22.4%) that comprises events where members of the younger generation observe in the older affected relatives the symptoms and their progression, and the attached suffering.\nThe category on encouraging involves episodes where the older family members influence their younger relatives regarding the performance or not of the PST. It entails of four subcategories: 'Encouraging' (7.9%), which consists in persuading and even pressuring the young individuals to carry out the PST; 'Supporting any decision' (3.9%)-do it, don't do it, or delay the decision; 'Providing practical support' regarding the process of PST (eg, scheduling and going with the individual to the consultations and/or receiving the PTS results); 'Discouraging' doing the PST (2.6%), usually in an implicit way, this is, the older family members postpone any attempt of the younger members to discuss the topic or simply ignore their search for information.\nThe supporting category takes four subcategories into account: 'Emotional support' (5.3%) when the result of the PST is positive (carrier), usually by giving hope to the younger relative, which is associated to the new available treatments; 'Advising on personal life decisions' (3.9%), typically the older protagonist advise the carriers to not have children and to warn the non-carriers about a possible relationship or marriage with a carrier; 'Emotional and instrumental support during the disease' (3.9%) that implies taking the younger affected relatives to consultations, preparing their meals and generally showing empathy; 'Supporting the decision of undergoing the available treatments' (2.6%) that entails the encouragement and motivation of younger individuals to not give up. The category related to information comprises events where the older family members transmit, or do not transmit, information to the younger generation on the disease and its possible treatments and/or motivate them to search for (more) information. It encompasses three subcategories: 'Informing' (6.6%), which occurs when the older family members transmit what they know about the disease (eg, history in the family, characteristics of the disease and old myths); 'Motivating the search for disease-related information' (5.3%) that report situations where the older generation motivate, support and collaborate with the younger generation in order for them to get more information and experience regarding the disease (eg, participating in conferences, searching for information on the Internet and talking to affected persons outside of the family unit); and 'Silencing' (1.3%) that refers a situation where the old individual even when directly questioned did not talk about the disease.\nThe next step was to carry on the cross-tabulations, starting with the gender of the older and younger protagonists of the incidents. The older protagonist in 52 incidents (68.4%) is a female; and the younger protagonist is in 46 incidents (60.5%) also a female. Data show that the flow of the intergenerational transmission occurs mostly between females (34 incidents), or from older females to younger males (18 incidents); some events from older males to younger females (12  incidents) and between males (12 incidents).\nThen the same procedure was performed regarding kinship between older and younger protagonists (Table 3). The kinship of the older family members with the younger ones involves mostly mothers (41 incidents) and fathers (16 incidents). Concomitantly, the affinity of the younger family members with the older ones is mostly daughter (33 incidents) and son (24 incidents). The most frequent associations are mother and daughter (25 incidents); mother and son (16 incidents); father and daughter (8 incidents); father and son (8 incidents); and also aunt and niece (7 incidents).\nIn terms of the disease status, the most frequent links involve (Table 3): older family members affected and younger members with an unknown illness status (28 incidents); older family members affected and younger members who are also affected (12 incidents); older family members affected and non-carrier younger family members (8 incidents); and older non-carriers and younger affected family members (6 incidents).\nThen the analysis focused on the association between the role played by the older generation and the kinship between old and young protagonists (Table 4). Data show that the mother plays the modelling role mainly towards her children (daughter and son); in terms of subcategories, mothers are mostly living transmitters of the disease experience (12 incidents) and normalise the illness experience (10 incidents). The father tends to play the following roles towards his children (son and daughter): modelling, mostly by normalising the illness experience (5 incidents); informing, mostly by motivating the search for disease-related information (3 incidents); and encouraging to carry out the PST (3 incidents). Data also indicate that the aunts/ uncles tend to assume roles towards nieces/nephews that are similar to those played by the mothers towards their children, this is, modelling by being living transmitters of the disease experience (5 incidents) and by normalising the illness experience (4 incidents).\nFinally, the roles played by the older generation of the family towards the younger generation was analysed considering the disease status (Table 4). The older affected protagonists mainly play the following roles towards the younger protagonists: modelling by normalising the illness experience, mostly towards members of the younger generation that are non-carriers (6 incidents) and affected (5 incidents); modelling by being living transmitters of the disease experience towards those young protagonists that still do not know their genetic status (12 incidents); encouraging to carry out the PST towards those that still do not know their genetic status (4 incidents). Furthermore, results indicate that the older family members that are non-carriers tend to assume the following roles: modelling, mostly by normalising the illness experience towards the younger protagonists that are non-carriers (2 incidents), affected (1 incident) and presymptomatic carriers (1 incident); supporting the younger protagonists that are affected, mostly through emotional and practical support during the disease (3 incidents) and supporting the decision to undergo the available treatments (2 incidents).\n---\nDISCUSSION\n\nThe main findings of this study suggest that the intergenerational flow takes place mostly between women, from mother to daughter, and from older affected persons to young pre-symptomatic carriers. Data also suggest that the older generation in FAP families play four main roles towards their younger counterparts: modelling, encouraging, supporting and informing. 26 Similar roles have been suggested in previous studies that mostly focus on hereditary cancers. 7,12,27,28 The most frequent role is modelling, mostly by normalising the experience of the illness and by being a living transmitter of the disease experience (dramatising is less frequent). Modelling emerges as a role performed unintentionally, that is, normalising the experience of the illness is performed by being an example and by being a living transmitter of the disease because the younger generation tend to observe the behaviour of the older generation. Moreover, normalising the illness experience helps the younger subjects to 'put the illness in its place', 29 which means that they learn or better understand that it is possible to manage and make the disease compatible with other family, social and/or professional functions. The role of living transmitter of the disease experience helps the younger individuals to better comprehend what it is like to live with the disease in terms of symptoms, incapacity and suffering. It is probable that the older persons realise that even without a clear intention, they are influencing their younger relatives to understand what the disease is and are normalising their experience of living with the illness. This will most likely help the older persons achieve an integrity that is associated with the building of a legacy that will shape the way they will be remembered by the younger generation. 16,30 The role of encouraging is mainly associated with the carrying out of the PST. In fact, the older protagonists that assume this role seem to consider that, for the younger individual, it is decisive to know their disease status because it will determine their health management and decisions in family life (such as the decision to have children or not). Additionally, the older generation tends to encourage the younger generation to carry out the PST. Nevertheless, some choose to support any decision taken by the younger generation and a few others opt to discourage the performance of the PST.\nSupporting is a role that the older persons assume when the younger family members are confronted with the carrier result in the PST and during the chronic phase of the disease. This role has emerged also in the literature on intergenerational relationships and hereditary diseases, 12,27 which emphasises that older persons tend to receive care from the younger persons. Yet, when the younger relatives are in need and the older persons are still independent, they assume the role of informal caregivers. This role underlines the families adaptive capacity, that is necessary when severe problems exist, and the reciprocity and mutuality that depicts intergenerational relationships. 16 The role of informing about the disease or the risk of having the disease is less frequent, and has emerged in the literature on hereditary diseases to describe that the older family members disseminate the family health history. 7,8,12,17,27 The older relatives have a lived and Table 4 Roles played by the older family members regarding the younger: kinship and disease status non-documented experience about the disease, have deceased relatives and also family, community and social stories and myths regarding the condition. These roles seem to be assumed more explicitly, but performed with diversity: some members of the older generation choose to share what they know and/or motivate the younger subject to get (more) information. However, some others may decide to keep silent, which may be due to the associated suffering when remembering experiences related to the disease. Research, mostly with hereditary cancers, has shown similar findings. 14,27,31 In terms of the flow of the intergenerational roles, data suggest that this mostly arises from older to younger women. Therefore, it seems that women from the older generation influence those from the younger generation, and as some literature has already suggested, women are gatekeepers and the dominant gender in the context of exchanging health information within the family. 7,9 This result is consistent with the traditional description of gender roles in families and also with the adopted family dynamics when faced with chronic severe diseases. 29 Moreover, while men tend to assume more instrumental and external roles, women perform more emotional and expressive functions, which they tend to keep within the family unit. 10,18 This study shows that mothers play their roles mostly towards their daughters (less frequently, towards their sons) in terms of modelling their children's experience of the disease (living transmitter of the disease and normalising the experience of the illness). And, fathers perform their role towards their daughters and/or sons by mostly assuming the role of modelling, but also informing and encouraging. The available literature has suggested that intergenerational influencing strategies have gender differences: women tend to influence most at an emotional level, while men exert their influence by being problem oriented. 10,18,29 Thus, the exercise of the roles tends to occur from parents to children, that is, between contiguous consanguineous generations. 28 However, also some roles performed by aunts/uncles towards nieces/nephews sometimes emerge, and are aligned with the roles performed by the mothers towards their sons/daughters: modelling health-related behaviours by being living transmitters of the disease experience and normalising the illness experience. It is probable that this type of influence takes place because the parents are unable to exert their family roles due to impairments related to the disease.\nFurthermore, the results suggest that the roles played by the older generation differ considering the disease status. The influence of older family members that are affected was the most frequent, possibly because as they are experiencing the disease, they model the behaviour of the younger family members that still do not know their genetic status; hence disclosing what it is like to live with the disease (living transmitters) and, implicitly, influencing the younger relatives to make decisions regarding the PST. The older family members that are affected also influence the pre-symptomatic carriers by helping them to understand the disease and by showing them how it can be faced, thus normalising the illness experience. The older family members that are non-carriers tend to assume other roles. They also fall into the category of modelling by normalising the illness experience towards the younger generation that are non-carriers, affected or presymptomatic carriers, but mostly the older generation that are noncarriers assume the role of supporting (emotionally and instrumentally) and encouraging (to undergo the available treatments) the younger generation that are affected.\nThe results of this study suggest that the older generation in FAP families play relevant roles in terms of promoting health behaviours and risk management towards their younger relatives. As FAP represents a public health problem in the high prevalence clusters, clinical genetic services and the health-care system more broadly might want to consider these roles and the intergenerational flow so that this information can be used to enhance health promotion behaviours in at-risk families.\n---\nLimitations and further research perspectives\n\nThe main limitation of this study was the reduced size of the sample (18 participants) despite the data saturation. In addition, the sample comprises mainly family members from the intermediate generation (most participants were 31-52 years old). A larger sample with more participants from older and younger generations would have allowed a better understanding of the roles performed by the older family members as well as the intergenerational transmission flow. In addition, since inherited disorders are a family matter, future studies could explore in/congruencies within each family, to highlight individual perspectives within families, and how they might be interacting and influencing health-related behaviours. Also, future research could include the perspective of family members that choose not to carry out the PST in order to explore the type of influence this decision has. Upcoming studies could also explore the evolving role of the older persons as new and more efficient treatments are made available. Furthermore, coming studies should examine the professionals' perspectives on the roles of the older family members. And also examine the role of the older generation in other genetically inherited diseases.\n---\nRoles\n\nKinship from older to ( \u2192 ) young (n) Disease status from older to ( \u2192 ) young (n)\nModelling health-related behaviours Normalising Mother \u2192 Children (10)  Affected \u2192 Non-carrier (6) Father \u2192 Children (5)  Affected \u2192 Pre-symptomatic ( 5)\nNon-carrier \u2192 Affected ( \n---\nCONFLICT OF INTEREST\n\nThe authors declare no conflict of interest.",
        "Introduction\n\nThe world is currently facing a pandemic due to the rapid spread of coronavirus disease 2019 (COVID- 19), caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2). As of 23 rd March 2021, an estimated 124,291,475 confirmed cases and around 2,735,205 deaths have been attributed to COVID-19 affecting more than 219 countries and territories across the world [1]. Malaysia has reported 334,156 confirmed cases with 1,238 cumulative deaths with a case fatality rate of 0.4% [1]. Although the case fatality rate was low in Malaysia compared to other developed countries like USA or UK, people were anxious as the virus could spread rapidly from one person to another through direct or indirect contact [2].\nIn Malaysia, the first COVID-19 case was detected on 25 January 2020 [3]. With a surge in cases thereafter, physical distancing rules, restrictions on social gatherings, appropriate use of face masks, Movement Control Order (MCO), Conditional Movement Control Order (CMCO), extended movement control order, and border closures were implemented by the Malaysian Government between mid-March to August-2020 to curb the spread of the disease [4]. However, Malaysia has seen a resurgence of COVID-19 cases and is currently facing a third wave of infection and is under the second CMCO from 9-November-2020 in all states except Perlis, Kelantan, Pahang and Sarawak. Malaysia has launched COVID-19 vaccination program on the 24 th February 2021 [5]. The impact of all these spatial distancing policies and the uncertainty of returning to normalcy have direct and indirect impact on social life as well as mental wellbeing of the community people. Those interim actions such as MCOs or lockdowns, physical distancing and quarantine have reportedly led to heightened fears, stress and anxiety amongst individuals globally.\nA recent review found women, younger individuals, those living in rural areas, those with lower socioeconomic status, those at higher-risk of COVID-19 infection and longer media exposure to be associated with higher levels of anxiety and depression [6]. Individual studies have shown that the COVID-19 pandemic affected people in different countries in different ways with some groups being more vulnerable than others. In Australia, pre-existing mental health conditions, increased smoking and alcohol during the lockdown and high levels of fear and being female were associated with higher levels of psychological distress [7]. Similarly, in the UK, females, younger age, lower annual income, smokers and co-morbidity were associated with poor mental health [8]. While in Italy, female gender, negative affect and detachment were associated with higher levels of stress [9]. In some studies in China, frequent and prolonged social media exposure during the COVID-19 pandemic was found to be strongly associated with anxiety and depression [10].\nPrevious studies have reported the negative influence of pandemics on psychological wellbeing, which can lead to acute depression and anxiety [7,11]. Evidence suggests that frontline did not receive direct or specific funding for this work. The specific roles of this author is articulated in the 'author contributions' section.\n---\nCompeting interests:\n\nThe authors have read the journal's policy and have the following competing interests: RS is a staff member of Telstra Health (https://www.telstrahealth.com/). There are no patents, products in development or marketed products associated with this research to declare. This does not alter our adherence to PLOS ONE policies on sharing data and materials.\nhealthcare workers, who were directly involved in the collection of samples, diagnosis, treatment, and care of patients during an outbreak were at higher risk of developing psychological distress and mental health symptoms [12]. Previous evidence documented immediate psychological impacts amongst frontline healthcare workers with symptoms of anxiety, distress, depression, fear of spreading infection to family, friends and colleagues [7,13]. Lower sleep quality due to anxiety and stress, which eventually reduced self-efficacy exponentially among the medical staff has also been reported [14].\nOnly recently studies have emerged to show the negative impact of the pandemic on children, older people, pregnant women, university students, people with weight issues and the general population as a whole. An Iranian study revealed effect of fear of COVID-19 was significantly associated with depression, anxiety, suicidal intention and mental quality of life among the pregnant women [15]. In a study with older people, fear of COVID-19 significantly mediated the association between perceived health status, and insomnia, mental health and COVID-19 preventative behaviours [16]. A recent study among university students from Indonesia, Taiwan and Thailand found that Thai students had the highest level of anxiety but limited resources to fight the COVID-19 pandemic, whereas Taiwanese students were more negatively affected by information gathering from the internet; such less perceived satisfactory support was associated with more suicidal thoughts among Indonesian students [17]. Stressors of COIVD-19 pandemic could also result in behaviour impairments of children and adolescents, which could potentially impact psychological wellbeing in early life and adulthood [18].\nStudies on the impact of COVID-19 on mental health are limited in Malaysia and most of them were conducted amongst students. In one study using online survey, out of 983 Malaysian students, 20.4%, 6.6%, and 2.8% experienced minimal to moderate, marked to severe, and most extreme levels of anxiety, respectively. Female gender, age under 18 years, pre-university level of education, management studies, and staying alone were significantly associated with higher levels of anxiety. The main stressors included financial constraints, remote online teaching, and uncertainty about the future regarding study and career also affecting the mental health [2]. In another study amongst Malaysian university students, the prevalence of anxiety was much higher; 30.5% were experiencing mild, 31.1% moderate, and 26.1% severe anxiety; age >20 years, Chinese ethnicity, decreased family income, co-morbid conditions, and spending time watching COVID-related news and infected friends and relatives were found to be associated with increased anxiety [19]. In another study in Malaysia age <25 years and females were more likely to have higher levels of fear of COVID-19; however, 70% of the respondents were also students in this study [20].\nThere is limited evidence regarding the impact of COVID-19 on psychological distress, fear and coping strategies as a whole and amongst community members and healthcare workers in Malaysia. We, therefore, conducted this study to understand the extent of the mental health burden in the community settings in Malaysia during the COVID-19 pandemic. The study will identify population subgroups more at risk of developing poor mental health outcomes and enable policy makers to guide resource planning and design psychosocial interventions targeted to these high-risk and vulnerable groups of population.\n---\nMaterials and methods\n\n\n---\nStudy design and settings\n\nA cross-sectional study was conducted between August and September 2020. An online survey link was shared in different online platforms, including Facebook, Twitter and LinkedIn inviting online users to participate in this study.\n---\nStudy population\n\nStudy participants included patients, university students and healthcare professionals residing in Malaysia. To be eligible, participants had to be 18 years or above and were literate enough to respond to an online questionnaire in English. The participants who took <1 minute to complete the questionnaire, were excluded during analyses.\n---\nSampling\n\nSample size was calculated using OpenEpi [21]. Considering 32.6 million population of Malaysia [22], 30% estimated prevalence of stress amongst Malaysians [23,24], at 95% confidence intervals and 80% power, the estimated minimum sample size was 323. Snowball sampling technique was used to recruit the study participants. Once any participant filled up the online questionnaire, h/she forwarded the survey link to own personal/professional networks.\n---\nData collection\n\nGoogle form was used to develop the study questionnaire. The first page included participant information statement and the consent form. Participants, who provided consents, could move to the next screen. There were two screening questions to determine eligibility of the study participants, one was age and the other was location of residence. Eligible participants accessed the full study questionnaire and responses were collected anonymously. The online survey link was shared through university/hospital staff/students' emails, text messages, What-sApp and other social media platforms such as Facebook, Twitter and LinkedIn. Patients visiting any healthcare settings or university students within the defined study period were informed about the study and of the online link by the respective healthcare professionals or university faculty members.\n---\nStudy tool\n\nWe used the same survey questionnaire (except residence location/region in Malaysia) which was used earlier by the Australian investigators included in this study [7]. Three validated tools were included in the survey questionnaire. The Kessler Psychological Distress Scale (K10) tool having ten items was used to assess psychological distress [25], the Fear of COVID-19 scale (FCV-19S) having seven items was used to assess the levels of fear [26], and the Brief Resilient Coping Scale (BRCS) having four items was used to assess the levels of coping [27]. Each of those tools collected responses using a 5-point likert scale and the scoring was categorised as discussed in earlier study [7]. Reliability of using these tools had also been examined in a recent study [28]. The questionnaire was pre-tested and no changes were made.\n---\nData analyses\n\nData from Google forms were downloaded and analysed using STATA v.12. Continuous variables were described using descriptive statistics such as mean standard deviations, and proportions. Scoring in the K10 scale was re-defined into low (score 10-15) and moderate to very high (score 16-50), the FCV-19S scale to low (score 7-21) and high (score 22-35) and BRCS scale categorised into low (score 4-13) and medium to high (score 14-20) resilient coping. We used univariate and multivariate logistic regression to investigate the associations. The multivariate models were adjusted for socio-demographic variables such as age, gender, living status, country of birth, education, and employment status.\n---\nEthics\n\nEthics approval was obtained from the Human Research Ethics Committee (HREC) at Universiti Sains Malaysia (USM/JEPeM/COVID19-40). Data were collected anonymously and could not be linked back to identify any participant. Contact details of Befrienders was included at the end of the online questionnaire, allowing participant/s to access necessary support in case of distress during filling questionnaire.\n---\nResults\n\nA total of 720 individuals participated in this study. Mean age (\u00b1SD) of the participants was 31.7 (\u00b111.5) years, and most of them (56.7%) were in the age group 18-29 years. More than two-thirds of the participants (67.1%) were females. A quarter of the study population (27.1%) was from Penang, and another quarter (22.9%) was from Perak in Malaysia. Almost all of them (90.8%) were born in Malaysia. A third of the study population (30%) identified themselves as frontline or essential service workers, and a third (31.9%) was identified as patients. Details of the characteristics of the study population are presented in Table 1.\nAbout two-thirds of the study participants (62.1%) experienced moderate to very high levels of psychological distress. Only a quarter (27.1%) reported high levels of fear of COVID-19 and two-thirds of the participants (65.1%) were identified as having medium to high resilient coping (Tables 234).\nTable 5 shows the univariate and multivariate analyses regarding factors associated with psychological distress. Moderate to very high levels of psychological distress was associated with impacted financial situation due to COVID-19 (AOR 2.16, 95% CIs 1.54-3.03, p<0.001), alcohol drinking in the last four weeks (AOR 3.43, 95% CIs 1.45-8.10, p<0.01), being a patient (AOR 2.02, 95% CIs 1.39-2.93, p<0.001), and having higher levels of fear of COVID-19 (AOR 2.55, 95% CIs 1.70-3.80, p<0.001). However, those in the older age groups i.e. 30-59 years (AOR 0.51, 95% CIs 0.27-0.95, p<0.05), those of \ufffd60 years old (AOR 0.07, 95% CIs 0.01-0.37, p<0.01) and those who had medium to highly resilient coping (AOR 0.54, 95% CIs 0.38-0.77, p<0.01) were less likely experience higher psychological distress (Table 5).\nTable 6 shows the univariate and multivariate analyses regarding factors associated with fear of COVID-19. Study participants who had been tested negative for COVID-19 but were self-isolating (AOR 3.12, 95% CIs 1.04-9.32, p<0.05) and those who had moderate to very high levels of psychological distress (AOR 2.56, 95% CIs 1.71-3.83, p<0.001) also had high levels of fear. Conversely, study participants who were born in Malaysia (AOR 0.39, 95% CIs 0.18-0.86, p<0.05) and who drank alcohol in the last four weeks (AOR 0.26, 95% CIs 0.10-0.68, p<0.01) had lower levels of fear in this study (Table 6).\nStudy participants who provided care to a family member/patient with known/suspected case of COVID-19 had medium to high resilient coping (AOR 1.87, 95% CIs 1.01-3.46, p<0.05), whereas participants with moderate to very high levels of psychological distress had low resilient coping (AOR 0.54, 95% CIs 0.38-0.76, p<0.01) (Table 7).\n---\nDiscussion\n\nThis cross-sectional survey found that a large proportion of Malaysian residents experienced moderate to very high levels of psychological distress as a result of the COVID-19 pandemic. Malaysians, whose financial situation was impacted by COVID-19, those who drank alcohol in the past four weeks, those who self-identified as patients and those with higher levels of fear, were more likely to experience higher psychological distress. Higher levels of psychological distress were also associated with higher levels of fear and so were people who self-identified as patients. A large majority of the participants also reported as having medium to highly resilient \n---\nLiving status 718\n\nLive without family members (on your own/shared house/others) (18.9)\nLive with family members (partner and/or children) (77.9)\n---\nBorn in Malaysia 720\n\nNo Have an income source (employed/Government benefits) (50.0)\n---\nPerceived distress due to change of employment status 699\n\nA little to none (69.0)\nModerate to a great deal (31.0)\n---\nSelf-identification as a frontline or essential service worker 720\n\nNo (70.0)\n(Continued )\ncoping during this pandemic especially those who provided care to family members affected by the pandemic. Findings of our survey in Malaysia are comparable to similar studies conducted in other parts of the globe. Financial difficulty is associated with anxiety as well as a predisposition to depression after several months of quarantine exacerbated by undue uncertainty [29]. Studies among the general population in China and India have shown that poor economic status and difficulties in meeting living expenses during the COVID-19 pandemic significantly increasing the degree of psychological distress [30,31]. Likewise, studies during the SARS and MERS epidemics have also shown that increased psychological distress was associated with increased financial difficulties. This could be explained by the emergence of a sense of uncertainty and lack of security during the pandemic [32]. Hence, our finding further supports the inverse association between increased financial difficulties during COVID-19 and the occurrence of psychological distress.\nIn line with our findings, Ahmed et al. conducted a study in a Chinese population where they had also reported high prevalence of alcohol use and alcohol dependence during the COVID-19 pandemic [33]. Given that this was a cross-sectional study, it was possible that psychological distress led to increased alcohol use as a coping mechanism to deal with COVID-19 induced psychological distress, but the converse was also likely that increased alcohol use worsened psychological distress [34].\nThis study also showed that people who self-identified as a patient i.e. having visited a healthcare provider in the past four weeks, were more likely to experience higher psychological distress. However, it was not clear from the survey questionnaire if patients had visited a healthcare provider for COVID-19 like symptoms or for other medical conditions. Being infected with COVID-19 or awaiting the possibility of becoming ill was likely to be more stressful because of the fear of mortality or morbidity associated with a disease [29]. Those infected with COVID-19 had higher levels of depression, anxiety, and post-traumatic stress symptoms when compared to those not infected. In fact, people with a history of being infected with COVID-19 had reported unresolved fear, guilt, and helplessness. They were likely to be affected by the stigma of being labelled as someone who had been infected and faced uncertainty about their prognosis and future [35]. Moreover, the findings of this study also highlighted that those who tested negative for COVID-19 but maintained self-isolation from others had higher levels of fear, and those with higher levels of fear of COVID-19 also had moderate to high psychological distress. Knowing the high infectivity capability of the virus, the asymptomatic presentation of some of the COVID-19 positive cases, and the consequences of the COVID-19 infection had created enormous fear among the general population and healthcare workers [36][37][38]. Unresolved fear which led to long-lasting stress might have predisposed individuals to psychological distress during the COVID-19 pandemic [39]. Hence, our study further strengthened the relationship between fear of the COVID-19 pandemic and increased psychological distress. Factors identified as protective factors against psychological distress in this study was older age (\ufffd30 years) and having higher level of resilience. Several studies on the psychological impact of COVID-19 in the general population reported that younger people (aged 21 to 40 years) were at higher risk of predisposing to depression and anxiety [40] highlighting consistency with other studies. Younger people may have greater focus on COVID-19 and higher degree of worry about the spread of COVID-19 presumably because of more and/or frequent access to news/social media, hence increasing their risk of psychological distress compared to older people [40].\nThose with higher resilience, particularly in the components of tenacity, strength, and optimism, have shown to experience less mental health complications during the COVID-19  I had recent overseas travel history and was in selfquarantine   pandemic [41]. Our study has also indicated that low resilience was associated with moderate to high levels of psychological distress while moderate to high resilience was not only associated with lower psychological distress, but also enabled the individual to provide care to family members or patients infected with COVID-19. Hence, our study highlighted the pivotal role of resilience in overcoming the psychological impact of the COVID-19 pandemic. The strength of this study was the use of validated tools to investigate the factors associated with psychological distress, fear and coping strategies in Malaysia. Due to nation-wide travel restrictions, online survey was the only feasible way for data collection, and we were able to recruit a large sample of Malaysian population during the critical pandemic period. However, there were some limitations in this study. As this study was an online survey, most younger people participated into this survey as they were more active on social media. The study was conducted in English, so those who were not well versed in English might not be able to take part in the study. It was beyond the scope of the study to check and ensure that the participants had sufficient ability in understanding English. Due to the self-reporting nature of the survey, possibility of reporting bias cannot be excluded. The survey responses were predominantly from west Malaysia, although the survey link was shared across all the states in Malaysia through various social media platforms and emails. This could be explained by the researchers' use of snowball sampling techniques which reflected their community acquaintances and accessibility to clinics/allied health service facilities more in West Malaysia than in the eastern part of Malaysia. Another important limitation of our study was, those who might have tested positive to COVID-19 or those whose family members or friends were tested positive with COVID-19 infection or who were interested to this topic were more likely to participate into this survey. We also acknowledge that we might have missed the more marginalized or vulnerable group of population in this study (e.g., those who were more isolated specially people from rural areas, from the areas of poor internet access, older people those who were not active in social media, or migrant or other minority groups); therefore, the findings of this study could be potentially underestimated and might not be representative to the general Malaysian Population.\n---\nConclusions\n\nThe study identified some of the key risk factors for developing psychological distress, fear and coping strategies during the COVID-19 pandemic in Malaysia. Vulnerable groups of individuals such as patients and those impacted financially during COVID-19 should be supported for their mental wellbeing. Behavioural interventions should be targeted to reduce the impact of alcohol drinking during such crisis period. Findings of this study would assist the researchers to plan future studies with vulnerable groups of Malaysians, specifically exploring the strategies to support their mental wellbeing during the pandemic and post-pandemic period. Specific interventions based on the emerging evidence arising from Malaysian and global studies can be tested to alleviate psychological distress, fear and improve resilience among Malaysian population.\n---\nAuthor Contributions\n\nConceptualization: Wendy Cross, Muhammad Aziz Rahman. \n---\nData curation:\n\n",
        "INTRODUCTION\n\nAlthough women consume less alcohol and drink less often than men, 1 women's drinking warrants serious attention from alcohol researchers and health care providers, in part because women are more susceptible to certain alcohol-related problems at a given level of consumption 2 and because women are less likely to receive help for problems with alcohol use. 3 While women may share many experiences and risk factors relevant to their alcohol use and associated problems, women are not a monolithic group. Multiple dimensions of social location (e.g., race/ethnicity, socioeconomic status, and sexual identity) profoundly shape women's lived experiences. 4 These can affect health and a wide range of health-related factors over the life course, such as social and environmental risk and health-promoting exposures, health behavior, resources that enhance health and help to manage disease, care-seeking, and the quality of health care received. Thus, unsurprisingly, among women there is heterogeneity of risk for problems related to drinking.\nThis article briefly reviews what is known about alcohol-related disparities among women and discusses mechanisms that could give rise to inequities in alcohol outcomes. In this article, disparity refers to social group differences in which groups that have greater social or economic advantages have more desirable health outcomes than groups without those advantages. 5 Research on alcohol-related disparities has focused on racial/ethnic and socioeconomic groups [6][7][8] and often has not been stratified by gender to examine disparities among women or men separately, as doing so would require very large samples for low-prevalence outcomes. Thus, this review reflects a predominant focus in the extant literature on race/ethnicity (often White, Black, and Latinx groups, with rare analysis of Latinx subgroups), socioeconomic status, and the limited study of disparities among women. Far less research has been conducted on sexual minority groups (defined by sexual orientation). Reflecting the work to date, unless otherwise stated, this review defines women based on physiological sex. Finally, this review focuses on problems associated with personal alcohol consumption and does not include the many secondary harms experienced because of other people's drinking.\n---\nDISPARITIES IN ALCOHOL-RELATED PROBLEMS\n\nIdentifying racial/ethnic and socioeconomic disparities in alcohol-related problems is not always a straightforward task, partly because of differential abstinence rates across racial/ ethnic and socioeconomic groups. For example, in the National Epidemiologic Survey on Alcohol and Related Conditions-III (NESARC-III), the percentage of people who drank alcohol in the past year ranged from 62% to 75% across racial/ ethnic groups and 56% to 81% across levels of education. 1 The National Alcohol Survey (NAS) reported 64% of heterosexual women and 78% of bisexual women drank alcohol in the past year. 9 In addition, race, ethnicity, and socioeconomic status are deeply intertwined in the United States. 10 In light of the above, the detection of alcohol-related disparities can be affected by the inclusion of abstainers in analyses and also by how investigators handle socioeconomic status when analyzing racial/ethnic differences. Although analytic decisions depend on research objectives (e.g., to establish general population rates, understand risk relationships, estimate residual racial/ethnic differences, or recognize the role of socioeconomic status in racial/ ethnic differences), sensitivity analyses are always a useful option to gauge the effects of such decisions on study results and enhance Alcohol Research: Current Reviews Vol 40 No 2 | 2020 interpretation. Effort was made in this review to be attentive to such decisions.\n---\nAlcohol Use Disorder and Negative Consequences of Drinking\n\nThe following section provides a review of research on the prevalence and risk of alcoholrelated problems in different subgroups of women defined by race/ethnicity, socioeconomic status, and sexual minority status. Problems examined in this literature include alcohol use disorder (AUD) and negative consequences of drinking. In nearly all of the studies reviewed, AUD was defined according to the Diagnostic and Statistical Manual of Mental Disorders, Fourth Edition (DSM-IV), 11 which includes and distinguishes alcohol abuse and alcohol dependence. In 2013, the Diagnostic and Statistical Manual of Mental Disorders, Fifth Edition (DSM-5) 12 was released, which replaces DSM-IV alcohol abuse and dependence diagnoses with a single AUD diagnosis that is classified as mild, moderate, and severe.\n---\nRace and ethnicity\n\nNational survey data show greater prevalence of DSM-IV AUD among White women compared to other racial/ethnic groups. For example, in Wave 1 of the NESARC, which was conducted from 2001 to 2002, age group-specific rates of DSM-IV alcohol abuse and dependence among women (including abstainers) were consistently higher in White women compared to Black, Latina, and Asian/Pacific Islander women in nearly all of four age groups examined. 13 The exceptions were American Indian/Alaska Native (AIAN) women, whose prevalence of DSM-IV alcohol abuse and dependence was greater than that of White women in three of four age groups, and Black women, whose DSM-IV alcohol dependence prevalence was higher than that of White women at midlife (ages 45 to 64) and older (ages 65 and older). However, many of these differences did not appear to be statistically significant. Taking into account standard error, the clearest differences were observed among White, Black, and Latina women, the three largest groups. DSM-IV alcohol abuse prevalence was higher in White women compared to Black women before midlife (younger than age 45), and higher than DSM-IV alcohol abuse prevalence of Latinas in all but the oldest age group (ages 65 and older).\nIn the same NESARC survey, the prevalence of DSM-IV alcohol dependence was significantly higher only in young-adult, White women (ages 18 to 29) at 6% vs. 4% in young Black women and 4% in young Latina women. 13 At 9%, the prevalence of DSM-IV alcohol dependence among young AIAN women was highest of all, but it had a wide confidence interval. By contrast, in 2000, 2005, and 2010 NAS data, White, Black, and Latina women (including abstainers and not stratified by age) showed statistically nondistinguishable prevalence and odds of having DSM-IV alcohol dependence and two or more negative consequences of drinking. 14 Because these studies were based on older data that, in some cases, were collected nearly 20 years ago, data from the 2017 National Survey on Drug Use and Health (NSDUH) 15 were analyzed to provide updated national estimates for women. As shown in Table 1, most of the significant racial/ethnic differences in DSM-IV alcohol dependence prevalence were no longer apparent when abstainers were excluded. When compared with White women who drink alcohol, only Asian women who drink had significantly lower rates of DSM-IV AUD, and AIAN women who drink had higher rates of DSM-IV AUD. In studies excluding lifetime abstainers, there is some evidence of greater alcohol problems among racial/ethnic minority women who drink compared with White women who drink. For example, Grant and colleagues conducted a longitudinal analysis of NESARC Waves 1 and 2 from the early 2000s and found that at Wave 2, young White women had the greatest risk for DSM-IV alcohol dependence onset compared with young Black and Latina women. 16 However, the risk for young White women was lower than that for older minority women. Both Black and U.S.-born Latina women ages 40 and older had greater risk of DSM-IV alcohol dependence onset than young White women (adjusted OR = 1.71 and 2.08, respectively). 16 In addition, older Black and U.S.-born Latina women had more persistent alcohol dependence (adjusted OR = 2.73 and 1.36, respectively), and older U.S.-born Latina women had greater recurrence of dependence (among those with lifetime dependence prior to Wave 1). This elevated risk among older minority women was in marked contrast to similarly aged, White peers, whose risk for alcohol dependence onset, persistence, and recurrence was much lower than that of young White women. The racial/ethnic patterning of risk was the same when DSM-IV AUD was the outcome, except that disparities were also evident among younger minority women ages 30 to 39. In this age group, Black women had greater AUD onset, and U.S.-born Latinas had greater AUD persistence than young White women. Notably, this NESARC study did not control for socioeconomic status indicators. 16 In a 2005 and 2010 combined NAS study of women who drink, which adjusted for demographics, education, and income and also rigorously controlled for heavy drinking, the only disparities found between Black and White women were in DSM-IV alcohol dependence (adjusted OR = 3.3), and this disparity held across the range of heavy drinking. 17 There was no significant disparity between Latina and White women in either negative consequences of drinking (an outcome similar to alcohol abuse) or DSM-IV alcohol dependence. (Due to sample size limitations of the study, 17 U.S.-born Latina women were not analyzed separately as they were in the NESARC study by Grant and colleagues. 16 )\nAs noted, all of the research on AUD in demographic subgroups reviewed above, including the 2017 NSDUH data on AUD, 15 is based on the DSM-IV diagnostic criteria rather than the DSM-5 criteria. Thus, it is not clear whether these findings (especially those based on data collected from the early 2000s) accurately reflect DSM-5 AUD patterns among women, as the latter have not yet been examined. However, results from two recent NESARC-III studies of women and men combined suggest that the patterning of AUD prevalence across racial/ethnic, socioeconomic, and other demographic subgroups may be similar across DSM-IV and DSM-5 criteria. 18,19 For instance, AUD prevalence among White, Black, and Latinx study participants based on DSM-IV criteria was 13%, 13%, and 12%, respectively, 18 and the prevalence based on DSM-5 criteria was 14%, 14%, and 14%, respectively. 19 Similarly, for educational levels, the DSM-IV AUD prevalence was 10% for less than high school, 13% for high school, and 13% for some college or more, 18 and the prevalence based on DSM-5 criteria was 12%, 15%, and 14%, respectively. 19 These results suggest that the presence or absence of disparities in women's prevalence of DSM-5 AUD might reasonably be gauged by recent research that uses DSM-IV AUD criteria (for instance, as captured by the 2017 NSDUH). But confirmation is needed, as the NESARC-III analyses were not restricted to women.\n---\nSocioeconomic status\n\nSimilar to the findings for race/ethnicity, the 2017 NSDUH data show significant differences in DSM-IV alcohol dependence and AUD by educational attainment, but when abstainers are excluded, nearly all differences become nonsignificant (see Table 1). 15 Importantly, in a recent systematic review, Collins concluded that although groups with greater socioeconomic advantages (defined by income, education, and other indicators at the individual, family, or neighborhood levels) had similar or greater levels of alcohol consumption than those with fewer advantages, the groups with fewer socioeconomic advantages were at greater risk for alcohol-related problems. 8 This finding has been referred to as the \"alcohol harm paradox\" 20 and is similar to the phenomenon among some U.S. racial/ethnic minority groups, particularly Black persons, of having greater risk for alcohol-related problems than White persons despite drinking less. 21 This socioeconomic status paradox has been studied mostly outside of the United States and has been observed for a variety of alcohol outcomes. A meta-analysis by Grittner and colleagues, drawing upon survey data from 25 countries, found that in several high-income countries, women who drink alcohol and who have less education were at greater risk for external drinking consequences (e.g., consequences affecting finances; work, school, or employment; close relationships; and risk of injury/fights). 22 In the full sample of countries, an inverse educational gradient was found when controlling for age and drinking pattern, as well as country-level, socioeconomic development factors.\nThe socioeconomic conditions of residential neighborhoods also are relevant. Analysis of the 2000 and 2005 combined NAS data found that women who drink alcohol and live in disadvantaged neighborhoods have twofold greater risk for alcohol problems (adjusted OR = 2.07 for two or more drinking consequences or DSM-IV alcohol dependence) than women who drink and live in more advantaged neighborhoods. 23  This study controlled for individuals' education, income, unemployment status, and demographics.\nA different study that used 2000 and 2005 combined NAS data further showed that among White women who drink alcohol, neighborhood disadvantage was associated with increased risk for negative consequences of drinking. 24 The authors noted that White women who drink and reside in disadvantaged (as compared to more advantaged) neighborhoods were challenged by greater family histories of alcohol problems, co-occurring drug use, and drinking to cope with stress, which are risk factors for alcohol problems.\nProviding a context for such findings, a longitudinal study of women in poverty highlighted the distinctive stressors faced by women who drink and have low incomes. 25 Stressful life events and neighborhood stressors (e.g., crime, drug trafficking, and shootings) were common, and these in addition to economic stress, contributed to psychological distress and increased women's risk for developing problematic alcohol use.\n---\nSexual minority women\n\nIn this article, sexual minority women, including bisexual women and lesbians, are defined based on sexual orientation. In a study by Wilsnack and colleagues, the investigators compared data collected from sexual minority women in the 2001 to 2002 Chicago Study of Health and Life Experience of Women (CHLEW) study with data collected from exclusively heterosexual women in the 2001 National Study of Health and Life Experiences of Women. 26 The investigators found higher prevalence of lifetime alcoholrelated problems, alcohol dependence symptoms, and hazardous drinking among sexual minority women. Bisexual women were most likely to report alcohol problems, with 70% reporting lifetime problems in contrast to 29% of heterosexual women.\nSimilar disparities in hazardous drinking were found in a more recent wave of the CHLEW study (2010 to 2012) and in a 2000 to 2015 NAS analysis. 9 Additionally, a separate study by Drabble and colleagues that used 2000 NAS data found that lesbians had 7.1 times higher risk of meeting criteria for DSM-IV alcohol dependence (bisexual women had 6.4 times higher risk) than heterosexual women. 27 A recent study that used 2015 to 2017 NSDUH data indicated disparities in DSM-IV AUD rates as well. 28 In that study, bisexual women had 2.2 times higher odds than heterosexual women and 1.5 times higher odds than lesbian women of having past-year AUD after adjusting for demographic characteristics. 28 Although this review focuses on sexual minority women, the newly emerging literature on alcohol use among gender minority women (i.e., noncisgender and nonbinary women) should be noted. A systematic review of transgender individuals (including gender minority women) by Gilbert and colleagues found estimates of binge drinking among transgender individuals ranging from 7% to 65%, with estimates of lifetime and past-year DSM-IV AUD prevalence at 26% and 11%, respectively. 29 More research is needed on these groups. As noted by Gilbert and colleagues, to facilitate research on alcohol use disparities among gender minority women and transgender individuals, new methods will be needed, as many of the current alcohol use measures to assess unsafe drinking rely on physiological sex-specific cut points.\n---\nHealth, Morbidity, and Mortality\n\nDisparities in alcohol-related health outcomes, morbidity, and mortality are studied less commonly than disparities in AUD and the negative consequences of drinking alcohol. Few studies focus on women; instead, studies typically include women and men and control for gender. Nonetheless, in analyses restricted to women, racial/ethnic and socioeconomic disparities in risk have been reported for some alcohol-related health conditions and outcomes. For example, based on suicide decedent data from the National Violent Death Reporting System, AIAN women had approximately twice the odds of acute alcohol intoxication relative to White women at the time of death. 30 Also, increased alcohol use is known to be associated with Alcohol Research: Current Reviews Vol 40 No 2 | 2020 mortality among people with HIV. 31 This risk disproportionately affects Black women, whose incidence rate for HIV far exceeds that of White women (estimated at 783.7 and 43.6 per 100,000 for Black and White women, respectively). 32 Research also indicates socioeconomic differentials in alcohol-related morbidity and mortality. An English study of hospital admissions from 2010 to 2013 that examined wholly and partially alcohol-attributable conditions found the greatest socioeconomic disparities among women with wholly alcoholattributable chronic and acute conditions. 33 These results suggest that socioeconomic status differences in harmful drinking patterns contribute to differential morbidity.\nApplying a similar comparative approach, Probst and colleagues conducted a metaanalysis of 15 studies from 7 countries and found greater socioeconomic disparities in women's alcohol-attributable mortality than in their allcause mortality. 34 Across different measures of socioeconomic status (e.g., individual-level education, occupation, employment status, or income), socioeconomically disadvantaged women had 1.8 times the relative risk of alcoholattributable vs. all-cause mortality when compared to more advantaged women. Similarly, a Scottish study of women and men combined found that socioeconomically disadvantaged participants who drink moderately had much greater risk for alcohol-attributable harms (i.e., hospital admissions or deaths) compared to socioeconomically advantaged participants who drink moderately or even heavily, regardless of the socioeconomic status measure used and even after controlling for differences in binge drinking, obesity, smoking, and other risk factors. 20 Other research has investigated disparities in the protective health effects of moderate drinking. Although protective effects for cardiovascular disease mortality and for diabetes onset have been found, 35,36 some studies indicate health benefits for Whites but not for racial/ethnic minorities. [37][38][39] Race/ethnicity differences in the protective effects of alcohol have also been observed in two studies of all-cause mortality. One study used NAS data 40 and the other was a gender-stratified study based on data from the National Health Interview Survey. 41 The latter study found that moderate drinking was associated with the lowest mortality among White women (a mortality rate of 40.1 per 1,000 person-years). In Black women, moderate drinking was associated with a mortality rate of 93.8 per 1,000 person-years), more than double the rate of White women with a similar drinking level and also higher than the mortality rate associated with high-risk drinking among Black women (67.6 per 1,000 person-years), although confidence intervals for Black women's rates were widely overlapping. 41 In contrast to these disparities, the United States has seen a racial/ethnic crossover in liver cirrhosis mortality rates for women. Although rates for Black women were highest in 2000, they have since dropped, and rates for White, non-Latina women and for White, Latina women have risen, exceeding the rates for Black women. 42 These results are consistent with reports of increased consumption and alcohol problems among White women based on the 2000 and 2010 NAS survey series. 14,43 \n---\nPOSSIBLE EXPLANATIONS FOR DISPARITIES\n\nAn obvious potential explanation for these disparities is that they reflect population differences in harmful drinking patterns. Sexual minority women, for instance, are more likely to drink alcohol, to drink to intoxication, and to drink heavily compared to exclusively heterosexual women (adjusted OR = 1.8 and 2.0 for intoxication and heavy drinking, respectively). 27 Yet, it is unlikely that consumption patterns alone account for disparities. Indeed, the finding of greater harm despite lower or similar levels of drinking lies at the heart of the alcohol harm paradox. As noted, the latter refers to socioeconomic disparities in alcohol outcomes but is similar to the phenomenon observed for some racial/ethnic minority groups of disparities in alcohol problems at the same level Alcohol Research: Current Reviews Vol 40 No 2 | 2020 of heavy drinking among both women and men. Related to this, it is important to note that previous research finding elevated alcohol consumption among AIAN relative to White individuals has been based on specific AIAN tribes or geographicarea subgroups, whose prevalence of alcohol use varies. 44 Recent analyses of the 2009 to 2013 NSDUH and the 2011 to 2013 Behavioral Risk Factor Surveillance System indicate that, nationally, AIAN and White participants had similar odds of binge drinking and heavy drinking (i.e., drinking five or more drinks on 5 or more days). Moreover, White participants had lower abstinence relative to AIAN participants, with an adjusted odds ratio for abstinence among White participants relative to AIAN participants of 0.64 (95% CI: 0.56, 0.73). 45 Thus, consideration of other ways that disparities in alcohol-related problems can arise is needed. Recent research calls attention to potential explanations involving the life course, differential vulnerability, and access to care. As noted earlier, this review reflects a predominant focus in the literature on racial/ethnic and socioeconomic disparities. Future studies are needed to assess relevance to other disadvantaged social groups.\n---\nHarmful Drinking Patterns Over the Life Course\n\nReflecting core concepts of life-course developmental theory, 46 both the age at which heavy drinking occurs and the duration of heavy drinking across the life course are relevant to disparities in alcohol-related problems. This makes sense intuitively, as the longer a person engages in health risk behaviors, the greater the chances of experiencing related problems. Also, certain age periods are likely to pose more or less risk for different kinds of alcohol-related problems. Bouts of heavy drinking, for instance, are likely to be tolerated less and to have more consequences when coupled with greater responsibilities to others, such as family and employers.\nNotably, three recent studies based on National Longitudinal Study of Adolescent to Adult Health data examined racial/ethnic differences in the heavy-drinking trajectories of young women, with somewhat mixed results (possibly reflecting methodological differences, such as adjustments for socioeconomic status). [47][48][49] Two studies showed that heavy drinking of young White women consistently exceeded that of Black women. 47,48 One study indicated that the rapidly declining trajectory of White women converged with the trajectory of Latina women by age 30, 47 and another showed a convergence of White, Latina, and Black women's trajectories by their early 30s. 49 A fourth study based on the 1979 cohort of the National Longitudinal Study of Youth (NLSY) examined women's heavy-drinking trajectories from ages 21 to 51. 50 This study also found that heavy drinking among White women exceeded that of Black and Latina women in their early and mid-20s, but the trajectories of all 3 groups declined thereafter, with no significant racial/ ethnic differences in heavy drinking between ages 30 to 51. However, sensitivity analyses excluding lifetime abstainers and women who never drank heavily showed a crossover in the heavy-drinking trajectories of Black and White women. 50 The trajectory for Black women rose during their early 20s, a period when White women's trajectory declined, thus causing a crossover at age 30. Thereafter, Black women's trajectory declined and reconverged with the flattening trajectory for White women at age 40. Consistent with these results, a 2010 NAS analysis of heavy drinking trajectories among women who reported ever drinking in their lifetime found that Black women, compared to White women, had twofold greater odds of persistent, frequent, heavy drinking (vs. declining heavy drinking) beyond their 20s and into their 40s (adjusted OR = 2.65, p < .01). 51 Taken together, these life-course drinking studies highlight racial/ethnic differences in the heavy-drinking trajectories of women in their early and mid-20s, which are consistent with the greater DSM-IV AUD risk observed during this period among young White women. Importantly, early adulthood is a time when health is relatively robust, and many women have yet to take on large, adult responsibilities. Drinking trajectory studies that extend beyond the 20s are rare, but there is some evidence of Black-White disparities in the age and duration of heavy drinking among women who reported ever drinking in their lifetime. These disparities were found for women in their 30s, possibly extending to their 40s.\nProspective studies beyond young adulthood are needed, especially for younger cohorts, as racial/ethnic differences in heavy drinking may be changing. 1,52 Nonetheless, the observed Black-White disparity in heavy drinking after young adulthood is consistent with the findings from a NESARC study of women who drink (described earlier), showing greater DSM-IV AUD onset among Black women in their 30s and 40s, as well as greater AUD persistence among Black women in their 40s and older, compared to White women in these same age groups as well as younger (ages 18 to 29). 16 These disparities are particularly significant when juxtaposed with other life-course findings. Namely, by midlife, there are striking racial differences in cumulative lifetime exposure to socioeconomic disadvantage, 53 and disparities in health become more pronounced. 5,54 \n---\nCumulative Disadvantage\n\nPopulation differences in exposure to health risk factors and their cumulative effects are an important mechanism in health disparities. 5 Cumulative disadvantage refers to the notion that social status positions such as race/ethnicity and socioeconomic status profoundly influence opportunities and resources over the life course and, thus, also affect exposures to health risk factors. 55 Growing up in poverty in neighborhoods with inferior schools, greater crime and violence, and limited economic opportunities can lead to poor quality and low-paying jobs, a lack of health insurance, and ongoing exposure to stressors. Black women and men with low incomes are particularly affected by these factors due, in part, to racial residential segregation 56 and geographic inequalities of opportunity. 57 Consistent with this, research has indicated that a large majority of Black children who were raised in poor neighborhoods continue to reside in similar neighborhoods as adults. 58 In an early articulation of the effects of cumulative disadvantage and its relationship to health disparities, Geronimus proposed the \"weathering hypothesis\" to account for the accelerated health deterioration of Black persons relative to White persons. 59 This is exemplified by high rates of chronic disease found in young and middle-aged Black women residing in lowincome, urban areas, which contribute to their early mortality rates. According to the hypothesis, the widening racial health disparity seen through middle adulthood reflects the cumulative effect of adverse exposures from conception onward. These adverse exposures include chronic social stressors (e.g., discrimination), environmental hazards, inadequate health care access and treatment, and unhealthy behaviors. Notably, greater alcohol availability, targeted advertising, and less access to healthy food in low-income and minority neighborhoods can contribute to and aggravate unhealthy behaviors. [60][61][62] Research has since shown that chronic, enduring stress affects the body's physiological stress response, with adverse effects on the cardiovascular, metabolic, and immune systems. 63 Moreover, the physiological consequences of chronic stress, which are referred to as allostatic load and assessed via biomarkers, have been found to be greater among poor and non-poor Black women than White women, and have been associated with accelerated aging. 64,65 Consistent with these findings, data from the 2017 National Health Interview Survey showed that 14% of Black women (and 13% of Latina women) reported fair or poor health, in contrast to 8% of White women. 66 Even when the sample was stratified by poverty status (i.e., poor, near poor, and not poor, with poor defined as having income below the federal poverty threshold), Black women and men tended to report worse health than White women and men.\nAs suggested, cumulative disadvantage can also affect health indirectly through risky health behaviors that people use to cope with stressors. 67  A longitudinal study based on NESARC data found that the effect of poverty on heavy drinking incidence was worse for Black women who drink than for their Latina and White counterparts. 68 A different longitudinal study based on the 1979 NLSY cohort data reported that cumulative poverty across the life span was positively associated with onset and persistence of alcohol dependence symptoms after young adulthood (in a combined sample of women and men who drink). 69 Further, a study based on 2010 NAS data found that cumulative socioeconomic disadvantage partly explained the disparity in persistent heavy drinking until midlife between Black and White women. 51 This confluence of disparities in cumulative disadvantage and health in middle adulthood provides an important backdrop for understanding disparities in alcohol problems after young adulthood. It raises the question of differential health vulnerability-the idea that certain social groups are more susceptible to health-related consequences when they are exposed to risk factors such as, in this case, heavy drinking. 70 To the extent that health \"weathering\" begins to accelerate after young adulthood and at a faster rate for demographic groups that have more enduring chronic stress, heavy drinking beyond young adulthood may contribute to alcohol-related health disparities at midlife and later. In keeping with this, a recent NLSY study by Kerr and colleagues found that among Black and Latina women, but not White women, diabetes onset was associated with a history of heavy drinking in the previous 10 years, even when controlling for health risk behaviors, socioeconomic status, and other demographics. 71 Differential health vulnerability may reflect various mechanisms that require future study. It may be rooted in biological interactions with alcohol that affect health. For example, heavy drinking can exacerbate certain health conditions such as hypertension, type 2 diabetes, and chronic kidney disease, which are more prevalent among Black Americans. Also, as discussed by Jackson and colleagues, differential vulnerability may reflect unmeasured health risk behaviors like smoking and unhealthy eating, which may cooccur with heavy drinking and are thus potentially confounding variables. 41 Alternatively, unhealthy behaviors could, in some instances, be effect modifiers that interact with alcohol to alter risk for health conditions. For instance, the aforementioned NLSY study by Kerr and colleagues found an interaction between alcohol and obesity for diabetes risk for women. 71 Bensley and colleagues' study of male, Veterans Health Administration patients who had HIV provides further illustration of this complexity. 31 Black patients with low-risk drinking (defined as a score of one to three on the Alcohol Use Disorders Identification Test consumption questions [AUDIT-C]) had greater mortality than White patients who had similar drinking levels, indicating differential vulnerability. The disparity was attenuated after adjusting for the greater presence of hypertension, hepatitis C, tobacco use, and other drug use among Black patients. To better understand alcohol-related disparities and the epidemiologic paradox of greater problems despite lower levels of drinking for some groups, research is needed to examine population differences in health and health behaviors and potential interactions with alcohol consumption patterns.\n---\nOther Social and Biological Factors\n\nStudies have documented gene variants that are more prevalent among Black persons 21 that affect the metabolism of alcohol, leading to a buildup of acetaldehyde in the bloodstream. While the gene variants have been associated with lower rates of alcohol dependence and heavy drinking, experimental research by Pedersen and McCarthy has found that the variants also are associated with more intense subjective responses to alcohol. 72 Specifically, they found that Black participants experience greater stimulating effects from alcohol than White participants, even after controlling for differences in past-month alcohol use. Further, greater increases in stimulation are associated with more alcohol-related problems among Black participants. As the researchers suggested, this acute stimulation could contribute to disparities in Alcohol Research: Current Reviews Vol 40 No 2 | 2020 the negative consequences of drinking alcohol at a given level of consumption. 72 In addition, Black women in this study experienced greater sedating effects from alcohol than White women. In view of the greater cumulative and chronic stress experienced by Black women compared with White women, 51,65 this finding of greater sedating effects of alcohol might be a factor in Black-White disparities in persistent heavy drinking and AUD among older women who drink.\nSocial position and sociocultural context also affect the likelihood of experiencing alcohol problems, particularly negative social consequences, at a given level of consumption. For years, researchers have called attention to the greater negative consequences of drinking borne by racial/ethnic minority groups who have less permissive drinking norms and are subject to greater societal scrutiny and stigmatization. 73,74 People with greater resources and higher status are better able to shield themselves from the negative consequences of drinking that others experience. 75 For example, negative consequences could be minimized at work (because of greater flexibility and autonomy and less scrutiny), in family duties (by paying for childcare or home-delivered meals and groceries), and when going out for the night (by hiring a driver).\nThese differential standards and consequences of drinking may be seen among women, perhaps more now than in the past when gendered roles and drinking norms were more similar across women. Reflecting on recent decades, Schmidt observed that social and economic changes resulting in greater freedoms for women have led to the \"equal right to drink\" only for women in the middle and upper classes. 76 By contrast, women with low incomes and women who receive welfare benefits, particularly racial/ethnic minority women, arguably have been more surveilled, stigmatized, and penalized for alcohol and other drug use.\nFinally, stress experienced due to being a member of a stigmatized minority group may help to explain alcohol-related disparities between sexual minority women and exclusively heterosexual women. Minority stress theory applied to drinking behavior suggests that the heavy drinking patterns of sexual minority women (relative to heterosexual women) are related to the stress of holding one or more minority identities. 77,78 Minority stress theory has been used in many studies. Research shows that sexual minority women experience stressors such as discrimination and harassment because of their sexual orientation, and that these women are more likely to report psychological distress than heterosexual women. 74 A study of sexual minority women and sexual minority stressors associated with substance use and mental health outcomes (e.g., unfair treatment, events of prejudice, and victimization) has provided further empirical support of this theory. 79 In this study, sexual minority stressors mediated the adverse effects of more masculine gender expression (i.e., a set of culturally assigned qualities to the category of masculine) on mental health and substance use outcomes. Other studies have found that sexual minority women experience additional stressors associated with increased alcohol use. In comparison to exclusively heterosexual women, sexual minority women are more likely to have experienced child sexual abuse, depression in their lifetime or in the past 12 months, and early onset of alcohol use. 26,80 Together, this varied literature suggests that social and biological factors may contribute to alcohol-related disparities among women in several ways. These factors may increase exposure to high levels of stress and discrimination (and drinking in response), they may increase sensitivity to the physiological effects of alcohol, and they may increase exposure to punitive societal responses to an individual's own alcohol use.\n---\nDifferential Access to and Quality of Care\n\nDifferences in access to care and in the quality of care received constitute another important explanation for disparities in alcohol-related problems. Although health care access and quality account for a relatively small percentage of the Alcohol Research: Current Reviews Vol 40 No 2 | 2020 variation in life expectancy in the United Statesestimated at 10% 81 -health care is a valuable resource. Indeed, having a regular source of primary care has been associated with reduced racial/ethnic and socioeconomic disparities in health. 54 The Institute of Medicine's report, Unequal Treatment, famously documented racial/ethnic disparities in the quality of health care received in the United States, even after accounting for differences in socioeconomic status, insurance, disease stage, comorbidities, and facility type. 82 Such findings have motivated the national goal of ensuring equitable access to high-quality care to mitigate disparities in early or delayed diagnosis, types of treatment, and care outcomes. 83 Part of the problem of health care disparities is structural, related to income, insurance, and the type and quality of care that is affordable and geographically accessible. Another part of the problem is social, related to implicit (unconscious) bias on the part of health care providers and how this bias affects patient-provider communication and interaction, treatment decisions, and health care outcomes. 84,85 Related to both structural and social factors, health care utilization also reflects patient perceptions, attitudes, and willingness to seek care. In the case of racial/ethnic disparities in alcohol-related care or treatment, cultural acceptability (including language compatibility) and perceived stigma toward people with AUD may be particularly relevant. 86,87 Whereas considerable research has investigated racial/ethnic and gender disparities in the receipt of alcohol-related care, far less is known about disparities among women specifically. In a rare, gender-stratified analysis of alcohol treatment utilization, Zemore and colleagues' analysis of NAS data found racial/ethnic disparities in treatment use among women with a lifetime AUD. 88 When compared with White women, Latina and Black women were significantly less likely to obtain specialty alcohol treatment, even after controlling for survey year, age, socioeconomic status (i.e., education and income), and insurance status (adjusted OR = 0.31 and 0.38 among Latina and Black women, respectively; p < .05). Moreover, this disparity was also observed for Alcoholics Anonymous use (adjusted OR = 0.38 and 0.37 for Latina and Black women, respectively). 88 Other studies (using samples of women and men combined) have further shown disparities in treatment completion, which is an important predictor of post-treatment substance use and health outcomes. 89,90 A variety of factors might contribute to racial/ ethnic disparities in treatment use specifically among women. One factor is the stigma of AUD, which may be a particularly salient deterrent for social groups that have more conservative drinking norms and that might already be socially marginalized. Notably, there is evidence of more conservative drinking norms for Black women compared to those for White women 91 and less permissive attitudes toward Latina women's drinking, which tend to be held by lessacculturated Latina women. 92 The stigma of AUD could lead to concealment or denial of alcohol problems and to family concerns about privacy and pressure to not seek treatment. All of these issues may be magnified for women due to the more intense social control of women's drinking.\nOther potential treatment barriers are a lack of childcare and concerns that children could be taken away. These concerns are not unfounded, given research showing that Black mothers who use alcohol or other drugs are reported to child protective services more often than similar White mothers. 93 In addition, women generally are more likely than men to experience treatment barriers because of transportation difficulties and inadequate insurance. 94 The latter may be particularly relevant to racial/ethnic minority women, as studies have found that Latinx and Black individuals are more likely than White individuals to report logistical and structural barriers. 95,96 Considering the pronounced racial/ ethnic disparities in alcohol problems among women after young adulthood, additional disparities in alcohol-related care and treatment compound the problem. This large unmet need among minority women, which may reflect a variety of causes, must be addressed. \n---\nCONCLUSION\n\nThis review provides evidence of alcohol-related disparities among women. The research in this area is relatively sparse, but disparities in AUD prevalence, the negative consequences of drinking, and alcohol-related health, morbidity, and mortality outcomes are apparent. This review also highlights the importance of a life-course perspective for understanding disparities in alcohol problems. By examining what happens within and between social groups across the life span, the widening of social group differences in cumulative socioeconomic disadvantage, health, and alcohol-related problems-especially after young adulthood-becomes more noticeable. Future research is needed to examine how these various disparities may be interrelated.\nImportantly, a life-course lens also requires attending to social roles and health as these change with age. Attention to such changes can help to advance understanding of how alcohol consumption results in negative consequences and why some groups are affected more than others. Finally, social position and sociocultural context remain important considerations because they can affect internal and external responses to drinking. Social position and sociocultural context also influence access to, use of, and the quality of alcohol-related and general health care. All these factors can affect the persistence of alcohol-related problems and the progression of disease.\nIn thinking about potential remedies, education emerges as one important factor. Some research has found that education, compared with income, is more strongly and negatively associated with the onset of disease (i.e., the likelihood that an individual will develop a chronic health condition). By contrast, income is a stronger predictor than education of how a disease progresses once an individual has the condition. 97 In light of the benefits of education for health and health behavior, 50,98 improving access to quality education at an early age and supporting higher educational attainment is an important strategy for improving health and addressing health disparities among racial/ethnic minorities and socioeconomically disadvantaged persons.\nIn addition, increasing insurance coverage and access to affordable, quality health care for underserved groups, a goal of the Patient Protection and Affordable Care Act, represents another crucial path to reducing health disparities. However, efforts devoted to improving health care access and quality will yield limited gains so long as stress and social stigmatization among minority populations persist, and profound differences in neighborhood conditions and available opportunities remain. These are the fundamental causes that need to be addressed to truly eliminate alcohol-related and general health disparities.\n---\nFinancial Disclosure\n\nThe authors declare that they have no competing financial interests.\n---\nPublisher's note\n\nOpinions expressed in contributed articles do not necessarily reflect the views of the National Institute on Alcohol Abuse and Alcoholism, National Institutes of Health. The U.S. government does not endorse or favor any specific commercial product or commodity. Any trade or proprietary names appearing in Alcohol Research: Current Reviews are used only because they are considered essential in the context of the studies reported herein.",
        "Introduction\n\nThe lesbian, gay, bisexual, transgender, queer (LGBTQ) community is a marginalized group of people who face much discrimination around the world [1]. Although Thai society has become more open to most forms of divergence and diversity, and LGBTQ self-expression at educational institutions, in the workplace, and at healthcare facilities is tolerated, inequalities and discrimination toward them are still prevalent [2]. Discrimination is frequently directed at those who live their lives in a way that is different from the majority, such as being LGBTQI [3], from a marginalized ethnic group [4], or a sex worker [5]. Being discriminated against can lead to not seeking help for health problems because of the fear of humiliation and unfair treatment when visiting healthcare facilities [6][7][8][9][10].\nThe interconnectedness of several social characteristics, such as race, gender identity, and class, is known as intersectional identity and is regarded as a contributing factor to inequalities and interrelated systems of discrimination [11][12][13]. Intersectionality comprises many social statuses at once since people do not experience discrimination based on an independent social status, such as transgender women (TGWs) who are also a member of an ethnic minority [14]. Identifying individuals with an intersectional identity will enable us to comprehend them in the social contexts of family, school, workplace, and community.\nDiscrimination and experiences of mistreatment at educational institutions can be particularly challenging for LGBTQ students; their experiences include hate speech and sexual harassment from teachers and other students [15][16][17][18]. Sixty-one percent of TGWs were discriminated against when they were students (the highest among LGBTQ people [1]), and because this is a sensitive period for most individuals, bullying and victimization due to their self-expression and behavior caused the most pain. People from marginalized ethnic groups are more likely to be undereducated, be stateless, not have the correct documentation, and be unable to find steady high-paying employment [4], which can make their lives difficult.\nTGWs frequently face discrimination concerning employment, such as limited access to job opportunities, being turned down by potential employers, being laid off, and being denied career training or promotion opportunities [2]. From a survey on the economic inclusion of LGBTQI groups in Thailand by the World Bank in 2018 [19], 77% of transgender people were denied job applications due to their gender identification, while 40% said they had experienced harassment, bullying, and/or discrimination at work. From a survey of LGBTQI people in the US in 2021 [20], 45.5% of LGBTQI workers experienced unfair treatment at work, including being fired, not hired, or harassed because of their sexual orientation and/or gender identity at some point in their lives. Interestingly, 8.9% of employed LGBTQI people reported that they had been fired or not hired because of their sexual orientation or gender identity in the past year. Of these, 11.3% were colored and 6.5% were white, thus reflecting discrimination based on their intersectional identity due to race. From the interviews concerning discrimination against TGWs who have become sex workers in Thailand, it appears that although the stigma of being involved in the sex industry is prevalent in Thai society, the high compensation and ability to express their gender identity have encouraged some of them to do so [21]. However, the downsides are no access to basic social security, no legal support, and being shunned by Thai society [5]. In contrast, the findings from a previous study in Thailand infer that LGBTQI people with a higher socio-economic status experienced less discrimination and were more unlikely to experience discriminatory encounters [22].\nA previous study by Riggs et al. (2018) revealed that transgender and nonbinary people living in Australia and the UK were more likely to be subjected to family violence than other individuals [23]. In Thailand, a survey among LGBTQI people in 2019 indicated that 56.4% of TGWs suffered more discrimination from their families (53.2%) compared to other gender minorities (50.0%, 49.8%, and less than 45.0% for non-binary, bisexual men, intersex, and other groups, respectively) [1]. Moreover, they are often forced to dress contrary to their gender identity and to change their appearance to that of a man by family members (often violently) and are also bullied by people in their community. The majority of transgender individuals have suffered victimization due to their gender identification, including verbal harassment, being stalked, and harassment by strangers on the street [24]. Moreover, TGWs are still unable to change the title of their gender identity on government documents, visas, and passports, which has created a barrier to their rights to healthcare, education, work, freedom of movement, and non-discrimination [25].\nSome Thai Buddhists believe that homosexual people are unable to control their sexual impulses and tendencies [26]. Nevertheless, in 2010, several news reports about the inappropriate behavior of novices and monks, such as monks having sex with novices and monks and novices making themselves look effeminate, wearing makeup, and wearing their robes tightly or in a way that suggested that they had breasts were published. Accordingly, some Thai talking heads discussed the inappropriateness of this behavior and that it is indicative of a morally degenerate Buddhist Sangha [26][27][28]. The sight of effeminate monks reinforces the idea that monasticism is the purview of \"real men\" only, and so TGWs who have undergone gender affirmation surgery are not allowed to enter the monkhood. Nevertheless, those who have not can be ordained if they have \"good morality\" and their appearance is not overtly different from a cisgender monk [27,29]. Many parents do not accept their sons' sexual orientation or gender identity and force them into monkhood in the hope that the monastic life will cure them of their immoral sexual tendencies [26]. In addition, some TGWs have felt obliged to become monks since childhood due to the economic status of their family and/or their ethnicity. However, TGWs who become monks are usually discriminated against by their families and communities due to their chosen gender identity.\nTGW patients are usually stereotyped and subjected to service discrimination, procrastination [30,31], profanity, ignorance [32], and/or even negative reactions from other patients when seeking medical care [33]. Furthermore, many researchers have found that people in minorities with multiple gender identities had more negative experiences than those with a single one [13,34,35]. What is even worse, people with multiple gender identities lack access to healthcare information and knowledge about fees [5], which affects their medical treatment outcomes [32].\nThe Mplus Foundation, Chiang Mai, and the SISTERS Foundation, Chonburi, are organizations that specialize in the prevention of the spread of human immunodeficiency virus (HIV), which can lead to acquired immunodeficiency syndrome (AIDS); other sexually transmitted diseases (STDs); and sexual health, as well as advocating LGBTQ rights in Thailand. In cooperation with both foundations, a study was conducted on the issues of discrimination against diverse identities. The study is categorized into four discrimination scenarios: (1) at an educational institution, (2) in the workplace, (3) in everyday life, (4) and at a healthcare facility. The data were collected via in-depth interviews. The outcomes should reflect the problem of discrimination against TGWs with an intersectional identity and related organizations and help stakeholders to provide plans to mitigate discrimination against them in the long run.\n---\nMaterials and Methods\n\n\n---\nParticipants\n\nThe qualitative data used in this study were collected by the Mplus Foundation in Chiang Mai and the SISTERS foundation in Chonburi, Thailand, between 1 April 2022 and 31 May 2022. Nineteen participants aged 18 years old or older with intersectional identities, including (1) ethnic minority TGWs (9), (2) TGW sex workers (6), (3) ethnic minority TGW sex workers (2), and (4) ethnic minority TGW monks (2), were included in the study (Figure 1). Before beginning the interviews, the reasons for this study were explained to the participants, after which written consent was obtained from them. \n---\nData Collection and Measurements\n\nThe interviews were carried out by an in-depth interview specialist (N.M.) who has been trained in counseling as well as mental health issues by the Mplus foundation. Since the data collection was conducted during the COVID-19 epidemic situation in Thailand, \n---\nData Collection and Measurements\n\nThe interviews were carried out by an in-depth interview specialist (N.M.) who has been trained in counseling as well as mental health issues by the Mplus foundation. Since the data collection was conducted during the COVID-19 epidemic situation in Thailand, there were two methods of interviewing, in-person and by phone, which depended on what was convenient for each participant. When conducting the in-person interviews, the interviewer and the participant were in a private room, wearing masks, and staying more than 1 m apart for the entire process. This complied with the guidelines for preventing the spread of COVID-19 in Thailand. The schedule for the phone interview was arranged at the convenience of the participant and to ensure that the interviewer and participant were alone to maintain their privacy concerning personal information. The interviews with Buddhist priests were conducted over the phone and had to be formal, using religious terminology to ensure the security of the monk's status while providing information.\nThe participants were interviewed in Thai for between 45 and 60 min. Interviews were digitally audio recorded and then transcribed verbatim, and pseudonyms were allocated to the interviewees to preserve their anonymity. The semi-structured interviews consisted of two parts: (1) demographic information, including age, ethnicity, education, and occupation, and (2) experience of perceived discrimination. Part 2 of the semi-structured interview was a modified version of the perceived ethnic discrimination questionnaire-community version (PEDQ-CV) by using the questions in the section concerning discrimination in the past among ethnic groups living in Asia [36,37].\n---\nStatistical Analysis\n\nWe categorized the data into four discrimination scenarios: (1) at an educational institution, (2) in the workplace, (3) in everyday life, (4) and at a healthcare facility. We rechecked the information and encoded the themes and sub-themes from the semi-structured indepth interview transcripts using the NVivo program (release 1.3) prior to performing the statistical analysis.\nThe descriptive statistics of the characteristics of the participants are presented as frequencies and percentages for categorical variables and medians and interquartile ranges for the age variable. Thematic analysis was used to identify patterns or themes from the interviews. A re-iterant process was used for discussing areas of agreement and disagreement.\n---\nEthical Approval\n\nThis study was approved by the Chiang Mai University Research Ethic Committee (CMUREC 65/012). Informed consent was obtained from all of the subjects involved in the study.\n---\nResults\n\n\n---\nParticipants\n\nThe median age of the 19 participants was 30 years old (IQR = 28-40). The majority of the intersectional identities were ethnic minority TGWs (9/19; 47%), followed by TGW sex workers (6/19; 31%). Most of them were from the Thai Yai ethnic group (8/19; 42%), followed by Thai Yong and Thai Lue (21% and 5%, respectively). Most of them were self-employed as sex workers (8/19; 42%) and monks (2/19; 11%). Most of them had lived in Thailand for 11 years or more (7/19; 36%) (Table 1). The in-depth interviews were divided into four main themes with several sub-themes (Figure 2).  \n---\nDiscrimination at an Educational Institution\n\nDiscrimination was reported via the use of inappropriately harsh words by teachers, friends, and/or seniors at school because of having an intersectional identity, as per a TGW sex worker aged 30 years old. \"Teachers tried to persuade me to transform back to being a man because society would not accept me, would bully me, and it would be difficult to \n---\nDiscrimination at an Educational Institution\n\nDiscrimination was reported via the use of inappropriately harsh words by teachers, friends, and/or seniors at school because of having an intersectional identity, as per a TGW sex worker aged 30 years old. \"Teachers tried to persuade me to transform back to being a man because society would not accept me, would bully me, and it would be difficult to find work.\" (a TGW sex worker aged 27 years old); \"I am from a rural community and my skin color is quite dark, and so I was often teased about being a black ladyboy.\" (a TGW sex worker aged 29 years old); and \"I was new to Thailand from Myanmar and couldn't speak or write Thai, so I was often bullied for being \"stupid\".\" (a TGW monk from ethnic minority aged 29 years old) are some of the answers of the interviewees.\nSome have been discriminated against through physical violence, and some are misunderstood and judged by their outward appearance by friends and teachers at educational establishments. For example, \"A third-gender friend and I were often hit, kicked, teased, and caught naked by mischievous teenagers at school.\" (a TGW sex worker aged 27 years old) and \"A third-gender friend and I were often subjected to anger, violence, and being spoken rudely to.\" (a TGW sex worker aged 26 years old).\nSome have been impacted by both positive and negative discrimination at school. An example of a positive impact is \"Being teased by a friend motivated me to study hard, and I received first place in the school.\" (a TGW monk from ethnic minority aged 29 years old), and an example of a negative impact and unacceptable event is \"The teacher told me that if they said it and I couldn't accept it, then I should quit, and so I decided to stop studying and quit.\" (a TGW sex worker aged 30 years old).\n---\nDiscrimination in the Workplace\n\nAn example of workplace discrimination is people refusing to work with a group of individuals with intersectional identities. For example, \"When I applied for a particular job, I was rejected because they wanted a woman due to the work being delicate, and when I applied to be a security guard, the company wouldn't accept me because it required a man.\" (a TGW from an ethnic minority aged 35 years old); \"My female friend applied to be a hotel worker. The hotel needed more workers but when I went to apply for a job, I was not accepted because I'm a third-gender person.\" (a TGW sex worker from an ethnic minority aged 28 years old); and \"In addition, the organizational restrictions are another reason why LGBTQ people are denied employment. When I went to apply for a job, the interviewer said \"when you come to work, you must dress as a man and talk less. If you act as a third-gender individual at work, you will not be accepted.\" (a TGW from an ethnic minority aged 29 years old).\nHarsh words or bullying remarks made by colleagues or even customers are other sources of discrimination at work. For example, \"I've been accused of being homosexual and selling sex services, or some customers didn't read my profile. When they saw me as a ladyboy, they rebuked me with harsh words.\" (a TGW sex worker aged 27 years old).\nIn addition, sex workers are at risk of physical abuse and degradation. For instance, \"Customers have sometimes argued and threatened bodily harm, and have not paid or paid only half the fee when they found out I wasn't a woman.\" (a TGW sex worker aged 28 years old), and \"Some customers paid me less or had sex as a prank because they saw me as a transgender woman, which I can't accept.\" (a TGW sex worker aged 26 years old).\n---\nDiscrimination in Daily Life\n\nSexist bullying words or mimicry can make someone feel despondent, especially when directed at particular minority groups. For instance, \"I am often referred to as a homosexual. \" (a TGW sex worker aged 21 years old); \"I'm often subjected to sexism, especially in Tai Yai society.\" (a TGW from an ethnic minority aged 35 years old). Some of the participants have been humiliated by people they did not know. For example, \"People looked at my dress and laughed and pointed at me. He called his friends to look at me and said loudly, \"Is this a ladyboy or . . . ?\".\" (a TGW from an ethnic minority aged 29 years old).\nEven though perpetrators think of it as a prank, it is actually a type of sexual harassment. For instance, \"I was touched on my butt, chest, or genitals.\" (a TGW from an ethnic minority aged 30 years old).\nDuring the Songkran festival, this group is often subjected to physical violence. For example, \"I went to a Songkran festival with my friends when I was a teenager. People threw shoes and water bowls, and even placed a bucket of water on my head.\" (a TGW sex worker aged 28 years old).\nBeing rejected by family and local people is another form of discrimination experienced in daily life. Some forms are related to religion and/or traditional beliefs. For instance, \"People in the community say, \"Why not be a man?\", \"Why do you want to be a woman?\", or \"It's a sin\". It's a sin that will fall on us and we will need to make merit to atone for it. They frequently comment about it even though the truth is not related.\" (a TGW sex worker aged 27 years old), and \"People in the community say they don't like me and don't want anything to do with me. They spoke specific words concerning hermaphrodites.\" (a TGW from an ethnic minority aged 36 years old).\nIn addition, people with conflicting identities have restricted access, such as when they want to use public restrooms. This group of people often encounter problems in their daily lives, such as \"When I need to go to the public restroom, I am always confused about whether I should go to the men's or women's. If I go to the women's bathroom, the women will be angry and resentful. If I go to the men's bathroom, men look at me.\" (a TGW from an ethnic minority aged 52 years old); \"Including rights that I should have access to but are restricted due to them only being available for Thai nationals and people with characteristics that match their birth gender only. This causes obstacles and restrictions for people with intersectional identities. For traveling outside the area or across the province, we need to apply for a permit for leaving the area at the district level, which causes inconvenience and hassle.\" (a TGW from an ethnic minority aged 36 years old). Other examples are as follows: \"My friend and I encountered difficulties with our name title prefix when traveling abroad. The interviewer asked us more questions than the others and often asked us if we are going to sell sex services?\" (a TGW sex worker aged 26 years old). In addition, there are restrictions on marriage registration in Thailand that can make life as a couple difficult. For example, \"My partner with whom I live in Thailand has questions about various lifestyles. There is no law to certify same-sex marriage, which makes living together here quite difficult. In his country, there is a law that allows same-sex marriage.\" (a TGW sex worker aged 26 years old).\n---\nDiscrimination at a Healthcare Facility\n\nTGWs are often discriminated against at healthcare facilities because their name and title prefix do not match how they dress. For example, \"I went to the hospital and was called Mr.... and I had to walk past other people who looked at me and laughed, embarrassing me like I was a clown, and so I didn't want to receive healthcare services.\" (a TGW from an ethnic minority aged 40 years old). Discrimination against TGW sex workers with intersectional identities, especially those working in brothels, is prevalent. They are asked to undergo additional health checks that are not related to their symptoms. When seeking treatment for a panic disorder, one participant was asked to consent to a blood test to check for thyroid functionality and questioned about taking hormones despite seeking treatment for an unrelated condition: \"I went for treatment for a panic disorder. The additional blood test or unrelated health checks are just because I'm a transgender woman. This made my treatment cost much higher than for others.\" (a TGW sex worker aged 26 years old).\nAnother important issue concerning discrimination at healthcare facilities is restricting the right to medical treatment. For instance, \"I went to the hospital for treatment. I don't have the right to medical care or personal health insurance, and so I had to pay all my expenses because I'm from an ethnic minority.\" (a TGW from an ethnic minority aged 35 years old). When they want to help others by donating blood, their offer to help others is usually refused because they are LGBTQ. For example, \"When I donated my blood, the staff saw me as a third-gender person and didn't accept my blood even though I have no comorbid diseases.\" (a TGW sex worker aged 30 years old).\n---\nThe Effects of Intersectional Identities on Discrimination\n\nMost TGW participants (12/19, 63.2%) have experienced discrimination related to refusing them employment at certain workplaces, followed by being rejected by their family and local people in their daily lives (9/19, 47.4%) and verbal bullying in their daily lives (7/19, 36.8%). Compared to the other intersectional identity groups, TGWs from ethnic minorities have experienced fewer forms of discrimination (1-5), whereas TGW sex workers have experienced more (3)(4)(5)(6)(7). Moreover, the number of discrimination forms is not related to the number of intersectional identities (Table 2). C1, TGWs from an ethnic minority; C2, TGW sex workers; C3, TGW sex workers from an ethnic minority; C4, TGW monks from an ethnic minority. The \" \u221a \" sign was represented for the type of discrimination that the responses had experienced.\n---\nDiscussion\n\nThe findings from our study through in-depth interviews with 19 people revealed discrimination experienced by the intersectional identities group at school, at work, in daily life, and/or at a healthcare facility. The major discriminatory actions toward transgender people at school were insulting speech and/or even physical abuse. As well as being subjected to these, the intersectional identity groups had been insulted for belonging to an ethnic group, which had an impact on both positive and undesirable outcomes. The positive impact was that it encouraged them to study harder, whereas the negative impact forced them out of the school system. Likewise, in previous studies, these individuals are less likely than others to have a bachelor's degree [1] and more likely to have poor mental health [38][39][40][41][42]. Raising awareness of gender diversity and intersectionality in schools and communities could be beneficial in reducing discrimination issues. Anti-bullying legislation should be enforced to prevent violence and bullying towards transgender people, especially at the school level.\nWe found no evidence to support that a higher number of intersectional identities affects the higher forms of discrimination against TGWs with intersectional identities. However, those in some categories of intersectional identity (such as TGW sex workers) experienced more forms of discrimination than others. Whether they belong or do not belong to an ethnic minority, most of them (6/8, 75%) reported that they had experienced discrimination when seeking employment. This might be one of the reasons that made them decide to become sex workers, even though they faced a higher risk of discrimination in their work and daily lives. This is consistent with the findings from a previous report in that the high amount of compensation and acceptance of their gender identity has enticed some TGWs in Thailand to enter the sex industry [21].\nWhen considering employment discrimination, TGWs not only have limited access to employment but also experience conflict with colleagues and supervisors. Some employers require employees to dress according to their birth-assigned gender and also interfere with their chances of promotion [2]. For those who have chosen to become sex workers to support their families, degradation, physical abuse, and/or refusal to pay for services are quite common occurrences. Likewise, according to the outcomes of previous studies, these individuals often experience compounded discrimination and stigma that negatively affects the community [5,43]. As is generally known, although Thailand's sex workers generate 6.4 billion dollars annually [44], prostitution is still illegal, immoral, unethical, and not accepted by Thai society, so there is no social security or healthcare available to them. Indeed, many have been physically attacked and arrested by government officers [4].\nAs an example of discrimination against individuals with an intersectional identity according to race, a research group from the US reported that 11.3% of LGBTQI employees of color experienced unfair treatment at work compared to 6.5% of white LGBTQI employees [20]. Anti-discrimination laws in employment against a job applicant or an employee on the grounds of race, gender expression, and gender identity would raise the employment opportunities for TGWs from an ethnic minority. Identifying individuals with an intersectional identity and comparing them with TGWs without one would enable us to comprehend differences in discrimination in the social contexts of family, school, workplace, and community. However, since TGWs without an intersectional identity were not included in the present study involving in-depth interviews, exploring these differences between the two groups cannot be provided herein.\nBecause they have been rejected by society and unappreciated by their families and communities, they often relocate from home to try and avoid discrimination. In addition, the main religion in Thailand is Buddhism, and most Buddhists believe that sin or karma is to blame for a person's desire to have a different gender assignment from that given at birth [45]. This might have influenced how society perceives intersectional identity persons as different, thereby causing discrimination and difficulties with them integrating into society. Although there are still some people in Thailand who accept them and are working to mitigate the social stigma, changing the regulations to cater to third-gender people is not progressing quickly. Same-sex marriage and officially changing their gender assignment are unavailable, while labor laws only cater to cisgender individuals. These findings are consistent with those from previous studies [2,46].\nOur findings suggest that intersectional people experience worse healthcare outcomes, which is supported by previous study outcomes. For instance, LGBTQ people in the UK have less favorable healthcare experiences than cisgender people [9]. One of the causes is that most healthcare professionals are not trained to deal with transgender patients, and as a result, the latter commonly encounter ambivalence and uncertainty when seeking medical intervention [31]. Consequently, discrimination occurs during subsequent interactions with caregivers [33]. A fundamental right should be the ability to access appropriate medical care and counseling for physical and mental wellbeing, especially for trans people. Therefore, in an effort to address discrimination and healthcare disparity, healthcare that is recognized as LGBTQ-friendly should be established, with expert care providers and convenient access. In addition, blood donation remains prohibited in Thailand for the LGBTQ community due to the perception that they are at greater risk of HIV or hepatitis B infection, even though everyone carries the same risk of contracting these infections. However, some countries accept blood donations for these groups under certain conditions, so they do not feel any different. For instance, in the US, blood donors registering as transgender are not deferred, and eligibility is based on the criteria associated with the gender the donor has reported [47]. However, in Australia, a man who has sex with men or who is a sex worker needs to wait 3 months before donating blood [48]. Raising the understanding, the quality of service, and the coverage of the healthcare providers in both physical and mental health is also important to improve the health outcomes and the healthcare service accessibility among the transgender women with intersectional identities.\nAs can be seen from our study outcomes, intersectional identity groups are stigmatized and discriminated against in every aspect of their lives with harsh words or sexual violence, at school or work, in everyday life or in healthcare settings. Even though school is supposed to be a safe and equitable environment free of discrimination for all students, it fosters violence and stigma against TGWs or minority students. Furthermore, some teachers aggravate rather than help the situation, causing some of them to drop out. The compulsory education level in Thailand is grade 9 in high school. Meanwhile, the job market is extremely competitive, and a good career is impossible to achieve without adequate education. Thai society only accepts transgender people conditionally, provided that they are beautiful, wealthy, and successful. At the other end of the spectrum, job security is extremely limited for anyone who engages in sex work. Their daily lives are difficult, and a lack of basic rights and sufficient healthcare prevents them from living normal lives. Moreover, a sex worker having an intersectional identity makes the situation even worse [49].\nAlthough this is a small sample size interview with intersectional identities, the results of these in-depth interviews are extremely valuable since the participants with intersectional identities cannot be easily contacted. The study's strength is that there has not been as much research published on intersectional identities as there is on transgender groups.\n---\nConclusions\n\nAlthough transgender identity is socially acceptable in Thailand to a certain degree, stigma and discrimination are still important issues, especially for transgender women with an intersectional identity. The outcomes of this qualitative in-depth interview study reflect the problems associated with multiple sources of discrimination aimed at transgender women with an intersectional identity in Thailand, including harsh speech or physical abuse; occupational, social, and legal inequality; and healthcare provision disparities. Raising awareness about gender diversity and intersectionality, as well as enforcing antibullying legislation and anti-discrimination laws, should be continually pursued in order to protect the rights and improve the quality of life of transgender individuals with an intersectional identity. In addition, enabling the rights of transgender people by allowing changes in gender assignment and same-sex marriages, as well as providing better welfare, will reduce the inequality between the cisgender and transgender populations.\n---\n\n\nData Availability Statement: Not applicable.\n---\nAcknowledgments:\n\nThe authors thank all patients who participated in this study. They also thank all staffs in the MPlus Foundation and SISTERS foundation for all their support. This work was supported by Chiang Mai University.\n---\n\n\nInformed Consent Statement: Informed consent obtained from the participants was given prior to participation in the study.\n---\nConflicts of Interest:\n\nThe authors declare no conflict of interest.",
        "Introduction\n\nIn the era of globalisation, the importance of English as a \"world language\" (Brutt-Griffler, 2002) has been growing across the world. English communicative competence is considered one of the necessary skills for today's young students to successfully participate in a highly internationalised society (Kennedy, 2006;Kramsch, 2014). It has also been emphasised as one of the essential roles of contemporary schools to help their students to become intercultural speakers who will be successful not only in speaking the English language but also in developing a relationship with people of other languages and cultures. For example, Byram's (1997;2012) \n---\nmodel of Intercultural Communicative\n\nCompetence introduces the \"intercultural dimension\" into language teaching practice and focuses on supporting learners in developing their own identities as well as those of the 'others' in the increasingly multicultural society. Researchers who have examined English education systems in non-English speaking countries, however, have observed a significant discrepancy between a growing societal emphasis on English-as-a-foreign language (EFL) learning and the perceived ineffectiveness of English curricula-particularly in public schools that extensively focus on grammar and translation exercises without providing authentic English communication opportunities (Butler, 2011;Coskun, 2011;Hu, 2005;Jeon, 2009;Nunan, 2003;Wong, 2012). The literature has highlighted multiple negative consequences of that discrepancy, which include: an excessive financial investment in private English education; unequal access to quality English instruction or communication opportunities; and an unproblematised dominance of Western (American) language, cultural values, norms, and world views within pedagogical practices and curriculum designs.\nIn parallel with the growing importance of intercultural communicative competence in English education, there is an increasing emphasis on the educational application of Computer-Mediated Communication (CMC) across various educational sectors. This has been driven by the rapid development of CMC technologies (e.g., online forums, blogs, social network sites, instant messenger, web-conferencing tools, email, etc.) and their prevalence in current students' learning and communication practice (see Alothman, Robertson, & Michaelson, 2017;Asterhan, & Bouton, 2017;Loncar, Barrett, & Liu, 2014;Tang, & Hew, 2017). For example, Genlott and Gr\u00f6nlund (2016) used communication technologies and enabled continuous classroom interaction and real-time formative teacher assessment in primary literacy and mathematics education contexts. Cheng and Jiang (2015) developed and introduced an online platform based on instant messenger to an art and design education context. Aghaee and Keller (2016) examined the use of online peer interaction and collaboration in Bachelor's and Master's thesis processes. Cho (2016) designed a discussion-based online course to facilitated participants' situated learning for pre-service teachers; and, Lee and Brett (2013;2015) took a similar pedagogical approach to inservice teacher education. Similar attempts have been also made in international classroom contexts, in order to enable and facilitate intercultural communication among students of different languages and cultures (see Ambrose et al, 2017 in global health education; Briones, & Lara, 2016 in ethics education).\nIn this context, integrating Computer-Mediated Intercultural Communication (CMIC, or often, 'international telecollaboration') activities into EFL language curricula has been discussed as a particularly promising alternative to the currently dominant practices of English teaching and learning (more broadly, literacy education). An increasing number of studies have reported positive outcomes of adopting CMIC activities in language learning environments. For example, Yang and Chen (2014) associated Taiwanese students' increased level of learning motivation and English communicative competence with a series of CMIC activities involving other students in Dubai, Pakistan and the USA. Wach (2015) investigated the CMIC experiences of English learners from Poland and Romania and concluded that English learners' participation in an international telecollaboration project is effective both in developing their English communication skills and in increasing their intercultural awareness (in particular, an openness to cultural differences and a willingness to communicate with others with different beliefs). Based on her findings, Wach stressed the great potential of embracing various forms of CMC in foreign language teaching. Villar-Onrubia and Rajpal (2016) also reported positive outcomes of an educational initiative involving intercultural interactions between UK students and non-UK students who, it was suggested, improved both intercultural communicative competence and digital skills through their participation in the initiative. Liaw and English (2017) incorporated an international telecollaboration project in their communication classrooms and claimed that the participants, through selecting, reading, describing, and discussing their chosen objects of personal significance, increased their awareness of one another's identities and perceived their interlocutors as authentic individuals of different culture and language.\nMost of the current literature concerning CMIC activities in language curricula focuses on positive aspects of students' learning experiences and on the pedagogical potential of new CMC technologiesreporting both in many cases (see more examples in Bray, 2010;Jin, 2015;Machado, et al., 2016;O'Dowd, 2007;Schenker, 2012;Tanghe & Park, 2016). However, it has also been noted that \"online intercultural exchanges are extremely complex and dynamic educational contexts\" (Turula, 2017, p. 21), which are not without significant challenges. In fact, although positive narratives continues to dominate, a number of recent articles document significant pedagogical issues observed when a telecollaboration project is introduced into a conventional classroom. For example, Carr (2016) noticed that some students find it difficult to get access to certain CMC tools, and that conflicting calendars among different schools make it a challenge for some students to fully participate in multi-institutional projects. Kayumova and Sadykova (2016) observed that students struggled with communicating and working with their international partners with very different cultural beliefs and learning approaches, which caused a failure in their telecollaboration projects. L\u00e1z\u00e1r (2015) also suggested that students generally perceive that developing a relationship with their partner is challenging, especially when different cultural conceptions cause significant mistakes in intercultural communication. Rudenko and Krylova (2016) concluded that students' repetitive speech 'mistakes' decreases their subsequent willingness to communicate with their foreign partners. In a similar vein, Ke (2016) reported that some students find it too difficult to develop a positive and stable linguistic identity. They tend to position themselves as a 'deficient' non-native speaker and experience a high level of anxiety in intercultural communication settings.\nIt is then perhaps unsurprising that many researchers have emphasised that well-designed activities with ongoing teacher and technical support are essential factors for the successful adoption of CMIC activities in learning environments. The factors highlighted include tightly structured tasks and task sequences (Kurek & M\u00fcller-Hartmann, 2017), carefully planned activities with clearly set deadlines and a diverse range of available technologies (Kayumova & Sadykova, 2016), more guided and demanding tasks and in-depth classroom discussion (Peiser, 2015), and an emphasis on teacher guidance and presence (Turula, 2017). However, with a few exceptions (see Ware & Kessler, 2016), it is difficult to find a published work that concentrates upon those pedagogical challenges emerging from (or during) CMIC activities in classrooms-especially work providing a comprehensive description of students' experiences with those issues and teachers' effort to address them. Thus, this case study aims to investigate specifically such challenges by asking a question like \"What pedagogical challenges and social issues are observed in a CMIC-implemented EFL classroom in South Korea?\" Tate (2007) argues that the task of a researcher, as an analyst, is to carry out reflexive translation of a particular social event under examination, and so that how the researcher interprets the event is entirely dependent upon which analytic lens he or she takes. Bryson and De Castell (1998) in their article1 effectively demonstrate how narratives about the same educational phenomenon (i.e., educational computing in school contexts) can be divergent, or often conflicting, among researchers with diverse analytic lenses. Therefore, this article, inspired by the authors' ideas above, aim to provide different, and hopefully more critical, narratives about CMIC activities in a language learning environment. To do so, the present author deploys two different, but potentially interrelated, theories of pedagogy: Multiliteracies Pedagogy and Critical Pedagogy.\n---\nA Theoretical Framework\n\n\n---\nMultiliteracies Pedagogy\n\nMultiliteracies is first introduced by The New London Group (1996) as a way of conceptualizing a notion of literacy reflecting an increasing cultural and linguistic diversity in the current society with a multiplicity of text forms associated with various information and communication technologies.\nMultiliteracies pedagogy is a teaching approach based on the epistemological assumption: human knowledge is developed not as \"general and abstract,\" but as embedded in social, cultural, and material contexts. Further, human knowledge is initially developed as part and parcel of collaborative interactions with others of diverse skills, backgrounds, and perspectives joined together in a particular epistemic community, that is, a community of leaners engaged in common practices centered around a specific (historically and socially constituted) domain of knowledge. (The New London Group, 1996, p.82) Multiliteracies pedagogy is accordingly explained by the group as \"a complex integration of four Thus, a central focus of multiliteracies pedagogy is to design learning environments that engage students in meaningful literacy practices involving multiple forms (or modes) of meaning-making experiences. The general principles suggested by multiliteracies pedagogy are correspondent to the predominant pedagogical ideas shared by social constructivist learning theorists who perceive learning as a knowledge construction process through participating in collaborative practices of social communities (e.g., Lave & Wenger, 1991;Scardamalia & Bereiter, 1993;Vygotsky, 1978). However, the essential goal of gaining conscious awareness of social problems (i.e., social inequality) and reforming cultural practices that cause those problems is a rather unique component of multiliteracies pedagogy.\nEnglish language education informed by multiliteracies pedagogy, therefore, needs to provide learners with opportunities to cognitively and critically engage with multimodal forms of meaningmaking practices within collective learning communities, but also in personally meaningful ways.\nUltimately, the effective language learning practices would enable and encourage learners to invest in and negotiate their linguistic and cultural identities and develop critical language awareness, through which they can understand various power effects created by language use in their own cultural and social contexts (Cummins, 2001;2004;Cummins, Brown, Sayers, 2007).\n---\nCritical Pedagogy\n\nCritical pedagogy, grounded in the Frankfurt School's critical theory, aims to empower students to question and challenge dominant social beliefs and practices, which are often taken-for-granted by students themselves despite the negative effects that they have on their lives (Aronowitz & Giroux, 1991;1993;Freire, 1995;McLaren, 2003). This teaching approach foregrounds the unequal conditions of schooling, in relation to larger socio-economic inequality, and its oppressive social functions that serve to marginalize certain groups of students (Giroux, 1981). Bourdieu, in his book The forms of capital (1986), introduces three interlinked types of capital: economic capital, social capital, and cultural capital. Among them, cultural capital refers to the \"inherited\" properties of a person that are mostly gained from his or her family, providing him/her a higher status in a society. An unequal amount of cultural capital possessed by families from different social classes creates unequal conditions for their children's continuing acquisition of cultural capital (Bourdieu, 1991). Bourdieu's theory of cultural capital has been taken up by many critical pedagogues to unpack and decode the socialization (or marginalization) process of schooling (Apple, 1998;Lin, 1999;Sandoval, 2000;Willis, 2003).\nFamilies from dominant social classes provide their children with initial advantages of cultural capital (in this case, the particular forms of knowledge required to succeed in the school system). For example, literacy (or academic skills more broadly) is a form of cultural capital. Children from different social classes tend to start their schooling with unequal levels of abilities to read and write, which then results in unequal educational experiences among them subsequently. Those children with more cultural capital do better in their schooling, benefit more from the school curriculum, and in turn obtain further cultural capita-more and faster-than those children from marginalized social classes. In this sense, the current educational system does not enable students to overcome different inequality issues in their lives, which are deeply rooted in the large socio-economic inequality. On the contrary, it supports and reproduces unequal power relationships between dominant and marginalized social classes (Aronowitz & Giroux, 1991). Through this educational socialization process, therefore, the inequality among students from different social classes tends to be maintained and reinforced unless teacher and students consciously and collectively resist the reproduction mechanism and strive to disturb the socialization process.\nDiscourse is another useful concept that can be discussed in relation to one particular form of cultural capital: namely, those learning attitudes that are considered right or appropriate in the current educational system. Discourse, understood as a set of knowledge, rules and regulations, is a central notion in Foucauldian analyses of power relations between disciplinary institutions (e.g., schools or prisons) and individuals who participate in institutional practices. According to Foucault (1990), discourse governs and controls people's thoughts and behaviours through different forms of institutional rules and regulations that exert disciplinary power upon people's thoughts and behaviours. Among multiple competing discourses in a social regime, certain dominant discourses decide which thoughts and behaviours are more legitimate than others (Foucault, 1995). Dominant discourses consist of a particular set of norms, which create different human subjectivities and divide people into often dichotomous groups: the \"normal\" and the \"abnormal or problematic\". In any school context, there will be some particular set of norms identifying good and bad learning attitudes and behaviours. Those norms (i.e., dominant discourses) in turn identify and classify students-for example as \"a good versus bad student\"\nor \"an active versus passive student\" (see Comber, 1997).\nAs a consequence, discourse imposes unequal values on different learning attitudes and skills and those that are more valued often turn into forms of cultural capital that privileged students are more likely to possess, and that disadvantaged students find it much more difficult to obtain. Yet, despite the controlling and discriminating nature of the disciplinary (i.e., educational) practices in school context, due to the taken-for-grantedness of discourse, it is almost impossible for individual students to challenge or resist the dominant discourse (Dean, 2010). Therefore, critical pedagogues call for the collective efforts of teachers and students to become conscious about the unequal educational practices existing in their classroom, and to further question and challenge dominant social and educational discourse and their ramifications in the lives of those being educated.\n---\nResearch Context and Motivation\n\nA case study is a helpful research strategy to retain the holistic understanding of real-life events and to ask 'how' and 'why' questions that investigate the effects of certain events in a complex social context (Yin, 2003). This study employs the case study methodology to investigate pedagogical challenges and social issues that emerge when implementing CMIC activities in EFL classes, and their educational and social consequences upon students learning and language practices. This case study builds on the author's earlier work (Lee et al., 2016) that developed and integrated a series of CMIC activities in two EFL classes (one in South Korea and the other in Iran). The present author was an English teacher-researcher in the Korean class. Motivated by a desire to innovate her own teaching practice, the author developed a CMIC-implemented EFL teaching framework, the Computer-assisted Multiliteracies Programme (CaMP), utilising a notion of multiliteracies that specifically included English language, cultural, and media literacies (Table 1). The three pedagogical principles embedded in the CaMP were the following: i) Provide authentic English communication opportunities to EFL students whose everyday communication medium is not the English language;\nii) Introduce English as a tool not for learning native speakers' culture but for intercultural communication to understand diverse cultures in the world;\niii) Expand students' understanding of communication to include multimodal communication practices.\nIn order to support the CMIC activities between the Korean and Iranian students, an online learning environment was also developed: the CaMP-site (Figure 1). A range of technological tools were either embedded in or linked into the CaMP-site. The initial evaluation of the implementation of the CaMP framework in the two classrooms suggested positive changes in students' English learning attitudes and outcomes. For example, the participating students showed an increased level of motivation for learning English and expressed a clear sense of satisfaction with their CMIC experiences (see more in the following section). Those results were in line with other researchers' positive observations regarding similar pedagogical initiatives (e.g., Liaw & English, 2017;Villar-Onrubia, & Rajpal, 2016;Wach, 2015).\nHowever, there were also subtle but negative consequences observed in the CaMP, which were not the focus of the author's initial investigation but which intruded into the data at various moments. For example, implementing the framework in real-life school contexts across two countries was very costly, which made it very challenging for individual teachers to enact the framework in their classrooms.\nAdditionally, and perhaps more seriously, irreducible gaps in students' socio-economic backgrounds produced unequal learning experiences and outcomes among the students in the same classroom. The point of departure of this case study, therefore, is a desire to revisit the original case, and to pay more attention to those consequences. Thus, this article-unlike the previous work that has emphasised the positive results of adopting CMIC in EFL education contexts-illustrates and conceptualises a range of unintended educational effects that receive less frequent attention in the literature. This is not to undermine the claims elsewhere that perceive the implementation of CMIC in language education as broadly 'successful' and 'effective'; but to present an analysis that is both more practically realistic and more conceptually sophisticated than those earlier claims. Thus, for CMIC to be a genuinely 'alternative' approach to EFL instruction, it is crucial to better understand in which ways its implementation changes the more conventional EFL educational practices and, at the same time, in which ways it introduces new issues to EFL education situations.  \n---\nPrevious Study\n\nIn order to better situate the present case study in its original real-life context, this section provides a brief summary of the findings from the previous qualitative case study that mainly focused on positive aspects of students' learning experiences in the Korean EFL class.\n---\nParticipants\n\nAs a teacher-researcher, the author designed and taught a three-week-long English course utilizing the CaMP framework in a Korean public middle school at which she was teaching English. An Iranian EFL teacher-researcher, a colleague of the author from the same doctoral program in Canada, and her thesis supervisor, a well-known second language educator, participated in the original research project and supported the implementation of the CaMP in the two EFL classes. Twenty-three Korean students voluntarily registered for the course and agreed to participate in the project. Informed consent from their parents and the school principal about their participation in the research project was also obtained. All student participants were in grades 7 to 9 (aged 13 to 15). Although the level of English among students was varied, most students were at a beginner level. Over the three weeks, the Korean class met every day from Monday to Friday for a two-hour session, with a ten-minute break in the middle. The class was virtually connected to the Iranian partner class through the CaMP-site. In the partner class, twenty-two individual Iranian students (aged 14 to 15), attending different schools but located in the same neighbourhood, were recruited. The Iranian teacher-researcher first recruited a class of seventeen Iranian female students from her school, a private female middle school in Tehran. Subsequently, she recruited five more male students through her personal contacts with their families. \n---\nData Sources\n\nData was collected from multiple sources. Yin (2003) suggests that it is important to collect multiple data sources and to provide a detailed case description for a qualitative case study, particularly when the study focuses on a single case. The most important data source was the author's field notes, in which she chronologically recorded the entire process of designing and teaching the CMIC-implemented English course.\nWhile designing and preparing the special summer course, the author was also teaching four regular Grade 9 English classes at the same middle school. Each class had approximately twenty-five students.\nTheir learning behaviours and attitudes were also recorded in order to produce more comparative understanding about students' experiences in the CMIC-implemented English course. While teaching the three-week-long CMIC-implemented English course, the author closely interacted with the twenty-two student participants and observed their behaviours and attitudes in the daily sessions. Immediately after each session, the author had a Skype conversation with the Iranian teacher-researcher each day. The two teacher-researchers shared their experiences, thoughts, and feelings, discussed some of the issues noticed, and considered different courses of action that could be taken to address the issues. Additionally, the author wrote a daily journal entry recording the students' behaviours and attitudes in each session and the follow-up conversations with the Iranian teacher-researcher. Even though there were no formal interviews conducted, the author continued to have informal conversations with the student participants and two other English teachers at her school throughout the course: those conversations were also recorded in the author's field notes. \uf0b7 What are your future plan(s) about English learning?\nThese two sets of written texts were initially analysed using constant comparative methods (Strauss & Corbin, 1990). Three dimensions of the initial coding schemes consisted of acting subjects (teacher, student and group), the five stages of the CaMP framework, and the seven CMIC activities in the framework (see Table 1). The coding results were discussed with, and member-checked by, the Iranian teacher-research and her doctoral supervisor; and the results were subsequently revised. Other class artifacts, including student learning outcomes, video clips, discussion threads, and chat records, were also collected and used for the additional data triangulation process (see Golafshani, 2003).\n---\nResults\n\nThe detailed description of students' experiences and perceptions in the CMIC-implemented EFL classes can be found in the previously published article (Lee et al., 2016). This section recapitulates how Korean students' learning experiences in the CMIC-implemented English course differed from ones in a more traditional English class.\n---\n1) Instructional focus: Grammar-translation vs. Communication\n\nThe regular English class at the participant Korean school mainly consisted of grammar lectures and reading-and-translation practices lacking in communicative approaches to language learning as other researchers have previously reported about many EFL classes (e.g., Butler, 2011;Coskun, 2011;Jeon, 2009;Wong, 2012). The learning content was entirely provided by a teacher, while students remained silent throughout a 45-minute session (four sessions per week) except when responding to teacher's closed-answer questions. Many students seemed to have difficulties in fully concentrating on the teacher's lecture and a few students were dozing off or texting (doing something with their mobile phones).\nAlthough each unit of the English textbook used did include some activities meant to support communicative action, in reality what was observed tended to be rather mechanical and based on the prescripted conversations with little deviation. Due to the large volume of subject content that each session was required to cover, the teacher found it difficult to allocate sufficient time to the communicative activities, which doubtless constrained the exploratory nature of students' conversations.\nOn the other hand, the CMIC course, which was virtually connected to the English class in Iran, chiefly focused on communicative activities. Each Korean student was paired with an Iranian student (or two Iranian students) as a conversation partner and the pair exchanged cultural information and knowledge using the English language. Each student prepared and led their conversations with Iranian students according to their own interests, without the teacher's direct intervention. Students showed genuine enthusiasm and excitement about the course structure that provides opportunities to meet and interact with \"foreign friends\". This was clearly shown in the students' course evaluation results (see details in Lee et al., 2016). All twenty-three Korean students initiated the online conversations at least twice during the three week period. Except for 1 student who had a health problem, all twenty-two students were actively engaged in and contributed to all of the communicative activities despite the voluntary nature of the participation in this summer class.\n---\n2) English: school subject vs. intercultural communication tool\n\nIn the regular English class, students' learning achievement was evaluated exclusively based upon the accuracy of their responses to multiple-choice or short-answer questions in written exams. In this context, English tended to be perceived by many students just as one of the academic subjects that they need to study for getting a good score on those paper exams (see Kim, 2013 and this is also indicated in the students' responses in the course evaluation). However, participating in a series of CMIC activities effectively changed the common perception of English among EFL students-from a piece of knowledge including grammatical rules and lexical items often distant from their real-life communication practices to a communication tool that enables them to interact with people in other cultural contexts and so to learn about other cultures. In fact, most of Korean EFL students in the course (N=17 out of 23) mentioned, in the course evaluation questionnaire, that talking to Iranian students increased their motivation to improve their English communicative competence as a mean of learning more about other cultures.\nIn the collective writing stage (See Table 1 above), Korean students as a group produced videos about different aspects of Korean culture to introduce Korea to their Iranian partners: each group focused on i) popular Korean food, Kimchi, ii) traditional Korean clothing, Hanbok, iii) Korean pop music and singers, iv) travel information about Korea's capital, Seoul, and v) Dokdo Island, a territory whose ownership is subject to dispute between Korea and Japan. Korean students also watched a series of videos about Iran produced by Iranian students, which generated interesting discussions about the differences and similarities between Iran and Korea. The Korean class, then, invited the Iranian teacher via Skype to ask further questions about Iran and Iranian culture. This \"e-Conference\" activity was led by the Korean students who asked a list of prepared questions, with some linguistic support of their teacher, about particular aspects of Iranian culture, which are \"shockingly strange\" (students' expression). For example, Korean students wanted to know why female and male students in Iran cannot attend the same school or have direct conversations. Another interesting question was about whether Iranian women are allowed to wear jeans and short skirts in public. Here, all students became active learners of Iran's culture and fully engaged in the CMIC that was fundamentally guided by their own inquiries.\n---\n3) Communication: linguistic practices vs. multimodal practices\n\nUnlike the regular English class in the Korean school that focused exclusively on English literacy (i.e., how to read and write print-based texts), the CMIC course provided students with various channels and digital media for communication. Korean students participated in a series of \"Intercultural\nCommunication\" activities with their Iranian partners (and the teacher) using CMC technologies and social networking tools (e.g., chat tool, Skype), while students were also engaged in e-Text reading activities to search and learn about Iran and Iranian culture (e.g., hypertexts, Wikipedia, Google). In particular, for the collaborative media project that aimed to introduce Korean culture to Iranian students, each group of the Korean students chose a topic, collected the required information and visual resources, and created a short video clip-using multiple means to present and communicate their ideas with others (e.g., Movie Maker, PowerPoint). By experiencing these multimodal communication practices, students were able to expand their previously narrowly constructed understanding of English literacy to encompass media literacy.\nIt will be useful to illustrate this matter using an example. Han, a fifteen-year old male student, had relatively low English communicative competence and learning motivation: in the informal conversations with the teacher he openly stated that it was his mother who had forced him to register to the course despite his lack of interest in learning English. At the beginning of the course, it was noticeable that he\nwas not enthusiastic about writing his online profile nor talking with his assigned Iranian conversation partner. At the end of the first week, the class moved to the computer lab to search and read e-text about Iran. Initially, the students used domestic Korean search engines with Korean search terms and retrieved only a small amount of search results-there was, in fact, not much pertinent information (about Iranian culture) available in the Korean language. Subsequently, it was Han who searched resources using global search engines (e.g., Google and Yahoo) and thereby retrieved a large number of images and video produced in different languages (largely in English and Persian). Those images and videos portrayed popular culture in Iran, in which the class showed a great interest. Han also used, and encouraged his peers to use, Google Translate to understand what the searched images referred to. He actively engaged in the collaborative media project, through which he made meaningful contributions to developing the group presentation by collecting various media resources and editing the video. For his part of the video presentation, he proudly mentioned later to the teacher, he had practiced reading the presentation script more than 10 times and said that this was the first time for him to make an effort to learn English.\nAlthough, from the traditional linguistic perspective, there was no remarkable improvement in his English communicative competence during the relatively short period of the course, he showed a great deal of positive change in his attitude toward English communication, which he came to increasingly see as a set of multimodal practices.\n---\nPresent Study\n\nThe present case study is qualitative and explanatory, focusing on a single case of the Korean CMIC-implemented English class (Yin, 2009). It revisits the previous case and re-examines the previously collected and analysed data sources but using a different theoretical perspective-the principles of multiliteracies and critical pedagogy set out in section 3.\n---\nData Re-analysis\n\nThe present study aims to develop a deeper understanding of the course of events observed in the case and their social effects upon EFL learners and learning, which were not highlighted sufficiently in previously published accounts. This explanatory case study is based on the paradigm of critical educational research, which has emerged from recognising the critical limitations of both positivist and interpretivist paradigms. Those paradigms both often neglect (or underemphasize) the political and ideological conditions of many educational contexts (Gage, 1989). In contrast, the paradigm of critical educational research, heavily influenced by Critical Theory, sees educational research as a deliberately political activity aiming to reveal unequal social and educational conditions, to empower students to take control of their learning conditions, and to bring about transformative changes in schools (Cohen, Manion, & Morrison, 2011). This paradigm has directly informed my selection of i) a theoretical framework that guides my meaning-making process about the educational phenomenon under investigation and ii) the critical events within the focused case.\nand validity, this case study utilized the concept of 'critical friends' for ensuring the 'trustworthiness' of the research outcomes (Lincoln & Guba, 1985). The author first re-analysed the data and drew some preliminary findings about the pedagogical challenges and social issues experienced by EFL teachers and students in the CMIC-implemented course. The two other Korean English teachers at the participant school reviewed my findings and made some suggestions, which have been all incorporated in my results section below. The Iranian English teacher-researcher who taught the Iranian partner class was also invited to be a third reader of the results in the member-checking process. Next, the author developed tentative theoretical explanations about those challenges and issues utilizing the selected theories. To sharpen (or validate) the explanations and explore a more effective approach to adopting the CMIC activities in EFL class, the author had an in-depth conversation with the two Korean English teachers again after writing an initial version of the account presented in this paper, particularly focusing on the feasibility and the applicability of the arguments made to their own teaching practice. The following 'Results' section will focus on the pedagogical challenges and social issues that were observed and identified in the CMIC-implemented course. The 'Discussion and Implications' section will offer some theory-informed pedagogical principles that, it is argued, may guide other EFL teachers' effective adoption of the CMIC activities in their classes.\n---\nResults\n\nRegardless of the careful design and planning of the CMIC activities in the course, the implementation of such courses brought about numerous pedagogical challenges in the Korean class.\nDiverse issues of inequality, while not necessarily new or directly created by the implementation of the activities, were revealed and made more salient in the course as compared with the traditional classes. In the following section, a few examples, coming out of the author's secondary analysis of the collected data based on the analytic lens explained in the previous section, are provided.\n---\n1) Pedagogical challenges and a high cost of its implementation\n\nTo begin with, one of the critical challenges that most EFL teachers would face when implementing the CMIC activities in their classrooms is perhaps related to its high cost in terms of the required level of teacher's cultural and social capital as explained below. One of the essential components of this course was to connect the Korean class with Iranian students, which was mainly based on the two teachers' international connections. It would not be possible for individual EFL teachers in Korea without such a close contact with other teachers in different cultural contexts to implement such pedagogical initiatives in their classrooms. Additionally, but more fundamentally, such intercultural collaborative teaching, between EFL classes across two or more cultural contexts, requires teachers to possess a high level of cultural capital (or linguistic capital: that is, a sufficient level of English communicative competence). It should be noted that the author, Korean teacher, in this study possesses a much higher level of cultural capital, fundamentally gained from her relatively privileged educational and socio-economic background 2 , than the average level of capital that EFL teachers in Korean public schools have. Previous studies, in fact, suggested a large group of EFL teachers struggled with their low level of English communicative competence when applying a communicative language teaching approach in their classrooms (see Butler, 2011;Li, 1998;Nunan, 2003).\nSecondly, despite the relatively high level of the author's capital, there were a number of contextual problems which significantly hindered the effective implementation of the course. Some of those contextual problems are institutional, such as irreconcilable differences between the Korean and Iranian class sizes and academic calendar. Yet a number of the contextual problems are more subtle and less predictable, implicating social and cultural constraints.\nOne example is that right before the course started, the administrators of the Iranian female school raised concerns about their female students being in 'direct contact' (even if that contact was virtual) with male Korean students. They would not allow the course to be implemented unless this issue could be addressed. Thus, the Iranian teacher had to recruit individual students from outside the participating school, in order to facilitate pairing the nine male Korean students to Iranian students as one-to-one conversation partners. Even with these arrangements in place, managing some of the class-to-class communication tasks in ways that prevented direct contact between female students in Iran school group and the nine Korean male students required time-consuming management effort. Another significant example relates to the low level of some Iranian students' participation in the CMIC activitiesparticularly those with religious family background. The course was implemented in Iran just before the Ramadan period (the ninth month in Muslim calendar, which is considered the holy month), and so some families embarked on a spiritual trip, during which participants lacked internet access, or otherwise restricted their children's social and academic activities, including those using CMC tools. A couple of Korean students who had not received prompt responses from their Iranian partners for these reasons, showed a significant level of frustration and dissatisfaction with the course; nevertheless, the Korean teacher could not do much about this problem.\n2 She has earned her graduate degree from a Canadian university and has several years of living experiences in English speaking countries while most Korean EFL teachers have a bachelor's degree from Korean universities as their highest degree without having substantial experiences or contacts outside Korea.\nAnother example occurred when the Korean students uploaded their video clips to YouTube. At that point the Iranian government unexpectedly blocked the public access to the YouTube site as part of ongoing Internet censorship practices. Although the Iranian teacher sent out the video files to each student by email instead, due to the slow download speed in Iran, both the Iranian teacher and students experienced significant inconvenience. Also, the originally expected student interactions on the YouTube site, using that site's \"comments\" features, did not take place. This limitation in access to the learning materials and environment also significantly decreased the Iranian students' learning motivation and participation. Although students in both classes expressed great interest in meeting foreign friends and learning about each other's culture, leading those interested students towards active participation in the communication activities and supporting meaningful CMIC experiences was not a straightforward process for the teachers. Thus, it can be argued that the cost of implementing the intercultural communication activities in the actual classroom settings is high, which requires a sufficient level of teachers' social and cultural capitals and their thoroughgoing commitment to the course including considerable personal time and effort to innovate or change teaching practices.\n---\n2) Issues of unequal cultural capitals among students\n\nEven though most students were active throughout the course, it would not be accurate to conclude on that basis that all students equally participated in and benefited from the authentic English communication opportunities provided in the course. There were pre-existing gaps in the cultural (or linguistic) capital that each student brought with them to the course, and such inherent socio-economic inequalities in the actual social schooling context were reflected in, as well as influential upon, each student's learning practices and outcomes in the CMIC. Transferring the instructional focus from grammar-translation to communication, in ways that required students to be active English speakers, in fact, made the disparity in students' English language proficiency more salient in the course than within the regular English classes.\nThe following two cases may effectively demonstrate how such disparity in students' linguistic capitals influenced their learning attitudes and outcomes in the course.\nOne fourteen-year old female student, Min, had been sent by her parents to Australia during her primary school years, mainly for the purpose of learning English. Min had also been taking private English lessons since her return to Korea. On the other hand, another fourteen-year old male student, Jang, had never had actual English speaking opportunities prior to the course. In his informal conversation with the teacher, he implied that that was mainly because his family was not able to afford any costly private English education offering actual English speaking opportunities (e.g., a small group tutoring by a foreign teacher, a study-abroad programme). Min, from the beginning of the course, showed great enthusiasm for the series of intercultural communicative activities, whereas Jang rarely expressed his own ideas and remained quiet throughout the course. Min was a member of the group that produced a video clip about political issues related to Dokdo Island, which was the most complicated topic among the five chosen within the class, and she took the leading role in her group. In contrast, Jang took a rather passive role in his group, which produced a video clip about Korean pop music and singers: he introduced his favorite singer using a few simple English sentences.\nWhile Min was able to have meaningful conversations with her Iranian partner, Jang was only able to use the opportunity to exchange simple greetings and superficial information about Korea and himself with his partner. These differences between the two students' CMIC experiences in the course reflected the considerable gap between their levels of English communication competence, in this case a product of their different cultural capitals, and it was certainly not an easy task for individual teachers to address.\nTherefore, it is difficult to claim that adopting CMIC activities actually offered an \"equal\" access to communication opportunities to students at all levels. Each student's learning experiences in this course was fundamentally influenced by the extent of cultural capital that each of them had brought into the course from outside.\n---\n3) Issues of an unequal process of subjectification of a good student\n\nIf one simply adopts a (social) constructivist approach to teaching and evaluation, the answer to the question \"who are the good students?\" in the intercultural communicative course clearly differ from the ones in the regular, or more common, EFL classes. In other words, in the new course, active participants in the given communicative tasks are considered as \"better students\" than those passive listeners. In the regular EFL classes, receptive learning behaviours such as listening to teachers, taking notes, and reading textbooks silently are rewarded as legitimate classroom behaviours, whereas, active and self-directed or self-expressive behaviours (e.g., discussing, creating, and sharing) are more valued and rewarded in the intercultural communicative course.\nIt is important to note that that distinction is not merely an external, analytical point. The differences in legitimate student behaviours in the two classrooms were reflected in the language used by the teacher in each setting. At the most basic level, it is common to observe in the regular classroom, teachers saying \"be quiet\" or \"listen carefully\" while in the communicative course, the teacher continued to ask her students to \"be active\" or \"speak up\". Nonetheless, despite this shift in classroom norms or desired behaviours, it was noted that those active participants in the communicative course were, in fact, the same students who were considered good in the regular English classes. Regardless of the change of the instructional discourses, the students who followed the appropriate class norms and behaved accordingly were more positively perceived by teacher (or institution, which is a school in this case) than the students who did not in both discursive contexts. Going back to the two student cases discussed above, it was a certainly the case that Min was doing much better even in the regular English class as well as on her exam than Jang. The difference was more salient in the case of Jang whose learning behaviours (e.g., being quiet) were not perceived as problematic and bad in the regular class 3 , whereas it was clear that he was not (or more likely, not able to be) a good student in the communicative course without having a certain level of English communicative competence.\nIt may be also important to note that, in the third stage of the course, the collective writing (see Table 1) students with similar levels of English communicative competence spontaneously formed groups together for the collaborative media project. As a result, unequal cultural capitals were distributed among the five groups and subsequently, the complexity of the topic each group selected and the quality of each group's outcome tended to correspond to the level of its members' cultural and linguistic capitals. A more striking difference among the five groups was in the quality of their collaboration processes. For example, whereas students in the Dokdo group, to which Min belonged, seemed to know exactly how to collaborate on the project not only effectively but also efficiently, the group that dealt with the least complex topic (i.e., Korean traditional food Kimchi) continually consulted with the teacher on what to do. Inevitably, the students in the former group were more engaged in the whole learning process and produced a better outcome than the latter groupthe gap was significant.\nThis difference, again, indicates that each student possessed an unequal level of cultural capital referring to certain forms of academic skills and communication or collaboration abilities that were less likely taught in the traditional education settings. Despite the fact that the teacher was conscious of the different levels of prior knowledge and skills with which each group started their project, she was not able to give all groups the same grade but had to give the Dokdo group students higher grades. In the communicative course, the inequalities among Korean students related to different socio-economic backgrounds that each of them came from tended to be reinforced by the new set of learning norms and expectations for being a good student. 4) Issues of the unequal status of English speakers and the unequal formation of students' linguistic identities It should not be denied that EFL students as active participants in the course clearly had more 3 In fact, Jang's exam grade was in the middle range: his average grade in English subject was B.\nopportunities to express their thoughts, interests and needs compared to other EFL students in the regular classes where those opportunities were rarely available in the first place. Sue, a fourteen-year old female student with an extremely outgoing and bright personality, was one of the students who had often been admonished by her teacher for her bad classroom behaviours (e.g., not concentrating on the teacher's lectures, chatting, texting, etc.) in the regular English classrooms where she had never been given a chance to vocalize her ideas. Although she was not necessarily a good student in terms of her academic achievement 4 or classroom behaviours, she certainly was very popular among her friends. In the communicative course, Sue initially showed a great level of excitement about talking to her Iranian partner and working on the collaborative media project. She did not only actively participate in but led the classroom discussions and shared lots of interesting ideas and well-received suggestions throughout the course. For example, to introduce Korean traditional clothing Hanbok more effectively to Iranian students, she brought her own Hanbok and videotaped herself and her group members wearing Hanbok.\nShe repeatedly came to the teacher to request for diverse supports (e.g., using a school i-pad and accessing to an empty classroom, proofreading their English script, etc.).\nIt might be expected that such active and self-regulated learning experiences would empower students to fully exercise their autonomy in their learning processes, and possibly thereby enable them to overcome or at least partially ameliorate their differences in cultural capital. In other words, more autonomous and motivated students might potentially liberate themselves from their prior constraints and improve their educational and living conditions. The transformative nature of the attitude changes that Sue showed in the communicative course and subsequently, the positive changes of teacher perception on her as an active (and therefore 'good') student was expected to be an instance of that phenomenon:\nempowering students and potentially liberating them from the marginalisation function of schooling.\nHowever, a couple of days after her group's video was uploaded online to be shared with Iranian students, Sue asked the teacher to remove the video because she felt embarrassed about the strong Korean accent in her English speech in it. She directly said: \"my English sounds stupid! It does not sound like English at all.\" She also compared her accent with the accents of other students who had gone abroad to learn English; she clearly perceived that their accent more closely resembled the 'ideal' than her own.\nAlthough there was subsequently a classroom discussion about the Iranian students and teacher's accent, which encompassed the idea of English as a global language that needs to be more inclusive about diverse forms and accents (rather than viewed normatively in light of American or British accents), Sue remained 4 Sue's exam grade was in the middle range: her average grade in English subject was B.\nupset about her \"Korean accent\" which she persisted in perceiving as inferior.\nWhen it comes to the formation of EFL students' linguistic identities, the unequal status of English speakers in the broader EFL context seems to play a more powerful role in the process than the actual quality of students' performances in the communicative course. Similarly, the prevalent idea of 'native English' as some sort of gold standard among students remains difficult to challenge and deconstructjust as Sue stubbornly refused to accept her Korean accent as a legitimate English accent. In this case, it is difficult to argue that she fully exercised her autonomy in resisting the negative social meaning attached to non-natives' English and constructing a positive linguistic identity of herself, despite her active participation in the CMIC activities.\n---\nDiscussion and Implications\n\nThe adoption of CMIC activities certainly provided authentic English communication opportunities to Korean EFL students, introduced the idea of English as a tool for intercultural communication to learn other cultures, and effectively expanded students' understanding of communication to include multimodal practices. Thus, this new approach, in general, can be a promising alternative to more traditional forms of classroom-based EFL education, one which may effectively address some of the pedagogical challenges that are repeatedly identified in the literature on traditional EFL contexts. However, such adoption also introduced different pedagogical challenges and actually made some of the social issues of inequality in EFL contexts more salient. It is, therefore, necessary for teachers, instructional designers, and educational researchers who attempt similar pedagogical initiatives in language education contexts to be aware of those issues; they may have subtle but negative impact on students' learning outcomes and identities, in particular for students who are already socially and economically disadvantaged. In other words, the CMIC activities should be implemented in language classrooms in more pedagogically, socially, and culturally sensitive ways that aim to reduce the negative educational effects of its adoption. In this discussion section, therefore, the author seeks to address the obvious question: how can we conceptualise what those 'more sensitive ways' might actually look like?\nTo address that question, three teaching and instructional principles are proposed, drawn in turn from the two closely interrelated theories outlined earlier: Multiliteracies Pedagogy and Critical Pedagogy. Mostly fundamentally, the author argues that the effective adoption of the CMIC activities needs to be a theoretically informed practice, which is critically and clearly guided by those principles.\nThe principles, which will be elaborated in turn below, are: to set up a political goal for such adoption; to identify and prepare for 'teachable moments' during the activities; and to create new discourses, norms and teacher language that operate alongside the pedagogical initiatives.\n---\nSet up a political goal of the pedagogical initiatives\n\nThe first principle requires teachers not to be na\u00efve actors serving the unequal schooling mechanisms by uncritically exercising their pedagogical power upon students but to be transformative intellectuals5 who make \"the pedagogical more political and the political more pedagogical (Giroux, 1988, p. 127).\"\nConceptually speaking, the first step towards becoming a transformative intellectual is to be aware of social and cultural inequalities in the education context, including the issues discussed in the previous section and recognise the struggles that disadvantaged students experience in the EFL educational settings. The second step is to develop a self-image as a political actor who critiques and changes (or improves) the unequal educational conditions to alleviate the issues of inequalities and reduce the reproduction power of education. The third step is, then, to educate and empower students to also become political actors who contribute to the development of a more equal and less oppressive future. It may be necessary for teachers to have a solid theoretical ground, which can effectively guide their reflective practices, involving a complicated-certainly not so straightforward-course of action. This article suggests that critical pedagogy can offer effective guidance for that praxis.\nAccording to Burbules and Berk (1999), critical pedagogy is an effort \"to raise questions about inequalities of power, about the false myths of opportunity and merit for many students, and about the way belief systems become internalized to the point where individuals and group abandon the very aspiration to question or change their lot in life\" (p. 50). Critical pedagogy, in fact, originally emerged from the context of literacy education aiming to foster \"learning to perceive social, political, and economic contradictions, and to take action against the oppressive elements of reality (Freire, 1995, p. 17). On that basis, \"reading the world\" through literacy practices such as reading and decoding \"texts\" is a central strategy of critical pedagogy (Freire, 1998). In this sense, both the political aims and pedagogical strategies offered by critical pedagogy can be useful for the EFL teachers when implement the CMIC activities in their classroom. Of courses, taking up this theoretical approach would neither immediately remove some of the pedagogical challenges in the course nor directly reduce the cost of its implementation. However, this will enable teachers to perceive one's pedagogical praxis as a political action based on moral commitment to students' well-being and consequently, to set up a political goal of implementing such initiatives in a personally meaningful way, which may compensate some of their effort in the course. Furthermore, engaging in academic discussions among critical pedagogues in the similar education contexts can also be helpful for teachers to develop a sense of community or solidarity and further collaborative practices, which can be particularly beneficial (or even essential) to implementing such initiatives.\n---\nFocus on and be prepared for teachable moments for overt instruction\n\nThe second principle is closely linked to one of the four factors (or pedagogical strategies) that constitute multiliteracies pedagogy: overt instruction6 . According to the New London Group (1996), in a multiliteracies learning process, teachers need to provide active interventions at \"teachable\" moments to help students to gain conscious awareness and control over what is being learned and how it is learned.\nThrough meaningful interactions with teachers (or experts), students can denaturalise some of the takenfor-granted social and cultural beliefs and practices, which may have negative impact on their life, thereby opening them up for further critique and challenge.\nThe results of this case study actually demonstrate that, in retrospect, there were a great number of teachable moments in which the author, Korean teacher, could have more meaningfully intervened in her students' learning and reasoning processes-thereby engaging the students in more reflective and critical dialogues about language, culture, and educational and social issues of inequality. For example, in Sue's case, it might have been useful for the teacher to more proactively identify teachable moments for initiating discussion about the meaning and the importance of \"native speakers' English\" (and different natives' and non-natives' accents) on intercultural communication practices. Furthermore, rather than simply imposing new classroom norms (e.g., being active and speaking up) on students, it might have been more fruitful, at the beginning of the course, to have open conversations with them about their personal aims of learning the English language, their perceptions of the value of doing so for their life, and the norms that are desirable in the course. In this way, the notion of multiliteracies could be more effectively and meaningfully introduced to, and unpacked by, the students themselves. At the same time, establishing a shared understanding of the fact that there is a great diversity among class members-in terms of their reasons for learning English, the values of English language for their personal life, and their current level of English communicative competence-could be a strong basis for seeding ongoing discussions about some of the social issues of inequality that become evident throughout the course.\nUsing such overt instruction strategies seeks to engage students not only in their own learning processes, but also in the teacher's teaching or classroom management processes. Doing so can be also promising in this particular pedagogical context where the teacher tends to face various challenges that are less common in the regular EFL classrooms, and whose solutions might therefore not from the part of their established repertoire. For example, when the government of Iran prohibited the public (including the Iranian students in the course) from accessing the YouTube site, the author could have invited her Korean students to discuss that event, through which the students may be able to think about important notions such as freedom of expression and the censorship of media. In addition, inviting them to become part of the problem solving process-to figure out how to share the videos with Iranian students and have a further discussion with them-would have helped the students to become more active participants and potentially reduced the teacher's burden of implementing the course. We have already seen some meaningful changes in the earlier case of Han who led the classroom practices of using global search engines.\nAnother critical teachable moment that might be seen as inviting the teacher to more directly intervene, arose when students were forming the groups for their collaborative media projects. That intervention might focus on the equal distribution of cultural capital: the distribution not only of linguistic competence but also of other skills that the students have, into the five groups. Explicit discussion about possible ways of forming the groups so that everyone might have more meaningful and potentially fair learning experiences within course might be also useful in such a situation-useful not only for group formation, but for encouraging students to reflect upon their subsequent actions within the groups.\nIt would be a challenging task for teachers to intervene in students' learning at all the right moments throughout the course without the benefit of hindsight. That is because of the organic nature of this course, which does not have a fixed curriculum that prescribes what and how knowledge and skills need to be transmitted to students, and the sensitive and political nature of the topics (e.g., social and cultural issues). Therefore, in conjunction with what the first principle suggests, it is necessary for teachers to be prepared for those teachable moments by developing their own cultural and social awareness,\nappropriately sensitive yet effective language for articulating and unpacking different issues of inequality, and pedagogical strategies to engage students in critical dialogues without excluding anyone or any group in the classroom.\n---\nCreate new discourses, norms, and teacher language\n\nThe fundamental aim of critical pedagogy is to empower students to challenge and deconstruct dominant discourses (i.e., legitimate knowledge, rules, and subjectivities) and so, ultimately, to denaturalise and disrupt unequal social or institutional structures and power relations. Similarly, in multiliteracies pedagogy, students are expected to transform cultural practices: through participating in personally meaningful multiliteracies practices and through critically examining their own practices in relation to larger social and cultural contexts. The implication is that teachers, as transformative intellectuals, need to create-or at least contribute to creating-new discourses and norms in their school settings.\nThe third principle focuses on how teachers in the course can approach and fulfill this seemingly challenging and overwhelming task within their classroom teaching praxis. The most essential condition for teachers when approaching this challenge is that they are critical about their own pedagogical beliefs which are, inevitably, to some extent influenced by the dominant discourses in their social, historical and educational contexts. In particular, when it comes to the construction of students' subjectivities (and their own subjectivities as a teacher), the teacher should strive to avoid asking simple questions such as \"who is a good or bad student?\" or \"who is active or passive?\". Instead, an attempt should be made to ask more critical questions such as \"who may be perceived as a good or bad student?\", \"why or why not?\", or \"who may benefit or be disadvantaged by the assessment criteria in operation?\" Asking those questions may effectively reveal, at least to teachers themselves, some of the hidden social mechanisms of schooling that serve to reproduce hegemonic power relationships among students with unequal socio-economic status.\nAt the practice level, then, teachers can use this insight as one basis for critically evaluating the dominant forms of classroom language, which often contain common sense assumptions about schooling, teaching, learning, achievement, teacher-student relations, etc. (Giroux, 1988). Language is not a valuefree or neutral instrument, which simply conveys individual users' ideas or objectively represents social realities. Instead, it is both a value-laden political product and a tool, which mostly serves dominant cultural groups by circulating and reinforcing certain social beliefs and by influencing and changing people's thoughts and behaviors (see Woolard & Schieffelin, 1994). Therefore, teacher's classroom language needs to be an object of critical examination, through which teachers can continuously revisit and revise one's pedagogical beliefs and assumptions. Based on the examination, teachers need to create a new set of classroom languages and norms to make their own teaching practices more inclusive. In particular, it may be necessary for teachers in the current multicultural or globalised educational contexts where the diversity of student body is increasing, to have multiple sets of classroom languages and norms in an attempt to accommodate diverse student needs. In the CMIC course, for example, utilising different sets of criteria to facilitate and evaluate students' learning might be required in the different student cases discussed in the previous section.\nTeachers' critical examination can open up, in turn, the possibility for involving students in critical reflection about their own developing subjectivities. It can be beneficial for students to be invited to reflect on the construction of their own subjectivities (i.e., identities) as part of the course interactions.\nThose subjectivities in such courses might include being an 'EFL learner', an 'English speaker', a 'career developer ', a 'traveller' etc., and the open discussion of those issues implicates both their own personal motivation and the more collective issue of what it means to be a good or bad student (or English speaker). In this case, the diverse discourses that the students bring into classroom discussions can be organically fed into teacher's languages. Doing so may more effectively guide the adoption of the CMIC in language education in a more pedagogically, socially, and culturally sensitive way.\n---\nConclusion\n\nThis article addresses an existing gap in current literature, which to a large extent focuses on the positive outcomes of implementing CMIC activities in language education, while neglecting concomitant pedagogical challenges and negative consequences. The qualitative and self-reflective nature of the author's narratives in this article can, it is hoped, offer readers some useful insights into some of the pedagogical challenges experienced by teachers and students in the CMIC-implemented classroom. The author employed several methodological strategies (e.g., member-checking and data-triangulation)-in order to increase the trustworthiness of her findings that were also clearly informed and supported by the selected theoretical framework. Nevertheless, it can be easily seen as just \"a\" single story among many possible ones about what happened in \"a\" single classroom. In fact, the author's own attempt to reanalyse the previously collected data, effectively demonstrates that taking up a different analytical perspective allows a researcher (or analyst in Tate, 2007) to focus on very different moments of the selected pedagogical event and/or interpret the same moments very differently. In that sense, the motivation for publishing this article is not to argue that the present story is the only legitimate translation of the events but rather to open up a more productive conversation-welcoming multiple voices either informed by different theoretical perspectives or coming from diverse cultural contexts.\nThe author also hopes reading this article can help teachers, especially those planning or conducting similar pedagogical initiatives in their own classrooms, to become more aware of the unequal learning and living conditions among EFL students and to be better prepared to address some of the pedagogical challenges that may arise from their attempts. The three teaching principles suggested in this article will not provide teachers with universal solutions to all of those issues. Rather, they invite teachers to transform their pedagogical beliefs fundamentally, to understand the purpose of their teaching differently, and to take more sensitive approaches to teaching their students. At first sight, teachers may find the idea of putting those principles into practice demanding. However, it needs to be emphasised that the suggested principles do not impose \"add-on\" teaching responsibilities on teachers, increasing their workload, but rather provide useful teaching ideas that are suitable for the open-ended and studentcentred nature of CMIC activities. Two critical conditions for the successful adoption of the principles can be briefly recapitulated here. First, the principles need to be carefully taken into consideration from the design stage; so that more creative and/or direct learning activities can be developed and put in place.\nSecond, the principles need to be fully embedded in everyday interactions between teachers and students;\nstudents (not only teachers) should be invited as active participants in (or initiators of) the principleguided activities, such as creating \"teachable/learnable\" moments and new classroom norms.\nIt is clear that technological developments provide great pedagogical opportunities to improve EFL learning and teaching practices among other possibilities by allowing for more authentic English conversation (or intercultural communication) to take place among EFL students across the globe. The present author's experiences are consistent with the prevailing claims such that there is a wide range of technological tools available, including those specifically designed for the CMIC-implemented language classroom (i.e., CaMP-site). Indeed, utilizing those tools itself does not pose particular technical problems for teachers and students in such educational contexts. Going back to the starting point of this article, a lot of narratives in the current literature around CMIC-oriented language teaching are rather positive about the technological potential, while less critical or sensitive about the pedagogical challenges which have been the focus of this paper. Now, as both technologies and the educational applications of those technologies mature, it is timely for educational researchers to turn their focus from the technological implications of their research onto the more subtle but important social and cultural implications of their practice.",
        "Introduction\n\nHIV status awareness is critical for HIV prevention and care but HIV testing rates remain low in Uganda particularly among men [1]. The World Health Organisation [2] defines HIV selftesting [3] as an additional approach to facility-based HIV testing [4], in which individuals collect their own specimen, perform the test and interpret the results and recommends its use in order to improve HIV testing rates. HIVST is an emerging approach with potential for high impact on HIV prevention, care and treatment [5]. HIVST has been recognized for its potential to overcome traditional barriers to testing. In particular, HIVST has been documented to be convenient and less costly [6], feasible, accurate and acceptable among the general population [7][8][9] including pregnant women and their partners [10][11][12][13]. Nevertheless, the risks of HIVST as perceived by pregnant and lactating women and their male partners as potential recipients of HIVST as well as their health care providers could limit acceptability and scale-up of this new testing approach. A possibility for coercive testing, inability to test accurately and difficulty to cope with reactive results in the absence of HIV counselling are some of the key concerns documented about the HIV self-testing modality [14][15][16][17]. The cost of HIVST kits and challenges regarding linkage to care for those who test HIV-positive are other obstacles [11,12,[15][16][17].\nIn Uganda, HIVST is still a new strategy and has not yet been implemented to scale. The 2018 National Consolidated Guidelines for the Prevention and Treatment of HIV and AIDS in Uganda recommends the use of HIV self-testing as an additional approach to HIV testing services [18]. It further recommends that individuals whose HIV self-test results are positive should go for further HIV testing for diagnosis confirmation and counseling at the nearest health facility and if confirmed HIV positive, they should be linked to HIV treatment services [18]. The majority of pregnant women in many African settings including Uganda, who test for HIV as part of PMTCT programs are HIV-negative, but remain at risk of HIV acquisition during pregnancy and breastfeeding [19][20][21]. Acquiring HIV during pregnancy or breastfeeding increases the risks of adverse health outcomes for mothers and exposes their unborn or breastfeeding babies to higher risks of HIV infection [20,22]. Male partner HIV testing as part of antenatal care remains low. Scaling-up HIVST could thus offer both pregnant and non-pregnant women and their male partners an opportunity for initial and repeat HIV testing and those with HIV-positive self-test results can proceed to health facilities for HIV diagnostic testing. However, successful implementation of HIVST will depend on its acceptability by intervention recipients and providers.\nThis qualitative study was conducted consecutive to the 'Primary HIV Prevention among Pregnant and Lactating Ugandan Women' [23], a randomised controlled trial (RCT) conducted between 2012 and 2017 that assessed the effect of repeat HIV testing and enhanced counselling for the primary prevention of HIV acquisition in pregnant and lactating women and their male partners during late pregnancy and throughout the breastfeeding period [24]. A qualitative evaluation of the PRIMAL intervention indicated that repeat HIV testing and enhanced counselling at health facilities were acceptable and could enhance risk reduction among pregnant and lactating women [23]. Here, we explored perceptions of pregnant and lactating women, their male partners and health care providers regarding both initial and repeat HIV self-testing for women and their male partners during pregnancy and lactation in Kampala and generated suggestions for potential integration and scale-up of HIV self-testing in PMTCT programs.\n---\nMethodology\n\n\n---\nStudy design and population\n\nThis was a cross-sectional qualitative study conducted at Mulago National Referral Hospital in Kampala, Uganda, between April and December 2017. The study was consecutive to the PRI-MAL RCT, whose methodology and quantitative outcomes have been reported previously [23,24]. This follow-on qualitative study was conducted among selected 12 men, 22 women and 23 health care providers who either had participated in the PRIMAL study or were seeking or providing antenatal care services at Mulago Hospital (distinct numbers in each group provided in data collection section below). All study participants were from Kampala and Wakiso Districts (both of which encompass together the city of Kampala). All health workers including those who had worked in the PRIMAL study, had been involved in the national PMTCT program for over five years.\n---\nStudy setting\n\nMulago National Referral Hospital, located in Kampala, Uganda's capital city, is the teaching hospital for Makerere University College of Health Sciences and other medical training institutions. HIV counseling and testing services are provided routinely to all women attending their first ANC visit daily Monday through Friday as well as to all women considered at risk of intermittent infection when presenting for delivery at maternity. Pregnant women are encouraged by the hospital staff to attend ANC with their male partners and test for HIV together. On average, 90 pregnant women are seen per ANC day.\n---\nData collection\n\nData were collected from a total of 57 study participants through 10 in-depth interviews, five focus group discussions and five key informant interviews. The 10 in-depth interviews were conducted with five lactating women, four men who took part in the PRIMAL study, and one male partner of a pregnant woman enrolled in the general PMTCT program who preferred to be interviewed as an individual instead of being part of the FGD. Female study participants from the PRIMAL study were called and selected based on their availability and interest in participating in the study. The fifth male partner who took part in an in-depth interview was recruited among men who had accompanied their women to attend ANC at Mulago but had not taken part in the PRIMAL study. We explained the purpose of this study and asked the individuals mentioned above if they were willing to participate. All accepted were invited for an interview on a day and time convenient to them at Makerere University-Johns Hopkins University Research Collaboration (MUJHU Care) which is situated on Mulago Hospital complex. Separate interviews were conducted for women and their partners.\nFive focus group discussions (FGDs) of 7-10 participants each (total 42 participants) were conducted. Each FGD included a distinct group of participants: group 1) seven pregnant HIVnegative women attending ANC, group 2) 10 HIV positive pregnant women attending ANC, group 3) seven male partners of women attending ANC, group 4) 10 health workers involved in the provision of PMTCT services, and group 5) eight health workers who implemented the PRIMAL Study. The FGD participants in groups 1-4 were not part of the PRIMAL study while group 5 has been part of PRIMAL and this combination of participants facilitated comparison of perspectives regarding HIVST. Women and male partner FGD participants attending ANC were identified by attending health workers during ANC clinic visits and directed to members of the study team who provided information, recruited participants in the study and conducted the FGDs in a separate room at the hospital. Discussions were conducted by two qualitative researchers, one as a facilitator and the other as a note taker.\nIn addition, four health workers (a doctor, midwife, counselor and male peer counselor) involved in the provision of PMTCT services and one counselor who took part in the PRIMAL Study participated in key informant interviews.\nThe interview and FGD guides (included as Supporting Information) were developed by the study team informed by an extensive literature search. They explored awareness about HIV self-testing, perceived benefits and challenges of using self-testing and more specifically to reach male partners, and implications for integrating self-testing in the PMTCT program as well as the potential for scaling-up the approach. After answering the question on awareness, use of an oral HIVST kit was explained to study participants to enable them appraise its benefits and challenges. Interviews and FGDs with women and their male partners were conducted in Luganda, the main language spoken in Kampala, while those with health workers were conducted in English. All interviews and discussions were audio recorded and transcribed. On average interviews lasted 45-60 minutes and FGDs 60-90 minutes.\nData collection was phased, starting with in-depth interviews, followed by FGDs and finally key informant interviews. This approach to data collection enabled researchers to probe insights from one method of data collection to inform the subsequent data collection exercise. At the end of the interviews or FGDs, no new insights were emerging and no further data collection was undertaken.\n---\nData management and analysis\n\nData were transcribed and translated verbatim by a professional transcriber proficient in both Luganda and English. The first author (JR), who conducted all interviews and FGDs checked the transcripts for completeness. Data analysis was a continuous and iterative process guided by qualitative content thematic approach [25]. A code book was developed by the first and last author (RK) following reading of scripts several times to identify latent and manifest themes and sub-themes. Coding was done by JR who met regularly with RK to discuss emerging issues. Results were synthesised based on study themes and discussed at a study team meeting. Direct quotations were selected and used in presentation of study results. Throughout the process of data analysis, views of pregnant and lactating women, their male partners and health care providers were triangulated.\n---\nEthical approval and informed consent\n\nThe study was approved by Makerere University School of Medicine Research and Ethics Committee (SOMREC-Ref No. 2012-157), Uganda National Council for Science and Technology (UNCST-HS 1269), and the Committee on Human Research of the University of California San Francisco, USA (Study # 11-08151). On the day of the interview or FGD, each potential study participant was taken through the objectives, methods, procedures, benefits and risks of participating in the study and assured that no consequence was to result from their decision to participate or not. After verifying their understanding, those confirming their willingness to participate were asked to provide written consent before taking part in the study.\n---\nResults\n\nAll women and men had more than one child, most were married or cohabiting and had completed primary education. The age range of study participants was 24-40 years.\nPerceptions of women, men and health care providers regarding HIV self-testing are arranged below according to four major themes 1) awareness of HIV self-testing, 2) anticipated benefits of HIV self-testing 3) perceived disadvantages of HIVST and 4) suggestions to consider for integration and scaling up of self-testing in HIV programming (Table 1). \n---\nAwareness about HIV self-testing\n\nThere was limited awareness on HIVST among pregnant women, their partners as well as health workers involved in HIV care and research. Some participants associated HIVST to other individual self-testing strategies such as the rapid pregnancy and malaria tests. The few study participants who were aware about HIV self-testing, had accessed information mainly through friends and family members reflecting the dominance of informal networks as a source of health information.\nI\n---\nI heard it from my husband, he bought one testing kit and used it from home and after he said: \"I know you can't trust these results, you may think that I have faked them\". . .in actual sense I never trusted those results even though he works with health workers (FGD 1, HIV negative Pregnant Women).\n\nI heard two youth who were discussing. . . that self-testing is so easy. One said 'I cannot have sex with any girl before testing her'. .\n---\n.he moves with his HIV test kit. I heard them talk about it but I have never seen it (FGD 3, Male partners of women attending ANC).\n\nSimilar views on the importance of friends as a source of information on HIV self-testing was also evident in narratives of health workers who reported they heard friends talking about the advantage of the self-test kits.\nA friend told me that we can buy those HIV self-testing kits and use them from home; adding that we don't need to go to the hospital to test from there since that will be time wasting (FGD 5, HWs providing PMTCT and ANC services).\n---\nMistrust of HIV self-testing results\n\nThe few study participants who had heard about HIV self-testing or were encouraged to use HIV self-tests in general expressed mistrust about the effectiveness of HIVST; I heard that it [HIV self-testing] helps to do HIV testing but I have never seen it. . . One of my friends whom I told we should go for an HIV test, said that he had some strips that we can use. But I told him that I didn't trust them. . . (FGD 1, HIV negative Pregnant Women).\n---\nMy husband brought the kit home and tested himself but I never believed him because I know HIV testing should be done by trained health workers. . . (FGD 1, HIV negative Pregnant Women).\n\nEmerging from the above narratives is the fact that doubt about the effectiveness of HIVST was in part driven by lack of exposure and experience with self-testing and a belief that HIV testing should always be conducted by trained health workers.\nThe interviewer ensured that participants demonstrated an understanding of HIV self-testing before continuing with the interview or discussion regarding the potential uses of HIV self-testing.\n---\nDivergent perceptions about HIV self-testing\n\nStudy participants expressed divergent and often contradictory perceptions about the likely role of HIV self-testing in supplementing the existing HIV testing approaches. In all interviews and discussions, most study participants spontaneously mentioned the likely limitations or disadvantages of HIVST. Some study participants highlighted the potential advantages of the HIV self-testing approach but still most of them argued that the new approach would have multiple disadvantages as presented below.\n---\nPerceived advantages of HIV self-testing\n\nThe major perceived advantages of HIV self-testing were: enabling people to know their HIV status quickly, saving cost, being convenient and guaranteeing privacy.\nHIV self-testing can enable people to know their status quickly.. Most study participants mentioned that HIV self-testing would quicken the testing process and awareness of HIV status. This could reduce the spread of HIV especially among new and casual sexual partners as study participants explained.\n---\nIt (HIV self-testing) makes it faster for one to know his/her results at any time or to know your family's status at your convenience (KI 1-Male partner, ANC)\n\nHealth workers added to the same theme:\n---\nSay for example if someone got a girlfriend outside marriage and has those kits and before going for sex they test and in case they find one HIV positive, they will not go ahead with it (sex). HIV self-testing will help to protect people from getting infected (FGD 5, Health workers providing ANC and PMTCT services).\n\n\n---\nI think it (HIV self-testing) will help to reduce the number of new HIV infections. For example, if this is a new partner and they are going to have sex there and then in 'a one-night stand'. . . one of the key issues that we want to know at that time is their sero-status. So if they have the kits it's easier for them to test each other before going into the act other than going to the hospital which may be very far or closed (KII 03-Health Worker Providing ANC).\n\nAll study participants argued that HIV self-testing would save time for traveling to and from health facilities as well as waiting for facility-based HIV testing, especially in rural areas where health facilities are far and staff is limited: Some health facilities are very far especially in rural areas and one can spend the whole day traveling to and from the health facility for HIV testing. In this case, self-testing can help to save time. .\n---\n. (FGD 1, HIV negative Pregnant Women attending ANC).\n\n\n---\nHIV self-testing is a good idea because it is easy to use, it saves time that would have been spent lining up at the health facility (FGD 3, Male partners of women in attending ANC).\n\nThe time saving advantage of HIV self-testing was also mentioned by all health workers in individual interviews and focus group discussions.\n---\nYou do not need a number to line up, you do not spend a whole day in the hospital, it (HIV-ST) saves time. . . (FGD 4, HWS PRIMAL study).\n\nHIV self-testing was also seen as a strategy that has potential to help people in new relationships, those intending to get married or those in casual relationships who can test as couples before engaging in sex.\n---\nHIV self-testing-a cost saving strategy\n\nStudy participants mentioned that HIV self-testing would save money that would be spent on transport to and from health facilities for HIV testing. Study participants pointed out that the amount of money saved would even be more for couples seeking to test.\n---\nIt (HIVST) minimizes costs in terms of transporting yourselves to a testing facility and it helps when it comes to the issue of time since men are always in a hurry. . . (KII 1, Male Partner PRIMAL study)\n\nHIV self-testing is convenient and provides an opportunity for men to test Some study participants argued that HIV self-testing can be done any time thus making it a convenient approach unlike health facility-based testing which is subject to opening and closing hours. Self-testing was particularly considered more advantageous than health facilitybased testing for people with limited time, mostly men who are often busy looking for money to take care of their families. Most study participants viewed HIV self-testing as a potential gateway to increase testing rates for male partners who in general are hesitant to attend antenatal care clinics and test for HIV with their female partners. Most participants contended that HIV self-testing can be a solution for male partners who keep giving excuses that they do not have time to travel to the health facility.\nThere will be no way [. . .] the other partner will dodge because the testing kit is right there in your house and you can't dodge [i.e. pretend] that you do not have time. .\n---\n. (KI 3, Male partner PRIMAL study).\n\nSome male study participants viewed HIV self-testing as an opportunity for men to test themselves first before going for couple testing if they are uncertain about their HIV status. By doing so, HIV self-testing has potential to increase HIV status awareness especially among men who in general do not frequent often health facilities.\nWomen have the condition of testing during pregnancy and they actually test. Self-testing will help men because most of them want their wives to test and they say that their status is the same as their wives. .\n---\n. (KII 3-Male partner PRIMAL).\n\nThe potential advantage of HIV self-testing helping to test men was also mentioned by health workers:\n---\nIt [HIVST] can help people test themselves because we are still looking for avenues of bringing men on board. Self-testing is an opportunity for them to test (KII 5, Counselor)\n\n\n---\nHIVST guarantees transparency and confidentiality\n\nMost women, men and health workers alike in our study reasoned that HIV self-testing can ensure confidentiality of the result since it can be done individually, one can decide who to share with and in a private setting.\nIt (HIVST) will make you know yourselves within since you are testing together; it will not expose you to other people; you will know your status among yourselves. .\n---\n. (KI 1, Male partner PRIMAL study)\n\n\n---\nYou can get to know your status early without anyone else knowing; you can do it yourself and keep it to yourself (KI 2-Male partner PRIMAL Study).\n\nOne participant also explained that HIV self-testing would help to overcome likely connivance between some health workers and those seeking HIV testing to falsify 'test results'. Some men say when they ask their wives to go for HIV testing, women say husbands planned with doctors to say they are negative. . . (FGD, Male partners of women attending ANC).\n---\nSelf-testing may promote faithfulness among couples\n\nStudy participants noted that if made available, HIV self-testing can promote couple testing which could promote faithfulness and responsibility among couples, especially those who test HIV negative and are living apart.\n---\nI have a colleague who is a medical officer and her husband works upcountry but every month they test themselves together. That helps because this man will stay safe because he knows the check is on. Such couples can benefit from HIV self-testing kits (FGD4, HWs PRIMAL study).\n\n\n---\nIt can help families where one of the partners is a long distance trader or works far from home. When he or she comes you say \"Let us first test.\" Because I do not know how you have been there and you do not know how I have been here (KII 4, woman PRIMAL study).\n\n\n---\nOpportunity to confirm test results\n\nSome health workers in our study noted that in instances where one is in doubt of test results, self-testing can be an option to confirm HIV status.\n---\nSome people also do self-testing to confirm the results. Like one couple that came and tested together here and turned positive; they first denied and went ahead to do it on their own to confirm (FGD 5, Health workers providing ANC and PMTCT services).\n\n\n---\nPerceived disadvantages of HIV self-testing\n\nMost study participants emphasized the potential disadvantages of HIV self-testing likely to limit its use and integration in HIV programs. The key perceived disadvantages were 1) inability to conduct the test and interpret results accurately; 2) inability to cope with positive test results in the absence of professional counseling; 3) intentional transmission of HIV to partner; 4) failure to enroll in care; and 5) the possibility of coercive HIV testing of women by their male partners.\n---\nInability to conduct the test and interpret results accurately\n\nAll study participants mentioned that it would be difficult to use HIV self-testing kits properly, since many people lack knowledge about the kits. Most study participants also expressed fear that test results would not be interpreted properly, especially by people with low education and rural dwellers.\nBecause people do not know how to use the HIV test kits. . . You see like pregnancy test kits, very few people know how to use them. So, HIV self-testing will not work. Maybe here in towns or cities but in the villages it can't work. .\n---\n. (KII 4, woman PRIMAL study)\n\nThe other problem would be that someone might test himself and gives himself negative result yet he is positive. This strategy is good for someone who knows how to conduct the test very well but for many of our people it will be difficult (KI1, 3 Male peer Educator)\nSimilarly, health workers also expressed fear that it would be difficult for many people to conduct HIVST mainly due to low levels of education and lack of awareness about the testing approach, including test kit storage, the algorithm and interpretation.\n---\nI do not know how we can do that because. . .if some people do not know how to write their names and their dates of birth, how will they interpret the strip results? That individual needs to be tested by a health worker and referred for other services (KII 2, Health Worker)\n\nEmerging from the above narratives are concerns about the inadequacy of knowledge and skills about HIV self-testing which could lead to inaccurate interpretation of test results. As a results, there was a general perception that HIV testing should be conducted by health workers.\n---\nInability to cope with HIV positive test results\n\nStudy participants feared that HIV self-testing could lead to harm for the individual involved and others especially if test results are positive. There was widespread fear among study participants that it would be difficult for individuals to manage HIV positive results in the absence of pre-and post-test counselling by qualified health workers. All men, women and health workers in the study believed that the inability to cope with HIV positive results could lead to negative outcomes for the individual and his or her family members.\nAt individual level, several study participants, both men and women, feared that HIV selftesting might increase cases of suicide due to lack of counselling.\n---\nThe issue of finding yourself HIV positive where there are no counsellors-one might buy a rope and commit suicide. But if you are at a clinic in front of a health worker, he/she might tell you \"this is not the end of your life, you can still live for a long time. . .\" but if you do it alone, you might buy poison and kill yourself. (KII 2-Male Partner).\n\nSome study participants argued that HIV-positive results obtained through self-testing without professional counselling could lead to worries and depression.\nAt the interpersonal level, there was fear among study participants that HIV-positive results obtained through self-testing without professional help from health workers could increase cases of gender-based violence within couples as one member of a couple can accuse the other of infidelity.\n---\nIn case a man tests negative and then goes ahead to test his family members and find that his partner is positive. . . they can hurt each other. A man can end up killing his partner. It might cause harm between them. . .. It is not proper to leave people at home to do self-testing without counsellors, it might cause harm in the family. (KII 3, Male peer Educator).\n\nMen and women study participants noted that HIV positive test results following self-testing can lead to separation and breakdown in families.\nIt may also lead to separation of couples because at that time there is no counsellor, because those people before testing they need to be counselled but now if you go straight to testing and I find my wife with the virus and yet I am safe then that woman will not sleep in my house on that day. . . Even the woman might decide to go after knowing that the man is HIV positive.\n---\n(KII 1, Male partner PRIMAL study).\n\nYou might test from home and God forbid the husband is HIV negative yet the wife is HIV positive. . .that will be the end of that relationship. .\n---\n. (KII 1, Women PRIMAL study)\n\n\n---\nIntentional transmission of HIV to partner\n\nSome study participants also noted that if individuals test on their own, they may transmit HIV intentionally to their partners for fear of being abandoned or being accused of bringing the infection in the family. They could also intentionally mis-interpret the results.\n---\nIf this person tests himself or herself and gets to know that he/she is HIV positive;. . . he/she will say, I have also been infected why not infect other people as well! Why should I die alone?. . . (FGD women attending PMTCT).\n\n\n---\nWhen you already know that you are infected and you don't want to miss out on that person you have got, you can alter the results. It is easy to lie to someone if she does not know how to use the test kit, whatever you tell her, she might think it is correct (FGD male partners of women attending ANC).\n\n\n---\nFailure to enroll in HIV care\n\nA few study participants expressed concern that those who test HIV positive might find it difficult to enroll in HIV care due to lack of counseling, active linkage to care and not knowing where to go:\nWill people understand the way forward after testing HIV positive? My husband has tested positive and I am negative. . . then what next? Because these people will not be counselled before testing, they will prick and wait for the results but what will happen after the results? They will not go to the hospital for management. .\n---\n. (KII 1, Health worker)\n\nIn case he tested himself he will not encourage himself to go to the clinic and start taking ARVs. Some people will not even know which hospital to go to for treatment. Some fear going to hospitals near their homes because they do not want to be known (KII 2-Male Partner).\n---\nChallenges in disclosing HIV positive test results\n\nIt was noted that disclosure would be a challenge without health worker counselling especially to sexual partners for fear of gender-based violence or abandonment.\n---\nMost people will not tell others when they test and find they are HIV positive. They fear to be blamed for being the ones who brought the infection in the family. It takes a lot of encouragement and support from the health worker for disclosure to happen. But this will be missing in self-testing (FGD women attending PMTCT).\n\nOther challenges likely to limit the use of HIV self-testing were high cost of test kits and their limited availability especially in rural areas without pharmacies. Overall, most study participants were against promoting HIV self-testing especially due to the absence of pre and post-test HIV counseling and guidance to mitigate potential negative consequences of positive HIV test results.\n---\nSuggestions by study participants for integration of HIV self-testing in HIV programs and scale-up\n\nAs shown in Table 1, study participants suggested 1) community education about HIVST including use of community demonstration on HIV self-testing 2) provision of supportive information, education and communication materials in local languages and sensitization on the use of HIV self-test kits to guide community members, 3) make test kits accessible at no or low cost especially in rural areas without pharmacies and 4) training of health workers and community leaders on the benefits and use of HIV self-test kits to enable them to promote the testing approach.\n---\nDiscussion\n\nIn this qualitative study, we describe perceptions of pregnant women, their male partners and health care providers regarding HIV self-testing in Kampala, Uganda and participants' suggestions to facilitate potential integration and scaling-up of self-testing as part of the national PMTCT program.\nStudy findings revealed limited awareness about HIV self-testing among women, their male partners and health workers. This finding is not surprising given that this testing approach is still new and has only been part of pilot studies in Uganda [26]. The limited awareness on HIVST in this study reflects a need for raising awareness as part of scaling-up the HIV selftesting strategy. Most women and their male partners as well as health workers, expressed doubts about people's ability to use HIV self-testing kits appropriately and to interpret results correctly, especially among people with no or low education. Taken together, the limited awareness and widespread reservations about HIV self-testing imply that both health workers and community members sustained programs to educate potential users about HIV self-testing as part of scaling-up HIV testing in Uganda and similar low-income settings.\nOur study revealed divergent and often contested perceptions about HIV self-testing in supplementing the existing HIV testing strategies. Regarding the perceived benefits of HIV self-testing, most study participants mentioned that the testing approach can enable people to know their HIV status faster compared to health-facility based approaches and is cost-saving in terms of transport or time to go and wait at health facilities or testing centres. In this regard, HIV self-testing was considered suitable and with potential to reach men currently underserved by, or unwilling to test at, health facilities especially as part of maternal and child health programs [27] which largely target women. Some studies have indicated that due to difficulties of men accessing HIV testing services, some of them rely on results of their female partners and assume similar HIV status [28]. In this regard, HIV self-testing could be a gateway for men and other groups of people constrained by time to know their HIV status, and for those found to be HIV-positive to enroll in care.\nIn line with our findings, an earlier study conducted in Kenya, Malawi and South Africa on attributes of an ideal HIV self-test, revealed that self-testing would save on time spent waiting in long queues at health facilities and traveling to and from health facilities [17,29,30]. The time and cost saving benefits of HIV self-testing are of particular relevance to Uganda and other low-income settings given that COVID-19 and related restrictions have increased the cost of seeking health care. A recent qualitative study conducted in South Western Uganda revealed that COVID-19 is inhibiting access to and uptake of HIV testing services through lack of transportation, poverty and stigma related to the pandemic [31]. Fear of being exposed to COVID -19 at health facilities has also been noted [32]. Thus, prompt scale-up of HIV selftesting as part of existing HIV programs has potential to sustain access to HIV testing services in Uganda and other sub-Saharan African countries during the COVID pandemic.\nThe perception that HIV self-testing was convenient and guarantees privacy was recurrent in most interviews and discussions in our study. This desire for privacy among our study participants could be in part a reflection of continued stigma and the need for people to have autonomy over HIV testing and results. Being convenient, providing for privacy and autonomy as desired attributes of HIV self-testing have also been highlighted as key benefits of the testing approach by the World Health Organisation [4,33]. Our findings on the benefits of HIV self-testing are also consistent with what has been documented in other African settings. In Ethiopia, HIV self-testing was preferred among health workers due to confidentiality [34]. A community study done in Rakai, Uganda, also highlighted privacy, convenience and ability to test before sex as key benefits of HIV self-testing [35]. Recent reviews on HIV testing approaches [17] and HIVST [30] in sub-Saharan Africa revealed that HIVST can leverage social and sexual networks to reach those unlikely to be reached by HIV programs, increases opportunities for people to test when they want, can increase testing for men, youth and key populations [17], and provides men with an alternative, confidential and convenient testing model [30]. These widely appreciated HIV self-testing benefits by women, men and health workers in our study should be building blocks for community education and the scale-up of HIV self-testing as an additional testing approach.\nA unique benefit of HIV self-testing highlighted in our study is the potential to promote faithfulness among couples especially those living apart from each other including truck drivers and those in new relationships. Many study participants noted that these could undertake HIV testing as couples before engaging in or resuming sexual activity. As such, self-testing would challenge and encourage couples to be faithful to each other to avoid HIV infection. HIV self-testing has previously been found to be acceptable and feasible among truck drivers but stressed the need to provide supervised support and an opportunity for users to ask questions [36]. Our study findings also indicated that HIV self-testing can provide an opportunity for people in doubt of their HIV results to confirm their HIV status. The idea of some people doubting HIV test results given their own risk perception has also been documented in eastern Uganda [27] and can hinder HIV prevention for those who test HIV negative or enrolling in care for those who test HIV positive. It is important that, as part of activities to scale-up HIVST, community education emphasizes the need for individuals whose HIV self-test results are positive to go for further HIV testing and counseling at health facilities [18]. HIVST was also viewed to have potential to enable women and men to conduct repeat HIV-testing which is critical to enhance primary HIV prevention [11,21,24].\n---\nPotential disadvantages of HIV self-testing likely to hinder scale-up\n\nThe major perceived disadvantages of HIV self-testing by our study participants were; perceived inability to conduct the test and interpret results accurately, possible difficulty in coping with positive HIV test results for self and others, a possibility of coercive HIV testing of women by their male partners, and potential failure to enroll in care following HIV positive results.\nWomen, men and health workers in our study expressed fear that many people would be unable to conduct HIV self-testing and interpret results accurately owing to lack of knowledge and skills. Most study participants considered HIV testing highly technical and preferred that it should be left to trained health workers. Inability to conduct HIV self-testing accurately and difficulties to cope with positive test results in the absence of HIV counselling to prepare an individual for the test and managing test results have been documented by others as barriers to scaling-up HIV self-testing [14,16,37]. Study participants noted that the difficulties to conduct HIV self-testing and interpret results appropriately may be more pronounced in rural areas where educational levels are low. This worry may have considerable relevance for Uganda given that the vast majority of Ugandans (close to 80%) live in rural areas [38]. Consistent with our findings, a study done in South Africa on differentiated HIV testing approaches revealed that HIV self-testing and HIV counselling and testing can co-exist and complement each other [37].\nMost of the concerns related to inability to cope with the positive HIV test results and negative outcomes (separation, depression, fear of being abandoned or accused of bringing HIV to the family, intentionally transmitting HIV, violence and suicidal ideation) were linked to the absence of professional counselling and support by health workers for HIVST. These findings are not surprising given that since the advent of HIV in Uganda in the 1980s, pre and post-test counseling by health workers have been the norm and a backbone of HIV testing. These findings re-echo a need to ensure that HIV counseling is available and should be an integral part of scaling-up HIVST. Similar to our findings, a study conducted in other central Ugandan districts also documented fears of marital disruption and suicidal ideation and linked these to the absence of HIV counseling [39].\nThe risk of suicide following HIVST in the absence of counseling has also been documented in a qualitative study done in Kenya, Malawi and South Africa [40]. In another Kenyan study, two cases of intimate partner violence were reported by women delivering HIVST kits to their partners [41]. On the contrary, a community-based study promoting HIVST conducted in urban Malawi reported no cases of suicide or intimate partner violence attributed to HIVST [42]. Similarly, no serious adverse events were reported post-HIVST in Uganda [39], an indication that these fears may be overestimated in qualitative studies. Coercion as a social harm was reported by 3% of the respondents in the same Malawian study [42]. It is important that mechanisms for monitoring and responding to social harms following HIVST be part of plans for scale-up of the testing approach.\nOther disadvantages of HIV self-testing raised in our study include limited access to test kits and their cost which may make the testing approach out of reach to low-income earners and those living in rural areas. High cost as a barrier to use and scale-up of HIVST has been highlighted in other African studies [16,17,43]. The concerns about affordability as a likely barrier to use of HIV self-testing are relevant given that 21.4% of Ugandans are estimated to be living below the poverty line [44]. The implication here is that making HIVST accessible and affordable will be key to its utilization.\nTo counter the disadvantages likely to hinder scaling-up of HIV self-testing in Uganda, study participants recommended public education about the testing approach, provision of information and education materials including those with clear instructions and visual aids, as well as training of health workers to promote HIV self-testing. The implication here is that the potential scale-up of HIV self-testing in HIV programs should go hand in hand with a strong behaviour change communication component highlighting the benefits and how to minimize the potential disadvantages of the approach. Similar to our findings, a study on men's experiences of HIVST in Rwanda recommended health education and awareness creation about the testing approach and ensuring that test kits are available at a low cost. Also making self-test kits accessible over the counter or in the same manner as condoms including distribution through home visits, use of call centres for ordering kits and collection from local leaders, supermarkets and pharmacies [43] are other strategies to improve access. A review of men's perspectives on HIVST in sub-Saharan Africa recommended the use of community-level campaigns to educate men about HIVST and implementing strategies to ensure potential HIVST users are counseled and supported to test [30]. Having user friendly HIVST kit instructions [16] and translating them into local languages [43] are other key strategies to improve use of the HIVST.\nOur study findings should be interpreted in light of the following strengths and limitations.\nUse of qualitative methods of data collection facilitated an in-depth understanding of perceptions regarding HIV self-testing and what might be needed for its scale-up. The inclusion of pregnant and lactating women, their male partners and health care providers in the study enabled triangulation of data which helped to improve the trustworthiness of our findings. However, the study was conducted at a public hospital in the general antenatal care setting, thus the results apply to the population at this type of health facility but may not reflect the perceptions of women and men of higher socio-economic status. As well, the study was conducted at a tertiary hospital with experienced health workers often involved in research, and whose awareness about HIVST may be higher than what may be found at lower level and in rural health facilities with minimal exposure to research. The study was also conducted in Kampala, the capital city of Uganda, whose population tends to be more educated and informed. Thus the awareness and perceptions of this population may be different from those of rural dwellers. Nevertheless, the fact that our findings especially regarding the benefits and disadvantages of HIVST were similarly documented in other Ugandan settings [13] and other African countries is reassuring that our findings may have wider applicability beyond the study area.\nData for the study was collected in 2017 before Uganda started rolling out HIVST. With ongoing rollout, awareness might have slightly improved given that the rollout process has been slow. Thus more studies are needed to assess changes in awareness and stakeholder perceptions about HIVST as the testing modality becomes more available in Uganda and other African settings. In this study, we did not demonstrate how the HIVST kit works which would have improved study participants' familiarity with the kit. Indeed, some of the study participants kept referring to use of blood samples possibly reflecting as a common approach of collecting HIV testing samples through finger pricks which they have been able to experience or witness at most health facilities in Uganda. Future studies should include a demonstration session on the use of HIVST kits. However, given that our findings on awareness, benefits and limitations of the HIVST testing approach are consistent with what has been documented in Uganda [11,13] and other African settings [15][16][17]30] supports the validity of our findings.\n---\nConclusion and recommendations\n\nAlthough there was limited knowledge and experience about HIV self-testing among pregnant and lactating women, their male partners and health workers in Kampala, HIV self-testing was perceived as having the potential to help individuals and couples, especially those in casual and distant relationships to know their HIV status and that of their partners faster, more conveniently, saving cost and time, and guaranteeing privacy and confidentiality. The approach was also deemed to possibly encourage partners to be faithful to each other and could be an important tool for the integration of repeat postpartum testing into PMTCT programs. The major potential disadvantages of HIV self-testing perceived were a risk of harm to self or others including domestic violence and family breakdown in case of positive HIV self-test results as well as poor linkage to care due to the absence of pre and post-test counselling. Thus successful integration and scale-up of HIV self-testing in Uganda will require raising awareness and ensuring adequate linkages among health workers and community members about this new testing approach. Provision of information, education and communication materials including use of visual aids, making test kits available at no or low cost, and ensuring available and functional counselling support to users of HIVST especially in rural areas are key to scaling-up. Further research is needed to understand changes in awareness and perceptions about HIVST as the strategy is being scaled-up in Uganda and other African countries.\n---\n\n\nAll relevant data are within the paper and its Supporting Information files.\n---\nAuthor Contributions\n\nConceptualization: Joseph Rujumba, Jaco Homsy, Femke Bannink Mbazzi, Elly Katabira, Josaphat Byamugisha, Mary Glenn Fowler, Rachel L. King.\nData curation: Joseph Rujumba, Jaco Homsy, Rachel L. King.\nFormal analysis: Joseph Rujumba, Rachel L. King.\nFunding acquisition: Jaco Homsy, Rachel L. King.\nMethodology: Joseph Rujumba, Jaco Homsy, Femke Bannink Mbazzi, Zikulah Namukwaya, Alexander Amone, Gordon Rukundo, Elly Katabira, Josaphat Byamugisha, Mary Glenn Fowler, Rachel L. King.\nProject administration: Joseph Rujumba, Jaco Homsy, Femke Bannink Mbazzi, Zikulah Namukwaya, Alexander Amone, Gordon Rukundo, Rachel L. King.\nResources: Jaco Homsy, Alexander Amone, Gordon Rukundo.\nSupervision: Joseph Rujumba, Jaco Homsy, Zikulah Namukwaya, Alexander Amone, Elly Katabira, Josaphat Byamugisha, Mary Glenn Fowler, Rachel L. King.\nWriting -original draft: Joseph Rujumba, Mary Glenn Fowler, Rachel L. King.\nWriting -review & editing: Joseph Rujumba, Jaco Homsy, Femke Bannink Mbazzi, Zikulah Namukwaya, Alexander Amone, Gordon Rukundo, Elly Katabira, Josaphat Byamugisha, Rachel L. King.",
        "Introduction\n\nIt is well recognized [1] that Latino Americans and African-Americans carry a disproportionate burden of chronic disease in the US [2], including higher death rates associated with these diseases among Blacks as compared to non-Hispanic White Americans [2,3]. Despite these persistent racial and ethnic health disparities, ethnic minority groups remain underrepresented in research intended to reduce disease burdens [3]. Given the current focus on personalized medicine [4], medical research increasingly requires biological samples from research participants [5] which necessitates extensive resources to collect a diversity of relevant genotypes [6]. Those who choose to consent to biospecimen storage and continuing studies may be overrepresented in ongoing and future research [3,6]. Therefore, understanding which groups are most likely to consent for specimen storage and continuing research is important to interpret results of the studies using these specimens.\nPrevious studies demonstrate that racial and ethnic groups are not equally willing to give biological specimens. Lower participation rates among ethnic minorities were attributed to concerns regarding exploitation by medical researchers, discrimination, confidentiality, inequities between those benefiting from the research and those participating in research, and a lack of direct benefit from the research or disinterest in genetic research [3]. Some of these publications examined the National Health and Nutrition Examination Survey (NHANES) data collected from 1999 to 2008 [7][8][9], a large dataset. Data collection for NHANES includes a medical examination interview conducted in English, Spanish, mandarin Chinese (both traditional and 2\nThe Scientific World Journal simplified), Korean, and Vietnamese [10]. Hence, observed disparities among ethnic groups likely extend beyond language barriers and can provide information about health and participation willingness across these groups.\nFindings of previous studies regarding consent rates by racial and ethnic groups may not be relevant due to key changes in NHANES since 2007. First, participants are no longer given a separate consent form for DNA storage for future use. Consent rates increased overall after this new format was implemented. Second, the consent forms for collection of samples included more detailed information regarding genetic testing [9]. Third, NHANES sampling was also adjusted to oversample all Hispanics, rather than only Mexican-Americans, as had been done previously [11]. Fourth and most importantly, self-reported ethnicity data was changed to include Mexican-American, other Hispanics ethnicity (those who are self-identified as Hispanic but not Mexican-American), and Asian options for the first time in the 2011-2012 dataset [10]. Despite these key changes in consent forms, sampling strategy, demographic categories, to our knowledge, NHANES consent rates by race and ethnicity have not been reexamined and compared.\nThis study aims to determine if there is a difference between minority racial and ethnic groups and Whites in their consent rates for biological specimen storage and continuing research that required a biological specimen among adults who participated in the 2011-2012 NHANES cycle. Analysis of these data may provide new information about the relative willingness of minority ethnic groups to consent to biospecimen storage and donation for continuing studies [12]. This study may also identify potential differences in consent rates within the Hispanic population, which comprises the third largest ethnic minority population and second fastest growing population in the US [13].\n---\nMaterials and Methods\n\n\n---\nSources of Data.\n\nThe NHANES is a national, ongoing study by the Centers for Disease Control and Prevention (CDC) that is designed to assess the health and nutrition status of residents of the US. This study includes a survey and physical examination information that are collected both in the home and at mobile medical examination centers, respectively [14]. The NHANES uses a sample that is statistically determined a priori to give a nationally representative sample. This is a complex, multistage probability sampling design of noninstitutionalized civilians within the 50 states. First a sample is gathered within a county, then within a segment of the county, then households within the segment, and finally individuals within the household [15,16]. A weighting scheme is used to give a representative proportion to certain population subgroups of special interest. This increases the reliability and precision of estimates of health status indicators for these populations [14,15]. To be included, participants must live in the US. Certain populations (e.g., the elderly, non-Hispanic Blacks, and Hispanics) are purposefully oversampled to provide a representative population. US citizens that are incarcerated, institutionalized, or working in the military are not eligible [14].\nIn this study, only participants aged 20 years and older were included in our analyses since (1) NHANES collected education data on those aged 20 and older [10] and (2) consent for participation in \"Specimen Storage and Continuing Studies Using DNA\" was only offered to those over 20 years old [17]. Further, participants with missing data for any variable of interest were not included (see Figure 1).\n---\nVariables.\n\nThe dependent variable was consent on either one of two consent forms regarding biospecimen storage and continuing studies (additional to the NHANES and exam consent) [17,18]. Information on consent status is not publically available because the specimen IDs are linked to the sequence numbers in a separate database to maintain confidentiality. To obtain consent data, the authors provided a list of sequence numbers for all participants from the 2011-2012 survey cycles to the CDC where they were then matched with consent status by the CDCs information management programmer.\nIndependent variables were demographic information regarding study participants: race/ethnicity, age, gender, educational level, and income poverty ratio, as obtained from the 2011-2012 NHANES dataset that is available online. Categorical groups for race and age were defined in accordance with the groups created by the CDC, and both were selfreported [15]. Prior studies have discrepant results regarding the influence of age on consent to biospecimen donation. To provide an appropriate comparison to studies which have examined the relationship between age and consent [7][8][9][19][20][21][22][23][24][25], we categorized age in the same manner. In the analyses of the NHANES data collected from 1999 to 2004, sex differences in consent rates varied across intervals [8]. Therefore, influence of sex on consent was included. Regarding education level, previous studies found high school degree or less were more likely to donate (91.6%, CI: 90.3-92.9) versus some college reference group (89.1%, CI: 87.9-90.4) [8] or donate after second request (4.36, 95 percent CI: 1.33, 14.27), and those with some college or college graduates were less likely to consent compared to those with less than a high school education (odds of consenting OR 0.49 CI: 0.27, 0.86 \ud835\udc43 = 0.014) [21]. In contrast, McDonald et al. found that respondents with some college education and college graduates were more likely to donate a blood or saliva sample compared with those with less education (OR 1.60 CI: 0.81, 3.14, \ud835\udc43 = 0.18) [26]. Therefore, educational attainment was categorized as in previous studies. Existing evidence on the role of income on participant willingness to donate a specimen is mixed [7][8][9]23]. However, the study by McQuillan et al. found that in NHANES (2001-2002), higher incomes were less likely to consent to future research including genetic studies [8]. Therefore, as per their analysis, income was included as income poverty ratio.\nSince unidentifiable, publicly available data were used for this analysis, conditions for exemption from Institutional Review Board (IRB) review have been met in accordance with the National Institutes of Health, Office of Extramural Research (NIH/OEP) Regulations on Human Subjects Protection and Inclusion [27].\nThe Scientific World Journal \n---\nData Analysis.\n\nTo determine the relationship between consent and race/ethnicity, we used SAS 9.3 statistical software [28] to run a logistic regression using consent for genetic research as the dependent variable with education level, gender, income poverty ratio, and ethnicity as the independent variables, using NHANES weighting according to guidelines [15,29]. A forward selection approach was followed in the model building process. Independent variables significantly associated with the outcome (\ud835\udc43-value of < 0.25) were selected for the initial model. A variable was included in the final model if the Wald chi-square test statistic showed a level of significance of \ud835\udc43 < 0.05 and the regression coefficients changed significantly when it was removed from the multiple logistic regression model. We then checked for paired interactions among the variables using the Wald chi-square test and likelihood ratios. Any significant interaction term was included in the final model. To test for goodness of fit in our model, we obtained the Hosmer-Lemeshow test statistic using the lackfit feature on SAS software. We used an alpha of 0.05 and the Wald test to generate confidence intervals at the 95% significance level for the ORs.\n---\nResults\n\nOf the 9,756 NHANES participants 2011-2012, 5,560 individuals over 20 years old were included in our analysis. After those who responded \"refused\" (\ud835\udc5b = 2) or \"don't know\" (\ud835\udc5b = 3) to education level were excluded, 5,555 individuals were included in the analysis. The included sample was 49.3% male and 9.7% Mexican-American, 10.4% other Hispanics, 36.7% non-Hispanic White, 26.2% non-Hispanic Black, 14.3% non-Hispanic Asian, and 2.7% multiracial/other races. Of the included individuals, 99.4% consented to give a biospecimen for future research. Table 1 outlines the characteristics of included NHANES participants along all variables of interest.\nAge group, gender, education, and race/ethnicity were each individually significantly associated with consent to give a biospecimen in simple logistic regressions (\ud835\udc43 < 0.25, see Table 2 and Figure 2). There were no interactions between the variables of interest across the categories. Age group and race/ethnicity were found to be significantly associated with consent (\ud835\udc43 < 0.05, data not shown).\nControlling for age group, other Hispanics were statistically less willing to donate a specimen (OR 0.236, 95% CI:\nThe Scientific World Journal 0.079, 0.706), as were Non-Hispanic Asians (OR 0.212, 95% CI: 0.074, 0.602). Subjects self-identified as other/multiracial were also statistically less likely to consent (OR 0.189, 95% CI: 0.037, 0.957). These findings indicate that the odds of other Hispanics consenting to biospecimen research were 0.24 times the odds of non-Hispanic Whites given the same age group. In this analysis, among race/ethnic groups, those in the 20-39 years and 40-59 years age groups are more willing than those in the over 60 years age group to provide consent for storage of a biospecimen for use in future studies with ORs of 2.215 (95% CI: 1.006-4.879) and 9.375 (95% CI: 2.163, 40.637), respectively.\n---\nDiscussion\n\nGiven that the 2011-2012 NHANES dataset is the first to include the category of non-Hispanic Asians for race/ethnicity, the lower consent rates by non-Mexican Hispanics, non-Hispanic Asians, and other/multiracial individuals are consistent with previous findings that some ethnic minority groups may be less willing to consent to provide a biospecimen [2, 7-9, 19-23, 25, 30] while adding depth to these results through the introduction of more specific ethnic minority groups.\nAn interesting finding in our study is that non-Hispanic Blacks had consent rates on par with non-Hispanic Whites.  These results were not consistent with previous studies where Blacks have had lower consent rates to specimen donation [2, 7-9, 19-23, 25]. These data may reflect aggressive recruitment of non-Hispanic Blacks or the ability of researchers within the study to build trust with recruits [31,32], perhaps concomitantly with increasing public interest in genetic research and personalized medicine [31][32][33]. Additional research is warranted to confirm these findings.\nIn fact, there is a paucity of information on consent rates among ethnic minorities but in the few studies to date, studies have included very few minorities other than African-Americans [2, 4, 7-9, 19-23, 25]. Consent rates among non-Mexican Hispanics have not been explored. As public awareness and salience of genetic research for all US populations increases [34], it is important to reassess attitudes of ethnic minority groups concerning these issues. While minorities make up a growing proportion of the US population, there continues to be a lack of information on how to best meet the health care needs of this segment of the population [30,35,36]. These observations regarding different consent rates between age groups add new evidence where previous studies had conflicting results [2, 7-9, 19-23, 25].\nLimitations of our study include that we were unable explore the reasons for nondonation. Future studies should include qualitative assessments of the reasons for withholding consent for storage for future studies. These may include health literacy, participant involvement in study development, and the use of community health workers for data collection and recruitment, all of which have been suggested in other studies as potential factors influencing consent [30,36,37]. Although we cannot explain differences in consent rates, our findings have significant implications for future studies that include biospecimen collection.\nResearch utilizing biospecimen holds promise for assisting in better care for a host of diseases, but results will be less generalizable without the inclusion of minorities [37]. This analysis also illuminates the fact that categorization of minority ethnic groups is important in uncovering the reasons for nonparticipation by varying subgroups. This study demonstrates that already underrepresented groups may be less willing to consent to this important research. Therefore, it provides rationale for future studies to understand the reasons for nonparticipation and consequently, to support future research that requires biospecimens.\n---\nConflict of Interests\n\nThe authors declare that there is no conflict of interests regarding the publication of this paper.\nSubmit your manuscripts at http://www.hindawi.com\n---\nStem Cells International\n\nHindawi ",
        "Introduction\n\nThe National Survey of Family Growth (NSFG) is designed to complement birth certificate data collected through the National Vital Statistics System by collecting information on the factors that affect birth and pregnancy rates, including sexual activity, marriage, divorce, cohabitation, contraceptive use, and infertility (1)(2)(3)(4)(5)(6)(7)(8)(9). Measuring contraceptive use is one of the NSFG's central goals because it is a key factor affecting birth and pregnancy rates (1)(2)(3)(4)(5)(6). Women and their male partners may use different contraceptive methods to prevent and plan for pregnancy.\nIn addition to presenting use of contraception overall, this report describes the types of contraception used. Different contraceptive methods have different effectiveness rates for preventing pregnancy (10)(11)(12). This has potential implications for demographic group differences in unintended pregnancies (13)(14)(15)(16). Contraceptive use or nonuse may also be related to desire or ambivalence about becoming pregnant (17,18). Information on ever use of barrier methods, such as male condoms, is also presented, providing a basic description of ever use of methods that can help prevent the transmission of NCHS reports can be downloaded from: https://www.cdc.gov/nchs/products/index.htm.\nsexually transmitted infections, including HIV (19).\nThis report is one of two complementary NSFG reports on contraceptive use produced by the National Center for Health Statistics, concerning ever use and current use of contraception. The most recent report on current use was based on survey data collected in 2017-2019 (4) and focused on the methods, if any, that women were using during the month of interview (current contraceptive status). These two reports together provide a more comprehensive description of contraceptive use, because women may use different methods over their reproductive life course.\nThis report provides an update on ever use of contraception based on the most recent 4 years of NSFG data; the last National Center for Health Statistics report on this topic was based on 2006-2010 data (5). In addition to describing ever use of contraception, this report also presents data on selected contraceptive methods that women tried and then discontinued due to reasons other than seeking a pregnancy. Reasons for discontinuing use of those methods are described.\n---\nMethods\n\n\n---\nData source\n\nThis report uses data from the 11,695 female respondents ages 15-49 of the combined 2015-2017 and 2017-2019 NSFG. Most analyses in this report are based on data collected from the 10,122 women who had ever had vaginal intercourse with a male partner (sexually experienced women). Data for the combined 2015-2019 NSFG were collected through in-person interviews of a representative sample of men and women ages 15-49 in the U.S. household population. Only one person was selected for the main interview in each sampled household. Detailed information about the 2015-2017 and 2017-2019 NSFG has been published elsewhere (20,21). NSFG is jointly planned and funded by the National Center for Health Statistics and several other programs of the U.S. Department of Health and Human Services (see Acknowledgments). The questionnaire for women collected information on their pregnancies and births, marriages and cohabitations, contraceptive use, infertility, general and reproductive health, and social and demographic characteristics (22). The overall response rate for men and women in 2015-2019 for ages 15-49 was 64.3%, and the response rate for women was 65.9% (23).\nAlthough describing changes over time in contraceptive methods ever used is not a primary focus of this report, Table 7 shows ever use of contraceptive methods for 2006 \n---\nMeasures\n\nThis report focuses on contraceptive use among sexually experienced women. The focus is on data from women because a goal of this report is to describe contraceptive methods ever used up to the time of interview (lifetime use). The questions on contraceptive use asked of men in NSFG refer to use on specific occasions (such as first or most recent sexual intercourse) or with specific partners, not lifetime use. Information on the association between contraceptive use and each of the demographic variables included in this report, including Hispanic origin and race (4,5,(24)(25)(26)(27), education (4,5,(27)(28)(29), religion (4,5,(30)(31)(32), and urban-rural residence (33)(34)(35), has been published elsewhere. Descriptions of variables used in this report follow.\n---\nContraceptive use and sexual experience\n\nThe NSFG questionnaire for women asks a series of yes or no questions on whether she or a male partner had ever used each of more than 20 methods of contraception up to the time she was interviewed. Respondents are instructed to report use for any reason and to answer yes even if they used the method only once. The series begins with 11 separate questions asking about the following methods: birth control pill, condom, vasectomy, Depo-Provera, withdrawal, calendar rhythm method, Standard Days or CycleBeads method, safe period by temperature or cervical mucus test, contraceptive patch, vaginal contraceptive ring, and emergency contraception. Women are then asked a \"select all that apply\" question listing 10 other specific methods (plus a residual \"other methods\" category for any method not previously asked about) to identify which she had ever used.\nIn addition to presenting estimates for contraceptive methods individually, a summary category is presented indicating women's ever use of \"any most or moderately effective reversible method\" of contraception, defined in this report to include the pill, contraceptive implants such as Norplant and Implanon (5-and 3-year implants), 1-month injectable Lunelle, 3-month injectable Depo-Provera, contraceptive ring, contraceptive patch, and intrauterine device (IUD). These methods are included in this category because they have the lowest typical-use failure rates of available nonpermanent methods (10,11). Male and female sterilization, two of the most effective methods of contraception for preventing pregnancy (10,11), are generally nonreversible and not included in this subcategory; estimates of sterilization use are shown separately. Since the use of these most or moderately effective reversible methods requires a woman to see a medical professional to obtain the method or a prescription for the method, this summary measure also indicates the number and percentage of women who have used methods requiring periodic or regular contact with a medical professional.\nThe measure of sexual experience used in this report indicates ever having had vaginal intercourse with a male partner (recode variable HADSEX).\n---\nDiscontinuation of contraceptive methods due to dissatisfaction\n\nAll women who ever used at least one contraceptive method were asked if they ever stopped using a method because they were not satisfied with it. Women were instructed not to count discontinuing the method because they wanted to get pregnant. Women who reported ever having discontinued use of the pill, male condom, or IUD were asked about the reason or reasons they were not satisfied with that method. Women could report more than one reason for discontinuation. Women who discontinued use of one brand or formulation of a particular method due to dissatisfaction, but then used another, are included. For example, a woman who may have stopped using one formulation of birth control pills due to side effects and then began using a different formulation would be considered to have discontinued using the pill due to dissatisfaction.\n---\nDemographic variables\n\nEducation-Categories are no high school diploma or GED, high school diploma or GED, some college, no bachelor's degree, and bachelor's degree or higher, as measured by the highest degree a woman had finished at the date of interview (recode variable HIEDUC). Results are presented only for respondents ages 22 and older because many younger women have not completed their education.\nHispanic origin and race-Classified according to 1997 Office of Management and Budget guidelines for the presentation of race and ethnicity data in federal statistics (36). These guidelines allowing respondents to report more than one racial or ethnic origin are reflected in the recode variable, public-use recode HISPRACE2. The NSFG question asking about race includes 14 response options and allows the respondent to choose one or more options. A separate question asks about Hispanic origin. The recode variable HISPRACE2 collapses this information into four categories: Hispanic; non-Hispanic White, single race; non-Hispanic Black, single race; and non-Hispanic other or multiple races. HISPRACE2 along with additional nonpublic variables are used to define the categories presented in Table 2: Asian non-Hispanic single race (subsequently, Asian), Black non-Hispanic single race (subsequently, Black), White non-Hispanic single race (subsequently, White), and Hispanic (including by nativity).\nNativity among Hispanic women-All NSFG respondents are asked if they were born outside of the United States (variable BRNOUT). In this report, nativity is presented only for Hispanic women.\nUrban-rural residence-A respondent's place of residence at the time of interview in NSFG is categorized as a principal city of a metropolitan statistical area (MSA), other MSA, and not MSA (recode variable METRO). For this report, principal city of MSA and other MSA were classified as urban residence, and not MSA was classified as rural residence. The respondent's address was classified according to U.S. Census 2010 population counts using Office of Management and Budget definitions (37).\nReligious affiliation-Presented as four groups: no religion, Catholic, Protestant, and other religion, based on the categories in the RELIGION recode variable.\nImportance of religion in daily life-Based on responses to a question (RELDLIFE) asked only of respondents who reported some religious affiliation: \"Currently, how important is religion in your daily life? Would you say it is very important, somewhat important, or not important?\"\n---\nStatistical analysis\n\nStatistics for this report were produced using SAS software, Version 9.4 (38). All estimates for 2015-2019 were weighted to reflect the reproductiveage female household population of the United States for July 2017, the approximate midpoint of 2015-2019 interviewing. More detailed information on sampling errors in NSFG has been published elsewhere (22).\nPercentages were compared using two-tailed t tests at the 5% level. No adjustments were made for multiple comparisons. Survey clusters minus strata were used as the degrees of freedom for significance testing of pairwise comparisons. A weighted least squares regression method was used to test for linear trends across education, using the number of categories minus two as the degrees of freedom. Terms such as \"greater than\" and \"less than\" indicate that a statistically significant difference was found. Terms such as \"similar\" or \"no difference\" indicate that the statistics being compared were not significantly different. Lack of comment regarding the difference between any two statistics does not mean that the difference was tested and found not to be significant. All estimates presented meet National Center for Health Statistics guidelines for presentation of proportions (39).\nThis report presents basic descriptive statistics on the ever use and discontinuation of use for specific contraceptive methods in the United States using data from 2015-2019. It does not attempt to demonstrate cause-and-effect relationships. Estimates have not been standardized for differences across groups, such as by age. Changes between the two time points in Table 7 \n---\nResults\n\n\n---\nPercentage of women who ever had sexual intercourse and ever used contraception\n\nThe Table provides the percentage of all women by age who ever had sexual intercourse with a male, the percentage who had ever used contraception (regardless of whether they have had sexual intercourse with a male), and the percentage of sexually experienced women who had ever used a method of contraception. Among the full sample of women ages 15-49, 88.0% of women had ever had sexual intercourse, including 97.2% of women ages 25 and older.\nThe percentage of all females who had used a method of contraception-regardless of whether they had sexual intercourse with a male-increased with age, from 38.0% among teenagers ages 15-17 to 71.8% at 18-19, 86.5% at 20-24, and 97.3% at 25-49.\nThe percentage of females who had used contraception exceeded the percentage who had sexual intercourse at ages 15-17 and 18-19, because some females use contraceptive methods before first sexual intercourse for reasons such as regulating menstrual periods or reducing cramps or pain during menstrual periods, or in preparation for first intercourse (6). Among women ages 15-49 who ever had sexual intercourse with a male, virtually all had ever used contraception (99.2%).\n---\nEver use of contraceptive methods among sexually experienced women\n\nTable 1 presents estimates of ever use of any method of contraception and specific methods of contraception for sexually experienced women ages 15-49. In addition, Table 1 includes estimates for selected subcategories, including use of a most or moderately effective reversible method, long-acting reversible contraception, IUDs, and fertility awareness-based methods.\n\u2022 About one in five sexually experienced women had a female sterilization procedure (21.3%), and 14.6% of sexually experienced women had sexual intercourse with a male partner who had a sterilizing procedure (Table 1).\n\u2022 Nearly 9 in 10 sexually experienced women had ever used a most or moderately effective reversible method of contraception (87.8%). \u2022 One in four sexually experienced women had ever used longacting reversible contraception (contraceptive implant or IUD) (24.9%) (Figure 1). Higher percentages of sexually experienced women ages 20-29 (27.6%) and 30-39 (28.9%) had ever used longacting reversible contraception compared with sexually experienced women ages 15-19 and 40-49 (19.2% each).\n\u2022 About one in four sexually experienced women had ever used the injectable contraceptive Depo-Provera (24.5%). \u2022 About four out of five sexually experienced women had ever used the pill (79.8%). \u2022 About one in five sexually experienced women had used an IUD (20.4%). \u2022 Nearly all sexually experienced women had ever used a condom with a male partner (94.5%). \u2022 About two in three women had ever used withdrawal as a method of contraception with a partner (65.7%). \n---\nDifferences by Hispanic origin and race\n\nDifferences in the percentage of sexually experienced women who had ever used specific methods of  \n---\nDifferences by education\n\nEver use of contraceptive methods is shown among sexually experienced women ages 22-49 by educational attainment in Table 3.\n\u2022 The total percentage of women ages 22-49 who had ever used any contraceptive method ranged from 97.3% to 99.6% across educational attainment. \u2022 The percentage of women who had used female sterilization decreased with greater education, from 41.7% for women with less than a high school diploma or GED to 13.1% for women with a bachelor's degree or higher. \u2022 A lower percentage of women without a high school diploma or GED had ever used the pill (67.1%) compared with women with higher levels of education (86.8% for women with a bachelor's degree or higher). \u2022 The percentage of women who had ever used Depo-Provera decreased with additional education, from 39.9% of women with less than a high school diploma or GED to 12.7% for women with a bachelor's degree or higher. \u2022 The percentage of women with less than a high school education who had ever used withdrawal (47.2%) was lower than the percentage among women with higher education levels. The percentages among women with a high school (65.2%) or college (65.0%) education were not significantly different from each other. \u2022 An increasing trend was seen in the use of emergency contraception by education, ranging from 12.2% for women without a high school Hispanic, foreign born 2 Non-Hispanic, single race diploma or GED to 26.0% for women with a bachelor's degree or higher.\n\u2022 Ever use of fertility awareness-based methods for pregnancy prevention increased with educational attainment (Figure 3). An increasing trend in use was seen by education, from 11.1% for women without a high school diploma or GED to 23.6% for women with a bachelor's degree or higher.\n---\nEver use by current religious affiliation and importance of religion\n\nTable 4 shows the percentage of sexually experienced women who had ever used specific methods of contraception by current religious affiliation and the importance of religion in daily life.\n\u2022 Across religious affiliations, 99.7% of women with no religious affiliation, 99.3% of Protestant women, 98.8% of Catholic women, and 97.6% of women affiliated with other religions had ever used a contraceptive method (Table 4). \u2022 The percentages of Catholic (20.8%) and Protestant (25.7%) women who had female sterilization were higher than the percentage for women with no religious affiliation (14.2%). The percentage of Protestant women who had female sterilization was also higher than the percentage among women with other religious affiliations (17.8%). \u2022 The percentage of Catholic women who had ever used a condom with a male partner (91.3%) was similar to the percentage for women with other religious affiliations (89.8%), and both were lower than the percentages for women with no religious affiliation (96.3%) or Protestant affiliations (95.8%). \u2022 Similar percentages of women across religious affiliations shown in Table 4 had ever used a fertility awareness-based method (17.9%-20.2%). \u2022 The percentage of women who had ever used the pill was 76.3% for Catholic women, lower than the percentage among Protestant women (81.8%). Percentages were similar for Catholic women compared with women with no religious affiliation (80.2%) or other religious affiliation (75.9%). \u2022 A higher percentage of women with no religious affiliation had ever used the IUD (24.6%) than Catholic (19.3%) or Protestant (18.2%) women. \u2022 A higher percentage of women for whom religion was very important in their daily lives had a female sterilization (27.0%), compared with those for whom religion was somewhat important (19.8%) or not important (14.6%) (Figure 4). \u2022 Women for whom religion was very important had a lower percentage of ever using any most or moderately effective reversible contraceptive method (86.1%) compared with those for whom religion was not important (90.8%). \u2022 The percentage of women for whom religion was very important who had ever used the pill (78.0%) was lower than the percentage among those for whom religion was somewhat important (81.2%) or not important (84.4%). \u2022 The percentage of women who had ever used the male condom with a partner was lower for women for whom religion was very important (92.2%), compared with those for whom it was not important (97.7%) or somewhat important (95.9%). \u2022 Ever use of fertility awareness-based methods was higher for those for whom religion was very important (20.7%) compared with those for whom religion was not important (12.6%) or somewhat important (17.1%).\n---\nEver use by place of residence\n\nTable 5 shows contraceptive methods ever used among sexually experienced women based on place of residence at the time of interview, categorized as urban or rural residence. Figure 5 also presents estimates for ever use of selected methods by place of residence at the time of interview.\n\u2022 Nearly one in three women living in rural areas had female sterilization (30.1%), higher than the percentage for women living in urban areas (19.5%) (Table 5, Figure 5). \u2022 A higher percentage of women living in rural areas had ever used the pill  (84.8%) compared with women in urban areas (78.8%). \u2022 A higher percentage of women living in rural areas had ever used the 3-month injectable Depo-Provera (29.4%) compared with women living in urban areas (23.5%).\n\u2022 A higher percentage of women living in urban areas had ever used emergency contraception (25.0%) compared with women in rural areas (15.9%). \u2022 Similar percentages of women in urban and rural areas had ever used a condom (94.5% and 94.9%, respectively) or withdrawal (64.8% and 65.9%) with a partner (Table 5).\n---\nDiscontinuation of contraceptive methods\n\nTable 6 shows the percentage of women who had ever discontinued the pill, condom, or IUD due to dissatisfaction (not due to seeking a pregnancy) among those who had ever used the method, as well as reasons for discontinuation.\n\u2022 Of the 52.6 million women who had ever used the pill, 34.1% ever discontinued use because of dissatisfaction with the method (Table 6).\n\u2022 The three most common reasons for discontinuing the pill due to dissatisfaction were side effects (64.1%), difficulty using the method (such as taking the pill as directed) (15.7%), and disliking changes to their menstrual cycle (12.4%). \u2022 Of the 60.2 million women who had ever used a condom with a partner, 8.5% had ever discontinued use due to dissatisfaction.\n\u2022 Of the women who discontinued using condoms due to dissatisfaction, the most common reasons were that the method decreased their sexual pleasure (44.3%), and their partner did not like using them (35.7%). \u2022 Among women who had ever used the condom, 14.7% had discontinued use due to worry that the method would not work. \u2022 Of the 13.0 million women who had ever used the IUD, 32.8% had ever discontinued use because they were not satisfied with the method.\n\u2022 Of women who discontinued the IUD, 64.4% cited side effects. One in five women who discontinued the IUD did not like the changes to their menstrual cycle (19.9%). 1 Significantly higher compared with other importance of religion categories (p < 0.05). NOTES: Sexually experienced is defined in this report as ever having vaginal intercourse with a male partner. Total includes women who did not know or did not report importance of religion in their daily life. See \"Demographic variables\" in this report for details on the question about importance of religion in daily life. \n---\nTrends\n\nThe focus of this report is on describing women's contraceptive experience using 2015-2019 NSFG data rather than on trends over time. However, Table 7 \n---\nSummary\n\nThis report provides a description of ever use of contraceptive methods in a woman's lifetime up to the time of interview using NSFG data from 2015-2019, the most recent data available. Information is presented on the types of methods women used at any time in their lives for any reason, as well as similarities and differences in contraceptive method use by Hispanic origin and race, nativity for Hispanic women, education, place of residence, and religious affiliation and importance. Information is also shown for discontinuation of the pill, condom, and IUD due to dissatisfaction with each method.\nThe main findings of this report can be grouped into three broad areas. First, virtually all sexually experienced women had ever used contraception (99.2%). This held true across Hispanic origin and race, education, religious affiliation, and urban-rural residence. Additionally, nearly 9 out of 10 sexually experienced women had ever used a most or moderately effective reversible method of birth control.\nSecond, variation in the specific types of methods ever used was seen across Hispanic origin and race, education, religious affiliation and importance, and urban-rural residence. Similar percentages of Hispanic and White women had ever used an IUD, higher than the percentages for Black and Asian women. Looking at education, female sterilization was about three times as common among women without a high school diploma or GED (41.7%) compared with those with a bachelor's degree or higher (13.1%). Ever use of Depo-Provera was roughly three times as common among women without a high school diploma or GED (39.9%) compared with those with a bachelor's degree or higher (12.7%). Higher percentages of women living in rural than in urban areas reported having female sterilization (30.1% compared with 19.5%) and ever using the pill (84.8% compared with 78.8%). A higher percentage of women living in urban areas had ever used emergency contraception (25.0%) compared with women in rural areas (15.9%).\nThird, as noted previously, among the 52.6 million women who had ever used the pill, 34.1% (or 17.9 million) discontinued use because of dissatisfaction with the method, most often because of side effects (64.1% of the women who stopped using it), difficulty using the method (such as taking the pill as directed) (15.7%), and disliking changes to their menstrual cycle (12.4%). Side effects were also the most common reason for discontinuation of the IUD among women who had ever discontinued its use. This report on the ever use of contraception, along with the earlier report on current contraceptive status for 2017-2019 (4), provide two key pieces of the portrait of contraceptive use for reproductive-age women in the United States. The report on current use describes contraceptive methods being used during the month of interview among women and provides a detailed view of recent contraceptive status. This report on ever use provides a description of contraceptive methods ever used throughout women's lives up to the date of interview among all women who had ever had vaginal intercourse with a male partner. Together, these reports provide a more complete description of recent and lifetime contraceptive use among women in the United States.  1 Male sterilization includes vasectomy or any other operation that makes it impossible to father a baby. Female sterilization includes tubal sterilization, hysterectomy, ovary removal, and any other operation that makes it impossible to have a baby. 2 Includes contraceptive implant, IUD, injectable, pill, contraceptive patch, and contraceptive ring. 3 Includes contraceptive implant and IUD. 4 Includes ever use of other or unknown type of IUD, not shown separately, and ever using one or more types of IUD. 5 Includes calendar rhythm, Standard Days, or CycleBeads methods. 6 Includes safe period by temperature or cervical mucus test. 7 Includes cervical cap and other methods.\nNOTE: Sexually experienced is defined in this report as ever having vaginal intercourse with a male partner. SOURCE: National Center for Health Statistics, National Survey of Family Growth, 2015-2019.  0.0 Quantity more than zero but less than 0.05. 1 Includes women of other or multiple race and origin groups, not shown separately. 2 Male sterilization includes vasectomy or any other operation that makes it impossible to father a baby. Female sterilization includes tubal sterilization, hysterectomy, ovary removal, and any other operation that makes it impossible to have a baby. 3 Includes contraceptive implant, IUD, injectable, pill, contraceptive patch, and contraceptive ring. 4 Includes contraceptive implant and IUD. 5 Includes ever use of other or unknown type of IUD, not shown separately, and ever using one or more types of IUD. 6 Includes calendar rhythm, Standard Days, or CycleBeads methods. 7 Includes safe period by temperature or cervical mucus test. 8 Includes cervical cap and other methods.\nNOTES: Sexually experienced is defined in this report as ever having vaginal intercourse with a male partner. People of Hispanic origin may be of any race.       ---Data not available; type of IUD ever used was not asked in 2006-2010. 1 Male sterilization includes vasectomy or any other operation that makes it impossible to father a baby. Female sterilization includes tubal sterilization, hysterectomy, ovary removal, and any other operation that makes it impossible to have a baby. 2 Includes contraceptive implant, IUD, injectable, pill, contraceptive patch, and contraceptive ring. 3 Includes contraceptive implant and IUD.\n---\nNational Health Statistics Reports \uf0be Number 195 \uf0be December 14, 2023\n\nFor more NCHS NHSRs, visit: https://www.cdc.gov/nchs/products/nhsr.htm.\nFor e-mail updates on NCHS publication releases, subscribe online at: https://www.cdc.gov/nchs/email-updates.htm. For questions or general information about NCHS: Tel: \n---\nCopyright information\n\nAll material appearing in this report is in the public domain and may be reproduced or copied without permission; citation as to source, however, is appreciated. \n---\nNational Center for Health Statistics\n\n",
        "Introduction\n\nSocial innovation has evolved throughout history as a means to address social problems and needs. However, its definition, characteristics, and scope did not become clear until the 1990s, as noted by Garc\u00eda-Flores and Palma in 2019. Nevertheless, it was only in the year 2000 that social innovation began to gain relevance in key documents and policies aimed at addressing pressing social issues, as pointed out by Moulaert and McCallum (2019). The consolidation of this concept occurred after the 2008 crisis when the European Commission identified it as a means to promote a more socially oriented market economy.\nToday, there is broad consensus among politicians, public officials, scientists, and civil society in general regarding the positive impact of social innovation. However, there is still a lack of agreement on the specific areas and sectors where social innovation practices can generate the greatest impact. This is where social innovation emerges as a novel means to carry out actions through alternative forms of social organization. In this sense, it is of particular interest to identify the fields in which social innovation can be most effective. Therefore, the main objectives of this article are to identify and analyze, based on their relevance, the areas in which social innovation has the greatest potential for development. Additionally, it aims to highlight the potential implications that social innovation could have in achieving the Sustainable Development Goals (SDGs) established by the United Nations in 2015.\nThis approach is of vital importance because the SDGs represent a set of critical goals and challenges to address fundamental issues such as poverty eradication, gender equality, access to education, and environmental sustainability. Social innovation can play a key role in achieving these objectives by providing creative and effective solutions to complex and urgent social problems.\nThe structure adopted to address this article is organized as follows: first, this introduction is presented, serving as the starting point for the study. Next, in the second section, a comprehensive review of existing literature on the subject is conducted. In the third section, the research methodology employed is described in detail. The fourth section is dedicated to presenting the results obtained, identifying 17 impact areas and analyzing their relationship with the Sustainable Development Goals (SDGs). Finally, in the concluding section, the main conclusions derived from the study are presented, and the bibliography used as a reference is provided.\n---\nTheoretical Framework\n\n\n---\nThe Impact of Socially Innovative Initiatives\n\nThe existing body of literature on social innovation encompasses a wide array of perspectives and definitions (Pastor & Balbinot, 2021). Nevertheless, a common thread runs through these diverse viewpoints: social innovation is fundamentally characterized by its ability to address societal challenges and fulfill social needs, ultimately aiming to enhance overall wellbeing (Garc\u00eda-Flores & Palma, 2019). Social innovation initiatives often embrace a collaborative, grassroots approach involving citizens, primarily seeking to tackle a broad spectrum of social issues by providing alternative solutions distinct from traditional market or public sector offerings (Mart\u00ednez et al., 2019). According to Boni et al. (2018, 68), social innovation comprises a set of practices and activities designed to address social problems and human needs. However, these authors underscore that social innovation extends beyond mere immediate problemsolving, as it should involve the enhancement of social relationships and structures.\nWhile there is a consensus on the positive potential of social innovation for enhancing overall well-being, a clear consensus is lacking regarding the specific domains where its impact is most significant (Hern\u00e1ndez-Ascanio et al., 2016, 178). Moreover, due to its interdisciplinary and crosscutting nature, delineating its impact within a single specific area is challenging, as pointed out by Moulaert et al. (2017). In this context, it becomes paramount to explore the domains that existing literature has identified as conducive to the emergence and effectiveness of social innovations. In Table 1 below, we present areas highlighted in the literature as contexts where social innovations can flourish. \n---\nAreas\n\nAreas Authors point out Better healthcare (Mulgan, 2006;Howaldt et al., 2016) Contributes to reducing unemployment (Grimm et al., 2013;Howaldt et al., 2016: 187) Empowers the role of women in society (Mulgan, 2007;Peeters & Ateljevic;2017 (Buckland & Murillo, 2014;Howaldt et al., 2016;Millard, 2018;Eichler & Schwarz, 2019 (Mart\u00ednez et al, 2019;Chueri & Araujo, 2019, 28) Social inclusion and poverty reduction (Mulgan, 2006;Gonz\u00e1lez et al., 2014;Howaldt et al., 2016) Source: Author's elaboration.\nTable 1 shows that different authors have identified multiple areas of social innovation impact, covering a wide range of domains and issues. Despite the apparent versatility and capacity of social innovation to address a wide range of needs and problems, it is essential to have a more precise understanding of the areas in which social innovation initiatives are more likely to excel and achieve the desired success.\nThe importance of identifying the areas where social innovation can have a greater impact lies in the ability to strategically allocate resources, such as time and funding, to achieve effective results. This avoids scattering resources across a wide range of social challenges and instead increases the efficiency of interventions, as well as their visibility and attractiveness to those responsible for designing and implementing them.\nFurthermore, this precise understanding facilitates the alignment of social innovation initiatives with broader development goals, such as the Sustainable Development Goals (SDGs). This helps to address critical challenges in a way that is consistent with global goals.\nFor this reason, section 4 provides a more detailed analysis based on interviews with experts in the field. The aim is to identify and highlight the key areas where social innovation practices have the greatest potential for impact.\n---\nThe potential of social innovation to contribute to the Sustainable Development Goals (SDGs)\n\nThe existing literature highlights the remarkable capacities of social innovation in the context of sustainable development. This close connection stems from the global growth of social problems and challenges, which has put increasing pressure on various organisations to adopt and implement social innovation approaches (Eichler & Schwarz, 2019;Kruse et al., 2019). As Angelidou and Psaltoglou (2017) point out, civil society, the private sector and the public sector have begun to recognise that the current system of production is showing signs of exhaustion and requires a profound overhaul. As a result, the pursuit of the desired sustainable development, as shown by Rodrigo and Arenas (2014), involves significant changes in governance, which has led to a greater emphasis on social aspects in a possible alternative paradigm.\nThis growing interest in sustainable development reached an important milestone in 2015, when the United Nations General Assembly approved an agenda for sustainable development with 17 main goals and 169 targets to be achieved by 2030. The Sustainable Development Goals (SDGs) were endorsed by 193 countries, including emerging economies such as India and Brazil (Millard, 2018). Millard (2018) highlights that, according to the United Nations, sustainable development involves meeting the needs of the present without compromising the opportunities of future generations. This definition first appeared in the Brundtland Report in 1987. The UN resolution adopted on 25 September 2015 emphasises that to achieve sustainable development, progress must be balanced across economic, social and environmental dimensions. In this context, Millard (2018) argues that the failure to achieve sustainable development is due to the lack of consideration of any of these three dimensions, a perspective that applies to all the SDGs, which address a wide range of areas.\nThe SDGs represent a set of essential challenges and goals to ensure sustainable development in the medium and long term. They currently constitute the most ambitious global agenda agreed by the international community to mobilise collective action around goals shared by all countries (Gil, 2018). However, it is important to note that while the SDGs provide a common framework, each government must set its own national goals and decide how to incorporate these aspirations into its plans, strategies and actions (United Nations, 2015).\nSocial innovation is emerging as an essential phenomenon that public decision-makers can promote to contribute to the achievement of these goals (Moore, 2015;Millard, 2018). In this regard, Herrero (2019, 49) emphasizes the potential of social innovation to address the key European and global social challenges, with a particular focus on the SDGs. Eichler and Schwarz (2019), in their study of 128 social innovation initiatives, investigated whether they contributed to addressing any of the SDGs. They found that a total of 115 initiatives were linked to solutions for at least one of the SDGs, representing over 89% of the total. This led them to conclude that social innovation and the SDGs are fully aligned. In the same vein, Millard (2018) argues that the UN's 2030 SDGs have contributed to the recognition of the relationship between social innovation and sustainable development, which has led to increased attention on this approach.\n---\nMethodology\n\nA number of primarily qualitative analytical techniques were used to complete this research article. Specifically, a methodology combining literature review and semi-structured interviews was developed.\nWith regard to the literature review, an exhaustive study was carried out of the most cited academic works, as well as the main reports published by local, national and international institutions. This allowed for an understanding of the different impacts, areas and implications that social innovation can have.\nOnce the literature review was completed, and with an understanding of the issues at hand, semi-structured interviews were conducted to analyse the discourse and explore the areas where socially innovative initiatives could have a greater impact.\nFinally, once the different areas where socially innovative initiatives are more likely to develop were known, the possible alignment of these areas with the Sustainable Development Goals was analysed.\n---\nInterview methodology\n\nThe existing literature highlights the remarkable capacities of social innovation in the context of sustainable development. This close connection stems from the global growth of social problems and challenges, which has put increasing pressure on various organisations to adopt and implement social innovation approaches (Eichler & Schwarz, 2019;Kruse et al., 2019).\nA total of 24 semi-structured interviews were conducted in order to identify the sectors where social innovation can have a greater impact. These interviews were divided into several parts: In the first part, general questions about social innovation were asked; in the second part, inquiries were made about the most relevant aspects of the creation process of social innovation; and finally, there was a section dedicated to exploring the effects and results that, according to the interviewees, social innovation could generate. The script used for the semi-structured interviews can be found in Annex 1.\nThe interviewees were divided into three categories: leaders of initiatives recognised as socially innovative, agents and promoters of social innovation, and academics who have published prominent research in the field. The full list of interviewees can be found in Annex 2.\nThe interviewees were selected on the basis of the relevance of their experiences and projects, their academic and professional background, and the awards and recognition they have received from prestigious organisations in the field of social innovation.\nA total of 87 people were contacted, of whom 24 agreed to participate in the interviews. In addition, in some cases, ongoing research allowed the identification of experts and interesting profiles that met the established criteria. The interviews took place during the months of June, July, September, October and November 2018, both in person and remotely, with an average duration of 50 to 75 minutes.\n---\nData Analysis\n\nAtlas.ti software was used to analyse the content of the 24 interviews. According to Prados (2007), Atlas.ti is a computer tool capable of structurally analysing complex qualitative data based on grounded theory, which aims to generate theory while studying the phenomenon.\nContent analysis made it possible to identify the key elements of a phenomenon based on the words and phrases used by the interviewees (Noguero, 2002). The linguistic and semantic contexts of words or phrases expressing relevant concepts were examined. A total of 24 primary files corresponding to interview transcripts were analysed.\nFirst, the interviews were coded, which involved segmenting relevant fragments of information (quotes) and labelling them with codes representing attributes related to the content of the quotes. Each time a quote related to an area where social innovation practices could be developed, a specific code was assigned. This coding made it possible to establish links between different parts of the information.\nAtlas.ti was also used to count the absolute frequency of the codes assigned. This provided information on how often a particular attribute was mentioned in the content of the interviews. Then, to assess the relative importance of each attribute, its relative frequency was calculated, which refers to the number of times a particular code is repeated in relation to the total number of repetitions of all codes.\nThis statistical content analysis helped to identify the most important areas where social innovation could have a significant impact.\n---\nResults\n\n\n---\nAreas of Social Innovation Development Identified from Interview Analysis\n\nAfter analyzing the content of the interviews, we have identified a total of 17 codes related to areas where social innovation can have a significant impact. Each of these codes represents a specific domain in which social innovation could play an important role. Furthermore, we have categorized these areas into three main spheres based on the relationships observed during the content analysis: the socioeconomic sphere, the sustainability and well-being sphere sphere, and the social sphere. Table 2 shows the priority areas where social innovation initiatives can have a more significant impact. Columns two and three show the absolute and relative frequencies of these areas, respectively. These frequencies are used as indicators to assess in which areas social innovation is more likely to have a greater impact. A higher frequency indicates that respondents repeatedly mentioned that area as conducive to the development and impact of social innovation.\nThe identification and weighting of these areas provides valuable information for those responsible for promoting, driving or developing initiatives in specific areas. It allows them to understand whether experts see opportunities for significant impact through social innovation in specific areas. Knowing that there are socially innovative approaches that differ from traditional methods is a crucial resource that can guide those who want to initiate projects to achieve impact in a particular area.\nMoreover, by grouping all these fields into three main areas, we can see that social innovation faces three main types of challenges: economic and political, global and social. Below we will analyse each of these identified areas based on their relative importance within these spheres.\n---\nSocioeconomic sphere\n\nThis sphere encompasses the primary impact areas of social innovation in relation to the socioeconomic domain, comprising a total of six fundamental areas:\n1. Governance: Governance reflects the space where decisions that influence economic and social development are made. Historically, this domain was dominated by actors from the public sector. However, today, socially innovative initiatives are emerging that promote new models and approaches, challenging traditional governance. These initiatives drive a transition from a vertical and dominant focus of the public sector toward a more democratic and participatory model, creating inclusive governance. and generate new forms of production and consumption. These alternative models include self-consumption, the consumption of ecological and local products, collaborative consumption, and fair-trade practices. In addition to creating a new consumption paradigm, these practices impact areas such as rural development, local development, employment, and citizen empowerment. These areas are not only essential for the socioeconomic sphere but also interconnect and influence each other, highlighting the importance of social innovation initiatives in addressing contemporary challenges.\n---\nSustainability and Wellbeing Sphere\n\nThis sphere encompasses the primary impact areas of social innovation related to sustainability and wellbeing, comprising a total of six fundamental areas:\n1. Environment: Highlighted as the primary focus area, the environment constitutes a critical field. Social innovation has become a key driver for addressing sustainability issues that affect both present and future generations. Innovative initiatives play a crucial role in combating climate change, reducing pollution, managing water scarcity, and preserving biodiversity. 2. Education: It is identified as an essential domain where social innovation is indispensable. Education is being transformed by initiatives that aim to turn educational institutions into spaces for creativity and knowledge development. Furthermore, these initiatives improve collaboration among actors in the education system and promote technical and social skills for employability. 3. Housing: Access to adequate housing is a fundamental right facing significant challenges in today's society. Here, social innovation stands as a crucial tool to address this issue.\nCreative initiatives are facilitating housing access through solutions such as prefabricated housing for the homeless, collaborative housing models (cohousing), and floating houses in flood-prone areas. 4. Agriculture: Agriculture, beyond food production, is becoming an important field for social innovation. Initiatives such as urban agriculture, agrarian systems that ensure food sovereignty, and social agriculture are proliferating. These practices have an impact that extends to areas such as employment and the reduction of social exclusion. 5. Health: Healthcare is a fundamental concern in society. Social innovation positions itself as an effective tool to address issues and demands in this sector. It facilitates coordination between the public and private sectors for more effective solutions and promotes collective action by civil society to address unmet health needs. During the COVID-19 pandemic, many socially innovative initiatives emerged to address the psychological needs of the population. 6. Energy: Access to energy services is essential for wellbeing but is not always guaranteed for those with limited resources. Social innovation presents itself as a solution, promoting more decentralized energy models with a focus on self-consumption and alternative energy sources. These initiatives seek to eliminate economic barriers to energy Access These interconnected and high-impact areas demonstrate the importance of social innovation in transforming our world toward a more sustainable and prosperous future.\n---\nSocial Sphere\n\nIn this sphere, crucial areas of impact for social innovation initiatives related primarily to social challenges converge. Five areas that address fundamental needs and rights for social wellbeing stand out:\n1. Social Exclusion: Social exclusion, a result of systemic inequalities, is a central concern. Social innovation is recognized for its ability to address and reduce social exclusion.\nNumerous socially innovative initiatives work on creating social and symbolic capital to ensure the basic rights and freedoms of individuals. 2. Social Rights and Social Justice: Closely related to social exclusion, the emphasis on achieving greater social justice and social rights goes hand in hand with reducing social exclusion. Many social innovation initiatives designed to improve this area also have a positive impact on reducing social exclusion. 3. Women's Empowerment: Addressing gender inequality is a key priority in modern society. Women's empowerment is essential to overcome structural barriers. Many socially innovative initiatives focus on promoting women's empowerment as part of the national and international social agenda. 4. Persons with Disabilities: Personal identity and independence for people with disabilities are essential. Social innovation initiatives align with this goal, working to eliminate discrimination and exclusion. These initiatives seek to facilitate the lives of people with disabilities, promoting their personal and professional development for independent living. 5. Active Aging: The aging of the population is a challenge in modern society. Social innovation addresses these issues with practices that respond to the needs of aging societies. Initiatives promote activities to improve the physical and mental health of older people, encourage intergenerational spaces, provide transition measures into retirement, and offer creative workshops to contribute to the well-being and active participation of older people in society. These interconnected areas highlight the crucial role of social innovation in promoting equity, inclusion, and social justice in our communities. Social innovation acts as a driver to address complex social challenges and advance toward a more equal and just society.\n---\nAlignment of Social Innovation Impact Areas and Sustainable Development Goals (SDGs)\n\nWe have previously established the close relationship between sustainable development and social innovation. Now, it is essential to analyze how the impact areas of social innovation align with the Sustainable Development Goals (SDGs), allowing us to understand how these initiatives can contribute to addressing global challenges and advancing toward a more sustainable future.\nThe following Table 3 shows this direct connection between the SDGs and the impact areas of social innovation: This table shows that all identified areas of social innovation impact are directly related to one or more SDGs. Furthermore, some areas have links to multiple SDGs, highlighting the ability of socially innovative initiatives to address multiple goals simultaneously.\nGiven this close correspondence between social innovation and the SDGs, socially innovative initiatives emerge as essential tools for decision-makers at the national and international levels to achieve the SDGs and advance sustainable development. Social innovation becomes a valuable tool to effectively address social and environmental issues, promote positive change in society, and significantly contribute to the realisation of global sustainable development goals.\n---\nConclusions\n\nSocial innovation is a phenomenon that generates social value and well-being. Its positive effects go beyond its final impact, with great importance attached to the processes and new methods of participation and work developed to achieve better social results. This contributes to the fact that a number of common elements and characteristics are present in the logic of action of any socially innovative initiative, allowing a novel approach to the most important social objectives in a wide range of areas.\nIn this study, we have thoroughly explored the complex relationship between social innovation and the Sustainable Development Goals (SDGs), discovering a synergy that offers a promising path towards a more equitable and sustainable world. Our key findings shed light on the powerful impact of social innovation in achieving social and environmental goals with global reach.\nAt its core, social innovation goes beyond the mere generation of value and wealth; it is a catalyst for processes and methodologies that promote superior social outcomes. This innovative and participatory approach is a common denominator of all the initiatives we have studied, enabling them to address the most pressing social challenges in different fields in a unique and effective way.\nThis inherent logic of action of social innovation initiatives, combined with their close link to sustainable development, allows them to contribute both directly and indirectly to the achievement of socially desirable goals, such as the SDGs. Some of these initiatives may start with primary objectives related to the environment, employment or gender equality, while others, even if these objectives are not a priority, incorporate these aspects into their modus operandi (such as respect for the environment, labour inclusion and gender perspective).\nIn this work we have identified and assessed a total of 17 impact areas, which we have grouped into three categories: political and economic challenges, global challenges and social challenges. This identification and evaluation has led us to conclude that social innovation initiatives have a significant impact in a wide range of areas, with particular emphasis on the environment (0.0959), combating social exclusion (0.0909) and strengthening governance (0.0858). These areas, which have historically been challenging and where traditional policies have proved ineffective, find in social innovation initiatives a new perspective that can provide more effective responses to recurring problems.\nThe identification of these areas has also allowed us to conclude that social innovation initiatives, even when focused on a specific area, can contribute to the achievement of several SDGs. This close link between the SDGs and the areas of social innovation impact clearly shows that both public and private entities can find in this work a solid guide for developing socially relevant, innovation-based initiatives.\nFinally, it is important to highlight two fundamental contributions of this study. On the one hand, we provide solid evidence for entrepreneurs and social organisations wishing to implement actions in specific areas, offering insights on how to approach them innovatively. On the other hand, we identify areas where experts believe there is a greater likelihood of success for social innovation, which is useful for public decision-makers in developing plans and policies that promote and support social innovation initiatives in areas with direct implications for the SDGs.\n---\nAnnexes\n\nAnnex 1. Script -Interviews for Social Innovation and Sustainable Development: An Analysis of its Impact Areas and its Relationship with the SDGs.\nPART I: General Questions about Social Innovation 1. In your perspective, what does social innovation mean to you? 2. What do you believe are the main characteristics and/or properties that an initiative or activity must possess to be considered a social innovation? 3. Do you think the concept of social innovation is being overused? Are you concerned it might lose its value? 4. What do you think motivates agents to engage in socially innovative practices? In your case or that of your association/company/represented sector, what was the main motivation? 5. What role do you believe innovation support centers play in social innovation? And academics?\nWould you identify any other relevant agents for promoting and developing social innovations? What kind of support has your association/company received? 6. What do you think is the profile of socially innovative agents? What kinds of organizations, companies (technological centers, business cooperatives, etc.), and public entities do you believe can engage in socially innovative activities? 7. What stages do you think a socially innovative initiative typically goes through from its inception to consolidation? What is the most critical phase? PART II: Identification of Variables 8. In what types of regions do you believe social innovations tend to develop? 9. Do you think that in regions where there are more problems, more social innovations emerge? 10. What factors facilitated the launch of your socially innovative initiative? 11. What factors acted as barriers or hindered this process?",
        "Introduction\n\nAs the Indigenous peoples of Aotearoa New Zealand (Aotearoa), M\u0101ori comprise 17% of the population and 7% of this total are 65 and older (1). M\u0101ori face significant and stark social and health inequities relative to non-M\u0101ori populations in Aotearoa New Zealand (2,3). For example, M\u0101ori have a life expectancy that is seven years lower than other New Zealanders and yet the major causes of mortality are preventable and treatable (4). Further, \"M\u0101ori experience systematic disparities in health outcomes, determinants of health, health system responsiveness, and representation in the health sector workforce\" (p. 10) (5). These health and social inequities are heightened for kaum\u0101tua (elders) who also face additional challenges including social isolation and loneliness, end-of-life concerns, and chronic health conditions (6,7).\nThese health and social inequities have not decreased over the last 50 years and are largely explained by a variety of structural determinants (8). These include inequalities in social determinants (e.g., housing, income, and education), institutional discrimination from the effects of colonization, and insufficient access to health services (4,9,10). A key component of the explanatory factors is Te Tiriti o Waitangi (Treaty of Waitangi), which is the written agreement for the founding of Aotearoa. It is a disputed document that has different versions in English and M\u0101ori, but guaranteed M\u0101ori rights. Historically, the Treaty was not followed until 1975 with the Treaty of Waitangi Act (8). More recently, Te Tiriti has five principles: (a) Recognition and protection of tino rangatiratanga (self-determination); (b) Equityequal access to health care and equitable outcomes; (c) Active protection-governmental protection of the first two principles; (d) Partnership-government partnering with M\u0101ori; and (e) Options-providing options for services that are grounded in te Ao M\u0101ori (M\u0101ori worldview) (2).\nToday's kaum\u0101tua grew up prior to the Treaty of Waitangi Act and lived in a society that was more racist than in present and were affected by an education system that banned and punished people for embracing tikanga M\u0101ori (cultural protocols) and speaking Te Reo M\u0101ori (language) (11,12). This colonial historical trauma contributed to health inequities through cultural dissonance or feeling of separation from their own culture (13)(14)(15). Further, kaum\u0101tua have not been recognized for their contributions to a dominant society even while M\u0101ori culture upholds elders as, \"carriers of culture, anchors for families, models for lifestyle, bridges to the future, guardians of heritage, and role models for younger generations\" (p. 14) (16).\nThe treaty principles, colonial history, and existing inequities are reasons why numerous researchers have found a need and advocate for services and programmes that are culturally appropriate and safe and that address structural determinants and structural change (7,17,18). However, much of the predominant narrative and specific actions to alleviate inequities are grounded in a deficit approach (19)(20)(21). The deficit approach emphasizes Indigenous communities as 'difficulties' to be fixed relative to mainstream populations. The deficit-model approach sometimes blames M\u0101ori for the challenges they face and does not consider structural and systemic elements including loss of cultural connection resulting from colonization (8,22,23).\nIn contrast, this study offers a strength-based approach grounded in tikanga M\u0101ori and Te Ao M\u0101ori to guide solutions and this particular research. Specifically, the study focuses on kaum\u0101tua mana motuhake (actualization, autonomy, and independence) at an individual and collective level (24). He Huaraki Tautoko (Avenue of Support) is a collaboratively developed programme about intergenerational cultural knowledge exchange that also involves physical activity (24). Physical activity is correlated with various health benefits for older adults, from improved mental wellbeing (25), to a reduction in morbidity, mortality and falls (26). Particular physical activities can also be a means of strengthening cultural connection among M\u0101ori (27), while cultural activities and practices can be a driver that leads to \"incidental\" physical activity (28). He Huarahi Tautoko was constructed by researchers and six community providers along with their kaum\u0101tua through a participatory process. It is based on kaum\u0101tua as carriers of m\u0101tauranga (M\u0101ori knowledge systems) and involved sharing with each other along with a member of their wh\u0101nau (extended family) through w\u0101nanga (learning sessions). The m\u0101tauranga included Te Reo M\u0101ori, whakapapa (genealogy), pur\u0101kau (M\u0101ori lore), waiata (songs), and karakia (prayers). This cultural knowledge was selected as there is a link between cultural continuity and positive health outcomes (29)(30)(31), especially in the context of kaum\u0101tua who have a history of cultural dissonance due to colonial policies and practices. Further, this exchange of m\u0101tauranga was grounded in physical activities such as walks to significant cultural landmarks, gardening (e.g., traditional food preparation), and other cultural practices (dancing, cleaning the marae or community meeting house). Physical activity was important to enhance physical functioning and mental wellbeing. This project is a component of the Kaum\u0101tua Mana Motuhake Poi (KMMP) programme funded by the Aging Well National Science Challenge (https://www.ageingwellchallenge.co. nz/) (24).\nThe cultural knowledge and physical elements are important as part of a holistic model of health important for M\u0101ori communities. There are various models of M\u0101ori health that focus on a holistic perspective with perhaps the most popular being te whare tapa wh\u0101 (four walls of a house) (13,32). Te whare tapa wh\u0101 was chosen by this research partnership to guide this project and includes four elements: te taha wh\u0101nau, te taha hinengaro, te taha wairua, and te taha tinana (social, psychological/mental, spiritual and physical health respectively).\nThere are two aims of this study. The first aim is to present the baseline study from the He Huarahi Tautoko project to establish the initial comparison point and the psychometric characteristics for the measures. The second aim is to identify correlates for five outcomes related to te whare tapa wh\u0101 and mana motuhake: health-related quality of life (HRQOL) and self-rated health for physical and mental wellbeing, loneliness for social health, spiritual wellbeing for spiritual health, and life satisfaction for mana motuhake. Examining the correlates of these health and wellbeing outcomes provides indicators for researchers and practitioners developing programmes and services to address health equity for kaum\u0101tua. They can also reinforce whether the He Huarahi Tautoko project is addressing key attributes.\n---\nMethods\n\nThe larger study has a mixed methods pre-test and two post-test, staggered design; four providers receive the programme initially and two providers receive it later (24). A cross-sectional survey for the baseline measures was the study design for this specific study. The research is grounded in Kaupapa M\u0101ori (33,34) and a participatory research approach, He Pikinga Waiora [enhancing wellbeing (35)]. Kaupapa M\u0101ori emphasizes Te Ao M\u0101ori and tikanga (36), relies on self-determination and uses m\u0101tauranga M\u0101ori and M\u0101ori epistemology (37). He Pikinga Waiora centers Kaupapa M\u0101ori, while also including a partnership model amongst researchers and communities. A partnership of six M\u0101ori social-health service providers and university researchers from four universities comprised the research team. The project is registered with the Australia New Zealand Clinical Trial Registry (ACTRN12621000541808).\n---\nParticipants\n\nParticipants were 75 kaum\u0101tua from six M\u0101ori social-health service providers across Aotearoa. We originally planned to identify a sampling frame and randomly sample participants from each provider. However, the initial planning was prior to the COVID pandemic, and we began the programme in between lockdowns for the pandemic. Kaum\u0101tua were hesitant to participate in group interactions post-lockdowns as they comprised one of the most impacted populations by the virus. Thus, the providers made the determination that they should invite all willing kaum\u0101tua to participate and it became a purposive sample. We had originally sought to recruit 15 kaum\u0101tua from each provider. Four were able to do this and two providers were only able to recruit eight and seven participants. There were 45 women and 16 men (14 did not specify) with an average age of 69.80 (SD = 7.26).\n---\nMeasures\n\nMeasures were organized around our holistic models of hauora (health) and mana motuhake. For hauora, we included the following scales: self-reported health (38,39), HRQOL (40,41), spirituality (42), loneliness (two items from Waldergrave et al. (43) and one item from Hayman et al. ( 6)), perceived and desired social support (44), relationship quality with the person participating with them in the programme (45), cultural connection (29), cultural practices (10 items created for this study), self-reported exercise hours per week (46), and physical functioning (time to complete five chair stands and time to walk 3 meters) (47,48). We also included life satisfaction (49) and sense of purpose (50) for mana motuhake. The measures included two different types of scales:\n(1) 11-point semantic differential scales and (2) Likert-type scales ranging from 3-6 points. The Appendix includes the items from the survey.\nThere were 39 items. Participants could complete the questionnaire on their own in a paper/pencil format or have a M\u0101ori community researcher administer the survey via an interview. The survey was written with English and M\u0101ori versions (back-to-back); it was originally written in English and then translated and back-translated to ensure equivalence of M\u0101ori to English. A large font and sufficient spacing were used for ease of reading for kaum\u0101tua. Participants received a $50 voucher for completing the survey. The University of Waikato's Human Research Ethics Committee, HREC (Health) 2020#93 approved the research protocols. The data collection procedures followed a culturally appropriate approach employed in prior projects (51,52) to provide cultural safety.\n---\nData analysis\n\nCronbach's alpha was used to assess reliability. Items from scales with low reliability were retained as individual items. Descriptive statistics included means, standard deviation and bivariate correlations. Multiple linear regression models (forward method) were employed to were run to determine the correlates of self-rated health, HRQOL, spiritual wellbeing, loneliness, and life satisfaction. The remaining items/scales were included as independent variables if they had a bivariate correlation with an outcome variable (p \u2264 0.10).\n---\nResults\n\nDescriptive statistics and Cronbach's alphas are displayed in Table 1. Spiritual wellbeing, HRQOL, and self-rated health were rescored to a 100-point response scale following prior approaches (53). Other response scores are based on the original response scale described in Table 1. The reliability for the relationship quality (\u03b1 = 0.21) and social support (\u03b1 = 0.37) were too low to warrant scales and thus the individual items were included for analysis.\nThe correlates for the outcomes are included in  emotional support and frequent interaction with their wh\u0101nau member as positive correlates, F (2,74) = 6.521, p = 0.002, adj R 2 = 0.13. The model for spiritual wellbeing included sense of purpose, not needing additional help with daily tasks, and cultural practices as positive correlates, F (3,73) = 15.643, p < 0.001, adj R 2 = 0.37. The regression model for life satisfaction was significant, F (4,72) = 23.203, p < 0.001, adj R 2 = 0.54. Life satisfaction had a positive association with sense of purpose, perceived emotional support, and relationship quality with their wh\u0101nau member; it was negatively related with frequency of interaction with their wh\u0101nau member. Finally, the regression model for loneliness was significant, F (5,71) = 12.291, p < 0.001, adj R 2 = 0.43. Low level of loneliness was positively associated with perceived emotional support, enjoyable interaction with their wh\u0101nau member, sense of purpose, not needing additional help with daily tasks, and exercise.\n---\nDiscussion\n\nThe purpose of this study was to establish the psychometric characteristics and baseline scores for the measures in the He Huarahi Tautoko project. Descriptive statistics indicated the following: (a) high levels of life satisfaction, spiritual wellbeing, sense of purpose, and cultural connection; (b) moderate to good levels of self-rated health, HRQOL, engagement with cultural practices, and social support; and (c) low levels of loneliness and exercise frequency. There are limited direct comparisons to other populations or other studies with kaum\u0101tua in Aotearoa on these scales. The results are very similar to a recent study of kaum\u0101tua in a different project (54) illustrating why it is important to not presume deficits or using a deficit approach when working with kaum\u0101tua specifically or M\u0101ori more generally (22,23). Further, the responses demonstrate possibilities for improvement on the scales so the He Huarahi Tautoko project can positively affect these variables.\nSense of purpose had a positive association for loneliness, life satisfaction, self-rated health, and spiritual wellbeing. Sense of purpose was operationalised as making plans, developing a sense of direction, and having goals (50). Having a sense of purpose has found to be a significant correlate for physical activity, maintaining a healthy BMI, and avoiding sleep problems in the Health and Retirement Study (USA) (55). A sense of purpose can center on a variety of factors such as caring for family, contributing to the community, or continuing to work. For kaum\u0101tua, a key aspect of sense of purpose consists of contributing to the cultural knowledge and tikanga of the community (16). Given the historical cultural dissonance experienced by kaum\u0101tua due to colonization (11,15), the He Huarahi Tautoko project is timely and important.\nSocial factors such as emotional support, tangible support, and relationship quality with their family member who is participating with them in the study were key correlates for HRQOL, life satisfaction, spiritual wellbeing, and low levels of loneliness. Prior research shows social support has a positive relationship for various wellbeing and health outcomes for Indigenous and non-Indigenous populations (29,(56)(57)(58)(59)(60). For older people, high-quality social relationships are important for enhancing quality of life and life expectancy (61)(62)(63)(64). The importance of social relationships was highlighted by the COVID pandemic as many kaum\u0101tua were isolated during lockdown periods (7).\nFrequency of moderate to vigorous exercise was a correlate for self-rated health and low levels of loneliness. A large amount of extant literature identifies a positive association of exercise and physical activity with wellbeing, mental health, health outcomes, and cognitive functioning (65)(66)(67). The relationship with low levels of loneliness is likely due to the preference of older adults to exercise with others as a way to avoid isolation and maintain connections with others (68).\nSelf-rated proficiency in cultural practices was associated positively with spiritual wellbeing. Cultural practices were operationalised as knowledge and confidence in using and sharing tikanga and Te Reo M\u0101ori as well as with roles in the community. Te Ao M\u0101ori is grounded in a cultural and spiritual connection which is important for many kaum\u0101tua (16). The results of this current study are consistent with these perspectives and reinforce the aim of He Huarahi Tautoko to enhance self-rated proficiency of cultural practices. As noted earlier, the cultural dissonance experienced by kaum\u0101tua due to colonization and State policies reinforce the need for culturally resonant programmes to enhance learning about Te Reo M\u0101ori and tikanga M\u0101ori (15,69).\nThere are some implications from this study for Indigenous aging and enhancing cultural practices and physical activity. This study emphasizes a holistic perspective of health including cultural, social, and spiritual elements as well as physical and mental components. Aging well for kaum\u0101tua follows M\u0101ori models of health (32) and the project this study is based on integrates these elements into the programmes for addressing cultural practices and physical activity.\nFurther, this study focuses on mana motuhake, which is important in the context of colonial history and not following the Treaty of Waitangi. The cultural dissonance that was created through these historical practices has negatively impacted current kaum\u0101tua (11). In addition, much of the framing around inequities that have resulted from this history is based on a deficit perspective that also has negative impacts for kaum\u0101tua (19,69). Mana motuhake emphasizes autonomy, status, and independence of kaum\u0101tua to recognize their own concerns and thereby solutions for addressing their wellbeing. Kaum\u0101tua are acknowledged as having experience and knowledge and the keepers of M\u0101ori tikanga; thus they should be afforded the opportunity to participate in creating solutions to address health and social inequities (2).\nThe He Huarahi Tautoko programme was developed through a participatory process with kaum\u0101tua and kaum\u0101tua service providers that addresses key features they deem important (i.e., cultural knowledge exchange and physical activity). This programme was developed to support kaum\u0101tua mana motuhake through exchange of m\u0101tauranga with each other and with members of their own wh\u0101nau. The programme addresses key aspect of health, wellbeing, physical function, and culture that are important for kaum\u0101tua. The programme is culturally grounded and culturally safe (70), which helps to ameliorate some of the negative harms created from the colonial history.\nAlthough there are key strengths of the study and the larger project, there are a couple of limitations as well. The study uses self-reported measures which are subject to perceptual bias. However, mana motuhake suggests that kaum\u0101tua are able to describe their own wellbeing. In addition, we do include physical functioning tests to complement the self-report measures. Further, the study is a purposive sample and thus generalization to the larger population is not appropriate. There may be inherent recruitment bias as a result of the non-random participant selection.\n---\nConclusion\n\nThis study offered the baseline and psychometric characteristics from the He Huarahi Tautoko project, which is a programme that aims to enhance physical activity and cultural knowledge exchange for kaum\u0101tua in Aotearoa New Zealand. These results provide a baseline for later evaluation of the programme. Further, the study findings include key correlates of five wellbeing indicators grounded in the te whare tapa wh\u0101 model: sense of purpose, social support and relationship quality, exercise frequency, and proficiency with M\u0101ori cultural practices. This current study illustrates key issues for kaum\u0101tua wellbeing; the He Huarahi Tautoko programme is a culturally-resonant approach that is strengths-based (rather than deficit based) to address wellbeing. \n---\nGlossary\n\n\n---\nData availability statement\n\nThe datasets used and/or analyzed during the current study are available from the corresponding author on reasonable request.\n---\nEthics statement\n\nThe studies involving humans were approved by the Human Research Ethics Committee (Health), University of Waikato. The studies were conducted in accordance with the local legislation and institutional requirements. The participants provided their written informed consent to participate in this study. \n---\nAuthor contributions\n\n\n---\nConflict of interest\n\nThe authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.\n---\nPublisher's note\n\nAll claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers. Any product that may be evaluated in this article, or claim that may be made by its manufacturer, is not guaranteed or endorsed by the publisher.\n---\nSupplementary material\n\nThe Supplementary Material for this article can be found online at: https://www.frontiersin.org/articles/10.3389/fpubh.2023. 1307685/full#supplementary-material",
        "\n\nThe controversy over genetically modified organisms (GMOs) in China Chinese consumers have long relied on traditional media -TV, newspapers and radiosas major sources of information about GMOs (He et al., 2015;Ho et al., 2006), the governmentcontrolled coverage of which has been overwhelmingly enthusiastic and rarely critical (Liu and Cong, 2014). In a study on two state-owned newspapers from 2002-2011, Du and Rachul (2012) suggested that Chinese reports on genetically modified (GM) crops had emphasized the benefits of transgenic organisms and seldom portrayed GM crops in a negative light. However, given the rising amount of information available online, scandals about poisoned food have unsurprisingly abounded on social media, and implausible-sounding claims from a vocal minority of GMO critics have gained traction within China's social media networks (Cao, 2018). Indicating the influence of such (mis)information, 46.7% of the respondents in a recent China-wide survey espoused negative views on GMOs, but most of their doubts about the safety of GM foods were based on false information (Cui and Shoemaker, 2018).\nMuch of the controversy surrounding GM foods in China concerns their possible risks to human health, and research has shown that mistrust in governmental regulation and a lack of scientific knowledge have exacerbated negative public perceptions of GMOs (Gaskell et al., 1999). The survey results show that consumers were not only largely unfamiliar with GM INTR 30,5 technologies and their potential benefits but also skeptical of scientists and the government when it comes to GMOs (Cui and Shoemaker, 2018). Although China initially pursued relatively aggressive policies for the development of biotechnology and the government's enthusiasm for GM technology has not waned, Chinese authorities have so far failed to weigh in on the very public debate over such technology's safety (Ji et al., 2019). The chasm of credibility and lack of any trusted referee make it nearly impossible for the public to distinguish rumor from fact, and the public's ignorance about scientific knowledge in the first place has contributed to the virality of rumors about GMOs on social media (Du and Rachul, 2012).\n---\nRumor diffusion and the echo chamber effect\n\nRumor can be defined as unverified information containing instrumentally valuable statements of public concern (DiFonzo and Bordia, 2007). Although its precise definition continues to vary among scholars, rumors are widely considered to be stories or statements widely disseminated without confirmation or certainty as to their factuality. Rumors can be either true or false, and their distinct stylistic dimensions can prompt different patterns of diffusion. Similarly, classifying rumors as either true or false, Vosoughi et al. (2018) examined rumor cascades on Twitter to find that rumors identified as false spread more rapidly than ones identified as true. In view of those findings, demonstrably false rumors may be especially provocative and interesting.\nIn recent scholarly work on rumors, researchers have probed the spread of single and multiple rumors about a variety of topics, developed theoretical models of rumor diffusion as well as methods of detecting rumors and performed interventions intended to curb the spread of rumors (Ciampaglia et al., 2015;De Domenico et al., 2013;Gupta and Kumaraguru, 2012;Tambuscio et al., 2015). Although insightful, the existing literature on rumors has largely focused on individual differences in responding to rumors while ignoring the context of the communities in which rumormongers are embedded. Existing studies of rumors suggest that what individuals believe often relates to their local network (DiFonzo et al., 2014). More specifically, individuals make judgments according to the standards that they share with the people around them, and such communities of judgment provide the basis for whether individuals view the decisions of their in-group as legitimate (Difonzo, 2018). Likewise, early research on rumors revealed that the content of rumors was closely linked to social boundaries in offline contexts, and divisive rumors can demographically or socially split individuals and groups (Fine, 2007). As a case in point, rumors spread among women about being attacked by male sexual predators have been shown to polarize men and women (Fine, 2007). In online contexts, the activation of rumors was found to be driven by social reinforcement in the neighborhood (De Domenico et al., 2013). An individual is more likely to spread a rumor if most of his or her friends are tweeting the rumor repeatedly.\nFrom a social network perspective, communities operate as subgroups or clusters in which nodes are substantially more connected to one another than to nodes outside the network (Wasserman and Faust, 1994). The existence of clusters has been understood as evidence of echo chambers, in which individuals tend to preferentially connect to the nodes within a cluster and form an ideologically homophilous community (Bessi et al., 2015;Schmidt et al., 2017). In recent work, researchers have found empirical evidence about the formation of echo chambers on social media (e.g. Bessi et al., 2015;Medaglia and Yang, 2017;Schmidt et al., 2017;Schmidt et al., 2018). For instance, by tracking several years' worth of Facebook data to investigate whether users become polarized on the social media platform, Schmidt et al. (2017) revealed that the consumption of information on Facebook is dominated by the echo chamber effect as well as that users tend to visit very few pages and to create distinct communities.\nPrevious research has pointed out the link between the phenomena of echo chamber and rumor diffusion (T\u20ac ornberg, 2018). Homogeneous clusters of users self-select their social Viral misinformation and echo chambers networks because of confirmation bias and selective exposure, which provide fertile ground for rumors to thrive. Increased polarization, together with the propagation of misinformation through social networks, has thus drawn increasing scholarly attention. A large-scale analysis of rumor-related tweets and retweets found that rumors disseminated by echo chamber members spread faster and are more viral than those that are not disseminated by echo chamber members (Choi et al., 2020). Schmidt et al.'s (2017) study of news consumption on Facebook showed that rumor diffusion was mainly driven by the polarization of users on specific topics rather than the lack of fact-checking online.\nExisting studies have shown that the formation of echo chamber on social media varies by platform (e.g. social medium or news medium) and type of social networks (e.g. friendship network or information network). Echo chamber effect may dominate in friendship networks (e.g. Bakshy et al., 2015;Schmidt et al., 2017) but is less obvious in information networks (e.g. Colleoni et al., 2014;Williams et al., 2015). Microblogging websites such as Twitter and Weibo are both a social medium and a news medium (Kwak et al., 2010). Considering all those trends and findings, in our study, we investigated whether \"homophilous communities\" also exist in discussions of GMO rumors on Weibo. Instead of just focusing on one type of network, we examined the community structure in both user interaction-and topic-based networks. To guide that trajectory of our research, we devised the following research question:\n---\nRQ1.\n\nTo what extent do users' discussions involving rumors about GMOs on Weibo resemble echo chambers?\nInteraction between communities: like-minded and cross-cutting discussions As Bruns (2017) has pointed out, the boundary for echo chamber is not simply whether social (media) networks exhibit clustering tendencies, but to what extent such communities prompt the exclusion of outsiders. In response, some researchers have found highly segregated communities on social media, whereas others have shown that viral information on social media can permeate into different communities (Weng et al., 2013). Reflecting the latter view, Holbert et al. (2010) have concluded that the extent to which social media users deliberately avoid dissonant views online has been exaggerated in research. Altogether, it seems that interactions on social media can generate homogeneous groups but do not necessarily shut out ideologically incongruent opinions (Vaccari et al., 2016). Compared with offline networks, online networks are thought to expand exposure to more diversified information. Because social media afford connectedness among individuals with otherwise weak ties (De Meo et al., 2014), their users are likely to encounter novel viewpoints unlike what commonly circulates in more closely knit social circles (Kim et al., 2013). Social media also afford the possibility of being inadvertently exposed to different views (Brundidge, 2010). In particular, Weibo as a news medium (Kwak et al., 2010) supports the formation of information networks, in which plenty of weak ties and even hidden links exist. Briefly, weak ties refer to a person's relatively distant acquaintances who are likely to differ from him or her, whereas hidden links are relationships formed because of a shared interest or comembership on a social media platform (Huberman et al., 2008). In both cases, the existence of long-distance affiliations facilitates the inadvertent exposure to heterogeneous information and opinions (Brundidge, 2010;Colleoni et al., 2014). Accordingly, Lev-On and Manin (2009) have described online communication as a \"mixed blessing\" in light of its deliberative affordances, in that it provides both \"unintentional exposure to opposing views, as well as 'drivers' that channel users away from opposing views\" (p. 105). Interactions among echo chamber members are likely to amplify and reinforce opinions shared, whereas interactions with antagonistic viewpoints may lead to attitudinal change or intensify the controversy (Bonchi et al., 2019;Vaccari, 2013).\nDespite all those research efforts, scholars have yet to examine interactions among counter-attitudinal groups in the context of online science communication. Nevertheless, it is INTR 30,5 worth characterizing such interactions, which, in China, are readily available within GMOrelated online discussions. In that case, investigations should address the extent to which cross-cutting exchanges occur in the diffusion of rumors about GMOs and whether conversations about such a shared public concern facilitate the exchange of opinions across communities. On the latter point, studies addressing similar topics such as climate change (Williams et al., 2015) and traditional Chinese medicine (Chen et al., 2018) have suggested that conversations on social media do not facilitate cross-cutting exchange. Considering all of the above, we formulated a second research question to guide our study:\n---\nRQ2.\n\nTo what extent do Weibo users engage in cross-cutting exchanges in response to rumors about GMOs?\nIn addition to quantifying the structural aspects of interactions between communities, we investigated the nature and content of the discussions between and within communities. A comment can convey support or disapproval. If the comment clearly communicates and substantiates its claims, it is more likely to exert an influence on the deliberative potential of an online discussion. In research addressing users' online comments, scholars have focused on information-seeking and information-sharing behaviors and even the nature of affective factors such as emotions and feelings. For instance, categorizing users' comments on Twitter posts with rumors as emotion-related, information-related, deliberative or calling for action, Chua et al. (2017) found that most discussions were emotion-or information-related ones.\nAlong similar lines, Zhang et al. (2013) have suggested that users' interactions in a Facebook diabetes group were structured around information, emotion and community-building. In one of the rare studies on interactions between segmented communities, Zollo et al. (2015) analyzed and compared emotions in debates on Facebook about science versus conspiracy theories. They found that the more that users engaged in discussions, the more they tended to express negative emotions.\nIn other research, the anonymity of the digital environment proved able to lessen the quality of online comments (e.g. Rowe, 2015). Uncivil, hateful and prejudicial discourse was more likely to surface when users were engaged in conversations about highly polemic issues (Berg, 2016). Investigating like-minded discussions in a study using online investment discussion boards, Tang et al. (2017) found that skewed assessments, exacerbated by the insular nature of the online community and its social structure, resulted in underperforming investment advice. With respect to cross-cutting discussions, empirical evidence suggests that members of different communities may frequently retweet each other's posts, and that rational or meaningful conversations between communities seldom occur (Chen et al., 2018). Moreover, exposure to disagreeable, uncivil talk about politics induced anger and aversion, which in turn reduced satisfaction with online political discourse (Gervais, 2015).\nIn our study, we first classified Weibo users' comments on posts containing GMO-related rumors into supporting and dissenting comments. Such comments could be facts or emotional judgments implying positive or negative preferences concerning the issue at hand. Thus, guided by two more research questions, we examined the extent to which supporting and dissenting comments were respectively related to emotions, related to information or uncivil:\nRQ3. What is the nature and content of Weibo users' comments on GMO rumor posts in terms of information seeking and sharing, emotional expression and level of incivility?\nRQ4. Do supporting and dissenting comments differ in their content-related features (i.e. information seeking and sharing, emotional expression and level of incivility)?\n---\nViral misinformation and echo chambers\n\n\n---\nMethod\n\n\n---\nData collection\n\nOur project focused on scientific rumors about GMOs that circulated on Weibo in 2018. Using the term \"genetically modified\" in a search on Weibo, we identified 139,319 posts addressing GMOs published from January 1 to December 31, 2018. Next, we created a library of rumors about GMOs by collecting rumors from websites that had already published rumors about GMOs. Two research students were recruited and educated about GMOs. We randomly selected 2% of the collected posts and asked the coders to classify 500 of them as either rumors or non-rumors. The coding achieved a Cohen's kappa (\u03ba) value of 0.95. Disagreements among coders were resolved through discussion, and the coders coded the rest of the data separately. After integrating their coding results and fact-checking online content, we compiled a list of keywords related to rumors about GMOs. Trigram or bigram phrases were combined manually by the researchers, with the aim to identify as many rumor-related messages as possible. Using the rumor-related keywords and phrases as seeds, we identified 21,837 rumor-related discussions (15.7% of all GMO-related messages), including original posts and reposts.\nTo exclude rumors of relatively small size, we selected original posts (n 5 345) that had received more than 20 comments (Guo et al., 2018;Shin et al., 2017). This subset of rumors was then coded for veracity by two researchers. We searched for reliable sources to support our coding decisions and resolved disagreement through discussion. We coded each rumor as a \"true rumor\" if delivered information reflects scientific evidence (n 5 103, 29.9%) or a \"false rumor\" if what it described is unlikely or untrue (n 5 227, 65.8%). Otherwise, 15 rumors (4.3%) were classified as \"unclear,\" because no evidence could firmly support or refute the content.\nTo address RQ3 and RQ4, we randomly selected 20 of the 345 rumors and coded the attitude of their comments (n comment 5 3,170). Of those 20 rumors, 13 were classified as true rumors, five as false rumors and two as being unclear. All comments on the 20 rumors were classified by two coders as: endorsing (i.e. comments that repeat or confirm the original post), rejecting (i.e. comments that deny or reject the original posts) or unclear (i.e. comments without a clear attitude). We chose those categories to span a continuum of views and avoid ambiguity between categories. Presented with both the comment and the original post, two coders coded the comments into three categories based on the mentioned coding scheme. We tested the intercoder reliability and found a \u03ba value of 0.90. Based on the results of coding, users' attitudes toward GMOs were classified as supportive, opposed or unclear.\nTo compare the content-related features of the endorsing and rejecting comments, we coded the comments as emotional (i.e. the comment contains affective expression), information-oriented (i.e. the comment provides or requests additional information) or uncivil (i.e. the comment is abusive, threatening or prejudiced against others). This categorization was not mutually exclusive. Intercoder reliability test found \u03ba values ranging from 0.81-0.93, all of which were acceptable and suggest that the results of classification were robust. Last, we used the automated coding in the software TextMind (Gao et al., 2013) to identify positive and negative emotions in the comments.\n---\nThe co-commenter network\n\nTo explore patterns of community structure and membership related to the rumors, we established an affiliation network between rumors and users. In social network analysis, affiliation networks are two-mode networks that map the relationship between two types of nodes (Wasserman and Faust, 1994). In our study, the first type of nodes was Weibo users, whereas the second was GMO-related rumors. Our two-mode network consisted of 345 rumors and the 25,126 users who commented on them. Next, we transformed the two-mode INTR 30,5 affiliation network into a single-mode network such that if a user commented on rumors i and j, then rumors i and j were considered to have a tie. The rumor co-membership network was a non-directed and weighted network. By transforming the rumor-to-user affiliation network into a single-mode rumor-to-rumor network, we were able to infer the extent to which false rumors were connected to true rumors in discussions.\nTo identify the community structure of the rumor co-membership network, we calculated the assortativity coefficient of the network based on rumors' veracity. The assortativity coefficient measures the level of homophily of the network based on a node-level attribute (Newman, 2002) essentially, the Pearson correlation of behaviors between linked nodes (Aral et al., 2009). The assortativity coefficient r falls within the range \u00c01 \u2264 r \u2264 1 and is defined as: r \u00bc \u00f0sum\u00f0e\u00f0i; j\u00de; i\u00de \u00c0 sum\u00f0a\u00f0i\u00deb\u00f0i\u00de; i\u00de\u00de=\u00f01-sum\u00f0a\u00f0i\u00deb\u00f0i\u00de; i\u00de\u00de;\nin which e(i,j) is the fraction of edges connecting vertices i and j, a(i) is sum(e(i,j), j) and b(i) are sum(e(i,j), i).\n---\nThe comment thread network\n\nTo explore the community structure of the discussions about GMO rumors on social media, we constructed a comment thread network such that if user i commented on a rumor-related message posted by user j, then we acknowledged a link between i and j in the network. The rumor discussion network was a directed, weighted network. This network mapped the relationship between 1,631 users involved in posting and commenting on the 20 selected rumors about GMOs.\nWe identified influential users in the discussion network by using multiple social network metrics, including number of fans (>12,000, n 5 88), indegrees (>4, n 5 57) and kcore members (k 5 4, n 5 64). Number of fans measured the populousness of the user's direct audience, whereas indegrees was the total number of comments received by the user in the rumor diffusion network. Last, used to gauge so-called \"coreness,\" k-core was used as a measure to identify minor but interconnected core entities within a network. To be included in the k-core, every node has to be connected to at least k other nodes in the group (Hanneman and Riddle, 2011). Because researchers have suggested that the influence of a user in a network is a multidimensional construct that should be measured with multiple indexes (e.g. Wang et al., 2016), we used the three measures to identify influential users in social networks.\nTo measure the level of attitude-based homophily in the user interaction network, we also calculated the assortativity coefficient. Another way to examine the echo chamber effect is by assessing heterogeneity, defined as the balance between members holding supportive and opposing perspectives (Williams et al., 2015) and measured as:\nH \u00bc 1 \u00c0 js \u00c0 oj = js \u00fe oj;\nin which s is the observed frequency of supporters, and o is the observed frequency of opponents. That measure returns a value on a linear scale from 0 (i.e. perfect homogeneity, in which only supporters or opponents exist) to 1 (i.e. perfect heterogeneity, in which equal proportions of supporters and opponents exist).\n---\nResults\n\nWe identified 345 Weibo posts from 2018 containing GMO rumors that had received more than 20 comments. Among them, we found 103 true rumors (i.e. delivering scientific information), 227 false rumors (i.e. delivering unlikely or untrue information) and 15 unclear Viral misinformation and echo chambers rumors. Table 1 listed some examples of the three types of rumors. The average number of replies was 128.25 (SD 5 255.91) for the false rumors and 113.56 (SD 5 234.19) for the true ones.\n---\nThe co-commenter network\n\nThe rumor co-commenter network was a small but dense network, with 345 nodes and 761 co-memberships (density 5 0.030). The most centric rumors were connected to 18 other rumors in the network, which implies users interested in GMOs screened a large number of rumors at once. As shown in the rumor co-commenter network in Figure 1, the connections between the true and false rumors were intense, and no clear separation formed between their corresponding discussion communities. On the contrary, overlaps emerged between the communities circulating true and false rumors. The rumor co-commenter network's assortative coefficient, indicating the extent to which nodes were clustered based on veracity, was 0.15. That result suggests that the network had a low level of veracity-based homophily, and that users tended to comment on both true and false rumors.\n---\nThe comment thread network\n\nThe comment thread network, though a sparse one with 1,631 nodes and 3,170 edges (density 5 0.001), nevertheless represented a cohesive community, with values of reciprocity (0.097) and transitivity (0.003) that were higher than those in networks previously examined (e.g. Peng et al., 2016). Such results indicate that users grouped by the GMO-related topic were tightly connected with each other. At the same time, the assortative coefficient of the comment thread network based on users' attitudes toward GMO was 0.13, which implies a low level of polarization among users. Together with what the homophily analysis of the rumor co-commenter network revealed, such findings suggest a low level of the echo chamber effect in discussions about GMOs on Weibo.\nTo understand the distribution of opinions expressed about GMOs on social media, we analyzed the distribution of attitudes among influential users identified by number of fans,\n---\nCategory\n\nSearch keyword\n---\nTrue rumor\n\n\n---\nGenetically modified and sterilization\n\n[Genetically modified foods lead to sterilization] is actually a conspiracy theory, and crossbreed is also a type of genetic modification False rumor\n---\nGenetically modified and tumor\n\nThe incidence of tumors in Chinese children is increasing by 2.8% per year. In western countries, tumor is a disease that mainly occurs in elderlies over 50 years old. Nowadays, after the high incidence of tumor in young people, it starts to increase in children in China. The reason is because of the very difference of China from the world: China is the only country on the planet where most people eat genetically modified foods Genetic modification and infertility Genetic modification can cause infertility, mules are the genetically modified result of horses and donkeys, and few of them are fertile. The health of the next generation lies in your choice! Unclear Genome editing and detect [New genetic modification methods such as genome editing cannot be detected] Whose right to interpret the data after detecting genetically modified foods? Note(s): Coding of rumors' veracity was conducted by the researchers referring to articles and publications from reliable sources, such as Chinese Academy of Science (http://www.cas.cn/), Ministry of Agriculture and Rural Affairs (http://www.moa.gov.cn/) and Guokr (a popular science website: https://www.guokr.com/) indegrees and k-core. The heterogeneity score for the full network was 0.41, and for subgroups defined by number of fans, indegrees and k-core were 0.75, 0.71 and 0.51, respectively. Overall, opposing views dominated the discussions about GMOs, especially among users in the entire network and the subgroup defined by k-core. We found that opponents significantly outnumbered supporters in both of those groups (Figure 2).\nAs shown in Figure 3, the discussion networks exhibited segregations of users (i.e. nodes) according to their views on GMOs, presented by three colors. Whereas GMO opponents formed the majority in the full networks (Figure 3a), some supporters were located at the core of the networks as well (Figure 3b-d). More interactions (i.e. links) occurred among like-minded users (29.4%), although less often between users espousing different views (13.9%). In other words, the discussion network demonstrated homogeneity as well, which suggests that individuals on social media tend to discuss topics concerning GMOs with others similar to them. Among other results, half of the discussions presented unclear views on GMOs (49.7%), in which only emotions or emojis, mostly negative ones, and hate speech were presented. That finding highlights that a large portion of online discussions about GMOs do not convey meaningful information. In general, as heterogeneity analysis revealed, a mixture of homophily and heterophily emerged in users' discussions concerning rumors about GMOs.\n---\nCross-cutting exchange on genetically modified organism rumors\n\nTo answer RQ2, we examined the amount of cross-cutting exchange in users' responses to rumors about GMOs. Our results suggest that users responded to true and false rumors differently; true rumors were more often rejected (21.2%) than false ones (6.5%) but received fewer endorsements (9.3%) than false ones (61.7%) as well.\nTo elucidate the nature of interactions between members of the discussions about GMOs, we analyzed the content of the discussions (RQ3 and RQ4). Of the 3,170 post-reply interactions, 32.2% (n 5 1,020) were interactions between like-minded users, 23.2% (n 5 735) were between users with opposing views and 44.6% (n 5 1,415) involved at least one user who demonstrated an unclear attitude. The results of an additional one-way analysis of variance showed that cross-cutting interactions generally contained a significantly greater amount of emotionsand more negative onesas well as information and incivility than interactions among the like-minded or between users without a clear attitude. At the same time, interactions between like-minded users contained a significantly higher level of emotionality, both negative and positive, as well as incivility than ones between users without a clear attitude. Somewhat remarkably, the interactions between users without a clear attitude contained a higher level of information than ones between the like-minded (Table 2).\n---\nDiscussion\n\nOur findings contribute to knowledge about rumor diffusion on social media by presenting empirical evidence of the community structure formed within discussions of GMO rumors posted on China's Twitter-like Weibo. We empirically tested the echo chamber effect in shaping social media users' responses to rumors about GMOs in the Chinese context. Our findings suggest that some social media users are substantially more likely to engage with like-minded others. Users active in online discussions of GMOs, either as activists or as skeptics, tended to exhibit strong attitudes, while neutral views were largely absent. Meanwhile, cross-cutting exchange did occur, albeit to a lesser extent than like-minded exchange. In general, our findings confirm that microblogging platforms such as Weibo can also function as public forums for discussing GMOs that may expose users to ideologically cross-cutting viewpoints at times.\n---\nThe echo chamber effect in rumor diffusion\n\nOur results show that discussions about GMOs on social media are characterized by a weak level of (a) veracity-based homophily in terms of rumor content and (b) attitude-based homophily in terms of users. Moreover, our results reveal the coexistence of like-minded interactions and cross-cutting exchange. Accordingly, some Weibo users are more likely to form cohesive communities that support their views, as consistent with the notion of echo chambers on social media. At the same time, we observed a large portion of users who interacted with others who held different views, which increased the likelihood of crosscutting exposure overall. Such empirical evidence clarifies insights gained from offline settings (e.g. Huckfeldt et al., 2004;Vaccari et al., 2016), namely, that heterogeneity persists on social media despite the prevalence of homophily. Previous research addressing a friendship network on Facebook and follower-followee relationships on Twitter suggested strong attitude-based homophily (e.g. Schmidt et al., 2018;Yang et al., 2017). However, our analysis of the rumor co-commenter network found that far weaker homophily exists, at least on Weibo. Microblogging sites such as Twitter and Weibo afford the establishment of social connections as well as information sharing. Users may form strong ties with others whose views are consistent with their own, such as in friendships on Facebook and reciprocal ties on Weibo, but also encounter others with different views via weak ties, including in communities connected by shared interests.\nBecause increasingly more individuals rely on social media for news and information, the structural features of social and information networks are likely to play an increasingly Existing research has revealed that exposure to a diversity of views is associated with a lower likelihood of holding a polarized view (Levitan and Visser, 2009;Williams et al., 2015), which could prevent the virality of rumor diffusion (Schmidt et al., 2017). Our findings suggest the affordance of microblogging websites to expose individuals to perspectives from the other side and the potential to change attitudes.\nOur findings additionally suggest that the interaction networks formed on social media should not be determined based solely on the relationship indicated by reposting or replying. Although most studies on the topic to date have involved using reposts or replies to construct networks, in our study we went a step further by constructing an interaction network with reference to comment-based relationships. Specifically, we constructed a co-commenter network and a comment thread network. Because a comment can often indicate a user's attitude toward the issue at hand, it affords researchers the opportunity to examine the valence of ties in the interaction networks. By examining the valence of comments that bind participants in a network, we found evidence suggesting the existence of cross-cutting exchanges. In other words, the comment space provides a more or less open forum in which users confront or express dissenting views. Knowing that, researchers should begin to incorporate a more comprehensive measurement of interaction networks based on different types of relationships. The method used in this study can be applied to gauge the echo chamber effect and to assess the mechanisms (e.g. cross-cutting exchanges) allowing to break out of the proverbial echo chambers. A practical implication of the finding is that promoting scientific knowledge in the comment section may function to reduce the echo chamber effect on social media.\n---\nAttitude heterogeneity among participants and opinion leaders\n\nTo examine the polarization of attitudes toward GMOs, we analyzed the opinions of all users as well as opinion leaders identified by different criteria (i.e. influential users) in GMOrelated discussions. As a result, we observed strong attitude homogeneity across the network and in four-core network, in which most participants were sceptics. More attitude heterogeneity surfaced among influential users who received more comments and had more fans.\nThe role of opinion leaders in shaping attitudes toward GMOs merits attention. The twostep flow model of communication (Katz, 1957;Katz and Lazarsfeld, 1955) holds that media influence is exerted via the intermediary actions of opinion leaders who interpret and disseminate new information to others. In our study, we observed that though the most influential users on social media, indicated by number of fans and indegrees, were largely celebrities, news media outlets or governmental accounts, the overall discussion about GMOs was dominated by sceptics. There was a tension of attitudes toward GMOs between traditional opinion leaders and general users on Weibo, which echoes survey findings that Chinese consumers are cautious in trusting the opinion of scientists, news media and governmental organizations (Cui and Shoemaker, 2018). Although traditional opinion leaders have been vocal on Weibo, anti-GMO sentiments fueled by nongovernmental organization media, activists and grass-root expos es and conspiracy theories (e.g. \"GM crops are a conspiracy of American imperialism to annihilate Chinese\") have spread as a result. Traditionally, the tackling of rumors depends on news media, governments and scientists. The findings suggest that the effectiveness of these traditional opinion leaders in refuting rumor was on the decline. We should instead rely on a wider gamut of gatekeepers INTR 30,5 to disseminate GMO-related knowledge and stimulate a dynamic conversation on social media. Like-minded and cross-cutting interactions: information-or emotion-related? Our results additionally show that in the cross-cutting interactions, 64% of the messages provided or requested information, which was a significantly higher rate than in interactions among the like-minded (19%). Such empirical evidence was inconsistent with prior studies in the political context (e.g. Rojecki and Meraz, 2016;Shin et al., 2017) and the general context (e.g. Zollo et al., 2015) which found seeking information was largely absent in rumor-related discussions. This inconsistency might be related to the particularity of the Chinese context, in that discussions about GMOs on Chinese social media often involve enumerating facts or providing additional information in debates. The pro-GMO camp often shares and disseminates information on the scientific consensus formed around GM technology, whereas the anti-GMO camp often cites factual evidence that stresses the uncertainties and possible environmental and health risks of GMOs. This finding provides guidelines for the practice of promoting popular science of GMO on social media. Practitioners could provide more reliable sources for fact checking. The evidence-based knowledge of GMOs should be disseminated to restore the public's confidence in science's ability to address public health and environmental concerns.\nBy comparison, our results included that most like-minded interactions (60%) expressed emotions, and only 19% of like-minded interactions had comments that requested or provided information. Such findings support previous assumptions that interactions between like-minded people tend to reinforce a shared view regardless of the narrative, which may fuel the viral spread of rumors. At the same time, our results also indicate that cross-cutting interactions contain a higher level of incivility and negative emotions than other kinds of interactions. Rational debates about the benefits and drawbacks of GMOs remain an ideal on Chinese social media. Our study contributes knowledge to the literature by distinguishing like-minded from cross-cutting interactions and by examining the nature of those interactions.\n---\nLimitation and future work\n\nIt should be noted that social media content varies by type, including text, images, audio, video, or a combination. Because the scope of our study was limited to the text, images and videos should be examined in future studies, even if they require a greater variety of analytic methods. At the same time, the discussions that we observed on Weibo represent only a slice of the larger cyberspace in which rumors go viral. It is possible that the discussion and diffusion of rumors are shaped by different types of media platforms and communication, either independently or conjunctly (e.g. other social media platforms, news media websites and private communication). In addition, a longitudinal study of rumor diffusion could illuminate the dynamics of online community formation and conflict resolution. Such research might ask whether cross-cutting interactions reach consensus over time or become segmented as a result of ideological differences. Another question is what the driving factors behind the viral spread of rumors on social media are, both individual-and network-wise. By targeting a longer period in a design that is not cross-sectional, researchers could address those and other lines of inquiry.\n---\nConclusion\n\nOne major contribution of this study is that it systematically examined the network structure formed around the discussion of rumors about GMOs on one of China's most popular social media platforms. On the one hand, this study established the existence and extent of the echo Viral misinformation and echo chambers chamber effect in the interaction networks. On the other hand, we found that rumors not only bounced around through these echo chambers. Moreover, Weibo also served as a public forum for GMO discussions in which cross-cutting ties between communities existed. Most studies to date have involved constructing interaction networks based on reposts only, which precludes any further examination of the valence of ties. We constructed an interaction network with reference to comment-based relationships that encompassed two types of networks: the co-commenter network and the comment thread network. Because a comment can often convey a user's attitude, our strategy afforded us the opportunity to examine the valence of users' ties in the interaction networks. We also distinguished like-minded interactions from cross-cutting exchanges, and our findings testified to their coexistence. Our network analysis revealed that though a certain level of homophily existed in the interaction networks, referring to the observed echo chamber effect, Weibo also seemed to serve as a public forum for GMO discussions in which rumors could spread from community to community. A considerable amount of interactions emerged between the pro-and anti-GMO camps, and most of them involved providing or requesting information, which could mitigate the likelihood of opinion polarization. By extension, our findings suggest that microblogging sites such as Weibo have the potential to challenge the flow of rumors. Last, our study revealed the declining role of traditional opinion leaders (e.g. scientists, state media and government institutions) and the lack of alternative sources for fact-checking content on Weibo. Such trends pose the potential concern that rumors about GMOs may more easily go unchecked and viral on Chinese social media. In response, researchers should explore the roles of different types of opinion leaders in rumor diffusion, which could help to guide the development of efficient debunking strategies.\n---\nAbout the authors\n\nXiaohui Wang is a Research Assistant Professor at the Department of Journalism, Hong Kong Baptist University. His research focuses on the consumer health informatics, social media analytics and humancomputer interaction, which reflect the interdisciplinary feature of communication research. Dr. Wang's Viral misinformation and echo chambers",
        "Introduction\n\nSouthwark Council, Lewisham Council, the Royal Borough of Greenwich and Goldsmiths, University of London came together to form the South-East London Teaching Partnership (SELTP) in 2015. Funding for SELTP was provided through the social work teaching partnership initiative (Department for Education and Department of Health, 2015). The rationale for creating teaching partnerships was the belief that the quality of social work education could be improved if universities and local authorities worked more closely together to deliver social work education in the classroom. SELTP created an initiative for practising social workers, working in different roles and at different organisational levels, to teach BA, MA, and Step Up social work students at Goldsmiths. Step Up to Social Work is a 14-month postgraduate intensive full-time social work qualifying programme, funded by the Department for Education in England (Department for Education, 2021a). Social workers who taught at Goldsmiths were designated the role title 'teaching consultants'.\nThe paper initially considers why social workers and their practice might be affected by teaching social work students in a university environment. The paper then outlines the methodological approach undertaken, before presenting the reader with first-person reflections from the two social workers. Specifically, the paper presents reflective case study narratives of two 'teaching consultants' -Alex and Elizabeth -who were invited to reflect on their teaching experiences and the consequences of these experiences for their social work identities, values and practice.\nAlex's and Elizabeth's reflections are then discussed by drawing on reflective and social theories. The paper concludes by briefly considering the policy implications arising from the teaching consultant initiative, especially around workforce retention.\n---\nEvidence and Possibilities for Social Workers to Learn from Social Work Students\n\nWe know the benefits, for example, generating new ideas; questioning practice; sharing theoretical and research knowledge (Bogo, 2015) that social work students bring to social work agencies and practitioners while on their practice placements. However, we know little about the rewards, which might accrue to social work practitioners from teaching social work students in the University classroom. These benefits might not amount to much if teaching is seen as a unidirectional, unreflective activity in which social workers simply discuss their working lives. However, effective teaching depends on critical reflection (\u00c7imer et al., 2013), and for Fook (2015), critical reflection consists of two elements, both of which might be taxing the social worker before she enters the classroom to teach.\nFirst, preparing to teach may facilitate practitioners to reflect on the constituents (e.g., legislation; procedures; linking theory to practice; practice wisdom) of social work practice. Preparing to teach, in other words, invites social workers to think about what they do, how they do it and why they do it. Second, according to Fook (2015), critical reflection can make power relations more visible, not only in terms of a professional's relationship to versions of themselves over time but also in terms of how they see their positioning in relation to others within existing social structures. Seen through an anthropological lens (Eriksen, 2015, p.63), preparing to teach social work students about social work may afford practitioners an opportunity to reflect upon the status and role aspects of being a social worker in relation to others (e.g., managers, colleagues, clients, and other professionals).\nHowever, even if we accept Fook's (2015) explanatory model, it does not sufficiently explain why a university setting might offer rich benefits for social work practitioners who undergo a teaching experience. In workplace supervision, social workers are invited to reflect on their work activities by their managers. However, since social work agencies and higher education institutions comprise different forms of social life, other work reflections might arise from teaching students in the classroom. While social workers might prepare and reflect on what knowledge they want to impart before they enter the classroom, the University environment licences the formulation of unusual or unfamiliar questions, opening the possibility of disrupting what professionals intended to say and how they wanted to present themselves, through an exchange which promotes deep learning and critical reflexivity (Clare, 2007). The social context in which practitioners find themselves with students should therefore not be underestimated. This is where Bourdieu's (1988) concepts of habitus and field may be helpful in that they permit us to imagine that students approach their learning experiences in ways reflecting the mental and cultural structures of the university. While power differentials do exist between social work students and qualified professionals in placement settings (e.g., Parker, 2010;Perry & Hughes, 2021) a key element of the University environment is that it permits students to ask questions in a space, which is quite different to student-professional encounters in a social work placement setting. For Bourdieu (1988, p.784) the university classroom represents a fielda structured space -which frames the social work students' habitus in terms of how they behave in the classroom. This can be illustrated by comparing elements of the social worker-student dynamic within the classroom to the placement setting, opening the possibility for qualitatively different reflections to emerge for the practitioner from classroom encounters. For example, students are usually organised as a collective learning group in a university settinginteractions lead to other questions emerging with the potential for ideas to bounce off each other -and this is very different to the placement environment where practice educators marshal learning. During placements, Perry andHughes, (2021, p.1051) note that \"many findings identify the centrality of the practice educator relationship to students' satisfaction and perception of placement in addition to successful learning\".\nAdditionally, while some social work students find it harder to speak in large student groups (Olivier et al., 2017), the classroom space can be organised in a way where it can feel more permissible for students to ask questions of powerful visiting lecturers. Here the focus more on the practitioners' own experiences and less on the students'another significant difference to what happens during a supervision session in a social work placement setting.\nThe HEI environment therefore opens a space for a social work practitioner to critically reflect as a visiting lecturer on their everyday social work practice. For Foucault (Packer, 2018, p456) knowledge about a profession such as social work, is constituted by more than what can be known but also by how a social worker sees themselves and what they can do. By considering themselves as an object of knowledge, in terms of what they know, their identity and practice, the experience of teaching can lay the groundwork for a new subjectivity to emerge for the practitioner in terms of how they see the potential for social work. In the classroom a social worker may be called upon \"to observe himself, analyse himself, interpret himself, recognise himself as a domain of possible knowledge\" (Foucault, 1984). Drawing from Bourdieu (1988, p.782), classroom encounters between students and visiting practitioners as lecturers may invoke the emergence of a dialectic between objectivity (e.g., how am I seen by another as an \"other\") and subjectivity (e.g., this is what constitutes me as a social worker) with the possibility of a new habitus emerging for practitioners. For Fook (2015, p.441) this ontological reframing is evident by an increased critique of underpinning assumptions related to practice frameworks and prevailing power relations, which 'can provide a platform for transformative social action' within social work practice. Therefore, unusual questions raised in the classroom may lead to moments for unexpected critical reflection and action by the social work practitioner.\n---\nMethods\n\nReflective case study narratives (Becker & Renger, 2017) offer a useful way to empower practitioners to articulate their feelings and reflections about these new experiences. Presenting a first-person account of events is also likely to engage the reader as it offers an effective means 'to transform experience into knowledge in a colloquial, narrative style' (Becker and Renger, 2017, p.138) and increase the readers' willingness to reflect on the issues raised by the practitioners. Vernacular writing is likely to connect more with the reader.\nA successful ethical approval form was submitted to the University to undertake research on the experiences of teaching consultants in the classroom. Two practitioners were invited to write down their reflections about their teaching experiences and the effects that these experiences had on them and on their practice. The practitioners were identified through relationships built up during the teaching partnership initiative with the academic co-writer of this article. The invitation to practitioners specified a word limit of approximately 750 words because the intention was to keep their reflections succinct, often a challenge in presenting reflective and engaging case narratives (Brandon, 2014). Yin (2009) proposes several formats in terms of presenting case studies; one of these is a non-sequential structure used for descriptive purposes and this structure best describes the presentation of the practitioners' reflections. Both participants were offered the same instructions for writing their accounts and what is presented in the findings section are their first-person unedited responses (except for clarity purposes) to this instruction. The accounts of both practitioners were analysed using basic thematic analysis, keeping in mind that social workers who teach in a University setting may experience new insights into the constituents of social work, such as what it means to be and to practice as social workers. Both practitioners had an opportunity to comment on this analysis, which is presented in the discussion part of this paper.\nFinally, both practitioners should be applauded for presenting their accounts. The way they do so is revealing, powerful, humbling, and illuminative. Reading their accounts offers a powerful testimony of their teaching experiences and arguably adds a rich and thick qualitative contribution to understanding the impact of teaching partnership initiative on practising social workers who taught in the classroom.\nElizabeth and Alex have moved on to different roles and organisations since writing their testimonies. Both have extensive experiences of working across a range of roles in social work practice.\n---\nSocial Workers' Reflective Accounts of Teaching Experiences and Their Effects\n\n\n---\nReflective Case Study A -Elizabeth\n\nWalking into my first session as a Teaching Consultant, I felt like an imposter. I questioned myself on my knowledge. How could I, as just a Social Worker, teach anyone anything? I was certain that I was going to be discovered at any given moment. I had premonitions that a student would stand up during one of my teaching segments and declare that I was 'winging it'.\nLuckily, the opposite occurred, and the students seemed enthralled in my stories of frontline Social Work. Many of the questions the students posed were of the practicalities of the role and how I had managed uncomfortable situations with service users. I found that I was able to answer such questions fluidly providing countless examples from practice.\nI have found delivering sessions on anti-discriminatory practice a deeply cathartic experience. I have been able to use my teaching platform to identify with the student experience. I shared with them that as a newly qualified Social Worker I felt I had to do away with my own identity as working-class black woman and take on a 'cloak' of professionalism. This was due to my experiences of being educated in white institutions and secondly being part of a social care workforce where the management team were predominately white. I subconsciously absorbed the message there was no room for me to be authentic. However, I now understand and value my 'otherness' as a strength when engaging with service users and their complex circumstances. I felt I was able to use the teaching partnership as an honest reflection of how through due process, I was able to be comfortable in my personal and professional identities and negotiate these statuses when working with service users whose backgrounds differed from my own. The teaching partnership has enabled me to share and discuss these quandaries and provide tools to the next generation of Social Workers to navigate such issues. In addition, I am of the view the role of a Teaching Consultant is pertinent, as it provides 'real time' insight into how Social Workers are perceived in the ever-changing socio-political climate.\nI felt that the sessions have also validated my professional identity and reminded me that I am an experienced and skilled practitioner and should lean on this. It has also enhanced my confidence as a practitioner to take on more complex work and have the gumption to make difficult decisions within my casework.\nMoreover, the teaching partnership has changed my perspective on the discipline that is Social Work. Entering into the teaching partnership, I realised I had taken a passive approach in my role as a Social Worker. I assessed using pre-established legal frameworks, I followed best practice guidelines and hoped that I was able to make some lasting positive change to the service users I encountered.\nHowever, I now see Social Work has an ever-evolving subject that requires the knowledge, insight, and experiences of those who practice the discipline. I believe this is essential, if the subject is to continue to be a relevant driver for social change within society.\nTherefore, I now view myself as a pioneer who is continuously trialling various Social Work techniques and theories. Whilst simultaneously evaluating their effectiveness within the theoretically safe and nurturing grounds of an educational establishment.\nI now take more time to actively reflect on all my interactions with the service users and find ways to incorporate lessons learnt into future teaching sessions.\nNonetheless, the dual role of practitioner and teacher has an array of complexities, as I often feel that I am in a cycle of endlessly critiquing my own practice. I feel a new sense of duty to ensure that my practice is beyond reproach. I now fear becoming a hypocrite that teaches best practice to students but does not do so in within my own work.\nI have attempted to reconcile these feelings by acknowledging my best will differ from day to day depending on caseload, management support and allocated time.\nI am often asked if I would go into teaching -as if the role of a Teaching Consultant does not reflect such a practice. I now inform enquirers that to teach, one must be committed to continuous learning and development. I feel that I require the active role as a practitioner to feed into my ability to effectively teach students, thus bridging the gap between Social Work theory and practice.\n---\nReflective Case Study B -Alex\n\nI have worked for a London local authority for ten years in different roles, in a mental health hospital social work team as a Care Coordinator in an Early Intervention Service and in the borough's out of hours Emergency Duty Team. When I heard about the South East London Teaching Partnership (SELTP) and the goals of the partnership, I felt inspired and motivated and wanted to become involved. I did my training at Goldsmiths, and I reflected on my time as student there and recognised the value of practitioners in social work education. I was able to commit time to SELTP as I work out of hours. I have always loved learning and am actively involved in my own continuous professional development and the opportunity to return to the classroom and to learn from students was an exciting and motivating opportunity. I have undertaken a number of teaching and teaching-related tasks, which have included coteaching with social work academics on different subjects (e.g., LGBTQ+ and Social Work; Mental Capacity Act; Mental Health Social Work; Law and legal thresholds and Introduction to Social Policy, peer-group reflective learning).\nFor teaching, I prepared anonymised, real-life examples of practice, which helped to bring social work to life. I discussed how I approached my work with cases by reflecting on how legislation, theory, procedures, and ways of working came together. I brought it back to the European Convention on Human Rights and got really passionate about this. Students did not know the history of where human rights legislation came from, and how that is a grounding, and I'm looking at my work as a social worker, being a social worker from a human rights perspective, and that's really good because you get re-energised into why you are doing this in the first place. My insights and reflections were appreciated by students in their feedback, as they came to understand how I work and why I do what I do, and, crucially, to imagine themselves in the role of a social worker.\nIn terms of my own practice, teaching has given me an opportunity to reflect once again on the application of theory to practice. For example, revisiting fundamental theories around power and care and control and linking to my practice as an Approved Mental Health Professional. My experience and understanding of the nuances, practicalities and complexities of the work has led me to a deeper appreciation of theory and research. Conversations with social work students benefited me and improved my work. Describing to students the ways in which I use my Self in my social work has enabled me to have a better understanding of my own social work identity and how this intersects with other parts of my identity. In practice, this has meant I am more relaxed, confident, and true to myself as part of my work.\nI have more energy and enthusiasm for my role and in my workit is invigorating being around the Goldsmiths campus and working with keen and enthusiastic students who ask questions and challenge how I think about things. Students have a thirst for knowledge beyond 'what we do' to 'why we do it'. Some of my colleagues are also Teaching Consultants and our team meetings benefit from discussions about what we've done at Goldsmithsexamples include discussions about race and mental health in the context of the law and considering thresholds for social work intervention. Our involvement with the university has renewed our enthusiasm for social work and what it can achieve. This is because you can see the students learn, develop, and grow and you see them on placement doing excellent work with service users and bringing their energy and enthusiasm to the profession and making a real difference in peoples' lives.\nThe experience I have had as a Teaching Consultant makes me want to continue to work for a local authority that has links with a Teaching Partnership, as I think the link with the university is so important and it shows that my employer values my experience and my input to the next generation and they are engaged in developing the profession I think it is a really positive part of working for Lewisham and one of the reasons I stay working for the borough. \n---\nDiscussion\n\nAt one level, classrooms are places where teachers and students interact to produce learning (Komarraju et al., 2010). However, seen more critically, a classroom setting is also a place with a function, both of which are elements of a discursive formation (Foucault, 1972), with discursive opportunities varying between places. The discursive formations of higher education and social work are different, opening up possibilities for different kinds of questioning or 'problematizations' (Foucault, 1998) to emerge about the possibilities of social work. Places such as classrooms are not sites where humans just perform, but are also catalysts and products of social action, in which humans are provided 'a sensory experience that can both orientate and alienate' (Luhman & Cunliffe, 2013, p.135). Universities can generate discursive practices where social workers interact with social work students to explain, justify, and reflect on practice in ways different to what happens in social workers' everyday lives. Classrooms are therefore, not impassive sites, but places that cultivate particular kinds of questioning, opening possibilities for different kinds of reflections and practices to emerge for the visiting social worker. Students in the classroom pose questions, which problematize what it means to think, be and act as a social worker. As a result, social workers who teach students at university may be rewarded with opportunities to reflect on their self-efficacy as professionals. These opportunities to reflect about the constituents of what it means to be a social worker can even loom before the social worker enters the classroom.\nElizabeth initially described feeling like an 'imposter' in taking on the teaching consultant role, suspicious about the depth of her knowledge and scared that students would reveal her inadequacies. Initial fears were dispelled as she successfully conveyed stories about her practice, a powerful learning tool to relay practice knowledge. Interactions with students revealed to her how much she knew about working through challenging situations with service users. Their teaching experiences led Elizabeth and Alex to acknowledge their professional capabilities, predicated on their extensive social work experience. Teaching revealed how much they understood the complexity of social work practice and led to increased confidence to take on more complex work or additional responsibilities. Teaching provided a confidence booster. Elizabeth and Alex assessed their worth as objects of knowledge and realised they were worthy, yet the knowledge they know goes well beyond social work knowledge or the curriculum. For example, there is a sort of emerging craftsmanship present for Alex, as she synthesises her academic and practice knowledge (Trevithick, 2008). Teaching allows Alex to return to theories of power, care and control, enabling her to generate new insights by relating this academic understanding to her practice as an Approved Mental Health Professional.\nPerhaps it is something to do with social work as a profession and/or with Elizabeth and Alex as individuals, but both their reflective case study narratives reveal how interactions with students led them to develop a greater sense of self-belief about their capabilities as social workers. For Foucault (Packer, 2018, p456) knowledge about a discipline or a way of life such as social work is revealed not just by what we know, but also by ways of acting and being a social worker. What is revealing about Elizabeth's account is how her emotions and identity came under the spotlight when she was in the classroom. Therefore, when Elizabeth enters the classroom, she becomes an object of knowledge (Foucault, 1984) not only for students but also to herself. What constitutes this object is also interesting. It is formed by what she understands as social work -revealed through Elizabeth's storiesand by her taking stock of her identity and values. Elizabeth examined how her professional and personal identities collide, creating a space to reflect on presence of intersectionality in her formation and the presence of structural barriers which impacted on her capability to be an \"authentic\" black working-class female social worker. Similarly, classroom encounters provided an opportunity for Alex to consider how one's identity as a social worker sits alongside other parts of one's identity to form one's self-constitution alongside other parts of her identity, with all going into the mix to form her self-constitution. For Alex and Elizabeth, encounters in the classroom provided a space to reflect on the changing nature of their professional identity in terms of their professional journey to date while creating opportunities to define what might happen in the future.\nIn particular, the classroom was not just about examining who they were; the classroom provided an opportunity to recalibrate their ambition for social work. Teaching afforded Alex an opportunity to reflect on the significance of human rights legislation, and reconnecting her to values, which had led her to become a social worker in the first place. The consequences of teaching for Elizabeth revealed itself in terms of reshaping her professional beliefs about the potential of social work as a discipline and reimagining her own values and visions as a practitioner (e.g., an active rather than passive engagement as a social worker). Teaching facilitates Elizabeth and Alex to get in touch with specific elements of critical reflection in social work practice (Fook, 2015), namely they are questioning the assumptions underpinning their practice and how power is distributed. By doing so, Elizabeth and Alex are reconstituting the ontological possibilities for social work practice.\nStepping out of their everyday working environment enabled Alex and Elizabeth to reflect on their political and ethical stance in relation to social work and the possibilities of social work practice. The discursive formation of the classroom setting generated discursive practices for Alex and Elizabeth to think critically about power relations in their work. Rather than just passively applying policy guidelines and legislative frameworks, Elizabeth wants to become a more powerful and pioneering practitioner to make a positive change in society.\nThis ontological recalibration opens a door to transformative practice actions. Evidence of different ways to practice emerge after Alex and Elizabeth return to their local authority settings. Alex acknowledged improvements in her work and the benefits of team discussions. There was a renewed focus to her work, and she embodied enthusiasm and confidence. Alex and Elizabeth also felt more strongly about the benefits of reflection. Alex thought more about the connections between theory and practice. Elizabeth reflected more about her interactions with service users. Alex revealed a desire to act as a sincere and authentic practitioner.\n---\nConclusion\n\nThe two reflective narratives reveal that a government initiative promoting quality within social work education (Interface Associates UK Limited, 2020) can result in unanticipated outcomes. Practicing social workers who teach social work students in a university setting also benefit and learn from these encounters, and this learning takes different forms. The discursive formation associated with higher education enables students to ask questions about practice. Students have the ability to problematize what it means to work as a social worker. Teaching social work students in the classroom provides a learning opportunity for social workers. Without having an opportunity such as this, social workers run the risk of existing and performing the same actions for a long time. Through these classroom encounters, social work students are the catalysts to generate an \"effective problematization by thought\" (Foucault, 1998, p.388).\nPossibly this may cause challenges for some practitioners, but for otherslike Alex and Elizabethsuch questioning revealed the depth of their social work experiences, which left them feeling reassured. Their self-efficacy concerning their social work abilities was strengthened. In addition, classroom interactions can lead social workers to re-examine their understanding of social work and to consider the possibilities of a new form of social work. In practice, this can result in social workers returning to their everyday lives and thinking and operating differently as social workers.\nGiven the crisis of retention especially in children's and families social work (Department for Education, 2021b), the opportunities for social work practitioners to teach social work students in the classroom may become an important element in social work continuous professional development policies. Elizabeth's and Alex's accounts point to some unanticipated benefits from teaching students; both gained opportunities to reform their subjectivities as social workers, not only in terms of their appreciation of what they knew but also in terms of taking stock of their values and identities as social workers and imagining new possibilities for social work. In a systematic review, Turley et al. (2022) did not identify or recommend any service interventions to increase social work retention. However, Elizabeth's and Alex's own accounts reveal individuals who were re-energised to return to social work after teaching social work students in a University classroom. Both were reconnected to the values, which brought them into social work. Both identified visions to approach social work in a different way, with an increased self-efficacy to do so.",
        "Introduction\n\nMarriage is an aspect of a series of cultures synonymous with traditions in Indonesia and other parts of the world. For example, Malaysia's popular Tepak Sirih tradition reflects the Malay lifestyle and values (Wahyuni, 2017). In China, people use many symbols in marriage practices (Yong, 2014). Tepak Sirih and innumerable symbols used by Malaysians and Chinese are forms of marriage culture (Tan et al., 2018). Likewise, Indonesia, with its various ethnicities and religions, has diverse patterns of customary marriages practiced by the Dayak (Mardawani & Jaya, 2019), Sundanese (Gunawan, 2019), Javanese (Robiyanti, 2020), and Gayo (Selian, 2017) to portray their cultures. According to Rochmiatun (2020), Uluan Musi, one of the regions in South Sumatra, has a distinctive cultural form and structure of marriage patterns.\nIn Colonial literature, Uluan and Iliran were formed as conceptions built up in society in the past. Uluran is a term to denote the territorial area of the upper reaches of the Musi River in South Sumatra. In rural areas, there is a single culture of civilization/culture; the people are only one or two ethnic/tribes. Meanwhile, Iliran is a term used to denote the downstream/Musi River estuary territorial area in South Sumatra. The urban center, a meeting place for various civilizations/cultures, has a more plural society. It was not found when this term was first used as an identity that shows the pattern and structure of society (Rochmiatun, 2019). However, there are indications that uluan and iliran emerged along with the presence and development of Palembang, which plays a role as a center of activity, especially economic activity. The role of Palembang, which is the center of economic activity, then makes it, at the same time, the center of politics, society, culture, and religion in the form of strict societal polarization (Syafran, 2021). As a center, iliran is considered a more modern area than the traditional Uluan. Stereotypes that exist in society are models that must be followed. Iliran and Uluan can also be understood from the political system of the Palembang Sultanate (Rochmiatun, 2020). The Sultanate of Palembang is believed to have various powers to control and coordinate all resources of political and economic power. The Ilir area is generally known as the toll area, while the Uluan area is commonly known as the Sindang area.\nThe Uluan Musi community was initially called deutro Malay, the \"second wave\" that inhabited the archipelago. Estimated to have come in the metal age (approximately 1,500 BCE), it replaced the Proto Malay, the first of the two \"waves\" of migration that were thought to have occurred during the Indonesian occupation. It settled in the interior of the archipelago area around Southeast Asia, which is now divided into the country of Malaysia and the western region of Indonesia. Deutro Melayu brought about progress in terms of culture and was even identified as a society that introduced iron tools into daily activities (Setiawan, 2019). Furthermore, this community has a lengthy history of development that led to its diverse and mixed cultural forms (Suroyo et al., 2021). This is understood as a consequence of continuous interaction, which created new heterogeneous and unique cultural forms. The legal culture concerning the marriage tradition of the Uluan Musi also has a lengthy history, portraying traces of pre-Islamic ornaments, such as cultivating yellow rice and using coconut leaves to decorate the venue (Arif & Darwati, 2018).\nIn the event of a customary marriage, Islamic law is unambiguous. Besides its requirements and pillars, the Uluan Musi community usually adheres to other Islamic values (Basori, 2020).\nThis can be seen in the inherent social rules, good habits that have existed for a long time, and other perspectives supporting Islamic values. These are perceived as a source of legal norms that apply to society (Isnaeni & Hakiki, 2017;Pala, 2020). This is evident in the enactment of the Sindang Merdeka [customary law used in the Southern Sumatra region] and the Simbur Cahaya Acts [customary law used in the Uluan region of South Sumatra], which had been in effect long before Indonesia gained independence. These two laws were the initial source that triggered the development of Islamic legal norms that apply to the Uluan Musi community (Adil, 2018;Fitria & Otoman, 2021;Wulandari & Marzuki, 2020). Likewise, the marriage procession practiced by the Minangkabau community adheres to Islamic teachings, including its legal requirements and pillars. The members also adhered to the Nagari exogamy by prohibiting marriage practices involving individuals from the same family line and getting a mate from outside the Nagari. This is because marriage is not only an issue related to the men and women involved but also the families of both parties and the prevailing social and legal systems.\nUluan Musi is a Muslim community; its traditions and laws are inseparable from Islam (Farida et al., 2019). Traces of Islamic law are evident in the Simbur Cahaya Act, specifically in Part I Article 32, which references the customary marriage system (Farida, 2012). However, four basic rules were identified as a guide in the Uluan Musi marriage arrangement. The first is the provision regarding its legality involving customary structures. The second has to deal with the amount of money spent and various types of traditional food prepared for the ceremony. The third regulates engagement and its legal consequences. The fourth manages civil law and customary settlement procedures (Farida & Hassan, 2012). The Simbur Cahaya Act, legally related to the marriage system, is declared a great tradition of the Uluan Musi community (Yusdani, 2004).\nThe Simbur Cahaya Act was perceived as a source of legal norms that are adhered to by the Uluan Musi community, explicitly concerning its customary marriage consequences (Adil, 2018). This arrangement is evident in documents, speech stories, and traces of traditions still practiced (Farida, 2012). The two customary law documents were perceived as a form of continuity and change from the great tradition stipulated in the Simbur Cahaya Act. The first document is centered on the customs and traditions of the Musi Banyuasin Regency (Permata et al., 2021). The second document is based on the Halawe Customary Law of Lubuk Linggau City (Rosmaidar, 2020). These two documents that regulate the customary law depict a change in the Simbur Cahaya Act, perceived as a grand tradition formally applicable in Uluan Musi. This change in the marriage tradition eliminates the distinctive cultural identity. The loss of this culture alters the social order, which results in the disruption of history and even leads to cultural destruction (Abdullah, 2015). In the past, the community highly respected the eldest brother figure as a symbol of control, honor, and protection for the family. However, the change in legal culture led to the loss of this symbolic function and cultural control (Mahyuddin, 2017). The difference in al-'urf [Islamic law that already exists and is known to support human rules] is evident in the alteration of the structure from conventional to formal. This occurred due to several factors, including the erection of conventional customary institutions that are no longer functional. Secondly, it was also due to the number of traditional elders who withdrew without customary regeneration. Thirdly, the people's empathy for customary substances slowly faded, and the younger generation was finally indifferent to Uluan Musi customs. This led to the acculturation between religion, custom, and culture, which cannot be denied by the Uluan Musi community and the Malay society, in general, due to their ability to embrace new things dynamically.\nThe research on customary marriages only tends to place this union as a sacred ritual and a tradition practiced by every community regardless of ethnic and racial background (Cholil, 2017;Melayu et al., 2021). It is recognized as the acculturation between culture and religion based on the power and legitimacy of the community (Banks, 1976;Friedman, 1975;Sukerti & Ariani, 2018). This is perceived as part of the cultural, economic, and political space, although it is rare for customary marriages to make changes to Islamic legal culture (Astuti & Amirullah, 2019;Watson, 1983). It is also considered an essential factor in altering legal culture (Octavianna et al., 2020;Zainuddin et al., 2020). Customary marriage needs to be seen as a natural process (Mubarok, 2017), not as one that is formed. There is freedom of space, and it presents the dynamics of Islamic legal culture, although not as objects of coercion to fulfill a series of traditions based on religion. This research is based on an argument that the Islamic legal culture concerning the customary marriage practiced in the Uluan Musi community implies that these practices and their noble values can be maintained in the dynamics of global reality (Murni & Supriyanto, 2010). This led to the dialogue that traditional culture needs to be perceived as part of social identity. Aside from reflecting on society, the acculturation process between customs and customary marriages emphasizes that something new can be harmonized in a specific cultural practice (Iryani, 2018). Finally, the existence of an integrated customary marriage is not only a matter of marital process and customs but rather a formulation of community identity that upholds local wisdom without rejecting new things. The Uluan Musi is expected to continue maintaining its tradition in terms of customs and religion and as a reflection of a group with distinctive character and principles.\nAccording to Lamsal (2012), cultural change and continuity are closely related to its structure. This is described as an institution created by communities to support sustainability (Azizah, 2020;Jaenudin et al., 2020). Meanwhile, culture is an activity practiced by cultural communities to express their norms (Salim, 2016). These two attributes in the Uluan Musi community tend to cause specific alterations in the Islamic legal culture, impacting social changes (Martono, 2018). These are further metamorphosed into religious nuances, perceived as a form of cultural structures practiced presently (Tendi, 2016).\nThe Uluan Musi community has experienced cultural and structural changes that impact traditional patterns, specifically in the marriage culture. Aside from confirming the Malay community's dynamic and open characteristics (Haryono, 2017), these changes also provide space for the acculturation process between religion and culture (Bukhari, 2017). The customary marriage practice, which has existed for a long time, can be shortened by changes in the Islamic Legal culture given the Uluan Musi community. Arif and Darwati (2018) stated that religion is essential in preserving culture. Also, several factors that alter Islamic law usually harmonize with the reality of different dimensions. This research is intended to complement previous research on changes in the legal culture of customary marriages, perceived as part of the traditional and communication media (Suryani & Vidya Riani, 2022). For example, Murdan (2018) analyzed specific changes in Islamic legal culture concerning traditional marriages of the Sasak tribe. Yayuk examined customary marriage as one of the gaps in child marriage (Kusumawati, 2019). Ni Nyoman (as cited in Sukerti & Ariani, 2018) analyzed customary marriage as a cultural existence. Other related research focused on culture and the correlation between customary, religious, and favorable laws. Subsequently, this research is intended to answer three main questions: (1) How is the practice of Islamic legal culture concerning the Uluan Musi marriage custom occasionally performed? (2) What changes are made? and (3) How does its impact affect the Uluan Musi community? These are expected to be able to resolve the issues associated with changes in the legal culture [al-'urf] relating to customary marriage practices in the Uluan Musi community.\n---\nUluan Musi community\n\nIn Colonial literature, Uluan and Iliran are conceptions fabricated in the past. It was not pinpointed when this term was first used as an identity showing societal patterns and structure (Rochmiatun, 2019). However, there are indications that Uluan and Iliran emerged along with the presence and development of Palembang, which played a significant role as the center of economic activities (Zulkifli & Nasution, 2001). This also made it a political, social, cultural, and religious center through strict societal polarization (Syafran, 2021). Meanwhile, Iliran is considered a more modern area than the traditional Uluan. The stereotype that exists in society is a model that needs to be adopted. Iliran and Uluan can also be analyzed from the political system of the Palembang Sultanate (Rochmiatun, 2020), which is believed to be influential in controlling and coordinating all political and economic resources. The Ilir and Uluan regions are known as the Toll and Sindang.\nThe power of authority differs between the Iliran and Uluan areas. For example, in the Iliran or collection area, the sultan has the power and authority to collect taxes and recruit labor. The reverse is the case in the Uluan or Sindang, where the sultan and the kingdom do not have the authority to collect taxes and exert occupation (Santun et al., 2010). The Musi River Basin is a deeply rooted regional identity covering four provinces: South Sumatra, Bengkulu, Jambi, and Lampung (Idris, 2019). Therefore, given its comprehensive area coverage, this research does not fully utilize the terminology of the Musi Ulu community in its regional sense. In South Sumatra, the term music is often used as the name of a river and cultural identity (ethnicity). When it is called \"orang musi or Wang musi\" [people from the Musi ethnicity] (Apriadi & Chairunisa, 2018;Firawati, 2019). Incorporating ethnic identity into the names of rivers is a typical South Sumatran adventure, such as with the \"Orang Ogan, ughang ugan\" [people who traditionally practice Ogan culture daily] or \"Orang Komering, Jolma komering\" [showing the identity of the people who practice the Komering culture] (Febriyanto et al., 2021).\nThe Musi community is a term for the ethnicity who live and occupy the area along the Musi River. It serves as a water source, and the people are then labeled as the Musi community (Sarkowi & Asmara, 2021). However, this is often pinned on the name of the river inhabited, as there are several of them in the South Sumatra region. The Musi community often identifies with the urghang Sekayu or the Sekayu Ethnicity (Apriyanti & Dienaputra, 2015). This is because Sekayu is an area used as the center of government and public services, currently included in the territory of the Musi Banyuasin Regency and the young Malay family (Deutero Malay) (Ku Samsu et al., 2020).\nMalays have existed in groups since approximately 500 BCE and are better known as Proto Melayu (Anis & Esa, 2012). Consequently, based on the origin of the Uluan Musi community, they are inseparable from their relatives from the Palembang, Basemah, Bangka, and other ethnicities that have existed in the past (Muhidin, 2019). By inheritance, this community forms a group of indigenous people with an identity institutionalized in customs, traditions, and tribal cultural practices. Presently, the geographical location of the Uluan Musi community is identical to the communities throughout Banyuasin, Musi Banyuasin, Musi Rawas, and Musi Rawas Utara Regencies, and the Municipality of Lubuk Linggau. More details on the distribution of Uluan Musi are shown on the map in Figure 1. \n---\nLegal culture and marriage: A sociological framework\n\nLegal culture is one of the three pillars of law in addition to legal structure and substance (Djatmiko, 2019;Rahardjo, 2017). These three legal pillars influence one another. The legal structure will make the law have clear and definite regulations, the substance of the law will make the law have compliance value because it departs from the needs of the community (Soekanto, 1977), and legal culture will enable the supremacy and law enforcement to operate in a proportional corridor (Al Arif, 2019;Harliza & Michael, 2020). Legal culture is a reflection of the functioning of the legal system (Jayadi, 2017). Thus, the legal culture becomes the legality of the legal product itself. In the end, the law will experience a reaction from the entry of new views related to legal dynamics.\nIslamic law is based on divine provisions where Allah is God who has authority over everything that happens in this world and clarifies and teaches good practices to his followers through the intermediary of the Prophet Muhammad as his messenger (Taufiq & Sy, 2019). On the one hand, Islamic law is static, fixed, and steady. This is because Islamic law is the decree of Allah SWT. However, on the other hand, Islamic law is re-understood, interpreted, and reconstructed within the framework of ijtihad to be sustainable with other scientific fields such as sociology, political science, anthropology, and the realities of social life (Buhori, 2017). Al-Quran, Hadith, Ijtihad, and Qiyas are legal antinomy; one side requires stability to create supremacy, but the other side must be elastic to achieve the substance of legal justice (Hajairin, 2021;Rahmatullah, 2021). In this context, Islamic law is said to be flexible and, at the same time, static to meet the needs of the development of human civilization. Social events that continue to develop, including those related to health, such as COVID-19, encourage the reinterpretation of several aspects of Islamic law (Hidayatullah, 2017;Jufri, 2017). Islamic law is generally flexible and elastic if it is not related to legal issues other than those related to legal provisions of ritual worship. Outside of ritual worship, Islamic law is very adaptable, including in this context associated with the legal culture of marriage.\nExistence is a space free of values and never has freedom from spatial practice patterns, different representations, and experiences of reality (Salim, 2016). Adat is a social reality constructed by the community simultaneously and binding (Efriani et al., 2021;Isdiyanto & Putranti, 2021). These customs and traditions are born, live, grow, and develop into ideologies that give birth to social values in a sustainable manner both as individuals and groups in the form of distinctive characteristics of customs according to geographical and territorial location (Haryono, 2017). This local wisdom can then be prioritized as an icon or symbol of a community (Salleh, 2018). Furthermore, cultural practices are upheld as a form of existence from cultural practices and customs that apply in the past, present, and future. Thus, the presence of adat and society is the beginning of identity formation. Of course, this is accompanied by various factors such as the rapid advancement of technology, the development of communication, and social, political, economic, and educational dynamics, which can be factors that change cultural practices that exist in the customs and traditions of a society.\nAffirmation of existence cannot be separated from the factors of religion, culture, and phenomena that exist in a society, where all of these were formed from the long process of the past and became a value system on the scale of custom, language, ethnicity, religion, and rules within the scope of social reality (Diharjo, 2019;Kamal, 2019). The existence of adat in marriage is a way to maintain traditions and culture as local wisdom and is a part that differentiates one society from another (Maladi, 2011;Rosmaidar, 2020). Humans become the central subject in realizing the existence of a thing as a form of respect for culture (Strasser, 2011). One is by continuously maintaining traditions even though these customs are sometimes simplified and adjusted according to the social scope. Religious and customary identity is inherent as one of the legitimacy to monopolize truth in cultural ritual practices; not infrequently, this also applies to the existence of religion (Gustiana, 2021). Existence is also challenging to separate from individual and communal problems that try to show their reaction in the social sphere (Thohir, 2020). Thus, the existence of the adat of society becomes a treasure that enriches the understanding of the dimensions of religion and culture itself.\nMarriage is considered sacred because there is an extensive choice that is taken and is valid for life, namely the option of a partner (Jotam Kalalo et al., 2019). Traditional marriages always experience a cultural shift, and this is because traditional marriages always keep up with the times. Several factors cause this cultural shift, including weak economic levels and freedom in choosing a mate (Roibin, 2015). In line with that, there has also been acculturation between religion and culture in traditional marriages. Islamic religious teachings are then used as a reference in traditional marriage processions and changes in the times (Isnaeni & Hakiki, 2017). Traditional marriages can also be a message from symbols in a community's traditional wedding procession. Later, the message will be disseminated after the traditional wedding ceremony.\nTraditional marriages that use religion as a reference will provide specific criteria and conditions for marriage because, often, traditional marriages do not use age restrictions and are not even recorded in the registration of marriages. It is not uncommon for the tradition of arranged marriages to be carried out for children ranging from 5 to 10 years old; it is not unusual for traditional marriages to be carried out for children aged 15 to 17 years (Imam Hafas, 2020). Until now, marriage law is considered to be underused in traditional marriages. Due to the impact on a dynamic and developing society, these basic rules have not been able to accommodate the needs of rules and guidelines for the community (Okwita & Adiningtiyas, 2019), especially in dealing with social changes and developments related to marriage (Safriani, 2018). As with the differences between matriarchal and patriarchal marriages, same-ethnic marriages for indigenous peoples are prohibited, interfaith marriages, and so on (Rahmi et al., 2022;Sandy, 2016). In addition to being a culture, traditional marriage is a step to achieve inner and outer peace, increase social solidarity, and create cooperation among communities. Therefore, traditional marriages continuously adapt to reality despite changes in practice.\n---\nMethods\n\nThis undertaking was sociologically intended to translate legal issues discussed correctly and investigate society's social realities (Barkah et al., 2022). Therefore, this research investigated the customary marriage procession of the Uluan Musi community. This exploration obtained a research permit from the ethical commission Raden Fatah State Islamic University Palembang, as the author's place of work and at the same time as a center for Islamic learning in the city of Palembang with activity number B-213/Un.09/PP.06/04/2022.\nData were collected concerning the existing facts in the practice of customary marriages and observations. The unstructured interviews were conducted from March to September 2018 with 15 Uluan Musi community, religious, and government leaders to obtain complete information about customary procedures. The research locations were in the Uluan Musi area: Banyuasin, Sekayu, Muara Beliti, Musi Rawas, Muara Kelingi, and Ulu Rawas. This was done to obtain more accurate data in finding the results of this study. Furthermore, this study also conducted interviews with 16 participants who had knowledge and understanding regarding traditional marriages of the Uluan Musi community; among the informants selected in the interview were community leaders, religious leaders, and youth leaders. Each participant was interviewed for 60 to 90 minutes, some concurrent. In addition, other data was also collected through personal documentation in the form of wedding photos, government policy documents, laws, and others.\nThe process of data analysis in this study took place through three stages, including the process of reducing data, namely the process carried out to organize data into a more systematic form; then the process of displaying data, namely the method of describing research data in the form of tables containing excerpts from summaries of interviews and secondary materials that have been classified; and finally, the process of verifying data, namely the process of concluding data based on trends from the data that has been organized. Of the three stages, conducting an inductive thematic analysis of the data made it possible to perform a restatement process for interpretation and reflection. The stages of analysis and analysis techniques used make it possible to formulate a conclusion about changes in society and Islamic legal culture in the practice of marriage in the Uluan community. Interview data for the informants' category of traditional leaders and youth leaders are shown in Tables 1 and2 below. This research focused on the Uluan Musi community in marriage, which commonly uses Islamic legal culture. This practice was carried out with various processions in the customs of the Uluan Musi community. In some marriages, the culture of Islamic law is still used by the Uluan Musi community. On the other hand, changes in the culture of Islamic law occur in the marriage practices of the Uluan Musi community. Sixteen informants were interviewed for this analysis: ten traditional leaders and six youth leaders. Factors of knowledge and involvement in the customary marriage practices of the Uluan Musi community selected the informants in this study. Socially, traditional leaders are essential in determining changes in Islamic legal culture in traditional marriages. The informants were also chosen to get further motives for changing the culture of Islamic law in the customary marriage practices of the Uluan Musi community.\nInterviews with traditional leaders were carried out as part of the primary source of this research. Meanwhile, the community leaders consisted of various regions in the Uluan Musi area, such as Banyuasin, Musi Banyuasin, Musi Rawas, Lubuk Linggau, Muara Beliti, and Ulu Rawas. Secondary legal sources from books, journals, and research related to customary marriage and cultural acculturation were involved by analyzing the laws and regulations that were previously observed. Furthermore, data were collected and analyzed descriptively in three steps: reduction, display, and conclusion. Data reduction is summarizing and sorting written documents to focus on the theme. This process illustrates the observations more sharply and also makes it easier to find data that has been obtained. The next step was data display, which made various classifications based on themes, tables, graphs, and maps. The last was drawing tentative conclusions based on the previous steps.\nAfter that, the data was presented as a description of an interview excerpt. So that it can be mapped and conveyed from the data analyzed in an interpretive manner to identify and explain the research context, data analysis in this study uses a phenomenological approach that sees changes in Islamic legal culture in the customary marriage practices of the Uluan Musi community as social facts that can be interpreted and contextualized.\n---\nResults and discussions\n\nThe ups and downs and development of society's culture show the dynamics of culture. The development of legal regulations, the changing of political leaders, the development of science and technology, and the rapid development of global communications are some factors that have caused this change. Likewise, the culture of Islamic law in the marriage tradition in Uluan Musi has experienced various changes in the procession and legal culture according to customs in the reality of society and Islamic law.\n---\nProfanization of customary marriage culture values at Uluan Musi\n\nMarriage is a sacred event that needs to meet several conditions and pillars in Islam to avoid annulment (Fahmi, 2019). The Fuqaha [jurists] who wrote classical and contemporary fiqh always paid special attention to the issue of marriage, which usually starts with the process of khitbah [proposing] (Muzammil, 2019). Fiqh covers the law (Hamdi, 2017;Putri et al., 2022), its limits, and the procedure for the khitbah. It is followed by explaining the marriage contract, including the terms and pillars (Tulab, 2017), guardian, dowry, and witnesses. Preliminary research on fiqh by Berhan et al. (2019) also explained the law of walimah 'ursy. The final section explains divorce, including the legal rules, types, and iddah (Jamhuri & Zuhra, 2020).\nResearch on fiqh is legal (Fikri, 2019) and associated with the formal legal aspects of the law. The final instruments for wadh'i law in Islamic law, consisting of sunnah, haram, makruh, and permissible, are used to determine the legal formalities of law, including marriage (Noor & Lee, 2021). The characteristics of such fiqh research in people's lives cause several dynamics and problems, specifically at the intersection with the culture exhibited by the community. Therefore, to overcome these problems and dynamics, the science of Usl al-Fiqh (Ushul Fiqh) compiled a set of ushul instruments as a method for solving istinbath [legal issues] (Nur'aini & Ngizzul, 2020). Regarding the intersection of Islamic law with local culture, the science of ushul fiqh formulated a series of 'urf' rules.\nIn the treasures of Islamic civilization, it is necessary to determine legal developments, which are inevitable in their meaning as part of human civilization (Muslifah, 2013). This intersection makes Islam have to \"dialogue\" with local traditions, where 'urf plays an important role as an instrument of Islamic law enforcement (Maimun, 2017;Najib, 2020). Marriage in the Uluan Musi community has undergone significant changes since the inception of a new culture through a prolonged transition from traditional to social reality (Izmuddin, 2018). The customary marriage is simply because the practice is mostly used to fulfill the Shari'a and pillars that apply in religion. Other practices are carried out without burdening the bride and groom or are often called \"kawin sepetang.\" The changes in cultural values in the customary marriage of the Uluan community can be seen in several interview excerpts.\n\"Currently, some indications show that the community is fed up with the complexity of customary law. They prefer something simple that does not require a lot of money and is easy to conduct. For us to be happy in marriage, without marital problems, we need to be busy and work together to achieve success. Sometimes, it's even enough to get married once.\" (AL, 64 Years)\nThe explanation above illustrates that the change in value occurs because the traditional procession in marriage has to go through a very long and complicated process; hence, people practice simpler and faster traditions. Apart from time, these considerations are also made to reduce significant expenses in marriage. Meanwhile, the following is an interview excerpt with traditional youth leaders:\n\"There have been interesting developments since the 2000s. Some people are of the opinion that our customs deviate from Islamic law. Therefore, for marriage, it is imperative to fulfill the conditions and the pillars without adding traditional events, which are capable of polluting religion.\" (CH, 48 Years)\nBased on the explanation, the change occurred due to factors other than the long and expensive customary marriage procession. It is also due to globalization, which opened a new understanding for the community in evaluating traditional practices. According to some people, certain traditional activities do not have religious backing; hence, they slowly become extinct or replaced. This is in line with the following excerpt by S, one of the traditional leaders of Musi Rawas:\n\"Marriages should be carried out in accordance with the terms and conditions of the Shari'a to avoid difficulty.\" (S, 60 Years)\nThe inclusion of profanization in the marriage culture of the Uluan Musi community also caused a change in orientation. It is more challenging to fulfill the pillars and requirements of religious law due to adultery and pregnancy outside marriage; hence, in most cases, the ceremony is conducted without hesitation (Panji, 2020). These factors often occur due to different religious understandings, such as differences in sects and schools of thought (Saifullah & Aksa, 2021). The practice of customs by society does not follow the teachings of the Shari'a, which means it comes out of aqidah. Therefore, profanization in the customary marriage procession of the Uluan Musi community certainly does not just occur but also opens a space for community dialogue in making decisions.\n---\nCultural coexistence in customary marriage at Uluan Musi\n\nCultural coexistence in the customary marriage at Uluan Musi is caused by two factors, namely in-group and out-group, which are carried out by local and external agents, respectively. This community also has mutually agreed-upon values, which are preserved as customs and legitimized to form the identity of indigenous people (Rochmiatun, 2020). The value system in the community can change in line with time (Setyaningrum, 2018) and is motivated by various internal and external factors. Changes in a customary culture, such as marriage, are simplified and formalized within months, as stated by A in the following excerpt:\n\"In the past, the marriage process was carried out for at least three months. It starts with the family gathering to determine the wedding day, customary weeks, contract process, and customary obligations.\nEverything should be no less than three months. Sometimes it takes even up to two years, and this is the reason many traditions are no longer conducted.\" (A, 60 Years)\nCultural simplification occurs due to a series of adjustments, while formalization is associated with changes in the political system of government. The coexistence in the customary marriage tradition of the Uluan Musi community can be observed from several factors. These include several new processes in marriage culture, such as pre-wedding photos as external factors included in the tradition and accepted by customs (Jannah & Nawir, 2019). Furthermore, the change in the complete chicken arbor tradition was replaced by giving money to both parties. Moreover, changes in the chicken arbor tradition include providing food and side dishes from family, relatives, neighbors, and closest relatives. Often, the chicken arbor is also a sign of invitation from the party who will carry out the wedding reception, and this is then replaced by giving money to both parties. The following is an excerpt by one of the bayan tue in the Musi Rawas area:\n\"Punjung ayam is a tradition that has disappeared or replaced with money in some areas of Uluan Musi. At Muara Kelingi, Musi Banyuasin (Sekayu), and Rambutan, a complete chicken arbor is prepared for cacapcacapan; the traditional event is carried out before the marriage ceremony.\" (Z, 72 Years)\nThe shifting of Punjung Ayam is understood as a form of cultural coexistence because it is replaced by money and cacap-cacapan. Cacap-cacapan is a traditional activity in the form of sprinkling flower water over the heads of the bridal couple after the marriage ceremony performed by the groom's family (father, grandfather, uncle, or another older adult male). According to AL, this was also implemented in the Palembang's culture and the cacapcacapan, where it is accompanied by a series of rhymes that are read poetically to show the role of parents in raising and educating their children. Finally, the coexistence is seen in the loss of the dish process, which is one of the hallmarks of the tradition. It is replaced by eating in the buffet by caterers or Wedding Organizers, eliminating the cooking tradition (SU, 40 Years).\nIdangan adat is a dish in the form of large and small food intended for eight people.  2018). This is where the stage ability is tested with callers for food, delivery officers, and spoons of gulai well organized to maintain order. Besides the value of manners and order, cultural education is also a system that can be obtained. In the traditional process, children will not receive idangan tue, callers to eat, and screwdrivers are not allowed to eat before others. The shift from idangan adat to buffets (French) in pre-wedding and complete chicken abolishes the cultural coexistence of serving food mentioned earlier. Suppose pre-wedding is a form of coexistence that duplicates the new culture. In that case, the koneng chicken arbor and idangan adat are a form of coexistence that eliminates the traditional format.\n---\nCustomary marriage culture at Uluan Musi and globalization\n\nThe Uluan Musi community is part of a trend reflected in the integrated and transparent face of the earth. Globalization can be analyzed culturally, economically, politically, and institutionally. The consequence of an integrated and transparent world is the possibility of a clash between traditional and modern cultural values. Many experts explain that the expanded idea of globalization as a complex process is often contradictory with multiple contradictory results (Ife & Tesoriero, 2008;Tesoriero, 2011). Globalization not only includes how significant changes affect daily life (top-down) but also ways in which these changes affect local activities globally (bottom-up) (Nahak, 2019). Consequently, the cultural differences that occur in culture show the transnational expansion of several aspects and practices homogeneously used to explain cultural acculturation that tends to occur due to the heterogeneity of diverse cultures in society. Likewise, AB and RS also conveyed this in the following excerpt:\n\"The customary marriage process takes a long time and also requires much money. Presently, a greater number of people have decided to simplify the process, except for the characteristics of traditional culture. The proper procession can now take up to a year.\" (AB, 67 Years; RS, 56 Years)\nThe format for holding traditional marriages that were previously carried out for a long time with long stages included belinjang/besinde [the period when bachelors have given jewelry and received women but have told their parents], nepek gan and nunak dera [a bachelor's customary sign to show his seriousness in establishing a relationship with dere. In the form of a small knife, a silver ring wrapped in a handkerchief], and ngeruan rasan [family deliberation activities to determine the procedure for marriage, and a series of other processions that can take a long time, based on simplified efficiency considerations making it slimmer and more compact]. In the past, it took more than a year for the grumbling rasan to nyangge; now, it only takes two or three months. This shows that the community also considers the pros and cons of practicing traditional marriages. The Uluan Musi community, in the end, prefers just to carry out the pillars and legal requirements of a marriage. At the same time, the traditional procession is only used as a regional symbol to maintain the existence of its customs. This is also seen from the economic capacity of the two parties who are getting married, as stated by traditional and youth leaders of Musi Rawas and Muara Beliti in the following excerpts:\n\"The change in the process of customary marriage, besides shortening the implementation time, also eliminates the culture of cooperation among the community. Currently, marriages use Wedding Organizers; hence, guests just attend the event and return home.\" (D, 40 Years)\nFrom the perspective of cultural globalization, the above simplification manifests the interaction between capitalist and industrialist institutionalization. The capitalist character that puts forward the economic capital and individual needs as a consideration for the implementation of customs changes the culture of the Uluan Musi community from society [lan] to individual [lan nga lan nga, lanku lanku; okop detang herka be jedila] (AL, 64 Years).\nGlobalization comes with everything in life, specifically for indigenous peoples (Islamy et al., 2021). It has changed cultural patterns in social reality, specifically in the process of customary marriage practices. This certainly impacts changes in the economy, education, and customary culture, although they are inseparable from the agents of the structure of change (Baharuddin, 2015). Customary marriages that were previously carried out by practicing customs in full slowly turned into simpler forms. Therefore, the application process for the ijab qobul, which took up to a year, can be condensed into several months. This is explained by D in the following excerpt:\n\"The work of prospective couples is often a consideration to simplify the series of traditional events. Those domiciled in different regions and islands have varying customs, which makes a family compromise in simplifying the wedding process. The important thing is that the conditions and pillars of Islamic law are enough; the rest is sunnah, which means you cannot do it.\" (D, 40 Years) Globalization, from a cultural perspective, simplifies the institutional interactions between industrialists and capitalists, where the practice tends to profit from capital as an economic calculation. This aligns with the community's needs in carrying out customary marriage processions, which were previously more inclined to a social perspective with globalization becoming more individual (Setiawan, 2020). Therefore, a compromising attitude is needed to deal with the cultural changes after adopting an external tradition, indicating it can become part of the marriage practice of the Uluan Musi indigenous community. The simplification of the customary marriage practice was due to the pressure of various interests compromised in a dialogue room, thereby obtaining a positive response from the local community to form a new value system in customs (Azkar et al., 2021).\nGlobalization is part of the acculturation process between religion and culture. The value of Islamic law is still embedded in the cultural tradition of the Uluan Musi people, even though customary marriage has undergone several changes since globalization. Changes in the Islamic law of the marriage culture still show its local identity, which is religious, tolerant, independent, democratic, responsible, etc (Rosmaidar, 2020). Therefore, profanization, coexistence, and globalization in customary marriage tradition at Uluan Musi have changed the views of Islamic Law related to al-'urf. These changes occur without losing identity in the practice of the tradition in line with the purpose of the Islamic religion as rahmatan lil 'alamin.\n---\nConclusion\n\nThe structural practice of customary marriage can be seen from two sides: the legitimacy of institutions and the individuality of its practices. Traditional culture shows personal expression in preserving culture, which changes with globalization and impacts the customary marriage practice of the Uluan Musi community. There are at least three essential points from the process of cultural change, such as the profanization of the customary marriage from external cultures included in the traditional procession both culturally and religiously. The second point is the coexistence of the indigenous community, showing an attitude of Openness to blend and survive in every movement of change in social reality. The third is globalization, which can be seen from the interaction between culture and the external environment to form a new pattern in simplifying the procession.\n---\nLimitations and further recommendations\n\nThis is also a space in legal research, not only on community units and legal institutions but also on micro-analysis related to institutional, legal functions, namely family law, which consists of positive, religious, and customary law. It also emphasizes the existence of customary marriages in the Uluan Musi community in the dynamics of globalization. The processions become a symbol of a particular society, established based on the customary order to practice traditional rituals. The existence of this custom also implies a discursive dialogue between the guidelines, which are always used as legal footholds. This research also contributes to aspects of customary, family, and Islamic law, specifically legal morality, from the perspective of civilization.\nThis research showed the existence of marriage as a symbol of the Uluan Musi community's identity. Unfortunately, it has not been considered sufficient to provide an overview of the practice pattern of customary marriages in the Malay community. Therefore, more detailed research must follow up on a more representative pattern of customary existence without relying on a region to gain a better understanding. This proposed research also needs to analyze how the harmonious relationship between culture and religion relates to the practice of marriage. Finally, it can complement the scope of customary marriages in the Uluan Musi community.",
        "Introduction\n\nDavid Bowie has been quoted as saying \"[m]usic itself is going to become like running water or electricity, [so] you'd better be prepared for doing a lot of touring\" (1). Bowie's prescient prediction of a post-Napster world led one economist to coin the \"Bowie Theory\" to explain the changing economic model used by the music industry (1). Namely, the Bowie Theory summarizes the industry's shift from relying on revenue streams from physical copies of pre-recorded music to that of live performances. And recent research has found that while recorded sales have fallen, revenues from live performances have held steady (2,3). But does such an approach only work for the most popular bands? After all, music insiders claim that even \"mid-level\" bands would \"be doing well to break even\" (4). Altruistic motivations aside, why tour? Are there any economic benefits to live performances?\nResearch on the music market has found that besides the direct benefits of touring (i.e. ticket and merchandise sales), tours provide an opportunity for an artist \"to expand their fan base\" (5). This latter \"indirect effect\" has generally been rather amorphous in the literature, but advances in network analysis allows us to frame this question in terms of contagion. In other words, do live events influence users who did not attend the event themselves, but are embedded in a social environment that experience \"infection\" (i.e. attendance of the live event by some members of the network)? Social contagion has been studied in different systems and under different dispersion scenarios; these include political mobilization through peer networks (6,7), adoption of health behaviours among members of online communities (8) and real-world social networks (9), leveraging peers for viral marketing (10), and the spread of hash tags on Twitter (11,12). Social influence has also been found to have a critical role in the art market (13). But are the social contagion processes more effective for a certain category of artists? Social influence signals are widely used in such settings and help promote popular products to maximize market efficiency. However, it has been argued that social influence makes these markets unpredictable (14). As a result, social influence has been presented in a negative light. However, when it comes to market activities, such processes can lead to considerable revenues.\nThe music industry in just the US has been valued at $17.2 billion as of 2016 (15), so there has been no shortage of incentives to optimize the revenue streams within it. However, it is the advent of illegal sharing of pre-recorded music by means of mp3s in the early 2000s that inspired renewed interest in developing new economic models for music consumption (16). In the modern music industry, there have been three major streams of revenue identified: (1) the Internet and digital music consumption, (2) CDs, records and conventional pre-recorded music consumption, and (3) live music consumption (17). The new economic models that emerged gave greater weight to live performances (18) and the industry reacted. As of 2016, live music revenues accounted for more than half of all US music revenue (15). Prior to the 2000s, there have been relatively few studies that sought to understand the mechanics of live events (5). This is not surprising as the conventional wisdom in the music industry before the Internet was that live performances should be treated as nothing more than promotions for prerecorded music (3).\nScholars identified the major reasons why artists choose to perform live as some combination of the following: (1) direct profits, (2) expanding listenership, and (3) strengthening their existing fan base (5). Prior to the Internet, it seems that the industry thought live performances could only satisfy the latter two factors, but, today, studies have clearly demonstrated that live music can capture a large share of direct revenues. Black et al. ( 5) find a trend of increasing ticket prices and unflagging demand, while industry reports illustrate live event revenues have continued to grow (19).\nThe question that remains is whether the traditional reason for concerts-namely, \"promotional\" effects-still exist. From a theoretical perspective, some researchers argue that there is something unique about live performances, as attendance has not waned despite the substantially higher costs (20). This may imply that despite major changes in the music industry and the increased convenience of digital music, live events are not simply a venue to sample music but a means by which individuals gain a new (or renewed) enthusiasm for the music (20). In other words, if the indirect promotional effects ever existed, it is likely that they still exist. Indeed, the increased ability to communicate information by the average concert-goer to a large number of personal contacts through online social networks may mean that the indirect effects are larger. These indirect effects have been assumed by theoretical economic models (18), but actually connecting music consumption to offline behaviour has proven challenging due to sparse data availability.\nMontoro-Pons & Cuadrado-Garcia (3) made significant in-roads evaluating the link between concert attendance and music consumption, concluding that concert attendance does not cause increases in CD purchases. They do note that their analysis does not capture other modes of music consumption and concert attendance may still have impacts on music listenership (3). Another study linked prerecorded music listenership with offline concert attendance to evaluate if fan preferences concord with songs actually played live (21). Most recently, Maas\u00f8 (22) looked at streaming patterns before and after a music festival in Norway. However, that study was a macro-level analysis and so it did not attempt isolate the peer effects from other possible confounds or attempt a more stringent causal identification strategy (such as the regression discontinuity design we use in this paper).\nWe build upon the methodology used by Rodriguez et al. (21), which made use of Last.fm data to link listenership habits to live event attendance. Specifically, we evaluate if live events have any impact on music consumption. We also look at differences in impact by type of band (popular or \"hyped\") and differences in attendee demographics. More crucially, we investigate whether live event attendance has any indirect effects on listenership among the attendee's friends. We divide non-pecuniary benefits into either direct effects or indirect effects. Direct effects increase a given individual's music consumption of that band as a result of their attendance. These effects include the expansion of the band's fan base (i.e., non-fans who go to a live event and become fans, consuming the artist's music after the event) and satisfying existing fans (i.e., fans who see the live event and subsequently consume the artist's music at a greater rate). Indirect effects include all gains in music consumption from the live event by individuals who did not themselves attend the event. These effects capture the recommendations from event attendees and other forms of communication about the events. We use Last.fm's song-listen data to capture an omnibus measure of music consumption and we rely on Last.fm self-reported live event attendance to capture our intervention of interest. As there are conflicting reports as to the profitability of touring depending on the artist's popularity, we also make the distinction between \"popular\" and \"indie\" (\"Hyped\") artists. To evaluate whether concerts actually have direct effects on the artist's fan base, we examine a given fan's listenership habits before and after live event attendance for 61,053 unique Last.fm users. We adopt a regression discontinuity design to determine if there are any shocks in listenership after event attendance, which would indicate that the performance either \"re-activated\" existing fans or generated new fans.\nTo evaluate indirect effects, we extract all of the attendees' Last.fm friends * (for a total of ~1.3 million users) and their listenership data. Any discontinuity in listenership at the time of the live event (that their friend(s) went to) can be interpreted as the indirect impact of event attendance on non-attendees. This measure captures indirect effects that result from either passive signalling or attendees actively recommending an artist to their friends.\nWe found strong evidence of direct effects for both Hyped and Top Artists. Attending an event of a Top Artist resulted in an immediate 1.13 song increase (p<0.001) in listenership by attendees.\nSimilarly, Hyped Artist events were responsible for a 1.05 song increase (p<0.001) in their attendees' subsequent song listens. The indirect impacts were much more nuanced. Event attendance of Top Artists' concerts impacted the music listening behaviour of attendees' friends who did not attend the * Last.fm has since adopted a follower/following social connection schema. At the time this study was conducted, Last.fm necessitated that both parties confirm their friendship to create a social link. event by .06 songs (p<.001). While this is a very modest impact, we must keep in mind that the average concert-goer in our dataset has an average 56.8 friends just on the Last.fm network. And in some cases, more than one individual in a given social community will attend the same event. To determine if the indirect effects increase as the number of attendees increase, we varied the number of individuals who attended the event and examined the indirect effect on non-attending friends. We found evidence that the indirect effects increase as the number of concert-going friends increase. A user who had 5 friends, saw an indirect effect of 1.6 songs (p<.025), an effect ~150% greater the direct effect of attending a concert. We did not see any evidence of indirect effects for friends of attendees of Hyped Artist events.\n---\nResults\n\nFollowing the Last.fm ontology, we bifurcate music artists into two categories: Hyped and Top Artists. \u2020 Hyped Artists have the largest increases in listenership, while Top Artists have the highest play counts (23). An example of a Top Artist in our dataset is Vampire Weekend, while the artist Yo-Yo Ma appears in our Hyped Artists list. For the two lists of artists, we extract live events between 2013 and 2014 documented on the Last.fm website; for each live event, we extract the list of all the Last.fm users who reported attending that event. Then we extract the attendees' basic demographic information, as well as all the songs they listened to a month before and after that live event. We also capture their entire Last.fm friends list and the listenership records of each of those friends. (This sampling strategy is a variant of \"labelled star sampling\" (24).)\nTo determine if there are differences in the characteristics of Hyped and Top Artist event attendees, we examine the summary statistics of the two sets of users. Identifying such differences helps build the case that music consumption and influence flows may be different across these two datasets.\n(Previous studies have found that fundamental demographic differences such as gender are associated with different motivations for attending live events (25).) As seen in Table 1, we find large differences in the gender composition of Hyped and Top Artist event attendees. Namely, Hyped Artists tend to attract ~54% of men to their shows, while Top Artists have a substantially more equitable gender breakdown with 49.7% of attendees reporting to be male. We should note that more than a tenth of our participants refrained from reporting their gender, so it is possible that it is not necessarily the \u2020 For detailed definitions of these and other terms used, please see Material and Methods. actual gender composition of live events that differs across Top and Hyped bands, but rather the willingness to report a particular gender. We then look at the friends-network of these attendees. As seen in Table 2, the local metrics of the two networks are relatively similar, with similar numbers of friends on average. Our sampling strategy precludes us from being able to cite any statistics about the global network structure-particularly metrics of transitivity such as betweenness (24). Figure 1 illustrates one of the sub-networks that make up our data sample: a Metallica concert in Ecuador with all the self-reported Last.fm attendees (in blue) and their non-attending friends (in red).  \n---\nDirect Effects\n\nWe first assess whether the live music event of a given artist increased listenership of the attendees of that artist's concert. \u2021 We find strong evidence of direct impacts on listenership among concert attendees of both Top and Hyped Artists. As seen in Table 3 and Figure 2   Due to Last.fm's integration with Spotify, it is possible to estimate the impact of this increase in listenership on artist revenue. \u00a7 Since Spotify states that an artist earns on average $0.006 and $0.0084 per stream (26), if we assume each play is the average of these two figures, or $0.0072, we find that a Hyped Artist earns $0.0076 per average attendee in additional revenue from a live event. Similarly, a Top Artist earns an additional $0.0081 per average attendee. It is important to note that these impacts \u00a7 Since the Last.fm song data includes not just Spotify song plays, but also users playing physical CDs and pirated material, the monetary estimates are used only to illustrate the magnitude of potential impacts. are likely much larger. As seen in Figure 2, listenership increases before and after the event. For instance, within two days of Top Artist concerts, music listenership is on average 2.01 songs per day, while at all other points in our data, it is 0.55 songs per day; for Hyped Artists, it is 1.37 songs per day within two days of a concert and only 0.30 songs per day otherwise. However, we must emphasize that pre-and post-event increases in listenership are not causally identified.\nAs an example, one event included in the dataset of Top Artist events is a Taylor Swift concert at the O2 Arena in London, which had a reported attendance of 74,740 (27). This means that if 66% ** of those attendees were Spotify listeners, this event generated an additional $401.34 from attendees' subsequent song listens.\n---\nIndirect Effects\n\nNow that we have established that attending a live event impacts listenership habits of attendees, we want to determine if the attendees then, in turn, influence their friends, who have not attended the same event. Therefore, we apply the same regression discontinuity design to all friends of the attendees. One important note: to ensure we are looking at active users, we include only those users who have listened to the artist at least once in the 2-month observation window. As seen in Table 4 and Figure 3, we find a statistically significant impact on the listenership of the non-attending friends of attendees of Top Artists, but not Hyped Artists. The indirect effect on friends of Top Artist attendees is a .060 additional song plays, or more than 5% of the direct impact on listenership. \u2020 \u2020 While this is a trivial impact on its own, it is important to emphasize that the mean Top Artist attendee has 56.8 friends. \u2021 \u2021 This means that one user's attendance translates to 3.4 more song plays on average, which translates to an increase of $0.025 per attendee. Using our earlier Taylor Swift concert example, the London concert secured an additional $1,210.40 in song streams. However, this analysis assumes only one friend attended the live event. We proceed by investigating whether the indirect impact increases as the number of friends who attended the event increases.\n** We base this estimate on recent proprietary studies. A recent Spotify study found more than 2/3rds of attendees of a Dutch festival in 2014 used Spotify (28), while Music Watch Inc. found that 56% of Internet users streamed music in 2012 and 69% in 2014 (29). \u2020 \u2020 Non-attendees listen to the artist at an average of .47 songs per day within 2 days of the event and .41 at all other points in the dataset. \u2021 \u2021 If some friends-lists are incomplete, this would bias our estimates of indirect economic effects downward. Since the data includes non-attendees who are friends with multiple attendees, we can pursue a series of subgroup analyses. The fact that we have a partially observed network would normally preclude such an analysis, but we know the full list of Last.fm attendees, so we know that there is no unobserved Last.fm friend who actually attended the event. A key limitation of our analysis is that event attendance is self-reported. It is very possible that a given individual appears to be a non-attendee in our data, but simply failed to report their event attendance. However, there is no reason why this measurement error could not go in the opposite direction, as well. Non-attendees can just as easily either incorrectly report their attendance (either as a social desirability lie or to simply demonstrate their intention of going). Future studies should investigate the prevalence of these effects to identify the direction and magnitude of the measurement error.\n---\nFigure 3: Graphical Depiction of the Regression Discontinuity Estimate of the Indirect Impact of Live Event\n\nTo determine whether the number of attendee-friends interacts with the indirect effect, we run a series of regression discontinuity analyses across non-attending users with various numbers of attendeefriends. We find that as the number of friends who attended the event increases, the influence on listenership increases monotonically. To better discern if this pattern stems from multiple attendees exerting influence on the non-attender, we perform a permutation test. Namely, we look at the discontinuity estimates across the number of attendee-friends in our actual dataset and then compare these results to a synthetic dataset where we held the friend network constant but randomly assigned that non-attending friend to a different live event date of the same artist in the same 30-day period such that no user in their friend network attended that live event. \u00a7 \u00a7 Table 5 and Figure 4 illustrate these results.\n---\nFigure 4: Graphical Depiction of Permutation Test Results \u00a7 \u00a7\n\nWe use the same bandwidth generated for the main indirect effects analysis to make comparisons across subgroups. Our analysis is robust to using a leave-one-out cross validation approach separately for each subgroup, though the results are weaker. We exclude all instances of non-attendees with more than 5 friends due to the likelihood that the non-attending friend may have simply failed to report attendance. (See for instance the cluster of attendees in the lower right corner in Figure 2.) Less than 0.2% of our sample have more than 5 friends who attended the same event.\nEven with the permutation test, we are not able to ascertain if there is a causal relationship between the number of friends and the increasing indirect effect. It is quite possible that having multiple friends attend a given live event is indicative of the level of marketing/advertisements about the event (i.e. more marketing may induce more members of a given social circle to attend an event). However, the impact of marketing is unlikely to be tied to the day of the event. Rather, marketing about a concert is much more likely to occur in the days leading up to the event, which may explain the rapid increases in listenership immediately before our discontinuities.\n---\nSubgroup Analysis\n\nAs a means of exploring further hypotheses, we conduct subgroup analysis across attendee's individual-level characteristics. Since these hypotheses were not made a priori, due to the risk of datadredging (30), we do not look at any statistical tests across groups. Rather, we hope to get a Bayesian baseline for potential differences, which should be evaluated rigorously in future studies.\nSince prior studies have shown that the motivations of live event attendees tend to differ across standard demographic characteristics such as gender (24,31), we check whether the impact of live events on music consumption differs across location and gender. *** Because there are clear differences in demographic characteristics between the attendees of Top and Hyped Artist shows, we pursue subgroup analysis separately for each batch of data. As seen in Table 6, we see few meaningful *** While users can also share their age, this data was not available on Last.fm's API at the time this analysis was conducted.\ndifferences across attendee demographics. While, there are some slight differences in the direct impact of Top Artist live events on self-identified males as compared to self-identified females, we should not make any rigorous comparisons of these statistics, as there may be large biases in gender selfreporting. If this difference is true however, this would mean that males have greater direct impacts of concert attendance. We also investigate whether the demographics of non-attending users translate to differential indirect effects (Table 7). In this case, we find that there are larger indirect impacts on non-US, non-attending users for both Hyped and Top artists. There is a slightly larger indirect effect on female non-attending friends in the Hyped Artist universe, but it is important to note that the indirect effects in the Hyped Artist universe were not significant. \n---\nDiscussion and Conclusion\n\nThis paper illustrates that even without any profits from tickets and merchandise sales at shows, there may be long-term economic benefits for touring bands. As we have shown, there are sizable, statistically significant impacts on music consumption of attending a live event. The size of the direct impact is comparable for both Top Artists and Hyped artists. However, while there are substantial indirect effects for Top Artists, there are no statistically significant effects for the up-and-coming artists. In concordance with the Black et. al study (5), we see that the rich get richer, while the average Hyped Artist struggles to expand their fanbase through touring.\nOne remarkable finding is that, if enough of your friends see a live event, there is a bigger boost in listenership of that artist than even if you yourself attended the event. This suggests that the word-ofmouth effects have the potential to be more important in indirect revenues than perhaps even direct ticket sales. One potential confound that we must be cautious of is that it is very possible that the non-attending listener may have actually attended the event. After all, if very many of a given user's friends attended the event, it may be indicative that the users themselves attended the event. Future studies should use alternative measures of attendance to assess the overall accuracy of concertattendance self-reports.\nAdditionally, it would be useful to better understand the means by which indirect effects spill over to non-attendees. Do attendees of live events actually talk to their friends about their experience at the concert or is the direct effect of listenership spilling over to non-attendee friends? In other words, the play counts of an attendee are public, so are non-attendee friends seeing the increased listenership and, in turn, listening to more of that artist themselves? Additionally, a user's attendance of a live event is also publicly available-are non-attendees seeing that their friend attended a live event by a given artist and so they listen to the artist themselves?\nFinally, future studies should also analyse just how much listenership increases across all (monetized) streaming platforms. Our results only cover those individuals who use Last.fm and have enabled music tracking on their preferred streaming service. These individuals may be systematically different from the usual concert attendee/streaming-music-listener.\n---\nMaterials and Methods\n\nThe analysis in this paper uses publicly available data from the music website Last.fm. We use three distinct types of data: (1) track listens, (2) event attendance, and (3) the Last.fm friends network.\n---\nTrack Listens Data\n\nLast.fm is a free music website with over 20 million active users (32) that keeps track of the songs played by its members. A user does not have to listen to the music directly from the Last.fm website for it to be recorded (or \"scrobbled\") in the user's track history-a user needs only to install the Audioscrobbler plugin on their music software (e.g., Itunes, Windows Media Player, etc.). The plugin keeps track of the music, as well as the time and date when the track is played-even when the user is not connected to the Internet. When the user next reconnects to the Internet, the stored data is dumped onto Last.fm's servers with date information updating retroactively. We extract this data to be used as our main outcome variable of interest.\nIn 2014, Last.fm partnered with Spotify (33), which led to the inclusion of the Audioscrobbler plugin in any Spotify installation by default, such that the user just needed to switch on Last.fm music scrobbling in their Spotify settings page.\nThe plugin and the API can be manipulated by users with programming experience to record tracks that someone has not actually listened to. We exclude cases where, in a given day, the individual appeared to have listened to a given artist for a duration that is greater than the length of a day. The average song length of contemporary popular music is 227 seconds (34) and so more than 380 songs in a day is unlikely to translate into actual listens. \u2020 \u2020 \u2020\nFor the purposes of this study, we made the a priori decision to look at a two-month time horizonwith one month of listenership data prior to the attendance of an event and one month of listenership data after the attendance of the event. Since we are unable to easily establish whether a Last.fm user is active, we apply our regression discontinuity only to users who listened to at least one song by the target artist in the 2-month observation window.\n---\nEvent Attendance Data\n\nLast.fm also serves as a platform for publicizing live events, which can be uploaded by fans, promoters, or the artists themselves. Last.fm listeners can indicate that they are going to attend or have attended an event. The event pages then retain the exact date and time of the event along with all the attendees.\nEvents prior to the existence of Last.fm can also be uploaded (e.g. the band Deep Purple have events as far back as 1968 with 8 self-reported attendees). To avoid recall bias (i.e., where users will retroactively mark that they attended only the most memorable events), we concentrated on recent live events. In 2014, we extracted live events from \"Top Artists\" and \"Hyped Artist\" that occurred January, 2013 -October, 2014. Hyped Artists correspond to bands who have the highest rate of growth while Top Artists are the most listened to artists on the site overall (23). We decided that \"hype\" is the best, easily-available proxy for \"indie\" artists who are recently popular enough to yield a large enough sample size of attendees, but are still far from the \"Top Artist\" charts. \u2020 \u2020 \u2020 All analyses are robust to the inclusion of these outliers.\nWe aimed to get comparable sample sizes of attendees in the Top Artist and the Hyped Artists datasets, so we ended up extracting the live events of 85 Top Artists and 300 Hyped Artists. (Top Artists had more documented live events and higher levels of self-reported attendance.) We exclude artists that have not actually had live events in recent years (e.g. Bob Dylan). We also insure that no artists are found both in the Hyped and Top Artist datasets. We only include events that had one eligible Hyped or Top Artist playing.\n---\n\"Friends\" Data\n\nLast.fm also serves as a music-based social network; users are able to \"friend\" other people and discuss music and artists primarily through \"shoutboxes.\" Research on these friend networks indicated that users primarily friend other users with similar musical taste (35), which makes this network particularly susceptible to indirect effects.\nWe extracted a network of friends for all attendees of events by Hyped and Top artists. (We suspect there were scraping errors for some attendees resulting in the extraction of an incomplete friends list.\nAll analyses are robust to excluding these suspect attendees.) We then extracted two months of listening history (one month before and after the date of the live event) of the corresponding artist for each friend of each attendee.\n---\nRegression Discontinuity Design\n\nIn order to assess the direct impact of live events on attendees' listenership, we use a regression discontinuity design, as is standard practice for similar phenomena (36,37). \u2021 \u2021 \u2021 We report all results using the bandwidth generated using the leave-one-out cross validation approach to minimize squared bias and variance (39). As recommended in Imbens, & Lemieux (36), we use the rectangular kernel for all analyses and verify its robustness using a triangular kernel. All analyses with the triangular kernel exhibit similar results. Since the same Last.fm user could have attended multiple events by a Top Artist in our dataset, we cluster standard errors on attendee. The results do not change meaningfully when we do not cluster the standard errors. Because each of these regression discontinuities used a slightly different bandwidth, as a robustness check, we use the Top Artist bandwidth with the Hyped Artist regression discontinuity and vice versa; the reported discontinuity impacts do not change meaningfully. \u2021 \u2021 \u2021 We use the Stata package \"rd\" coded by Austin Nichols (38).",
        "Introduction 1.Attachment and the Development of Sibling Violence and Self-Esteem\n\nAccording to Bowlby (1988), in infancy, the child begins to elaborate conscious and unconscious mental representations about himself and the external world in which he is involved. These mental representations are based on the subject's experiences with significant figures and, more specifically, on the availability and responsiveness demonstrated. Given this availability and responsiveness of primary caregivers, the child, when asked, develops a model of himself as someone deserving of affection and a model of the other as someone available to meet his needs. All these elaborated perceptions about oneself, others and the surrounding world are characterized by their ability to adapt to all life cycle phases, calling themselves dynamic internal models. These dynamic internal models are responsible for modeling the cognitions, affects and interpersonal behaviors that emerge in later relationships (Bowlby [1969(Bowlby [ ] 1982)). According to Bowlby (1988), all behaviors aimed at establishing or maintaining a close relationship with a representative person are called attachment behaviors. The secure base is characterized by its permanent availability and is only used when strictly necessary, as in distress situations (Bowlby 1988).\nIn adolescence, the so-called separation-individuation process begins, through which the young person becomes independent of his parents and extends his support network to other important people. At this stage, siblings are seen as a source of emotional support and are sought out for advice in the adversities that arise during this process (Seginer 1998).\nSoc. Sci. 2023, 12, 595. https://doi.org/10.3390/socsci12110595 https://www.mdpi.com/journal/socsci\nIt should be noted that this expansion of attachments is facilitated by the sense of security and trust provided by primary caregivers, their secure base (Fraley and Davis 1997;Hazan and Zeifman 1994;Mayseless 2004; Meeus et al. 2002;Nickerson and Nagle 2005). In turn, siblings become safe havens, and this relationship is characterized by its symmetry as they also provide and receive help (Mota and Rocha 2012).\nAlthough there are few studies on sibling attachment (Kiang and Furman 2007;Geraldes et al. 2013;Mota et al. 2017), the possibility that older siblings can become caregivers for youngest siblings, especially in stressful situations or in the absence of the primary caregiver, and even become their safe base, was first explored. In the relationship between siblings, a secure attachment is synonymous of a positive relationship, provided there is a relationship of trust characterized by reciprocity, symmetry and cooperation. However, some relationships are characterized by ambivalence, which does not mean that lasting affective bonds are not formed (Ainsworth 1989).\nAssuming that family dynamics, particularly social interactions with siblings, teach conflict resolution techniques that are later applied to peer relationships and the more diverse the sibling dyad is, the better prepared siblings are for these later relationships (Fernandes 2002;Mota et al. 2017). In a study of 374 families, Yeh and Lempers (2004) found evidence that healthy sibling relationships promote high levels of self-esteem and improved relationships with peers. Conversely, sibling dynamics may involve negativity, as this is often associated with conflict situations.\nSibling violence is often overlooked, resulting in a shortage of the literature on the subject (Linares 2006). Some authors use the term bullying interchangeably with sibling violence when defining this type of aggression (Monks et al. 2009;Toseeb and Wolke 2022;Wolke et al. 2015). Sibling violence is often characterized by a power imbalance between the victim and the aggressor and it is uncommon for siblings to have identical physical and/or psychological characteristics, such as age, height and physical strength (Monks et al. 2009). Moreover, according to Hoffman and Edwards (2004), the aggressors in sibling dynamics may analyze each other's characteristics to identify strengths and weaknesses, using this information to exert superiority over the victim before executing violent behaviors. Additionally, the lack of parental or adult supervision allows abusers to carry out their aggression at their convenience due to the extensive amount of time siblings spend together (Monks et al. 2009), as well as harsh parenting behavior (Tippett and Wolker 2015) or the use of the aggressive discipline (Relva et al. 2019). However, Krienert and Walsh (2011) emphasize that roles can be interchangeable in sibling violence and the aggressors may also be victims of violence between siblings.\nSibling violence can take the form of physical, psychological, sexual or relational violence (Relva et al. 2012). According to Wiehe (1997), physical violence is a deliberate act involving physically hurting someone with or without using an object. Psychological violence occurs when an aggressor uses intimidating language to scare, ridicule or belittle a sibling. There is still no consensus on the distinction between normal and abusive sexual contact (Relva et al. 2012). For Wiehe (1997), abusive sexual contact is considered inappropriate and can involve inappropriate touching, attempted penetration and using violence to force someone to have sex. Relational violence is a form of social violence that includes spreading negative rumors and publishing sensitive information online. In relational violence, the victim may not be aware of the aggression and the indirect nature of the perpetrator's actions makes it easy for them to deny involvement (Caspi 2012), but with probable consequences for the adolescents' adjustment (Gallagher et al. 2018).\nSeveral studies on sibling violence (e.g., Khan and Rogers 2015;Kiselica and Morrill-Richards 2007;Relva et al. 2014;Relva et al. 2013;Simonelli et al. 2002) investigate the traits of both perpetrators and victims. This study only considers sex, social class and position in the sibling dyad. Sex stereotypes predetermined by society, as noted by Kiselica and Morrill-Richards (2007) and Seixas (2009), are relevant here. Males are expected to be aggressive, while females are expected to show subtlety, fragility, and greater concern for others, as found by several authors (Kiselica and Morrill-Richards 2007;Kindlon and Thompson 2000;Sim\u00f5es et al. 2015). According to Rubia (2007), these characteristics present in females also seem to be due to their innate capacity for greater and faster aptitude in the development of verbal skills. Thus, according to recent empirical conceptions, males mostly assume the role of aggressor in sibling relationships (Dantchev and Wolke 2019;Kiselica and Morrill-Richards 2007;Relva et al. 2014;Relva et al. 2013). There is no consensus regarding the sex that faces the most victimization; however, females are the most affected in terms of experiencing psychological aggression (Khan and Rogers 2015;Relva et al. 2013;Simonelli et al. 2002). Other factors can contribute to this. A recent study conducted by Walters et al. (2020) intends to explore a mediational model of the effects and consequences of sibling victimization in a sample of 355 adolescents (165 females) with ages ranging between 10 and 15 years. According to the authors, the results indicated a negative association between parental monitoring and sibling victimization in girls. Regarding males, they predominantly perpetrate acts of aggression that result in serious physical harm (Relva et al. 2013;Relva et al. 2014).\nRegarding social class, Hoffman and Edwards (2004) recognize this analysis as a gap in the literature because the study samples mostly include elements of the average socioeconomic level. However, Green (1984) found an effect of financial resources on the severity of the aggressions perpetrated and victimized by siblings and this severity increased when economic conditions worsened. Also, Tippett and Wolker (2015), in a sample of 4237 participants (aged between 10 and 15), intended to identify factors associated with sibling violence, namely the socioeconomic background. The authors found that sibling victimization was related to families who experienced poverty or financial difficulties. However, the results regarding social class are inconsistent. In a large sample of 6838 children from the Avon Longitudinal Study of Parents and Children, a prospective United Kingdom birth-cohort, Dantchev and Wolke (2019) found that social class was not associated with sibling bullying, suggesting that \"social conditions matter less or not at all\" (p. 1068). Menesini et al. (2010) state that since the early 1980s, special importance has been attached to the relationship between siblings in their healthy development. Despite growing up and developing in the same family context, siblings may experience different practices, as well as experience differentiated treatment by the primary attachment figures (Dunn and Plomin 1990).\nSeveral studies focus on the position occupied by siblings in their sibling dyad (e.g., Bowes et al. 2014;Menesini et al. 2010). Menesini et al. (2010), by aiming to examine individual and relational factors in the function of violence between siblings, as well as its relationship with bullying in the school context, with a sample of 195 subjects aged between 10 and 12 years, found that victimization occurs more frequently in individuals with an older sibling. In addition, the authors also found that older and male siblings engage more in sibling violent behavior when the youngest sibling exists. They also found that females give more importance to the quality of the relationship established between siblings rather than conceiving importance to their birth order. Finkelhor et al. (2006) point in the same direction in a sample of 2030 children and adolescents aged between two and 17 years of age, as they found that compared to the rest, the older siblings attack the most. According to Fernandes (2000Fernandes ( , 2005) ) and Fernandes et al. (2007), there are characteristics common to siblings who occupy the same fraternal position. In this way, the older siblings are seen as the leaders and those to whom the beliefs and values of the parents are bequeathed are seen as the most responsible. The younger brothers are free from any burden, they are considered the most fragile and, therefore, the most protected. The middle brothers are characterized by the poorly defined role they experience in their siblinghood, incessantly searching for the position they should occupy, which gives them particularities such as low self-esteem and aggressiveness, revealing themselves to be quite centered in themselves, evidencing a scant concern for others.\nRegarding the association between sibling relationships and self-esteem, sibling relationships as a safe haven can be associated with self-esteem in adolescents (Mota and Matos 2015;Mota et al. 2017). Self-esteem is the subject's appreciation of him/herself, considering his/her attributes and virtues (Serra 1988). In fact, the positive sibling relationship is associated with lower levels of internalizing symptoms in adolescents during stressful events (Waite et al. 2011). However, maintaining the relationship between siblings is not always positive. Relationships between siblings can be damaging when primary care or the lack of it have not made it possible to organize young people's emotional experiences within a framework of emotional stability (Mota 2021). In these cases, sibling separation can be beneficial, protecting them from excessive rivalry, blame, abuse and/or violence in the relationship (Mota 2021). According to the literature, self-esteem exerts an effect on the involvement in aggressive behaviors between siblings since siblings involved in this dynamic evidence the presence of low levels of self-esteem (Avanci et al. 2007;Wiehe 1997). However, the results are not consistent. Laopratai et al. (2023), in a study that aims to examine sibling bullying and its association with self-esteem and depression during the pandemic in a sample of 352 participants (30.4% female) where 92 (26.1%) were victims and 49 (13.9%) were bullies of sibling bullying, did not observe an association between sibling bullying and low self-esteem. Possibly other variables may be associated, such as the context or individual aspects of young people.\nAccording to Rosenberg et al. (1995), global self-esteem is related to psychological well-being, while specific self-esteem is related to the behavioral area. This behavioral scope is associated with the subject's feelings in relation to a specific characteristic of the self, such as academic or sexual self-esteem. In this sense, Gentile et al. (2009), in a meta-analysis in which they wanted to verify the differences by sex according to 10 specific domains of self-esteem using 115 studies, were able to verify that when assessing overall self-esteem, there are fewer significant differences found about sex, which does not occur for specific self-esteem. However, this conception is not consensual since there is literature that attributes males to higher levels of self-esteem (Quatman and Watson 2001). Also, Ruiz et al. (2009), when analyzing the relationship between social reputation, bullying, psychosocial adjustment, loneliness, self-esteem and satisfaction with life in 1319 adolescents aged between 11 and 16 years, found that it was not possible to ascertain significant differences in variable self-esteem between females and males. Although this conclusion regarding family socioeconomic differences is not straightforward, it would also be expected that young people with higher socio-economic levels would have access to greater support and consistent care, promoting greater self-esteem. Esp\u00ednola (2010) supports this idea as, in a sample of 593 students aged between 9 and 13 years, they found that adolescents belonging to the middle social class show higher rates of self-esteem when compared to adolescents belonging to a lower social class. However, we know that many other personal and particularly relational factors are inherent in a socio-economic status and its relationship with young people's self-esteem, so we will try to fill in some of the gaps in the literature.\n---\nObjectives and Hypotheses\n\nThe main objective of this study is to analyze the role of self-esteem, position in the sibling dyad and sex in the development of sibling violence behaviors in adolescents (both from a victimization and perpetration perspective) (see Figure 1). First, we intend to analyze the associations between self-esteem and sibling violence. In this follow-up, the differences in self-esteem and violence between siblings will be analyzed according to such sociodemographic variables as sex, social class and position in the sibling dyad.\nAccording to the objectives, self-esteem is expected to correlate positively with the negotiation dimension and negatively with psychological aggression, physical aggression, sexual coercion and injury dimensions, also related to violence between siblings. It is also expected that self-esteem and violence between siblings present statistically significant differences in relation to sex, social class and position in the sibling dyad. Regarding self-esteem, it is expected that males present higher rates of it, as well as it is expected that respondents of a low social class have lower self-esteem. Regarding position in the sibling dyad, the middle sibling is expected to be the holder of a less positive appreciation of himself. Thus, it is expected that there is a greater involvement of females in negotiation behaviors, as well as in indirect aggressions. As for social class, a greater involvement in aggressive conduct between siblings by elements of lower social classes is expected. Regarding the position occupied in the sibling dyad, due to his low self-esteem and characteristic aggressiveness, the greater involvement of the middle brother in violent acts is observed. Finally, it is also expected that the negotiation dimension will be positively predicted by self-esteem, just as a negative prediction of self-esteem is expected in the face of the effectiveness of aggressive behaviors between siblings. Regarding the position in the sibling dyad, the greater involvement of the middle sibling in aggressive behaviors that aim at the resolution of conflicts between siblings, as well as the greater involvement of females in the negotiation technique and males in the effectiveness of aggressive behaviors of the direct type are observed. According to the objectives, self-esteem is expected to correlate positively with the negotiation dimension and negatively with psychological aggression, physical aggression, sexual coercion and injury dimensions, also related to violence between siblings. It is also expected that self-esteem and violence between siblings present statistically significant differences in relation to sex, social class and position in the sibling dyad. Regarding self-esteem, it is expected that males present higher rates of it, as well as it is expected that respondents of a low social class have lower self-esteem. Regarding position in the sibling dyad, the middle sibling is expected to be the holder of a less positive appreciation of himself. Thus, it is expected that there is a greater involvement of females in negotiation behaviors, as well as in indirect aggressions. As for social class, a greater involvement in aggressive conduct between siblings by elements of lower social classes is expected. Regarding the position occupied in the sibling dyad, due to his low self-esteem and characteristic aggressiveness, the greater involvement of the middle brother in violent acts is observed. Finally, it is also expected that the negotiation dimension will be positively predicted by self-esteem, just as a negative prediction of self-esteem is expected in the face of the effectiveness of aggressive behaviors between siblings. Regarding the position in the sibling dyad, the greater involvement of the middle sibling in aggressive behaviors that aim at the resolution of conflicts between siblings, as well as the greater involvement of females in the negotiation technique and males in the effectiveness of aggressive behaviors of the direct type are observed.\n---\nMaterials and Methods\n\n\n---\nSample\n\nIn the present study, 286 adolescents aged between 12 and 17 years participated (M = 13.55; SD = 1.12), of which 115 (40.2%) are male and 171 (59.8%) females. Regarding schooling (M = 8.11; SD = 0.82), 83 participants (29%) attend the 7th grade, 89 (31.1%) attend the 8th grade and 114 (39.9%) the 9th grade. This sample includes 163 (57%) individuals who occupy the position of youngest sibling in the sibling dyad, 35 (12.2%) are middle siblings and 88 (30.8%) are older siblings. \n---\nMaterials and Methods\n\n\n---\nSample\n\nIn the present study, 286 adolescents aged between 12 and 17 years participated (M = 13.55; SD = 1.12), of which 115 (40.2%) are male and 171 (59.8%) females. Regarding schooling (M = 8.11; SD = 0.82), 83 participants (29%) attend the 7th grade, 89 (31.1%) attend the 8th grade and 114 (39.9%) the 9th grade. This sample includes 163 (57%) individuals who occupy the position of youngest sibling in the sibling dyad, 35 (12.2%) are middle siblings and 88 (30.8%) are older siblings.\n---\nInstruments\n\nSociodemographic questionnaire-questionnaire designed for the appropriate purposes, which included variables considered pertinent to the investigation. It allowed access to information regarding the student (e.g., age, schooling), his/her parents (e.g., age, education, socioeconomic level, marital status) and his/her sibling dyad (e.g., number of siblings, age).\nThe Revised Conflict Tactics Scales (sibling Portuguese version)-CTS2-SP (Straus et al. 1996;adapted by Relva et al. 2013). This self-report questionnaire is addressed to subjects with one or more siblings, questioning them about potential aggressive behaviors experienced within their siblings. This scale consists of 76 items, subdivided equally by two subscales: victimization and perpetration. Each of these subscales consists of five dimensions: negotiation with 6 items (e.g., \"I showed that I cared about this brother/sister, even if we disagreed\"), psychological aggression with 7 items (e.g., \"I insulted or swore at this brother/sister\"), physical aggression with 12 items (e.g., \"I threw something at this brother/sister that could hurt him\"), sexual coercion with 7 items (e.g., \"I made this brother/sister have sex without a condom\") and injury with 6 items (e.g., \"This brother/sister had a fracture due to a fight with me\"). These items appear paired, that is, first questioning whether the individual was the author of a given behavior and later questioning whether he was a victim of the same type of behavior. The response options of this scale vary between 1 (once in the last year), 2 (twice in the last year), 3 (three to five times in the last year), 4 (six to ten times in the last year), 5 (eleven to twenty times in the last year), 6 (more than twenty times in the last year), 7 (not in that year, but it happened before) and 8 (it never happened) and it is, therefore, an 8-point Likert scale, corresponding to the frequency with which the behavior happened in a given period of time. It should be noted that for the present investigation, it was decided to recode points 7 and 8 in 0 because they were not related to the last year, which was not relevant to the current investigation. The internal consistency analysis for the present research showed Cronbach's alpha values of 0.78 for perpetration and 0.77 for victimization. Regarding the constituent dimensions of the scale of perpetration and victimization, Cronbach's alpha values of 0.79/0.84 for negotiation, 0.87/0.79 for psychological aggression, 0.84/0.88 for physical aggression, 0.67/0.58 for sexual coercion and 0.55/0.50 for injury were met, respectively. Through the confirmatory factor analyses, it was verified that the questionnaire, for the perpetration and victimization scales presents adequate adjustment indexes: Perpetration-SRMR = 0.07, CFI = 0.92, RMSEA = 0.09, \u03c7 2 (75) = 246.65, p < 0.001, \u03c7 2 /gl = 3.29-and victimization-SRMR = 0.06, CFI = 0.93, RMSEA = 0.08, \u03c7 2 (75) = 227.60, p < 0.001, \u03c7 2 /gl = 3.03. According to the authors, the Cronbach's alphas ranged, for the scale of perpetration/victimization, between 0.65 and 0.80/0.66 and 0.84, respectively (negotiation = 0.79/0.77, psychological aggression = 0.76/0.75, physical assault = 0.80/0.80, sexual coercion = 0.77/0.84 and injury = 0.65/0.66) (Relva et al. 2013).\nRosenberg Self-Esteem Scale-RSE (Rosenberg 1965; adapted by Rocha andMatos 2003, cited in Rocha 2008). This questionnaire, characterized by being a self-report instrument, aims to assess global personal self-esteem. Consisting of 10 items, five are positively worded (e.g., \"I feel I have some good qualities\") and the rest are negatively worded (e.g., \"Sometimes I feel like I don't pay attention\"), and the latter are quoted in reverse (items 3, 5, 8, 9 and 10). Originally, the response scale is of the six-point Likert type (from strongly agree to strongly disagree). In the present study, it was adapted aiming at standardization and simplifying the entire protocol's response. This thus appears in four points, namely: 1 (Strongly disagree), 2 (Disagree), 3 (Agree) and 4 (Strongly agree). The internal consistency analysis for the present research showed Cronbach's alpha values of 0.84, which is like the one found by the author (0.85) of which the adaptation was used (Rocha 2008). Through the confirmatory factor analysis, it was verified that the questionnaire presents adequate adjustment indexes: SRMR = 0.05, CFI = 0.98, RMSEA = 0.06, \u03c7 2 (32) = 66.104, p < 0.001, \u03c7 2 /gl = 2.07.\n---\nProcedures\n\nThe procedures in the research followed the General Regulation on Data Protection of the European Union and the Code of Ethics and Deontology of the Portuguese Psychologists Association for Research.\nThe present study sample was collected from middle schools in the northern zone of Portugal. The requests for authorization were sent to these establishments through letter and e-mail. When obtaining these authorizations, meetings were scheduled with the school directors to present the protocol and clarify possible doubts. It requested the informed consent of parents. The application was carried out with the collaboration of educational institutions and the standard instructions for applying the research protocol were given to the class teacher. Data collection occurred in the classroom in the presence of the supervising researcher. Students were informed that information was confidential and their participation in the study was voluntary.\n---\nData Analysis Strategies\n\nThe present research assumes a cross-sectional character since all the data were collected only in a single moment, so there was no follow-up of the sample subjects during a time interval. Data processing was performed using SPSS (Statistical Package for Social Sciences) version 28 for Windows. This process began with a database cleanup intended to identify and exclude missing data and possible outliers. The Mahalanobis distance analysis was used to identify multivariate outliers, since this allows the use of the means and variances of the variables, through which it is possible to identify subjects who present inadequate values (values that deviate significantly from the mean) (Field 2005).\nSubsequently, normality was tested by analyzing the values of skewness and kurtosis, complementing this process with several statistical analyses that aim to inform about the distribution of the data: Kolmogorov-Smirnov test, histogram graphs, Q-QPlots, Scatterplots and Boxplots (Mar\u00f4co 2007;Pallant 2020). The sample admitted all the assumptions of normality (-1 and 1). We performed 1st order Confirmatory Analyses of the instruments. Finally, correlation analyses (Pearson's correlations), differential analyses (student t-test, ANOVA and MANOVA) and prediction analyses (hierarchical multiple regression) were performed. The correlational analyses considered reference values stipulated by Cohen (1988), indicating correlation magnitudes between 0.10 and 0.29 as low, from 0.30 to 0.49 as moderate and values above 0.50 as corresponding to strong associations.\nThe same author's reference values for analyses of variance were also taken into account, considering the size of the magnitude of the effect through eta squared with values: 0.01 small effect, 0.06 moderate effect and 0.14 large effect (Cohen 1988).\n---\nResults\n\nCorrelational analyses were used to analyze the associations between self-esteem and sibling violence (perpetration and victimization). The results of the inter-scale correlations and the respective means and standard deviations are presented in Table 1. Regarding the association between self-esteem and the perpetration of sibling violence, there is a positive and significant correlation of a low magnitude for negotiation (r = 0.21, p < 0.001) and negative and significant correlations of a low magnitude for psychological aggression (r = -0.19, p < 0.001), for physical assault (r = -0.18, p < 0.001), for sexual coercion (r = -0.15, p < 0.05) and for injury (r = -0.18, p < 0.001).\nRegarding the association between self-esteem and the victimization of sibling violence, there is a positive and significant correlation of a low magnitude of negotiation (r = 0.24, p < 0.001), with psychological aggression (r = -0.19, p < 0.001), physical assault (r = -0.18, p < 0.001), and injury (r = -0.21, p < 0.001) which is correlated negatively and significantly with a low magnitude. Sexual coercion is the only dimension that is not statistically significantly correlated.\n3.1. Variance of Self-Esteem and Sibling Violence According to Sex, Social Class and Position in the Sibling Dyad\nAiming at analyzing self-esteem and sibling violence as a function of sociodemographic variables (sex, social class and position in the sibling dyad), we proceeded with univariate and multivariate differential analyses of variance (ANOVA and MANOVA), as well a Student's t-test.\nTo explore differences for the variable self-esteem as a function of sex, a student's t-test was used and the results suggest the absence of significant differences t (284) = 0.90; p = 0.37; IC 95% [-0.08; 0.20] \u03b7 2 = 0.04.\nRegarding the perpetration of sibling violence, significant differences are found in relation to sex for the dimensions of negotiation t (284) = -2.05; p = 0.04; IC 95% [-0.81;-0.02] \u03b7 2 = 0.20, psychological aggression t (284) = -2.02; p = 0.05; IC 95% [-0.57;-0.01] \u03b7 2 = 0.30 and sexual coercion t (152.52) = 2.58; p = 0.01; IC 95% [0.02;0.18] \u03b7 2 = 0.50. Thus, it is verified that females use more negotiation (M = 3.20, SD = 1.59), but also psychological aggression (M = 1.21, SD = 1.25), while males more often perform acts of sexual coercion (M = 0.14, SD = 0.39) (Table 2). Finally, physical assault t (284) = -0.22; p = 0.83; IC 95% [-0.17;0.14] \u03b7 2 = 0.05 and injury t (214.59) = 1.31; p = 0.19; IC 95% [-0.03;0.16] \u03b7 2 = 0.02 do not present significant differences regarding sex (Table 2). Regarding the victimization of sibling violence according to sex, there are significant differences for psychological aggression t (269.90) = -2.21; p = 0.03; IC 95% [-0.58;-0.03] \u03b7 2 = 0.40 and injury t (182.86) = 2.05; p = 0.04; IC 95% [0.00;0.22] \u03b7 2 = 0.60. Thus, it is concluded that females are the ones who suffer more psychological aggression (M = 1.14, SD = 1.26), while males are the ones who suffer the most acts of injury (M = 0.24, SD = 0.52) (Table 2). The dimensions of negotiation t (284) = -1.54; p = 0.13; IC 95% [-0.70;0.09] \u03b7 2 = 0.02, physical assault t (284) = 0.52; p = 0.60; IC 95% [-0.12;0.21] \u03b7 2 = 0.07 and sexual coercion t (284) = 0.74; p = 0.46; IC 95% [-0.04;0.22] \u03b7 2 = 0.07 do not demonstrate statistically significant differences regarding sex (Table 2).\nSocial class was classified according to the average monthly income of the parents. This categorization was made in three groups: low (up to and including the minimum wage), medium (from the minimum wage to 1000 euros) and high (more than 1000 euros per month). This division was based on the five-level classification on the Graffar household income scale. In the present study, these five levels were adapted into only three (low, medium and high) to simplify and be better understood by the respondents.\nA univariate differential analysis (ANOVA) was carried out to investigate self-esteem as a function of social class. The results suggest that there are no significant differences: F (2, 221) = 2.13, p = 0.12.\nRegarding sibling violence in relation to social class, there are no significant differences in both the perpetration F (10, 436) = 0.99, p = 0.45, \u03b7 2 = 0.53 or victimization subscales F (10, 436) = 0.63, p = 0.79, \u03b7 2 = 0.33.\nBy analyzing self-esteem as a function of position in the sibling dyad, it is evident that there are no significant differences F (2, 283) = 0.29, p = 0.75.\nRegarding the subscale perpetration of sibling violence, no statistically significant differences were found F (10, 560) = 1.36, p = 0.20, \u03b7 2 = 0.70 faced with the position in the sibling dyad.\nRegarding the subscale victimization of sibling violence F (10, 560) = 2.71, p = 0.003, \u03b7 2 = 0.97, there are significant differences in the position in the sibling dyad only for the dimension of physical assault F (2, 283) = 5.42, p = 0.005, \u03b7 2 = 0.84. Middle siblings (M = 0.72, SD = 0.98) experience more physical assault (victimization subscale) when compared to youngest sibling (M = 0.40, SD = 0.70) and older sibling (M = 0.28, SD = 0.45) (Table 3). Hierarchical multiple regression analyses were performed to ensure the response to the present study's objectives.\nIn the performance of each hierarchical multiple regression, the dimensions of violence between siblings were analyzed separately. All analyses introduced the following three blocks: sex, position in the sibling dyad and self-esteem. Note that the variables sex and position in the sibling dyad were dummy coded to ensure the analysis of sex (0-male, 1-female) and position in the sibling dyad (1-Youngest sibling; 2-Middle sibling; 3-Eldest sibling).\nIn the hierarchical multiple regression analysis regarding the negotiation (perpetration), sex makes a significant contribution, F (1, 284) = 4.21, p = 0.041, explains 2% of the total variance (R 2 = 0.02) and contributes individually to 1% of the variance for the model (R 2 change = 0.01). The position in the sibling dyad makes a significant contribution, F (3, 282) = 3.02, p = 0.03, explains 3% of variance (R 2 = 0.03) and contributes individually to 2% of the variance for the model (R 2 change = 0.02). Self-esteem makes a significant contribution, F (4, 281) = 5.96, p = 0.000, explains 8% of variance (R 2 = 0.08) and contributes individually to 7% of the variance for the model (R 2 change = 0.07).\nBy analyzing the individual contribution of each independent variable, it is observed that three have a significant contribution (p < 0.05) as an effect of negotiation (perpetration), presenting itself according to its importance: self-esteem (\u03b2 = 0.22), position in sibling dyad-youngest (\u03b2 = -0.20)-and females (\u03b2 = 0.13) (Table 4).\nIn the hierarchical multiple regression analysis regarding psychological aggression (perpetration), sex makes a significant contribution, F (1, 284) = 4.06, p = 0.045, explains 1% of variance (R 2 = 0.01) and contributes individually to 1% of the variance for the model (R 2 change = 0.01). The position in the sibling dyad makes a significant contribution, F (3, 282) = 3.13, p = 0.026, explains 3% of variance (R 2 = 0.03) and individually contributes 2% to the model (R 2 change = 0.02). No que concerns self-esteem as it makes a significant contribution F (4, 281) = 4.76, p = 0.001, explains 6% of variance (R 2 = 0.06) and individually contributes 5% to the model (R 2 change = 0.05). Analyzing the individual contribution of each independent variable, it is observed that two make a significant contribution (p < 0.05) as an effect of psychological aggression (perpetration), presenting itself according to its importance: position in the sibling dyadolder sibling (\u03b2 = -0.20)-and self-esteem (\u03b2 = -0.18) (Table 4).\nIn the hierarchical multiple regression analysis regarding physical assault (perpetration), sex does not make a significant contribution, F (1, 284) = 0.05, p = 0.827, does not explain the variance (R 2 = 0.00) and does not present an individual contribution to the variance of the model (R 2 change = 0.00). The position in the sibling dyad does not make a significant contribution, F (3, 282) = 0.89, p = 0.448, explains 1% of variance (R 2 = 0.01) and does not contribute individually to the template (R 2 change = 0.00). Self-esteem makes a significant contribution, F (4, 281) = 3.04, p = 0.018, explains 4% of variance (R 2 = 0.04) and contributes individually to 3% to the model (R 2 change = 0.03).\nBy analyzing the individual contribution of the independent variable, it is observed that self-esteem (\u03b2 = -0.18) has a significant contribution (p < 0.05) associated with physical assault (perpetration) (Table 4).\nIn the hierarchical multiple regression analysis regarding sexual coercion (perpetration) sex makes a significant contribution, F (1, 284) = 8.46, p = 0.004, explains 3% of variance (R 2 = 0.03) and contributes individually to 3% of the variance for the model (R 2 change = 0.03). The position in the sibling dyad makes a significant contribution F (3, 282) = 3.12, p = 0.027, explains 3% of variance (R 2 = 0.03) and contributes individually to 2% of the model (R 2 change = 0.02). Self-esteem also makes a significant contribution, F (4, 281) = 4.12, p = 0.003, explains 6% of variance (R 2 = 0.06) and contributes individually to 4% of the model (R 2 change = 0.04).\nBy analyzing the individual contribution of each independent variable, it is observed that two make a significant contribution (p < 0.05) as the effect of sexual coercion (perpetration), males (\u03b2 = -0.17) and self-esteem (\u03b2 = -0.15) (Table 4).\nIn the hierarchical multiple regression analysis regarding injury (perpetration), sex does not make a significant contribution, F (1, 284) = 1.085, p = 0.175, explains 1% of the variance (R 2 = 0.01) and does not present an individual contribution to the variance of the model (R 2 change = 0.00). The position in the sibling dyad does not make a significant contribution, F (3, 282) = 1.37, p = 0.253, explains 1% of variance (R 2 = 0.01) and does not contribute individually to the template (R 2 change = 0.00). Regarding self-esteem, it makes a significant contribution, F (4, 281) = 3.46, p = 0.009, explains 5% of variance (R 2 = 0.05) and contributes, individually, 3% to the model (R 2 change = 0.03).\nBy analyzing the individual contribution of each independent variable, it is observed that only self-esteem (\u03b2 = -0.18) makes a significant contribution (p < 0.05) as the effect of injury (perpetration) (Table 4).\nIn the hierarchical multiple regression analysis regarding negotiation (victimization) sex (dummy) does not make a significant contribution, F (1, 284) = 2.37, p = 0.125, explains 1% of the total variance (R 2 = 0.01) and contributes individually to 1% of the variance for the model (R 2 change = 0.01). Regarding the position in the sibling dyad (dummy), it does not make a significant contribution, F (3, 282) = 1.69, p = 0.170, explains 1% of variance (R 2 = 0.01) and contributes individually to 1% of the variance for the model (R 2 change = 0.01). With regard to self-esteem, it makes a significant contribution, F (4, 281) = 5.75, p = 0.000, explains 6% of variance (R 2 = 0.06) and contributes individually to 6% of the variance for the model (R 2 change = 0.06).\nBy analyzing the individual contribution of each independent variable, it is observed that self-esteem (\u03b2 = 0.24) makes a significant contribution (p < 0.05) as the effect of negotiation (Table 5).\nIn the hierarchical multiple regression analysis of psychological aggression (victimization), sex makes a significant contribution, F (1, 284) = 4.57, p = 0.033, explains 2% of the variance (R 2 = 0.02) and contributes individually to 1% of the variance for the model (R 2 change = 0.01). The position in the sibling dyad makes a significant contribution, F (3, 282) = 2.98, p = 0.0232, explains 3% of the variance (R 2 = 0.03) and individually contributes 2% to the model (R 2 change = 0.02). Regarding self-esteem, it makes a significant contribution, F (4, 281) = 4.57, p = 0.001, explains 6% of the variance (R 2 = 0.06) and contributes individually to 5% of the model (R 2 change = 0.05).\nBy analyzing the individual contribution of each independent variable, it is observed that two make a significant contribution (p < 0.05) as the effect of psychological aggression (victimization), with both presenting the same importance: position in the sibling dyadeldest sibling (\u03b2 = -0.18)-and self-esteem (\u03b2 = -0.18) (Table 5). In the hierarchical multiple regression analysis of physical assault (victimization), sex does not make a significant contribution, F (1, 284) = 0.27, p = 0.602, does not explain the variance (R 2 = 0.00) and does not present an individual contribution to the variance of the model (R 2 change = 0.00). Regarding the position in the sibling dyad, it makes a significant contribution, F (3, 282) = 3.73, p = 0.012, explains 4% of variance (R 2 = 0.04) and contributes individually to 3% of the model (R 2 change = 0.03). Regarding self-esteem, it makes a significant contribution, F (4, 281) = 5.08, p = 0.001, explains 7% of variance (R 2 = 0.07) and contributes individually to 5% of the model (R 2 change = 0.05).\nBy analyzing the individual contribution of each independent variable, it is observed that three make a significant contribution (p < 0.05) as the effect of physical assault (victimization), presenting themselves according to their importance: position in the sibling dyadyoungest sibling (\u03b2 = -0.29)-position in the sibling dyad-Eldest sibling (\u03b2 = -0.24)-and self-esteem (\u03b2 = -0.17) (Table 5).\nIn the hierarchical multiple regression analysis regarding sexual coercion (victimization), sex does not make a significant contribution, F (1, 284) = 0.55, p = 0.459, does not explain the variance (R 2 = 0.00) and does not individually contribute variance to the model (R 2 change = 0.003). With regard to the position in the sibling dyad, it does not make a significant contribution, F (3, 282) = 0.37, p = 0.772, does not explain the variance (R 2 = 0.00) and contributes individually to -1% of the model (R 2 change = -0.01). Regarding self-esteem, it makes no significant contribution, F (4, 281) = 0.31, p = 0.871, does not explain the variance (R 2 = 0.00) and contributes individually to -1% of the model (R 2 change = -0.01).\nBy analyzing the individual contribution of each independent variable, it is observed that none make a significant contribution (p < 0.05) as an effect of sexual coercion (victimization) (Table 5).\nIn the hierarchical multiple regression analysis regarding injury (victimization), sex makes a significant contribution, F (1, 284) = 4.85, p = 0.028, explains 2% of variance (R 2 = 0.02) and contributes individually to 1% for model variance (R 2 change = 0.01). Regarding the position in the sibling dyad, it does not make a significant contribution, F (3, 282) = 2.22, p = 0.086, explains 2% of variance (R 2 = 0.02) and individually contributes 1% to the model (R 2 change = 0.01). Regarding self-esteem, it makes a significant contribution F (4, 281) = 5.08, p = 0.001, explains 7% of variance (R 2 = 0.07) and individually contributes 5% to the model (R 2 change = 0.05).\nAnalyzing the individual contribution of each independent variable, it is observed that two have a significant contribution (p < 0.05) as the effect of injury (victimization), presenting itself according to its importance: self-esteem (\u03b2 = -0.21) and males (\u03b2 = -0.14) (Table 5).\n---\nDiscussion\n\nThe main objective of the present study was to test the effect of sex, position in the sibling dyad and self-esteem in the development of behaviors of violence between siblings. The results suggest that self-esteem is positively associated with negotiation on the perpetration and victimization of sibling violence. On the contrary, it is negatively associated with psychological aggression, physical assault and injury, also concerning the perpetration and victimization of sibling violence. Only about sexual coercion is selfesteem negatively associated with perpetration and there is no significant relationship with victimization. Thus, self-esteem seems to play an important role in the dynamics of violence between siblings. The presence of a positive perception of oneself (Serra 1988) seems to favor the establishment of dialogues between brothers and, consequently, to benefit the capacity to resolve conflicts that may arise between them. Self-esteem it also seems to be associated with aggressive conduct between siblings playing a protective role. When there is high self-esteem, there is a lower frequency of aggressive acts perpetrated and suffered by siblings. Knowing that self-esteem consists in the appreciation that the subject makes of himself, contemplating his attributes and qualities, having himself as capable or incapable of successfully executing what he proposes (Serra 1988), an individual with high self-esteem, demonstrating high levels of happiness and satisfaction with himself, will not have the need to develop any rivalry, which assumes a violent character, towards any of his siblings (Mota et al. 2017). The results of the present study are in accordance with previous studies. Yeh and Lempers (2004), in a sample of 374 families, found that a healthy relationship between siblings was predictive of high self-esteem because, by relating positively and adjusted, they developed positive feelings and perceptions about themselves. Also, in their research, Avanci et al. (2007) and Wiehe (1997) realized that low self-esteem characterizes siblings involved in violent behavior. Avanci et al. (2007), in a sample of 266 students aged between 11 and 19 years, also found that adolescents with high self-esteem engage less in victimizing behaviors in sibling violence. Wiehe (1997), in addition to verifying this assumption, also found that brother aggressors in their dyad have low levels of self-esteem.\nRegarding self-esteem, there were no significant differences according to sex. It is suggested that the results obtained in the present research are attributed to the fact of evaluating global self-esteem and not considering the various specificities of it (e.g., sexual, physical and academic) because an adolescent, in general, may not evaluate himself positively; however, he can be considered effective in some specificities (e.g., academic self-esteem). The study of Gentile et al. (2009) points in the same direction. The authors consider that to obtain significant results in this analysis and properly examining selfesteem, the specific self-esteem should always be considered. Although the results of the present investigation do not generate consensus in the literature (e.g., Quatman and Watson 2001;Ruiz et al. 2009), they are corroborated by Ruiz et al. (2009) in a sample of 1319 adolescents aged between 11 and 16 years and they, in their study, also verified the absence of significant differences in self-esteem in relation to sex.\nRegarding negotiation, psychological aggression and sexual coercion in the perpetration of violence between siblings, there were significant differences according to sex. Female subjects showed a greater predisposition to establish conversations with their siblings and the elements most resort to psychological aggressions. Therefore, males are the one who most use sexual assaults. In view of these results, it was expected that the female was more involved in negotiation behaviors. This can be explained based on empirical conceptions about sex differences. According to Rubia (2007), females, when compared to males, seem to have a greater capacity in relation to the performance of verbal skills. In addition, it is also known that the development of these same communicative skills progresses later in males, unlike females, who previously acquire a greater maturation of social skills (Legato 2009). In the follow-up, sex stereotypes created by society also seem to influence the greater involvement of females in negotiation behaviors. From its birth, the baby is exposed to certain behaviors taken to the detriment of its sex, according to Seixas (2009). From an early age, parents show different behaviors towards babies based on their sex. When faced with a female baby, parents tend to show more concern for others and engage in longer conversations than a male baby. This behavior is believed to contribute to aggressive behavior in boys (Kindlon and Thompson 2000). Females tend to adopt psychological aggression behaviors more often than males, suggesting that societal sex stereotypes play a significant role. While males are expected to display harsh and aggressive behavior, the same is not expected from females.\nFemales are expected to display indirect behavior characterized by subtlety (Sim\u00f5es et al. 2015). Therefore, knowing that psychological aggression is an indirect form of being aggressive toward a sibling, it was expected that individuals identifying as female would choose this type of aggression. However, the current results contradict the existing literature on the subject, which assigns the role of aggressor in violence between siblings to males (Kiselica and Morrill-Richards 2007). Kiselica and Morrill-Richards (2007) justify that such results are due to sex stereotypes, in which aggressiveness is once again associated with males and subtlety with females. Thus, the father figures infer that the damage caused by males is the most serious. This is because they see it according to sex stereotypes, in which males are the most physically aggressive and these physical damages are visible. As for females, they infer that they do not have sufficient strength to cause serious harm to their siblings. According to Kiselica and Morrill-Richards (2007), it is only by changing attitudes and accepting that it is that females perpetrate aggression against a sibling that this type of family violence can be tackled. Additionally, sex stereotypes already mentioned in this discussion also justify the greater involvement of males as an aggressor in sexual coercion. However, attention should also be paid to the inequality of power mentioned in the definition of sibling violence. It is understood that this inequality of power can be related to several characteristics of the actors; among them age, height, physical strength and superiority are specific to the aggressor (Monks et al. 2009). In this sense, physical superiority is seen as belonging to males when compared to females, which corresponds to sex stereotypes (Kiselica and Morrill-Richards 2007;Sim\u00f5es et al. 2015). It was expected that sexual assaults would be perpetrated primarily by males. The present results corroborate the existing literature on the subject, through which it can be observed that males more frequently maintain sexual acts (with siblings) without consent (e.g., Relva et al. 2013;Relva et al. 2014).\nRegarding psychological aggression and injury in the victimization of sibling violence in relation to sex, there were significant differences, with females showing a greater propensity to victimize psychological aggressions. In contrast, the males showed greater victimization of aggressive behaviors causing serious physical harm. Regarding the victimization of psychological aggression, experienced mostly by females, it is suggested that this is justified based on the conception that there is reciprocity in sibling violence (Krienert and Walsh 2011). In the present research, females are the ones that most perpetrate violence of the psychological type because, considering males are seen often with a greater physical robustness (Wiehe 1997), females are expected to opt for psychological violence. In addition, as already mentioned, according to Krienert and Walsh (2011), there is a reciprocity in sibling violence; in a way, they are both victims and perpetrators of aggression. Thus, and verified, this duality would be expected to be attributed to the same sex the same type of violence suffered and perpetrated. The present results are corroborated by the literature (e.g., Khan and Rogers 2015;Relva et al. 2013;Simonelli et al. 2002) when other researchers verified a predominance of females being victims of psychological aggression. The results of being a victim of injury (physical aggression with severe physical damage) seem to involve males exercising them, mostly in aggressive behaviors towards those of the same sex (Relva et al. 2014). Also, the existing literature on the subject points results in the same outcome. Several studies show that male siblings are more victimized by assaults with serious physical harm (Relva et al. 2013;Relva et al. 2014).\nIn view of the results obtained in the present study, it was also found that there were no significant differences in self-esteem as a function of social class. According to the existing literature on the subject, such results were unexpected. Esp\u00ednola (2010), in a sample of 593 students aged between 9 and 13 years, found that compared to individuals belonging to the low social class, subjects who were included in the average socioeconomic level have higher rates of self-esteem. However, these results are due to the particularities of the sample of the present investigation. Notably, 44.4% of the respondents belong to the middle social class, making it impossible to have a proportional distribution across all levels (low, medium and high) and an exact theme analysis.\nThe results also point to the absence of significant differences in victimization and the perpetration of sibling violence regarding the parents' social class. As mentioned, social class does not seem to predict sibling bullying (Dantchev and Wolke 2019). Thus, it is suggested that the results are related to the size of the sample and its distribution relative to the socioeconomic level of the parents. In the present study, 44.4% of the adolescents in the sample were in the middle class, 20.3% in the high social class and the remainder in the so-called low social class. It should be noted that a larger sample that contemplates, in equal ways, different social classes should obtain a better analysis of this theme. Hoffman and Edwards (2004) recognize the gap in the literature on the subject. According to the authors, the studies that contemplate the parents' social class in their analyses are scarce because, as in the present investigation, they present a limited sample and mostly belong to the middle social class, thus opting for the analysis of variables such as sex and age. However, Green (1984), aiming to detect and analyze the characteristics of children, as well as their parents, who assaulted siblings, causing them serious damage, in a sample of five children and adolescents, could infer that the severity of injuries increased when there were fewer financial resources. Indeed, in 2015, Tippet and Wolke found that greater rates of sibling aggression were associated with financial difficulties. In this follow-up, it is also suggested that because the sample of this study includes few elements of low social class, more obstacles are denoted to the existence of significant differences.\nRegarding self-esteem in relation to the position in the sibling dyad, there were no significant differences. It is suggested that such conceptions may be explained based on the present study's sample. It should be noted that the distribution of adolescents by the position occupied in their phratry is quite divergent since 57% of the respondents are characterized by being older siblings, only 12.2% are from the middle and the rest are the youngest. According to the literature, it would be expected that there will be differences in self-esteem inherent to the position occupied in the phratry. According to Fernandes (2005), the middle sibling is the one who presents lower self-esteem due to the poorly defined role occupied in his sibling dyad and consequent feelings of inferiority in relation to his siblings.\nThere were no significant differences in the perpetration of sibling violence in the face of their position in the sibling dyad. It is suggested that the achievement of such results is due to the non-association of the position occupied by the aggressor sibling with the position occupied by the victim of these same aggressions. The existing literature (Menesini et al. 2010) attaches importance to this analysis, verifying the presence of significant differences in sibling violence due to the position occupied by the sibling in the dyad. However, these analyses contemplate the position occupied by the two actors (aggressor and victim) in this dynamic, which does not occur in the present investigation. Another explanation is given for the lack of significant differences in the present analysis. Menesini et al. (2010) suggested that females, instead of highlighting importance to the position occupied by each of the siblings in their dyad, attach greater importance to the quality of the relationship established between them. Knowing that the sample of the present study consists mostly of female elements (59.8%), the position in the sibling dyad is expected to not acquire a prominent position. However, the existing literature does not corroborate the conceptions evidenced in the present study (Finkelhor et al. 2006). Thus, Finkelhor et al. (2006), in a sample of 2030 children and adolescents aged between 2 and 17 years of age, could verify that older siblings attack more frequently when compared to the others. The same was found by Tippett and Wolker (2015), where they found an association between the perpetration of sibling aggression and being the eldest child.\nAs for the victimization of sibling violence, there were significant differences regarding physical assault compared to the position in the sibling dyad. The middle sibling, as victims, were more often involved in acts of physical assault when compared to the youngest or the oldest. It should be noted that the middle sibling is, according to the literature, the only one who does not have his role well defined in his phratry because the eldest is considered as the successor of the parents, while the youngest is the protected son, the one who is free from any responsibility (Fernandes 2005;Fernandes et al. 2007). It is thus suggested that, due to this condition, the middle sibling feels that his interaction with his parents is insufficient, considering that they do not give him the necessary attention. Thus, they are inherently aware of a sense of abandonment, guided by feelings of inferiority towards their siblings, which in turn leads to the presence of low levels of self-esteem (Fernandes 2005), thus making them more vulnerable to the victimization of violent behaviors. Similar conclusions are described in the study by Bowes et al. (2014) with a sample of 2002 young adults aged 18 years. The authors found that victims of violence between siblings are characterized by belonging to a sibling dyad where an older brother is present. However, according to Tippett and Wolker (2015), the eldest siblings are also victims of sibling violence. The authors suggest that the youngest siblings desire the resources of the eldest siblings and, therefore, behave more aggressively.\nThe results of the present study also allow us to observe a predictive effect of selfesteem on the development of behaviors of both negotiation and sibling violence. This predictive effect is observed for both acts of perpetration and victimization, except for the victimization of sexual assaults; the existence of a positive role of self-esteem in negotiation behaviors is noteworthy. It is thus suggested, as already mentioned in the present discussion, that high self-esteem is associated with a greater capacity to resolve intra-family conflicts, thus favoring dialogue and communication between brothers and sisters. On the other hand, self-esteem is negatively associated with violent behaviors between siblings. In the face of such results, it seems that siblings who have high self-esteem, that is, who show high levels of satisfaction with themselves and the outside world, do not seem to have the need to develop any rivalry with any other sibling. Empirical conceptions present in the literature corroborate the results explained in that they argue that relationships between siblings influence self-esteem and a positive and healthy relationship between siblings benefits the quality of self-esteem, thus developing positive feelings and perceptions about oneself (Yeh and Lempers 2004). Also, about the possible presence of aggressive behaviors in the dynamics between siblings, Avanci et al. (2007) and Wiehe (1997) show that elements with high self-esteem less frequently victimize aggressions perpetrated by their siblings. Concomitantly, Wiehe (1997) also postulates that in addition to the victims, aggressors in sibling violence are also characterized by low levels of self-esteem. Additionally, Dantchev and Wolke (2019) argue that high self-esteem seems to be protective of the victim status.\nThe position's role in the sibling dyad was only significant for the perpetration of negotiation and psychological aggression and the victimization of psychological aggression and physical assault. In this follow-up, the middle brother reveals a greater predisposition to converse with the other siblings when compared to the youngest brother. It is suggested that this capacity for dialogue with others, present in the dyad, is because both the middle brother and the youngest one experience, from birth, social relations with the brothers. However, when these relationships are associated with the different fraternal roles that the middle brother can assume (Fernandes 2005), they give him characteristics such as cooperation and negotiation. In addition to the relational networks experienced since its birth, it is also noted that this adaptability to situations gives them prosocial characteristics such as the capacity for dialogue. However, empirical conceptions present in the literature contradict the present result; according to Fernandes (2005) and Fernandes et al. (2007), the middle brother presents a personality marked by aggressiveness and little concern for others, focusing essentially on himself. Thus, the presence of capacities relative to the middle brother is not expected, such as the predisposition to establish conversations to resolve conflicts arising in the fraternal dyad. Also, when equated with the eldest, the middle brother reveals a greater involvement in behaviors of the perpetration of psychological aggression. The characteristics inherent to the older brother in the literature are suggested to assume special importance in this relationship. Especially in the case of being male, the eldest son is seen by the parents as their successor, the one who will continue with their legacy (Fernandes 2005). Derived from this assumption, characteristics are inherent in the older brother that differentiate him from the middle brother. It is known that older people are revealed to be more conservative (Fernandes 2005) to the extent that they are instilled, by their parents, with the values and beliefs adopted by them. For this reason, the older one presents personality characteristics such as integrity, responsibility, greater concern for others and lower aggressiveness (Fernandes 2005;Fernandes et al. 2007). Conversely, the middle brother is given characteristics such as competitiveness and aggressiveness (Fernandes 2005;Fernandes et al. 2007). However, the present results are inconsistent with the existing literature on this problem. Finkelhor et al. (2006), using a sample of 2030 individuals aged between 2 and 17 years, found that older siblings most often play the role of aggressor in sibling violence. The middle brother reveals being more the victim of psychological aggression than the older brother. Regarding being victims of physical assault, the middle sibling reveals greater involvement when compared to the youngest and oldest. These results align with those reported by Dantchev and Wolke (2019) where the firstborn children were more likely to be perpetrators. The results can also be explained based on the assumptions already evidenced in the current discussion. Knowing that characteristics such as low self-esteem (Fernandes 2005) are found in the personality of the middle brother, this fact seems to predispose him to greater involvement in sibling violence (Wiehe 1997), more specifically, assuming the role of victim. The same is no longer true for the other siblings (older brother and younger brother) because, as already mentioned, they are at the extremes of the fraternal constellation and have their fraternal roles very well defined, which gives them security (Fernandes 2005), protecting them in situations demarcated by violence. These results point in the same direction as the empirical conceptions present in the literature, stating that high levels of self-esteem are inherent to a good relationship between siblings; on the contrary, in the face of a relationship demarcated by violence, it fosters the subject of a negative self-evaluation (Yeh and Lempers 2004).\nFinally, there was a sex role in the perpetration of sexual coercion and the victimization of injury, that is, of aggression causing serious physical harm. Thus, females, when compared to males, revealed a greater propensity to establish conversations and use this method to resolve conflicts arising within the sibling dyad. As already mentioned in the present discussion, it is suggested that, based on sex stereotypes, the adoption of negotiation behaviors by females was an expected result. Due to the belief that females should assume a behavior connoted by delicacy (Sim\u00f5es et al. 2015), the primary attachment figures interact early with the baby according to these same convictions (Seixas 2009). According to Kindlon and Thompson (2000), parents, in front of a female baby, maintain a longer conversation, always showing a concern for other individuals, which is no longer the case with a male child. In addition, the predisposition to a greater development of communicative skills also belongs to females (Rubia 2007), as they achieve, before males, a maturation of social skills (Legato 2009).\nRegarding the perpetration of sexual assaults, compared to females, males stand out in the adoption of abusive conduct of this type. Sex stereotypes, as already mentioned in this discussion, once again seem to have a strong effect on this association. The behavioral mode, determined by a society that must be employed by males, is connoted by aggressiveness and frontality, contrasting with the subtlety and delicacy particular to females (Sim\u00f5es et al. 2015). An inequality of power and a central characteristic of violence between siblings is also highlighted in this analysis. Noting that, normatively, males are physically stronger, the perpetration of sexual abuse is more achievable for them when compared to females (Monks et al. 2009). The existing literature on the subject in question denotes evidence that follows the same direction as the results obtained in this study, clarifying the greater propensity of males to engage as aggressors in sexual violence when compared to the females (e.g., Relva et al. 2013;Relva et al. 2014). Regarding the victimization caused by physical aggressions that cause serious physical harm, males also show greater involvement. It is suggested that these results can be explained by showing that males, recurrently, exert aggressions on individuals of the same sex (Relva et al. 2014). In this sense, there are several studies that corroborate the results presented here, demonstrating that males have a greater propensity to be involved, as a victim, in behaviors of physical aggressions that cause serious physical damage (e.g., Relva et al. 2013;Relva et al. 2014).\n---\nPractical Implications, Limitations and Suggestions for Future Studies\n\nRegarding practical implications, the presence of high self-esteem levels in adolescents' psychosocial development is highlighted, protecting them when they are in situations of sibling violence. This result is extremely important since self-esteem works as a protective factor. Additionally, developing programs that help children and adolescents improve the quality of sibling interaction can contribute to reducing sibling conflict. In this way, it is intended to highlight the importance of the adjusted development of self-esteem and alert to sibling violence. In addition, the sex of adolescents and the sibling position occupied seem to be relevant variables in the study of aggression between siblings. Teaching siblings to regulate emotions is also important to promote good sibling relationships. The current findings also suggest that siblings, where negotiation strategies are reduced, can be helped to develop these skills and contribute to reduced sibling violence. Parents can be taught to help children to acquire prosocial competencies.\nThe present study has several limitations. The first limitation is the cross-sectional character, not allowing the establishment of cause-effect relationships. The present investigation also uses self-report instruments. A convenience sample was also a limitation.\n---\nData Availability Statement:\n\nThe raw data, analysis code and materials used in this manuscript are not openly available due to privacy and ethical restrictions but can be obtained from the corresponding author. No aspects of this study were pre-registered.\n---\n\n\nThe project was submitted to a scientific board of the university.\nInformed Consent Statement: Informed consent was obtained from all subjects involved in this study.\n---\nConflicts of Interest:\n\nThe authors declare no conflict of interest.",
        "Introduction\n\nVarious studies have attempted to determine the common stressors that have a persistent and pervasive impact on the development of children [1,2]. Among the varied forms of stress that children can often fall vulnerable to at their tender age is being bullied at school [3]. Bullying, or behaviour intended to cause an adverse effect such as physical harm or mental distress to others [4][5][6], has been commonly observed among children ranging across different ethnic and crosscultural groups [7,8].\nBullying has been compared across nations. Craig et al. [9] have examined the rates of bullying and victimisation in 40 countries across the globe among nationally representative pupils aged between 11 and 15 years (\ud835\udc5b = 29,127). These countries generally fall under the category of middle-to high-income countries. The study unequivocally showed that exposure to bullying is a global problem, with the incidence ranging from approximately 9 to 45% among males and 5 to 36% among females. Other studies have reached similar conclusion [10,11]. However, an international comparison of bullying rates is hampered by two main inconsistencies: the fact that the term \"bullying\" lacks universal defining features and that there are variations in the methodologies employed to detect incidences of bullying. Nonetheless, in the countries where surveys have been conducted, despite the variety of 2\nThe Scientific World Journal methodologies and the lack of a standardised concept of bullying, the rate of bullying among school pupils appears to be remarkably high. Studies indicate that the prevalence of being bullied among school pupilsin North America ranges from 40 to 80% [12]. There is some indication that incidents of bullying appear to be declining among Euro-American populations [13]. In India, Kshirsagar et al. [14] recorded that approximately 30% of the children interviewed in their study reported having been bullied. In South Korea, another Asian country, the rate of various forms of bullying ranges from 5 to 12% in the population surveyed [15]. Siziya et al. [11] examined the prevalence of bullying among a nationally representative sample of adolescents in grades 7 to 10 in an African population in Zambia. The study reported that more than 60% of these children endorsed the view that they had been bullied over the previous 30 days.\nExamining the continuity of bullying over time is also important. In a longitudinal study conducted in Canada, Beran [16] explored a concept known as \"the stability of harassment\" in children. The data from this study indicate that those who were not initially bullied had less chance of being bullied in subsequent years. In contrast, the children who had previously been targets of bullying were more prone to experience bullying later. Other studies are consistent with this view [17]. There are empirical studies suggesting that the repercussions of being bullied for school pupilsare enormous, including potentially triggering the onset of mental disorders, which can in turn have negative ramifications on quality of life. There is also evidence that the psychological effects of bullying persist for many years after the actual incidence of bullying, with variability among victims depending on the social support provided for them [18]. Long-term selfreported antisocial behaviour, such as delinquency, violence, and drug use, was found to be related to bullying. Victimisation has also been associated with suicidal ideation, and this association was stronger for those adolescents who were bullied regularly on a weekly basis [19]. Although there is a strong belief that bullying has a significant impact on academic achievement, it is still unclear whether academic underachievement stems from scholastic failure or bullyingrelated absenteeism [20].\nTo our knowledge, with a few exceptions, minimal research exists in the literature to reflect the magnitude of bullying within the Arab/Islamic region. In Turkey, Arslan et al. surveyed pupils in the ninth and tenth grades of 6 high schools in the Istanbul province during the spring of 2007 and reported that 17% of the students admitted to having been bullied [21]. Gofin and Avitzour examined the rate of traditional bullying among Arab and Israeli students in Jerusalem in 2006, and traditional bullying was found among nearly 15% of these students [22]. Oman represents a fertile ground to explore the magnitude of bullying and its impact on indices of education engagement. In 2005, a brief study in Oman explored the bullying prevalence and demographics. The study showed that 40% of middle-school students in Oman had experienced bullying in the past 30 days [23]. However, little is known regarding the type and impact of bullying in Oman. Along with such questions, research exploring the places of abode or the educational settings is also imperative, as previous studies have alluded to the importance of differentiating areas of residence in determining factors of bullying [24]. In demographic terms, the Omani population is characterised as currently in a \"youth bulge\" phase, meaning that the majority of the population are children and adolescents [25]; this is the age group most widely acknowledged to be vulnerable to bullying. This gives a strong rationale for exploring the type and impact of bullying among emerging Oman's \"youth bulge\" population.\nTo our knowledge, no prior study has been undertaken to examine the magnitude and other correlates of bullying in Oman. This study aims to explore the incidents, characteristics, and impact of bullying among eighth grade pupils in Muscat Governorate of Oman. According to statistics available from the Ministry of Education, in the academic year 2006-2007 there were 101,597 pupils in the Muscat Governorate enrolled in public schools (which are schools run by the state under the direction of the Ministry of Education). In total, there were 49,783 (49%) male and 51,814 (51%) female students. For the present purpose, all participating pupils were children who were enrolled in the eighth grade. The sample size was estimated using the EPI Info statistical program Version 6.0 (Centers for Disease Control and Prevention, Atlanta, Georgia, USA) and based on the formula for calculating sample size in cross-sectional studies as elaborated by Kelsey et al. [26]. With a type-1 error of 5% (\ud835\udefc = 0.05) and 95% level of significance, it was estimated that 1,153 subjects would be required in order to detect a bullying prevalence as low as 25% at a power level of 90%. Therefore, the set target was to reach 1,153 participants in order to achieve the objectives of the study.\n---\nMethods\n\nThe participants were selected using stratified random selection. Schools in the catchment area were randomly selected with an equal representation of both genders. The second stage of the sampling was done by randomly selecting one of the eighth grade classes in the school and inviting all of the students in that class to participate in the study. All students were given invitation letters addressed to their parents/guardians describing the rationale for the study. The parents/guardians were asked to sign consent forms if they consent to the study. In addition to receiving parental consent, the pupils were also required to give consent before taking part in the study. The invitation was sent to 1500 students, of whom 1263 responded giving a response rate of 84%. Furthermore, 34 students were excluded due to missing sociodemographic data or incomplete segments of The Scientific World Journal 3 questionnaire. Finally, a total of 1229 students were included in the analysis, of whom female students constituted 55%.\n---\nInstruments.\n\nThe data were collected via a self-completed questionnaire specifically developed for the study using the available literature. The instrument was structured in a Likert questionnaire item, whereby respondents were asked to specify their level of agreement or disagreement on a symmetric agree-disagree scale for a series of statements. Questions were formed to cover the different attributes and characteristics of bullying that manifest as \"verbal, \" \"misuse, \" \"physical, \" or \"social, \" over the period of the last 12 months. Verbal bullying is operationalized here as statements that are perceived as threatening because of their negative connotations (\"I was teased, mean things were said to me, I was called name, I was threatened\"). Misuse is defined as bullying that falls under the category of interpersonal exploitation (\"Things were taken from me, my property was damaged, etc.\"). Physical bullying entails hurting a person's body or possessions (\"I was hit, kicked, pushed, slapped, spat on, etc.\"). Social bullying, also known as relational bullying, involves negatively affecting someone's reputation or relationships (\"I was excluded, ignored, had rumours spread, mean things said about me to others, others were made not to like me, etc.\"). The draft questionnaire was circulated to experts in the country who were likely to be well versed on issues pertinent to school bullying. After protracted discussion during a focus group, 10 items were deemed to be the most suitable and socioculturally relevant for investigating bullying in Oman.\nThe final section of the questionnaire covered demographic information including age, gender, and number of siblings as well as the birth order of the participants. In addition, some relevant information pertinent to education and performance was also sought from the students themselves.\n---\nProcedure.\n\nThe questionnaire was piloted on 50 pupils (25 males and 25 females). Following the pilot phase, a few changes to the questionnaire were made for clarity and comprehensibility. Also, it was decided that one member of the research team would read the questionnaires aloud to the class and that a period of time would be allotted after each question for the participants to write/select their answers. This method was adopted to ensure equal understanding of the questions among all included pupils, as reading ability was found to be variable among the students as deduced from the varying times taken to complete each questionnaire.\nIn the pilot study, the validity of the questionnaire was assessed by comparing the information obtained through self-completed questionnaires versus questionnaires filled out during interviews of all the 50 pupils included in the piloting. Therefore, all pupils were asked to complete the questionnaire by themselves, and later they were asked to fill out the questionnaire during the interviews. The information gathered using the questionnaire filled during the interview was considered the \"gold standard, \" to which the selfcompleted questionnaire was compared. Convergent validity was assessed using Spearman's correlations over the total bullying frequencies (\ud835\udc5f = 0.83, \ud835\udc43 < 0.01) which showed significant correlation, supporting the good convergent validity. Interrater reliability was established among pupils at a standard of 90% agreement on key questions related to bullying in the questionnaires filled during interview and the self-completed ones. The average percentage agreement between the pupil performance in both questionnaires (selfcompleted and during interview) was found to be 91%. The interrater reliability of questions related to bullying frequency and characteristics was also assessed using Kappa coefficient as a parameter which was found to be 0.82, indicating high reliability. Combining all psychometric parameters, the global psychometric assessment of the questionnaire indicated high reliability and validity.\nThe data were collected by three of the authors after receiving training on data collection via interviews. All attempts were made to adhere to the World Medical Association's \"Declaration of Helsinki\" for ethical human research, which encompasses confidentiality and data storage [27]. In brief, the pupils were explicitly assured that their participation was anonymous and voluntary, that the data gathered would be aggregated, and that they could withdraw from the study at any time, without prejudice. In the event that undue distress was experienced by the pupil while responding to sensitive questions, counselling support was offered if needed. The pupils were also asked not to discuss the questions among themselves in order to avoid peer influence.\n---\nData Analysis.\n\nChi-square analysis was used to evaluate the statistically significant differences between the proportions of categorical data. The nonparametric Fisher exact test (two-tailed) replaced the Chi-squared test in cases of small sample size where the expected frequency was less than 5 in any of the cells in the 2 \u00d7 2 tables.\nStatistical analysis was performed using the Statistical Package for the Social Sciences (SPSS) software Version 19.0 (IBM Corp., Chicago, IL, USA); a cut-off value of \ud835\udc43 < 0.05 was considered the threshold of statistical significance for all tests.\n---\nResults\n\nA total of 1,229 pupils were enrolled. In terms of breakdown of gender, approximately 45% were male (\ud835\udc5b = 554) and 55% were female (\ud835\udc5b = 675). The differential proportions of males and females were due to differential response rates among the two groups. Table 1 shows the sociodemographic characteristics of the study participants. The majority were 13-and 14-year-olds attending the eighth grade. There was an adequate representation of all the wilayats in the Muscat Governorate region. In total, 940 participants (76.5%) reported that they had been bullied, and bullying was comparable among females and males (76.7% versus 76.1%, resp.).\nTable 2 shows the characteristics of past bullying incidents among the participating pupils. The majority reported a frequency of 1-3 incidents of bullying over the past 12 months. The percentage of pupils who reported 4-10 incidents was 13%, and 14.7% reported that they had been exposed to more than 10 incidents of bullying. The distribution of bullying\nThe Scientific World Journal frequencies was comparable between male and female pupils.\nThe most common type of bullying was verbal, followed by social and then physical. The least reported type was misuse. Compared to the female pupils, verbal and physical bullying were more common among males. However, social and misuse bullying were more common among females. The difference in types of bullying among males and females was statistically significant. The most common site for bullying was reported to be in a school environment, followed by on the bus, in the neighbourhood, at home, and in shopping areas. Bullying on the bus was reported more by males, while females reported more bullying occurring at home. As Oman is a gender-segregated society, such variation might stem from the differences in how each gender spends their time. In Oman, the expectation is that females will remain mostly in the domestic sphere while males are accepted to spend more time in the public sphere. From a cultural perspective, this means that girls are expected to stay at home while the boys are allowed to freely spend time outside in the community. Table 3 shows the characteristics of the perpetrator of the bullying incident and the victim's reaction towards the incident. The majority reported that the perpetrator was older than them and tended to be a known person. This pattern of age and relationship of the perpetrator to the victim was more prominent among female pupils compared to males, and the differences were statistically significant.\nThe exact motive for the bullying was unknown to the majority of the victims. Nonetheless, the victim's physical appearance was the most reported reason for the bullying, followed by academic performance and the victim's style of speech. The least reported reason was a victim's disease or disability. Compared to female pupils, male pupils have reported higher proportions of academic performance, victim's style of speech, and victim's disease or disability as reasons for bullying, and the differences were statistically significant. The majority of the victims reported the bullying incident to another person such as a school staff member, a relative, or another student. About one-third of the victims fought back either verbally or physically. However, about one-fifth (19%) of the victims did not take any action against the perpetrator, and the difference between females and males was small (20.3% versus 17.3%, resp.). More female students tended to inform a relative or another student, while more male students tended to inform a school staff. Compared to females, more male pupils fought back physically. The variation in reciprocating the bullying was significantly different between males and females.\nTable 4 compares bullied and nonbullied pupils in terms of failed academic years and missed school days. Overall, the proportion of pupils who failed one or more academic years was more common among nonbullied pupils compared to bullied pupils. The differences were statistically significant (\ud835\udc43 = 0.02). On the contrary, missing 4-6 and 7 or more school days was more common among bullied pupils compared to the nonbullied pupils, and the difference was statistically significant (\ud835\udc43 = 0.01).\n---\nDiscussion\n\nOman has been internationally lauded for spending 3.5% of its gross national product (GNP) to furnish a universal free educational system for children, an important springboard for equipping the future generation with the required knowledge and skills to compete in the modern world [28]. Educational institutions have spread to all corners of the country and there is no gender gap in the gross enrolment rates for secondary education. As a result, illiteracy has generally waned among the younger generation. On an educational front, this progress is noteworthy, considering that four decades ago there were only 3 primary schools, and these were reserved solely for male children [29]. However, the present study addresses whether this education is provided in a safe environment in Oman.\nThe first aim of this study was to explore the characteristics of the bullying incidents among eighth grade pupils in the Muscat Governorate region in Oman. As there is currently no universally accepted definition of bullying, the rate of bullying appears to fluctuate in a complex way. Although the focus is often geared towards the victims of bullying, the presence of bullying perpetrators highlights the presence of antisocial behaviour in society. In either case, this behaviour is likely to burden the society with negative social and economic implications [30].\nBased on multicentre studies using the Global School-Based Student Health Survey, Fleming and Jacobsen reported that approximately 40% of middle-school students in Oman had experienced bullying in the past 30 days [23]. The present study suggests that approximately 76% of eighth graders  [11,16,31]. This study therefore unequivocally suggests that the rate of bullying in Oman is skewed toward the higher end of this range. Several studies suggest that certain sociocultural teachings, such as those found in \"collective societies\" or groups that highly value conformity, perceive bullying as an acceptable means to pattern idealised social norms [32,33]. Nonetheless, evidence from Japan suggests that Ijime (school bullying) coincides with the onset of acculturation and modernisation and with the weakening of the traditional bonds that are an integral part of a collectivist society [34]. In the last decades, which have been associated with the development of the oil industry, Oman has undergone a rapid socioeconomic transformation that has resulted in an unprecedented increased standard of living and acculturation. There is indication that such affluence has negative repercussions in denting challenging traditional outlooks that foster collective identity [35]. It remains to be seen whether the presently observed high rate of bullying in Oman stems from the sociocultural trajectory of the traditional to the more recent atmosphere of acculturation and modernisation. Further studies on this subject are therefore imperative.\nThis study showed that the most common type of bullying was verbal, followed by social and then physical. The available literature is replete with what constitutes bullying. One of the leading authorities in the field of bullying, Olweus, has categorised bullying into 3 types: physical, verbal, and social exclusion [4]. Rivers and Smith have classified bullying incidents as those stemming from direct physical bullying, direct verbal bullying, and indirect bullying [5]. Others have suggested that physical bullying, verbal bullying, isolation, rumour-spreading, and harming are integral parts of bullying behaviour [6]. As there has been no unified definition of bullying, there is an anecdotal and impressionistic observation that bullying among school pupilsin Oman generally falls under generic terms such as verbal and physical, but this is likely to have explicit manifestations such as being subjected to misuse as well as the social ramifications that this entails. For these reasons, the present study classified bullying under the categories of verbal, misuse, physical, and social.\nIn terms of gender, this study showed that verbal and physical bullying were more common among males compared to females, while social and misuse were the more common bullying types experienced by females. The gender difference in being bullied has been consistently shown in the literature. The commonly reported pattern is that females are more prone to \"nonphysical\" types of bullying [36,37]. Conversely, physical bullying appears to be more dominant among males. Studies of traditional societies indicate that prescribed gender roles have a direct bearing on the type of bullying an individual is likely to experience [38]. Thus, studies have suggested that females are likely to be victims of verbal abuse whereas males are likely to affected by physical abuse; this is concordant with the view that \"girls manipulate and boys fight\" [39].\nThis study has also attempted to determine the characteristics of bullying incidents and the perpetrators, as well as the victim's reaction towards the bullying. Significant numbers of bullying perpetrators for both genders were noted in the present study to be older than the victims. In agreement with the available literature, it was also found that most perpetrators of bullying tend to be those with whom the victim is familiar, echoing the idiom that \"familiarity breeds contempt\" [12,40,41]. Such situation appears to prevail in Oman.\nIt has been estimated elsewhere that 8 out of 10 disabled children are bullied and that being bullied is likely to create a 6\nThe Scientific World Journal substantial psychological burden [42,43]. From the present study, the link between disability and bullying was not endorsed; this therefore merits some speculation. In Oman, many children with disability are often not sent to school [44,45]. Therefore, it is possible that there were no participants within the study with overt disabilities. In traditional societies such as Oman, disabilities are equated with the incarnation of or possession by spirits which have capacity to \"infect others\" [46]. There is also the traditional belief that laughing at or mocking a disabled person will incur bad luck. Therefore, it is possible that individuals with a disability in Oman are spared from being bullied, unlike other countries. More studies are essential to scrutinise this view.\nA sizeable quantity of literature exists on the \"gut reaction\" of youngsters subjected to bullying [47,48]. As expected, victims who reacted to bullying constituted the minority, as only 37% informed others of the incident and 32% fought back either with a verbal response or by using physical means.\nIt is apparent in the available literature that there are many negative repercussions to bullying including in the domain of school \"engagement/disengagement\" [49,50].\nSchool engagement has been previously defined by Newmann in terms of students making a psychological investment in knowledge acquisition [51]. Accordingly, the learners \"try hard to learn what school offers. . . (and) take pride not simply in earning the formal indicators of success (grades), but in understanding the material and incorporating or internalising it in their lives\" [51]. Mehta et al. [52] have examined the hypothesis that bullying has an adverse effect on the variables relevant to school engagement. They unequivocally found that a \"bullying climate\" tended to breed lower commitment to the school and less involvement in school activities. Such occurrence was independent of the individual's gender, race, or school size or whether they were of an ethnic minority or otherwise [52]. The results of other studies are congruent with this view [53,54].\nTo evaluate the impact of bullying on indices of school engagement, this study explored whether indicators of school failure (repeating academic years) and absenteeism (absence from school) were more common among victims of bullying. The underlying assumption is that bullying predicts weakened academic performance vis-\u00e0-vis the resulting emotional distress and school disengagement. The present data suggest that the tendency for academic failure was more common among nonbullied compared to bullied participants. This finding, at face value, ostensibly discounts the previous view that being bullied is a strong predictor of negative academic performance or the indices of school engagement [43,[53][54][55], but there is some preliminary evidence that bullying has a more complex relationship with academic performance and that the link may not be a temporal one [56]. Some speculation for the present findings is therefore imperative. The reason that there were a higher percentage of pupils repeating an academic year among the nonbullied may be because the nonbullied included those who were already academically poor students. Moreover, some of them may not have been the victims of bullying but instead been perpetrators. However, investigating the second index of engagement, school absenteeism, indicated that the proportion of participants who missed 4-6 and 7 or more school days was more common among bullied participants compared to the nonbullied.\nVarious limitations emerging from the present study ought to be highlighted. Firstly, as aforementioned, the items of the questionnaires were drawn from the literature but were specifically chosen to suit the sociocultural context of Oman. By doing so, rather than employing measures with established psychometric properties, this study was limited by the virtue of having employed measures of which the validity and reliability have not yet been established. This limitation has obviously hampered the much-needed establishment of a universal taxonomy of bullying, relevant to international comparisons. In relation to this, despite the plethora of studies on bullying among school pupils, there is little consensus on the validity and reliability of the measures employed. Many authors have appointed the limitations of self-reported measures, such as the self-completed questionnaires used in this study [57]. Related to this, the student's own definition of bullying was also not taken into consideration and this might have affected the reporting of different types of bullying. Another related limitation was that the questions about possible motives for bullying were directed towards the victims, while it would have been more logical if they were directed towards the perpetrators. Future studies ought to be soliciting the review from both victim and perpetrators of school bullying.\nSecondly, Oman is a vast country with much subcultural diversity. This study is limited by the fact that it was carried out in the urban capital, Muscat, and its satellite region. On this ground, the generalisations of this study are limited to the urban population. The confidence in establishing a relation between bullying and poor academic performance might be limited by the fact that it has employed a cross-sectional approach. Such research methodology is not equipped to capture cause and effect as there is no mechanism to establish a temporal relationship. In addition, the observed association between being bullied and absence from school might have been affected by another yet to be recognized confounding factor. As a matter of fact, the study did not take into consideration that school absence may have been due to other factors (illness, accidents, outings, etc.). It is possible to assume that the reasons for school absence may have been different among bullied and nonbullied students. Finally, the present study appears to provide a snapshot of the victim of bullying. In future studies, it would be interesting to examine more extensively the characteristics of those who bully others. This could provide an overview of both sides of the bullying coin that would have implications for contemplating evidence-based antibullying curricula.\nIt is essential here to highlight some implications for school health. This study largely substantiates the view that bullying, or behaviour intended to cause adverse reactions such as physical harm or mental distress to another, transcends geography, ethnicity, or religious denomination [7]. This study supports the view that Oman is not immune to the vagaries of bullying, as the present results indicated that 76% of eighth graders had endured bullying in the last 12 months. Such a substantial percentage appears to be skewed towards the higher end of the range regarding bullying rates, according to the available international trend [11,16,31]. In a society known to be characterised by a \"culture of silence, \" such as in Oman, those affected by bullying are likely to have slipped under the \"radar\" of concerned parties, and, therefore, bullying has so far remained a furtive activity. It would be worthwhile to note that some bullying also occurs outside the vicinity of the school and that some of it may stem from known person. Therefore, teachers and school authorities ought to be educated in detecting the presence of bullying using culturally sensitive lenses, as well as in providing timely interventions. The curriculum should be specifically designed with the intention of providing education on the impact of bullying. The authorities in the Ministry of Education should employ the mass media at their disposal to combat bullying. Oman, as a society, has been known to thrive on its diversity; such an attitude could be used as the catalyst for prevention and intervention. As 76% of eighth graders have endured and suffered from harassment from others, concentrated effort is also needed in the Omani educational setting to address the issue of bullying.\nThe Scientific World Journal\n---\nConclusion\n\nThis study corroborates evidence from different parts of the world that bullying among school pupilsis rife and that it is irrespective of gender. The majority of harassment was described to occur within the vicinity of schools. Almost onethird of the victims did not know the reason for the bullying and only a small percentage sought help in the schools after the incident. While bullying has little impact on academic failure, it did have a negative repercussion on attendance. It is therefore imperative that educational environments in Oman are made conducive for learning, particularly considering that the bulk of its population is still within the school-going population.\n---\nEthical Approval\n\nEthics approval was provided by the local institutional review board (IRB), Research and Ethics Committee of College of Medicine and Health Sciences, Sultan Qaboos University (MREC no. 382).\n---\nConsent\n\nParticipant consent was obtained.\n---\nConflict of Interests\n\nThe authors declare that there is no conflict of interests regarding the publication of this paper.\n---\nAuthors' Contribution\n\nMuna Al-Saadoon was the PI. Muna Al-Saadoon, Marwan Al-Sharbati, Abdullah Al-Jabri, Sufyan Almamari, and Wafaa Al-Baluki were responsible for the design of the study and the acquisition of the data. Yahya M. Al-Farsi, Samir Al-Adawi, and Sayed Risvi contributed to the interpretation of the results. The initial draft of the paper was prepared by Samir Al-Adawi and then circulated repeatedly among all authors for critical revision.\nSubmit your manuscripts at http://www.hindawi.com\n---\nStem Cells International\n\nHindawi ",
        "Online\n\nFor related articles visit the archive and search using the keywords above. Guidelines on writing for publication are vailable at: journals.rcni.com/r/author-guidelines Hammersley and Atkinson (2007) point out that ethnography has much in common with other research approaches, and that the boundaries around it are not necessarily 'hard and fast'.\nAs nurses, the people we encounter as we go about our daily work all have knowledge, histories, relationships and cultural experiences that influence their experiences of health and illness. These real-life contexts should therefore be of central importance to nurses and nursing. Ethnography is a research approach that can 'get at' these issues, and the purpose of this article is to outline, in an accessible way, some of the principles and practice of ethnography. The article also aims to highlight ethnography's potential for improving nursing and healthcare practice.\nEthnography has its roots in the discipline of anthropology and, historically, classical ethnography was concerned with describing 'other' cultures (that is, those outside a home culture and usually regarded as less well developed) to understand their beliefs and practices. In the 1930s, the Chicago School of Sociology began to use ethnography to describe Western cultures and now, in the 21st century, the focus for contemporary ethnography has developed further. It is no longer merely focused on 'other' cultures but has shifted its concern to settings nearer to home -what Rapport (2000) called 'anthropology at home '. Some examples that are relevant to health care might include Becker et al's (1961) seminal ethnographic study of medical students, Boys in White: Student Culture in Medical School and Froggatt's (1997) ethnography of end of life care in hospice settings. More contemporary examples are listed in Box 1.\nModern ethnography is therefore largely concerned with local and 'near' communities, rather than distant and 'exotic' ones. It is interested in the routine of daily life and 'the ways that people understand and account for their day-to-day situations' (Maggs-Rapport 2000). Furthermore, as philosophical thinking has developed, different types of contemporary ethnography have emerged including, for example critical, feminist, focused, ETHNOGRAPHY IS A RESEARCH methodology concerned with describing people and how their behaviour, either as individuals or as part of a group, is influenced by the culture or subcultures in which they live and move. 'Ethnos' means people, race or cultural group and 'graphe' means writing; thus, ethnography literally means writing culture. In studying people in their cultural context, ethnography is a fundamental form of social research (Hammersley and Atkinson 2007) \n---\nPrinciples\n\nThere are a number of general principles that underpin ethnography, which the nurse has to understand if the focus and work of the ethnographic researcher is to be understood. These include assumptions about the nature of knowledge, its emphasis on investigating culture and the role of the researcher. This article examines each in turn and illustrates them, where relevant, with nursing practice examples.\n---\nNature of knowledge\n\nResearch is influenced by paradigms, which are ways of viewing the world and how knowledge is conceived and constructed. Ethnographic research is firmly rooted in the interpretative paradigm. This paradigm emphasises that there is no single 'objective' truth or reality; because human existence and practices are all different, it acknowledges there are a number of realities. Interpretivism regards cultural context as extremely important and that it cannot be removed or ignored from the research process. Thus, for example, when performing research to identify the most effective support strategies for families caring for people with dementia, the families' own beliefs, attitudes and previous experience of dementia will influence how they might respond to the strategies identified.\nIn taking account of meaning and context in this way, interpretivism is in stark contrast to the positivist paradigm that claims there is an objective reality 'out there' waiting to be discovered. Research in the interpretative paradigm is of interest to nurses who wish to understand more deeply the experience of health, illness, injury and disability.\nHuman beings experience the world through a web of meaning that includes rituals, symbols and languages. Ethnography seeks to investigate and interpret these meanings: it does not claim to be the 'true' picture but, in acknowledging that there is no universal knowledge, it provides the opportunity\n---\nExamples of ethnographic studies in nursing and health care BOX 1\n\nThe Social Meaning of Surgery (Fox 1992) Observational and interview data were collected over an 18-month period in the operating theatres of an English general hospital to undertake an in-depth description of practice, power and status in surgical practice.\nRethinking ethnography: reconstructing nursing relationships (Manias and Street 2001) This discusses some of the methodological challenges encountered during a critical ethnography of nurse-nurse and nurse-doctor interactions in a critical care setting in Victoria, Australia, including researcher-participant subjectivity, reflexivity and 'truth'.\nEvidence based guidelines or collectively constructed 'mindlines'? Ethnographic study of knowledge management in primary care (Gabbay and le May 2004) Non-participant observation, semi-structured interviews and documentary review were used to explore how GPs and practice nurses derive their individual and collective healthcare decisions.\nUsing participant observation to immerse oneself in the field. The relevance and importance of ethnography for illuminating the role of emotions in nursing practice (Allan 2006) Using participant observation, informal conversations with staff and patients and semi-structured interviews in a fertility clinic, this study explored the nursing role in infertility nursing in the context of theories of caring and emotion. It also explored the use of ethnography in illuminating the role of emotions.\nAn institutional ethnography of nurses' stress (McGibbon et al 2010) In-depth interviews, participant observation and focus groups were used to explore the nature of stress in paediatric intensive care nurses in one hospital in Nova Scotia, Canada.\nConducting critical ethnography in long-term residential care: experiences of a novice researcher in the field (Baumbusch 2011) An account of experiences as a novice researcher undertaking ethnographic research (using participant observation, in-depth interviews, documentary analysis and quantitative data) to explore the organisation of long-term residential care in British Columbia, Canada.\nAn ethnographic study of main events during hospitalisation: perceptions of nurses and patients (Coughlin 2012) Participant observation and unstructured interviews were used to explore patients' and nurses' perceptions of the care provided in a large teaching hospital in the United States.\nAn ethnographic study exploring the role of ward-based advanced nurse practitioners in an acute medical setting (Williamson et al 2012) Participant observation and interviews with five ward-based advanced nurse practitioners (ANPs) in a large teaching hospital in England was undertaken to explore the role of the ANPs and their effect on patient care and nursing practice.\nTurning over patient turnover: an ethnographic study of admissions, discharges and transfers (Jennings et al 2013) Field work (lengthy participant observation, interviews and document review) was undertaken in two medical and surgical units in the US to explore turbulence and change and, in particular, patient turnover, admissions, discharges and transfers. Art & science research series: 10 for a range of interpretations. In returning to the example of dementia support for families, if we are not able to step back and take time to understand families' attitudes to dementia and how these might influence their ongoing relationships with a loved one, we risk devising support strategies that are misunderstood and therefore misdirected.\n---\nInvestigating culture\n\nCulture is not a fixed entity or finite concrete thing; rather, it is the medium or context within which we engage in the complexities of everyday life. It is much more than race or ethnicity and includes language, behaviours, relationships, art, music, cuisine: the collectively valued ways in which we live. It both shapes and is shaped by our individual actions and behaviour. Ethnography seeks to understand this culture through a process of 'thick' (Geertz 1973) or rich description -that is, detailed description of the ins and outs of everyday life (Draper 2004). Thus, ethnography is about studying people's lives in the everyday contexts in which they live (Hammersley and Atkinson 2007). It describes people in their cultural context and attempts to understand how that culture is made up, how people interact with it, the relationship between the individual and the societal and, in the context of health and illness, how these come to be culturally defined and understood. In essence, ethnography is concerned with 'learning about people by learning from people' (Cruz and Higginbottom 2013).\nIn describing culture, ethnography is concerned with taken-for-granted things, things that are so ingrained and 'automatic' that we perhaps fail to realise their impact on our individual and societal or collective experience. Thus, one of the purposes of ethnography is to make the familiar strange. An illustration might be Coughlin's (2012) work on how events that we might take for granted as nurses have significant impact on the experiences of our patients (Box 1).\nAn important feature of ethnography is the interplay between the individual (or emic) and the societal (or etic), and how the two both inform and are informed by each other. The emic perspective refers to the insider's point of view: the reality seen, experienced, understood and expressed by the individual. It is an explanation of events from the individual's point of view. The etic perspective relates to the larger collective or societal picture. However, these two perspectives are not mutually exclusive. Individual performance and understanding is informed by collective cultural understandings and, in turn, individual and private experience can contribute to collective meanings of cultural practice. The way in which the individual shapes the collective and the collective shapes the individual can be understood as the individual-cultural dialectic (Draper 2000).\nReturning to the earlier topic of dementia, the emic perspective will include the individual's attitudes towards dementia, informed by their previous experience of knowing someone with dementia and how this now influences their interaction with a loved one with the condition. The etic perspective is the way in which society demonstrates its values towards people with dementia, how it portrays them in the media and the decisions it makes about resource allocation for their health and social care. Our individual experiences of dementia will be shaped by prevailing attitudes in society but, in turn, perhaps through campaigning for more effective person-centred approaches, this emic perspective can influence societal changes and ultimately reshape the collective (etic) perspective.\n---\nRole of the researcher\n\nA key principle associated with ethnography is its focus on the influence of the researcher. Assumptions about the nature of knowledge inform how knowledge is described, explained or generated. Research methods associated with positivism, randomised controlled trials for example, pursue the goal of 'objectivity' and attempt to eliminate sources of 'bias', one example of which is the influence of the researcher. The aim is to 'decontaminate' or 'bracket' the influence of the researcher to render the research scientific and objective.\nHowever, bracketing the influence of the researcher in this way is impossible, because investigation of the world can never be devoid of the influence of the investigator. All research approaches, from initial ideas through to decisions about design, method and analysis, bear the influence of the researcher (Draper 2000). In other words, the position of the researcher is never neutral. Ethnography makes the influence of the researcher explicit, rather than trying to erase it (Cruz and Higginbottom 2013).\nRecognition of the researcher's influence is achieved through the researcher acknowledging the ways in which their particular cultural beliefs, attitudes and assumptions shape their approach to the design, execution and interpretation of the research. It acknowledges that the researcher, as a principal instrument of data collection (McGarry 2007), has as much a role in the research as the participants being studied.\nReflexivity is therefore considered to be central to the ethnographic endeavour; ethnographers 'recognise that they are unable to put their own knowledge of the social world to one side in NURSING STANDARD may 6 :: vol 29 no 36 :: 2015 39 the hope of achieving objectivity' (Pellat 2003). Ethnography therefore embraces the researcher as part of the world being studied. Allen (2004) argues it is important to understand the researcher is an interpretative lens, that he or she has an effect on the issue being studied and that, in turn, 'the field' has an effect on the researcher. Rather than trying to put aside our own knowledge as researchers, our assumptions, beliefs and values are acknowledged 'up front'. Thus, ethnography acknowledges that researchers do not 'arrive empty-minded in the field' (James 1993); rather, the issues of 'who I am' and 'what I am' as my 'researcher self' should be carefully considered. With respect to conducting ethnographic research in nursing, therefore, our 'insider' knowledge of being nurses -our beliefs, attitudes and values -is acknowledged and enriches the research process.\n---\nPractice\n\nThis section explores what the principles of ethnography mean for the practice of undertaking ethnography. This is illustrated throughout with reference to a hypothetical research project conducted by Anna, a dementia specialist nurse, into the organisation of dementia care in hospital wards.\n---\nMethods\n\nA feature of ethnography is that the researcher engages directly with the culture or sub-culture being studied and becomes immersed in it. In classical ethnography the researcher would go and live in the culture they were studying, become immersed in it and be exposed to all its nuances. They would become completely engaged as a member of that culture and use a range of data collection methods to 'get at' it. As an 'outsider', their mission was to describe the culture and in so doing make familiar the strange. In contemporary ethnographic research, however, we are often members ('insiders') of the cultures that we are researching. The challenge for such 'anthropologists at home' is to make strange the familiar, to be able to stand back and describe taken-for-granted cultural practices and expose them to different theoretical analyses.\n---\nAnna, an ethnographic lens\n\nIn the context of ethnographic research into the way in which dementia care is organised in hospital wards, Anna is an 'insider' researcher, bringing her knowledge, skills and experience of being a dementia specialist nurse to enrich the study. She uses this as a 'lens' through which to describe the culture of dementia care on hospital wards and to uncover some of the assumptions that underpin the organisation of care.\nEthnographers use a range of methods for data collection, similar to those used in other forms of qualitative inquiry, to reveal these social and cultural practices and, perhaps more importantly, the meaning participants place on these. Often multiple data collection methods are used simultaneously to achieve as rich and detailed a description of the culture and its members as possible.\nData are collected in a purposive way from participants, or key informants, who are representatives of the culture under study. Participant observation is the method of data collection most closely associated with ethnography. This entails the researcher entering the culture to observe it, undertake detailed description and then provide an 'ethnography' of their interpretation of events. In contemporary ethnography, researchers are often already members, insiders, of the cultures or sub-cultures they are investigating. In this context, participant observation involves techniques to make transparent this insider-yet-outsider status, what Cudmore and Sondermeyer (2007) described as 'living in the borderlands'. It also requires the researcher to be clear about their role in the research setting, ensuring those being observed understand the purpose of the research and the role of the researcher. It often entails lengthy exposure in the field, where the researcher takes time to be present in the field, to become part of the 'furniture' so that the Hawthorne effect (how we modify our behaviour when we know we are being observed) is minimised.\nAnna's time 'in the field' Anna undertakes observation on two hospital wards to explore how care for people with dementia is organised, how healthcare staff work as part of the team and to what extent families and carers are involved in care. She spends time 'in the field' before commencing observations so that staff, patients and carers become familiar with her presence and understand the purpose of the research and their contribution to it.\nEthnographers may also use interviews, either individual or group, with key participants or informants. Such interviews can be semi or unstructured and are designed to explore the issue from the perspective of individuals or groups to uncover meanings, beliefs and assumptions. Interviews can be used to complement participant observation or can be the sole data collection method. Ethnographic research also uses documentary analysis, because key documents (for example, historical archives, policies) can be regarded as dominant emblems or symbols of the culture and/or sub-culture under study.\n---\nNURSING STANDARD\n\nArt & science research series: 10\nAll such approaches to data collection are designed to explore both emic and etic dimensions -that is, the individual and collective experiences, and the relationship between them.\n---\nAnna conducts interviews\n\nIn addition to observing how care is organised on the wards, Anna conducts semi-structured interviews with some patients, carers and staff to try and uncover their attitudes towards dementia and their views about the organisation of care. This helps her in her analysis of the culture of care on the two wards.\n---\nOperationalising reflexivity\n\nReflexivity, the researcher's acknowledgement of how he or she shapes and influences the research, can be exercised in a number of different ways. First, researchers can explicitly describe their own historical and cultural contexts and how these have influenced or shaped the research by writing this into the research report (or ethnography). Second, research diaries can be used to capture reflexive thoughts and observations in an attempt to achieve a critical distance. Entries made in research diaries can be subsequently incorporated into the final written ethnography and are often considered legitimate data in themselves. Third, because ethnography is not only a process but also a product (the ethnography), reflexivity is an important aspect of the way in which the ethnography is written.\nThe reflexive ethnographer is concerned with the ethnographic text and the extent to which it 'represents the reality of the participants' (Manias and Street 2001), and how the 'researcher self' is part of this. Writing the self in the ethnography -acknowledging our histories, assumptions, biographies and influence on the interpretive process -is a key feature of ethnographic writing, which is often characterised by writing in the first person. Ethnographic writing can therefore be described as 'messy writing', but it should provide as rich an account as possible, including making 'the writer a part of the writing project' (Denzin 1997).\nAnna writes her report Anna's research report is written predominantly in the first person, to signify her influencing role as the researcher. She also includes an autobiographical section where she describes her role as a dementia specialist nurse and makes explicit the assumptions, values and beliefs she holds about dementia. In addition to including descriptions of her observations and interview quotations from participants, she also occasionally makes reference to entries in her research diary.\n---\nPotential\n\nAs we have seen, ethnography can be used to investigate and illuminate the complexities of the social world. It can help us understand both individual and collective experiences and is therefore highly appropriate for use in nursing and healthcare practices. It can be used by a range of stakeholder groups to explore a wide variety of issues, including the experiences and management of care, professional identities, power relations and education. However, relatively little ethnographic research is done by nurses in their own settings (Cudmore and Sondermeyer 2007). McGarry (2007) argues that more research needs to take place where nursing happens. Ethnography NURSING STANDARD may 6 :: vol 29 no 36 :: 2015 41 therefore offers great potential for nursing and healthcare research.\nEthnography can provide an alternative view on familiar practices and problems and can complement other research approaches to construct a more comprehensive and holistic body of knowledge about a phenomenon. Nurses can undertake ethnographic research in their workplaces or in more unfamiliar settings, and ethnography can be used to raise questions about familiar assumptions underpinning practice (Manias and Street 2001). For example, in Cudmore's ethnography of the culture of emergency nursing, she writes: 'Like Alice in Through the Looking Glass, as an ethnographic researcher investigating my own workplace and therefore inevitably my colleagues, I found myself in an alternative world in which I would view the same environment from a different perspective' (Cudmore and Sondermeyer 2007).\nJohnson (2004), however, has suggested that such insider research, what he calls 'staying in your own nest', can often be undertaken merely on the grounds of convenience, because it is relatively easy to gain access to our workplace settings, our patients or our students. But Roberts (2007) argues that there are significant benefits associated with being immersed in the culture under study and where the 'cast of characters' (Lofland et al 2006) is already known.\n---\nConclusion\n\nCriticisms of ethnography often centre on the inability to generalise findings to other settings, calling into question its usefulness and 'scientific' value. This misses the point. Ethnography is a particular tool to approach a particular question or problem; the question should come first and then the nature of this question should determine the research approach taken. Ethnography cannot be used to answer questions where, for example, causality needs to be established. It can, however, provide rich, contextual and valuable insights where uncovering meaning and experience is at the heart of the research question. It therefore has great potential for nursing and healthcare research where practice is focused on the experience of both giving and receiving of care. Ethnography helps to illuminate the culturally shared experiences of others and in particular it relates the emic and the etic to each other. This is especially important where long-term conditions such as dementia challenge nurses to understand what they already do and how supportive and sensitive person-centred care might be enhanced in the future NS\n---\nAcknowledgement\n\n\n---\n\n\nIn places this article draws on material I initially wrote for an Open University postgraduate module on designing healthcare research. I would like to thank the two anonymous Nursing Standard reviewers for their helpful comments.\nNursing Standard wishes to thank Leslie Gelling, reader in nursing at Anglia Ruskin University, for co-ordinating and developing the Research series. Williamson S, Twelvetree T, Thompson J, Beaver K (2012) An ethnographic study exploring the role of ward-based advanced nurse practitioners in an acute hospital setting. Journal of Advanced Nursing. 68, 7, 1579Nursing. 68, 7, -1588. . ",
        "Introduction\n\nAs China enters the new century, the pace of ageing has been accelerating, and it has become the country with the largest elderly population in the world and one of the fastest ageing countries in the world. As a developing country, China is facing the pressure of \"ageing before getting rich\". The \"14th Five-Year Plan\" period is the first five years of China's new journey to build a strong modern socialist country, and it is also an important window to actively respond to aging, and a key stage for China to build a new model of old-age care. According to the Opinions of the Central Committee of the Communist Party of China and the State Council on Strengthening the Work on Aging in the New Era, the Medium-and Long-Term Plan for Actively Responding to Population Aging, and the Outline of the \"Healthy China 2030\" Plan, the 14th Five-Year Plan for Healthy Aging has been formulated, which provides a development plan for the construction and development of elderly care institutions in China. The \"14th Five-Year Plan for Healthy Ageing\" was formulated, providing a development direction for the construction and development of elderly care institutions in China. With the aging of the countryside and the deepening of empty nesting, the traditional function of the family as an old-age home is gradually weakening. As a result, the establishment of \"time banks\" in rural areas has become a feasible option. Through the construction of rural \"time bank\", to create a rural pension service team, solve the problem of manpower shortage, to realise the desire of the rural elderly to age in place, and at the same time, enrich the pension resources in rural areas, improve the quality of rural pension, and provide experience for exploring a new model of old age pension in China, and provide a new idea for building a healthy aging society in China. It provides new ideas for China to build a healthy aging society. In 1973, Ms. Asahiko Mizushima founded the world's first labour bank, VLB (VolunteerLabourBank), also known as Volunteer Volunteer Bank, which is the prototype of the modern time bank [1] . In 1980, the famous American scholar Edgar S. Cahn formally put forward the concept of time banking and promoted it to the whole world [2] . Effie and Gritzas have concluded that \"time banking\" can motivate people to participate in voluntary activities and make new friends, which is beneficial to the improvement of the community . Effie and Gritzas in the value of time banking research, concluded that \"time banking\" can drive people to participate in volunteer activities, make new friends and help improve the community [3] . Domestic scholars Zhou Pengfei and Yu Hechuan believe that time banking can actively develop the human resources of the elderly in rural communities, and through mutual help among the elderly, it can become a feasible solution to alleviate the problems of the elderly in rural communities [4] . In terms of the factors influencing participation in time banking, Gaoxin Yu takes social exchange theory as the analytical framework, links two key indicators and compares three different types of government involvement in time banking, and finds that time coin exchange standards, incentive mechanisms, policies and information norms are important influencing factors [5] . In terms of solution paths, Wtitham and Clarke argue that a time bank is an intermediary organisation that can provide service transactions for its members, who can earn \"time credits\" by participating in services in the present in order to satisfy their future service needs [6] . Professors Wu Zhendong and Chen Gong use the third allocation as an entry point to bring four insights into optimising the path of time banking: expanding the supply of resources, allocating resources in an orderly manner, government-led co-ordination and establishing a system of guarantees [7] .\nTo sum up, domestic scholars' research is mainly limited to the functions and values of \"time banking\" and its operation mechanism in combination with specific case studies. The literature on the factors influencing the willingness to participate in \"time banking\" is relatively small. This study will summarise the research of other scholars and combine qualitative and quantitative research to study the willingness to participate in time banking and the influencing factors of the elderly in Linyi, so as to supplement and enrich the research and analysis of the willingness to participate in time banking and the influencing factors of the elderly in domestic time banking. The study analyses the influencing factors of the elderly's willingness to participate in the domestic \"time bank\" and their influence factors.\n---\nCurrent situation of old age in Linyi\n\nThe aging of the population is deepening. By 2020, Linyi will have a rural population of 1,261,800 people aged 60 and over, accounting for 27.7 per cent of the population, of which 18.6 per cent will be aged 65 and over, exceeding the indicators for an ageing society by 10 per cent and 7 per cent respectively. The increase in the elderly dependency ratio from 13.7 per cent in 2010 to 18.44 per cent in 2020 shows that Linyi is facing the risk of population ageing from another perspective, and \"time banking\" provides a new way of thinking to solve the ageing problem.\nThe function of the family as an old-age household has weakened. Data from the seventh population census show that there are 3,899,700 family households in Linyi, with an average of 2.73 people per household, a decrease of 0.21 from the 2.94 people in the 2010 sixth national population census. The increasing miniaturisation of farming families has increased the pressure on children to provide for their old age, and the burden of old age has increased, so that the phenomena of \"paedophilia\" and \"the difficulty of raising children to provide for the elderly\" have occurred from time to time, and have become a social problem.\nThe supply and demand for institutions is insufficient, and according to Linyi's \"9055\" target system for elderly care services, 5 per cent of the elderly should live in elderly care institutions for centralised institutional care. Therefore, the number of elderly people over 60 years old in Linyi who choose to retire in institutions should be 106,200 people. However, according to the statistics of the Civil Affairs Bureau of Linyi City, the number of beds in Linyi City's old-age institutions that have been built and put into use is only 25,344, which is less than a quarter of the number of people who should be admitted to the institutions. With strong policy support, Linyi City, Shandong Province, actively responded to the national policy, and the Civil Affairs Bureau of Linyi City issued the \"Linyi City National Pilot Pilot Funding Guidance Programme for the Reform of Home and Community-Based Pension Services\" in December 2021, which provides financial support for the development of \"time banks\" in Linyi City, and clearly stipulates the construction standards of time banks and the recruitment requirements for volunteers. It also specifies the standards for time bank construction and volunteer recruitment requirements, providing guidance for the development of time bank pilots in Linyi.\nThe construction of elderly care facilities is complete, and the 2023 Linyi government work report recorded that Linyi City has strengthened the modernisation of rural areas, guaranteed that 143,000 households out of poverty will not return to poverty, renovated 1,341 rural dangerous houses, 159,000 rural clean heating, and reconstructed 1,961 kilometres of rural roads, which has provided an infrastructure to guarantee the countryside for elderly care in the countryside after that, facilitating the volunteers to carry out their service work. In addition, Linyi City has also strengthened the construction of rural medical teams, with a total of 11,870 rural doctors.\n---\nResearch design and data analysis of \"Time Bank\" in Linyi City\n\n\n---\nStudy design\n\nIt is known from the theory of planned behaviour proposed by Icek Ajzen that perceived behavioural control, subjective norms, and behavioural attitudes can influence the willingness to participate. Through the collation and collection of references, it is found that many researchers at home and abroad have applied the theory to the direction of elderly care and obtained relevant conclusions. Secondly, Davis's Technology Acceptance Model (TAM) proposes that behavioural intention, perceived usefulness and perceived ease of use can effectively predict testers' behavioural decisions. Finally, based on the current situation of rural development in Linyi, this paper combines the previous research and experimental results of many times to confirm that the degree of cognition is an important part of the influencing factor model.\nThe factors influencing the willingness of the elderly-to-be to participate in \"time banking\" are divided into seven variables with a total of 22 items. In order to ensure the reliability and validity of the sample data, this paper adopts the Likert scale to arrange, in addition to drawing on the existing items at home and abroad, but also adjusted according to the actual situation in Linyi City, and finally integrated into 22 items. The scale mainly includes seven dimensions: cognitive degree, behavioural attitude, perceived ease of use, perceived usefulness, subjective norms, perceived behavioural control and willingness to participate. Li Xinjian (2021) I believe that I have the energy and stamina to participate in the Time Bank and provide services.\nI believe I have the capacity to participate in the Time Bank and provide services.\n---\nWillingness to participate\n\nFor the time being, I'm willing to participate in the Time Bank. Manzira Elken (2023) I'm willing to participate in the Time Bank. If it's possible in the future, I'd like to participate in the Time Bank.\n---\nData analysis\n\nIn this paper, SPSS 26.0 was used to test the reliability of 8 variables and 22 question items. According to Table 2, the overall reliability of the questionnaire is 0.891, which indicates that the Richter scale has a good unity and meets the criteria of the reliability test. As can be seen from Table 3, the KMO value of the data in this paper is 0.831 greater than 0.8, indicating a strong correlation between the factors. Also the significance is 0.000 < 0.05, the validity of the questionnaire is good for follow-up research. popularised. Q4-Q6 belong to the category of behavioural attitudes, and the number of people who believe that \"time banking\" is valuable and worth promoting accounts for more than 70%-85% of the total sample, indicating that the vast majority of the elderly in rural Linyi City agree with the \"time banking\" model. Q7-Q9 belongs to the category of perceived usefulness, 78% of the elderly believe that time banking can relieve the pressure of old age, and 82% believe that it can help them better participate in social life, which further indicates that the elderly in rural areas of Linyi City need the \"time banking\" model of mutual support. Q10-Q11 belongs to the category of perceived ease of use. Q10-Q11 belongs to the category of perceived ease of use, 68% of the elderly believe that they need to be trained to participate in time banking, which indicates that there is a need for skilled caregivers in Linyi, and there is a need to set up a training school to improve the level of service and to expand the team of caregivers. 64% of the elderly in Q12-Q16 belong to the category of subjective norms and 64% of the elderly will be influenced by their family members in deciding whether or not to participate in the \"Time Banking\" model. \"The remaining influencing factors are the attitude of neighbours, the degree of concern of the village committee and the organisational ability of the village committee, which account for 31%, 43% and 47% of the sample respectively, indicating that most of the elderly people's willingness to participate is affected by the environment, and that it is necessary for the government to create an environment and public opinion propaganda to increase the motivation of the elderly people's participation.Q17-Q19 belongs to the category of perceptual control behaviours, and 46 per cent of the elderly people have enough time for participation. 46 per cent of the elderly have enough time to participate and provide services, 54 per cent of the elderly think they have enough energy and physical strength to participate in the Time Bank, and 57 per cent of the elderly think they have the ability to provide services.Q20-Q22 belongs to the category of willingness to participate, and the majority of the elderly are willing to participate in the Time Bank. Q20-Q22 belong to the category of willingness to participate, currently willing to participate in the \"time bank\" accounted for 42% of the sample, 30% said they were not sure, 21% said they were not willing to participate at present, and through the understanding of willingness to participate in 75%, and the number of people who are likely to participate in the future increased to 88%, which shows that the participation of older people in the \"time bank\" will increase with the awareness of the \"time bank\". It can be seen that older people's participation in \"time banking\" will increase as their level of awareness increases. In summary, the willingness of older persons to participate in the time-banking model of mutual support for the elderly is affected by the degree of cognition, behavioural attitudes, perceived usefulness, intuitive behavioural control and subjective norms. Therefore, to alleviate the pressure of old age and increase the participation of the elderly in \"time banking\", we need to start from these five aspects, solve the problems of insufficient cognition and low motivation of the elderly to participate, and make use of publicity to cultivate a favourable old age environment, increase the motivation of the elderly to participate, and solve the problems of old age in the countryside of Linyi City.\n---\nConclusions and recommendations\n\n\n---\nConclusion\n\nIn the context of aging, \"time banking\" is a useful supplement to the existing pension services, providing new ideas for the pension service system, which is conducive to enriching the pension mode and realising positive aging. This paper uses SPSS to analyse the reliability and validity of the data, and draws the following conclusions: attitude, perceived usefulness, perceived behavioural control, subjective norms have a positive effect on willingness, and perceived usefulness has a positive effect on attitude. Based on the conclusions of the study, we put forward suggestions to improve the willingness of the elderly in Linyi to participate in \"time banking\" from four aspects, hoping to make a certain contribution to the construction of \"time banking\" in Linyi City.\n---\nRecommendations\n\nIn recent years, Linyi City has been facing greater pressure on elderly care, and the level of aging has been increasing. It has already entered the ranks of a moderately aging society, with a relative shortage of pension services. In order to build a \"time bank\" in Linyi, accelerate the construction of pension facilities and teams in the region, alleviate the pressure on pensions in Linyi due to a lack of human resources, improve the city's ability to govern, and reduce the pressure on the municipal government, the following suggestions are put forward:\n(1)Suggestions for improving awareness and participation: Cultivate the spirit of exploration among the elderly, improve their ability to accept new things, strengthen the publicity of \"time banking\" by the government, focus on the usefulness of the publicity, and emphasise the value of \"time banking\" to improve the level of awareness and to transform the prospective elderly's preferences. The government should strengthen the publicity of \"time banking\", focus on the usefulness of publicity, explain the value of \"time banking\", raise the level of awareness, change the preferences of the elderly for old-age care, and cultivate the awareness of old-age care.\n(2)Suggestions for improving subjective norms: The Government and village committees should strengthen the publicity of the \"time bank\" online media, create \"mutual help groups for the elderly\", and work together to create a favourable atmosphere in society.\n(3)Suggestions for improving perceptual behavioural control: Standardized training for volunteer groups of prospective older persons at the grass-roots level to learn basic medical knowledge, including first aid and life care. At the same time, the prospective elderly should improve their physical fitness and advocate physical exercise, which is not only conducive to their physical health, but also to their participation in mutual aid services.\n(4)Suggestion for improving perceived usefulness: The village committee should implement various incentives to provide material and spiritual rewards to the elderly who participate in the Time Bank, so as to improve the perceived usefulness and motivate the elderly to actively participate in the programme.",
        "Background\n\nHuman Immune Deficiency Virus (HIV) can be transmitted from an HIV infected mother to her child during pregnancy, labor or delivery, and breastfeeding. The aforementioned mode of transmissions is collectively known as mother-to-child transmission of HIV or vertical transmission of HIV [1][2][3]. Preventing HIV transmission in pregnant women and their children often referred to as the prevention of mother-to-child transmission [4,5]. Mother-to-child transmission of HIV remains a significant problem in the low and middleincome countries, despite the development and growing availability of effective prevention methods for resourcelimited settings [6,7].\nMale involvement in sexual and reproductive health has recently been recognized as new strategy for enhancing maternal and child health by playing a role in preventing women's risk of acquiring HIV, but also in terms of her utilization of the PMTCT program: for the mother to test for HIV, to return for the result, for the couple to use condoms, to receive medication, and to increase adherence to proper infant feeding practices [7,8]. Ethiopia has adopted the global target for PMTCT which is nullifying mother to child transmission by the year 2030 [9]. For the aforementioned ambitious goal, male involvement is highly demanded especially in low and middle-income countries where the community is patriarchal.\nHowever, male involvement in the PMTCT program remains low in low and middle-income countries, including Ethiopia [10,11]. For instance, a study conducted in the Myanmar region of Asia revealed that male involvement in PMTC is 13% [12], A Brief Review of Initiatives in East, West and Central Africa showed that male involvement in PMTCT was low with a range from 1.8 to 32% [13]. Another systemic review conducted in Ethiopia showed that male involvement in PMTCT was 14 to 30% [11]. Recent studies on the topic suggest that there were several reasons for the low level of male involvement, including cultural barriers and norms, PMTCT/ANC knowledge, sociodemographic characteristics, male individual factors and health system [13][14][15]. This low level of male involvement in the prevention of mother to child transmission of HIV results in pregnant women not to get tested for HIV [16]. Those who dared to go for a test, if tested positive were afraid to disclose their serostatus to their husbands because they thought their husbands would accuse them of infidelity, face divorce and violence, some were not even allowed to continue with PMTCT interventions and increase in maternal to child transmission of HIV - [17].\nEthiopia has made major efforts through the scaleup of the PMTCT program, including reinforcing messages in well-designed community mass media campaigns, create an opportunity for constructive dialogue between men and women, forming women and male health development army and making services more male-friendly [18]. Few studies have been conducted from the male point of view for their low participation in PMTCT in Ethiopia [19][20][21]. In the study area, there is no study conducted so far on male involvement in PMTCT that could be attributable to the male involved in the uptake of PMTCT service. Therefore, this study would address this gap by assessing the involvement of male partners in PMTCT and its barriers at a community level in Enebsiesarmider District.\n---\nMaterials\n\n\n---\nStudy area and study period\n\nThe study was conducted in Enebsiesarmider district Northwest Ethiopia from February 10-30/2018. The administrative center of this district is Mertolemariam, which is located 364 Km to the North of Addis Ababa, the capital city of Ethiopia. Based on the 2007 census population projection, the total population of the district was 162,132, of which (male accounts 78,481 and female accounts 83,651. Regarding the health service, the district has 1 primary hospital, 8 health centers, 38 health posts, 4 drug vendors and 5 private clinics. The annual report from Enebsiesarmider District office in 2017 indicated that the health coverage of institutional delivery was 62%, ANC coverage 67.6% and PMTCT uptake 69%.\n---\nStudy design and study population\n\nThe community-based cross-sectional study design was conducted to assess male involvement in PMTCT and associated factors among males whose wives have given birth in the last six months before the survey. All males whose wives had ANC follow up during their last pregnancy and have lived in the study area for at least six months were eligible for the study.\n---\nSample size determination and sampling procedures\n\nThe required sample size for this study was calculated by using single population proportion formula: With assumption of 95% of confidence interval, margin of error tolerated (0.05), the proportion of male involvement in PMTCT 0.309 [19], non-response 10% and design effect 1.5 to accommodate for intracluster variability, the maximum sample size was 542.\nA stratified cluster sampling technique was employed to recruit study participants. The strata are urban and rural administrative units of the study area while the cluster is kebele (the smallest administrative unit in Ethiopia). The district has 37 kebeles (33 rural and 4 urban). The first sample size was proportionally allocated to urban and rural kebeles which results in 61 urban and 481 rural male allocations. Then the required numbers of kebeles that can accommodate the allocated sample to each stratum were selected by a simple random sampling technique. These results in 1 urban and 4 rural kebeles to be involved as actual data collection sites.\nAfter that, the number of males whose wives gave birth in the last six months found was taken from a family folder which was documented by the health extension workers (HEWs) with their household. Finally, all males whose wives gave birth in the last six months in selected Kebles were studied.\n---\nOperational definitions\n\n\n---\nMale involvement\n\nMale involvement Was measured using 'Yes', 'No' questions, Respondents were asked \"did you know your wife's ANC appointment the last time she was pregnant\"; respondents who respond \"yes\" will score 1 and \"No\" response will earn zero scores. The same pattern of questioning and scoring was made for the rest of the five PMTCT male involvement questions. According to the aforementioned statements, the involvement score for each respondent could range from 0 to 6. A total score of 4-6 was considered as \"involved in PMTCT\" and scores of 0-3 are labelled as not involved in PMTCT [22].\nCultural barriers Was measured using five-point Likert scale questions, Respondents were asked \"did you believe PMTCT information should first be given to men than to women\"; respondents who respond \"very disagree\" will score 1, \"disagree\" will score 2, \"undecided\" will score 3, agree will score 4 and \"very agree\" response will earn 5 score. The same pattern of questioning and scoring was made for the rest of the nine cultural barriers items. According to the aforementioned statements, the score for each respondent could range from 10 to 50. Finally, respondents who have scored greater than or equal to the mean value of the items have 'High cultural barriers' and those who have scored less than the mean value of the items have \"Low cultural barriers' [23].\nHealth system barriers Was measured using five-point Likert scale questions, Respondents were asked \"did you believe antenatal clinics should be opened on weekends and evening for men to attend these clinics with their partner\"; respondents who respond \"very disagree\" will score 1, \"disagree\" will score 2, \"undecided\" will score 3, agree will score 4 and \"very agree\" response will earn 5 score. The same pattern of questioning and scoring was made for the rest of the seven health system barriers items. According to the aforementioned statements, the score for each respondent could range from 8 to 40. Respondents who have scored greater than or equal to the mean value of the items have 'High health system barriers' and those who have scored less than the mean value of the items have \"Low health system barriers' [23].\nPMTCT knowledge Comprehensive knowledge of PMTCT was computed by summing up all relevant 10 knowledge items (item on ever heard about PMTCT Service, about MTCT of HIV transmission and prevention method). A correct answer for each item was scored as \"1\" and an incorrect answer was scored as \"0.\" Items were then summed up and converted into 100%. Accordingly, those respondents who scored greater than 60% of knowledge assessment questions were thought of as having Good knowledge and those respondents who answered less than 60% of knowledge assessment questions were thought of as having Good knowledge [24].\n---\nSocio-demographic factors\n\nThe demographic factors are age, education level, occupation, religion, residence, and income.\n---\nData collection tool and procedure\n\nThe data collection tool was developed by reviewing different work of literature [19][20][21] and it consists of five parts sociodemographic characteristics, knowledge related factors, cultural and programmatic barrier related factors, male individual-related factors and level of male involvement in PMTCT. The English language questionnaire was translated into Amharic language (language spoken in the study area) by an Amharic language speaker who has attended the Master of Arts in Amharic language and was translated back to English language by a person who attended Master of Arts in English language and comparison was made on the consistency of the two versions. The questions are both open and close-ended. Cultural and programmatic barrier assessment tools were adopted from a study done in Uganda and central Ethiopia [22,23]. Cultural barriers were assessed by ten reliable items (Cronbach's alpha = 0.903) and a programmatic barrier was assessed by eight reliable items (Cronbach's alpha = 0.810) to ensure the reliability of the scale [24].\nThe data were collected by 10 grade 10 completed students who are fluent in the local language (Amharic) and familiar with the study area. The data collection process was supervised by three BSc holders (Midwives). Data were collected via house to house visit by trained data collectors; For respondents who are not available at home at the first visit, a revisit was arranged at a minimum of three times.\n---\nData quality control\n\nTo assure the quality of the data, a structured and pretested questionnaire was used. Two days of training were given for data collectors and supervisors about techniques of data collection, instruments, ethical issues and purpose of the study. Intensive supervision was done by the principal investigator and supervisor. The collected data were checked for completeness, accuracy, and consistency throughout the data collection period.\n---\nData processing and analysis\n\nThe data were coded, cleaned, edited and entered into Epi data version 4.2 and exported to SPSS window version 24 for analysis. Descriptive results were presented using tables and figures. Model fitness was checked using a Hosmer-Lemeshow goodness-of-fit test. Crude odds ratios with their 95% confidence intervals were estimated in the bi-variable logistic regression analysis to assess the association between each independent variable and outcome variable. All variables with P \u2264 0.25 in the bivariate analysis were included in the final model of multivariate analysis to control all possible confounders. The adjusted odds ratio with 95% CI was estimated to identify the factors associated with male involvement in PMTCT using multivariable logistic regression analysis. The level of statistical significance was declared at P-value< 0.05.\n---\nResult\n\n\n---\nSocio-demographic characteristics of study participants\n\nA total of 525 study participants were involved in this study, making a response rate of 96.9%. The mean age of study participants was 35.7 (SD \u00b1 6.4 years). All, 525 (100%) of respondents were Orthodox followers by religion and Amhara by ethnicity. More than four-fifth, 466 (88.8%) of male partners were from rural areas. More than two fifths, 230 (43.8%) of the respondents were unable to read and write. The majority, 454(86.5%) of the study participants were farmers by their occupation. Regarding the family wealth index, nearly two-fifth 200 (38.1%) of households were 2nd quintile wealth range (See Table 1).\n---\nPMTCT and ANC knowledge of respondents\n\nRegarding PMTCT knowledge of participants, two hundred forty-five (46.7%) of them had good knowledge. Concerning ANC knowledge of respondents, it was calculated by summing up seven ANC questions and the mean value was calculated. Finally, those respondents scored mean and above were categorized as having good knowledge about ANC. Almost one third, 185(35.2%) of them had good ANC knowledge (Fig. 1).\n---\nCultural and health system variables of study participants\n\nRegarding cultural barriers (factors relates to men's opinion, perception, and the role that hinder the involvement of males in PMTCT), the mean sum scores of cultural barriers range from 10 to 46 with a median of 30 \n---\nMale individual variables\n\nAmong respondents, 225 (42.9%) of them had a fear of disclosure of HIV results to their wives. Almost two fifths, 211 (40.2%) of the participants were bothered to know what goes on in ANC. Only fifty-three (10.1%) of the respondents had gone together for VCT with his wife. More than three-fifths of respondents, 329 (62.7%) knew their HIV serostatus.\n---\nLevel of male involvement in PMTCT\n\nOne hundred sixty-two (30.9%) of the 525 men had attended ANC with their partners. Nearly one fourth, 123(23.4%) of respondents were new their partners ANC appointment. Most of them, 471(89.7%) provided financial support to their spouses to attend ANC. Almost half of the respondents, 276 (52.6%) were willing to use condoms during sexual intercourse for PMTCT of HIV. Among the total respondents only 137 (26.1%) of them, had male involvement in PMTCT.\n---\nFactor associated with male involvement in PMTCT services\n\nMultivariable analysis showed that the odds of male involvement in PMTCT were \n---\nDiscussion\n\nThe overall male involvement in PMTCT was found to be 26.1% [95%CI, 22.1-29.5]. This finding is quite low compared to what it should be. This finding implied that there is an unfinished agenda that district health office and local health care providers could work together to achieve full-scale male participation in PMTCT. It calls for urgent action to incorporate the important role of the male in preventing mother to child transmission of HIV in the study area specifically and in Ethiopia in general. This finding is in line with a study report from Eastern Uganda, Addis Ababa, Ethiopian and Tanzania [22,23,25]. This study was slightly higher than a study conducted in Gonder and Eastern Oromia Ethiopia [20,26]. The difference could probably be explained by the difference in the time gap as better attention has been given to male involvement in PMTCT these days, improvements in the health care systems because the Ethiopian government gives great emphasis on maternal and newborn health which leads to higher involvement in PMTCT. The finding in this study indicated that there is already a crated platform for male participation in the study area, and this could serve as a springboard to achieve full-scale male participation in PMTCT in Enebsiesarmider district. However, this finding is lower when compared with the study from Nepal, Thailand, and South Africa [26][27][28][29] and Hadiya (Lemo District), Arbaminch Ethiopia [19,21]. The discrepancy of these findings might be attributed to the difference in methods used and study settings, sociodemographic characteristics of the study participants, and availability and accessibility of health service infrastructures. This finding implies that there is a gap that the district health office and regional health bureau could work in collaboration with the local health care provider to enhance the involvement of husbands in the prevention of mother to child transmission.\nMales who had primary, secondary and above education were 2.02 and 2.45 times more likely to be involved in PMTCT than those who had no formal education respectively. Similar studies in Uganda, Nepal, Gonder, Arbaminch, and Debremarkos Ethiopia elsewhere have found that education level is an important factor of involvement in PMTCT services [20-22, 27, 30]. This may be related to as people more educated; they could easily understand both the transmission and prevention methods of HIV from mother to child. Moreover, educated males will have better awareness about the benefits of preventive health care including PMTCT and higher receptivity to new health-related information. Overall, as males receive more education, the chance may be higher because they could understand their role in sexual and reproductive health.\nIn this study male who had good knowledge about the PMTCT program were 2.57 times more likely involved in PMTCT than those who had poor PMTCT knowledge. This is consistent with the results of a study done in Arbaminch, Gonder Ethiopia and Tanzania, [20,21,25]. The possible explanation for this might be having good knowledge about PMTCT will help the partners to know the benefit of the PMTCT program, to involve in a discussion about safer sexual practices and assisting HIV positive pregnant women to get clinics. And as the male partners had good PMTCT knowledge about when does HIV transmission occurs from mother to baby, risk factors that increase the risk of HIV transmission during pregnancy and PMTCT core interventions the husband would have involved in PMTCT.\nMales who had good knowledge of ANC services were 2.10 times more likely involved in the PMTCT program than those who had poor ANC knowledge. This finding is similar to a study conducted in Arbaminch, Ethiopia [21]. This could be since male partners who had good ANC knowledge will have information that could assist them in making decisions regarding healthy behaviors including sexual and reproductive health education and promotion.\nCultural barriers were also found to be hindering male involvement in the PMTCT program in the study area. The finding of this study was comparable to the findings of studies conducted in Lemo District Ethiopia, Uganda and Tanzania [19,22,25]. In this study, those who had low cultural barriers were 2.20 times more likely involved in PMTCT than those who had high cultural barriers. The possible explanation might be those males who had low cultural barriers can accompany their partners during all maternal and child health services, communicate with their wives freely about the service obtained from the PMTCT program. Tanzania, a social and religious norm prohibited males from attending female health services and the widespread attitude that female reproductive health is not male responsibility and found to inhibit male involvement in PMTCT [31]. And also males who had cultural barriers could perceive themselves at risk or have an experience that increases the risk of acquisition of HIV they become more discouraged to join hands with a partner in a PMTCT endeavor.\nRespondents who had low health system barriers were 2.40 times more likely involved in PMTCT services when compared to their counterparts. The findings of the study are consistent with the study findings from Uganda, Lemo District and, Debremarkos Ethiopia [19,22,30]. The possible justification for this could be those males who had low health system barriers will have the more a man comes in contact with the health facility with his partner that helps male partners to have information about PMTCT.\n---\nStudy limitations\n\nDue to the cross-sectional nature of this study, establishing a true cause and effect relationship between adherence status and associated factors would be impossible. This study might also suffer from recall bias.\n---\nConclusion\n\nThe level of male involvement in PMTCT programs in Enebsiesarmider District, Northwest Ethiopia was low (26.1%) in 2018. Having secondary education and above, good knowledge of PMTCT/ANC, a high level of cultural and health system barriers were significantly associated with male involvement in PMCT. Therefore, the regional health bureau, district health office in collaboration with local health care providers should work on coordinated and targeted IEC and BCC programs to convince male partners to utilize PMTCT service and develop a strategy for community mobilization. Furthermore, enhancing male involvement through creating a husband's knowledge regarding the merit of prevention of mother to child transmission through the provision of adequate information for all male partners at the ANC clinic is recommended.\n---\nAvailability of data and materials\n\nThe full data set and other materials about this study can be obtained from the corresponding author on reasonable request.\n---\n\n\nAbbreviations ANC: Antenatal Care; HEW: Health Extension Worker; PMTCT: Prevention of Mother to Child Transmission; SPSS: Statistical package for social science; WHO: World Health Organization Authors' contributions HA: Conceives the research project, develops a proposal, supervised the data collection process, conduct the analysis and wrote the manuscript. NA: involved in proposal development, data analysis and wrote the manuscript. BM: involved in proposal development, data analysis and wrote the manuscript. AD: involved in data analysis, result writing and wrote the manuscript. All authors have read and approved the manuscript.\n---\nEthics approval and consent to participate\n\nEthical clearance was obtained from Haramaya University College of Health and MedicalSciences Institutional Health Research Ethical Review Committee with reference number IHRERC/024/2018. An official letter was sent to the Enebsiesarmider district health office and the data collection was begun after permission and cooperation letter was written to all kebeles on which the study was carried out. The study, purpose, procedure and duration, possible risks and benefits of the study were clearly explained to the participants using the local language. Then, individual informed written consent was taken.\n---\nConsent for publication\n\nNot applicable.\n---\nCompeting interests\n\nThe authors declare that they have no competing interests.\n---\nPublisher's Note\n\nSpringer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.",
        "Introduction\n\nU niversity students frequently struggle with their mental health as they navigate the transitional life stage from adolescence to adulthood within the university context (Anderson-Fye and Floersch 2011;Condra et al. 2015;Council of Ontario Universities 2017;Kessler et al. 2005;Lester 2011;Linden, Grey, and Stuart 2018;Ontario University and College Health Association 2017;Stallman 2010). I explore how the social context of the North American university influences these experiences of mental health struggles, revealing how the conflation of productivity and mental health in the university context leads to the stigmatization of unproductive students as \"lazy\" and not meeting normative expectations about what it means to be a \"good\" student. In exploring how normative expectations of constant productivity impact students' phenomenological experiences of mental health, I expand on anthropological understandings of mental health stigma as a sanction on the unproductive in neoliberal society (Grinker 2020). In this article, I introduce the conceptual contrast between \"Student Wellness\"-academic success-and \"Human Wellness\"-subjective well-being-as a means of understanding how university attempts to increase wellness often support neoliberal agendas to the detriment of their students' well-being.\nOver the last decade, the number of post-secondary 1 students with diagnosed mental disorders in Ontario has more than doubled (Condra et al. 2015;Council of Ontario Universities 2017). Compared to other groups in Ontario, postsecondary students are more likely to experience symptoms of a mental disorder, a major concern considering 83% of Ontario's youth-defined as individuals between the ages of 18 and 24 years-participate in postsecondary education (Ontario University and College Health Association 2017). Understanding the constructs of mental health embedded in the university context is important because they do not neutrally describe an experience, but are patterned by normalized expectations about what it means to be a \"good\" student to inscribe mental health with particular meanings (Beatty 2016;Kirmayer 1994;Scheper-Hughes and Lock 1987;Szasz 1979). While many interdisciplinary studies explore issues of student mental health as biomedical phenomena, in order to fully understand student experiences of mental health struggles in the university we need to not only look at the \"disease\" aspect of these struggles (Helman 1981;Kleinman, Eisenberg, and Good 1978;Kleinman 1978) but also how students experience them phenomenologically in their day-to-day lives as students. I use an ethnographic, phenomenological perspective in this study to contextualize my participants' experiences with mental health struggles as a single part of their lives where they fulfil other non-pathologized identities and social roles (Whyte 2009), such as that of the student. Critical phenomenology, while foregrounding experiential knowledge and individual subjective concerns (Bindhulakshmi 2012;Mattingly 2019;Raphael 2015;Tran 2017;Desjarlais and Throop 2011) articulates with the ethical and political context in which the experience occurs to create a more holistic picture (Mattingly 2019). Thus, by approaching student mental health phenomenologically while also taking note of the greater social structures at play within the university, I move between macro-level abstractions, such as neoliberalism, to how these forces shape micro-level, individual experiences.\nIn this article, I use the concept of neoliberalism to critique the entangled social and economic order (Ortner 2011) which promotes the liberal economic policies of open market competition and the privatization and deregulation of social services through the promotion of certain social values such as competition, extreme individualism, and hyper-productivity, and which positions the individual as entirely responsible for their own holistic well-being (Dolmage 2017;Ganti 2014;Nishida 2016). This system privileges corporate profit over human welfare, producing and maintaining systemic inequalities and barriers. Neoliberalism is an \"ideology of governance\" capable of molding subjectivities, as well as a structural force affecting peoples' lives (Ganti 2014). In the anthropological literature on mental health, neoliberal critiques have been used in the latter sense to analyze pharmaceutical companies (for example, Bergey, 2017;Cohen, 2017;Jenkins, 2010;Tseris, 2017), insurance companies (for example, Grinker, 2007;Lester, 2011;Young, 2001) and systems of communitycare in the United States (for example, Luhrmann, 2000) for their orientation towards economic profit over best practices of care and their placement of total responsibility for recovery on the patient's compliance. Here, I am primarily concerned with neoliberalism as an ideology, especially as it represents a social order-entailing particular values and normative expectations-within which the university operates (Dolmage 2017;Ganti 2014). Previous work in anthropology and disability studies suggests the normative North American university student is faced with expectations arising from this neoliberal social order (Dolmage, 2017;Geert Van Hove et al., 2014;Lester, 2011;Martin, 2007). The competitive environment of the university, reflecting that of the open market, may be understood as a site of \"natural selection\" where only the \"fittest\" are meant to \"survive\" (Dolmage 2017;Geert Van Hove et al. 2014). In the university environment, neoliberal values are reflected in the normative expectations that students are able-bodied, healthy, personally responsible, and able to follow a typical scholastic trajectory involving uninterrupted productivity (Dolmage 2017;Geert Van Hove et al. 2014;Gordon 2019;Lester 2011;Martin 2007). Students who struggle are seen as not up to the challenge and their failure is blamed on their individual traits rather than on the structural failings of the university (Christie et al. 2008;Dolmage 2017;Nishida 2016). In this article, I explore how these previously elucidated expectations, in particular hyperproductivity, are used to socialize students into appropriate neoliberal citizenship within the university, patterning student understandings and experiences of mental health.\nI argue that academic productivity as a value-rewarded with validation often in the form of grades-socializes students into this system where their constant economic productivity is demanded. While the academic work of undergraduates may not directly contribute to the economy, the normative expectation that they are hyper-productive is set by faculty whose academic productivity does place them in competition with others for their \"economic survival\" through granting and tenure processes (Nishida 2016). Economic productivity maps onto the university context by the ways in which knowledge becomes a commodity and universities focus on \"products\" or \"outcomes\" rather than learning processes, and further how this system socializes students into understanding their worth as a person as deriving from their productivity (Nishida 2016;Blum 2016).\nAnthropologist Roy Grinker (2020) has theorized that stigma as it relates to mental ill-health can be understood as one way in which individuals are socialized into hyper-productive neoliberal persons. Whereas Goffman's theory of stigma presents stigma as \"natural\" and places the burden for \"managing spoilt identity\" on the individual (Goffman 1968), Grinker (2020) draws from disability studies to theorize stigma as a social structure acting to sanction certain types of already existing people, establishing a socially constructed \"normal.\" Grinker (2020) argues that stigma as it pertains to mental illness has arisen out of the historically embedded structural conditions of modern capitalism and the creation of the ideal modern worker under neoliberal ideology. He contends that the concept of \"mental illness\" stigmatizes the economically unproductive, allowing them to be isolated and penalized. We might understand this as a specific instance of Scheper-Hughes and Lock's concept of the \"double illness,\" which contends that social and cultural responses to suffering, such as stigma, amplify the original suffering to create secondary negative experiences beyond the original \"symptoms\" (1986). As Nishida (2016) discusses, academics often experience guilt and shame due to self-stigma in association with not meeting their expectations of hyperproductivity. I explore in this article how similar stigmatization processes play out for students, increasing suffering while socializing them into the neoliberal social order.\nThe qualitative research I conducted with students at an Ontario university reveals that neoliberal values and expectations, particularly those pertaining to productivity, produce a social context which patterns my participants' phenomenological experiences with and understandings of mental health struggles. When reflecting on these experiences, students tend to define their mental health by their academic productivity levels rather than by their subjective well-being. For students, being mentally well means having the energy and motivation necessary to meet the neoliberal expectation that they maintain constant productivity. University attempts to increase wellness also reproduce this understanding, focusing on academic success over subjective markers of student well-being. Ultimately, these understandings of wellness pattern student experiences of mental health struggles as they attempt to maintain the image of the \"good student\" by prioritizing academic productivity over subjective well-being, further damaging mental wellness in a cyclical fashion.\n---\nMethodology\n\nThis study was conducted entirely online due to shifting public health recommendations during the COVID-19 pandemic. 2 I performed over 5.5 hours of online content research using the University of Guelph (UofG) Wellness Website, 14.5 hours of participant observation at online wellness events/ workshops run by UofG, and conducted 24 online, semi-structured interviews, averaging 40 minutes (a range of 23 to 56 minutes) with current UofG students between the ages of 18 and 24 years who self-identify as experiencing mental health struggles.\nThe Government of Canada defines mental health as [t]he capacities of each and all of us to feel, think, and act in ways that enhance our ability to enjoy life and deal with the challenges we face\u2026 a positive sense of emotional and spiritual well-being that respects the importance of culture, equity, social justice, interconnections, and personal dignity (Minister of Public Works and Government Services Canada, 2006); however, I define \"mental health struggles\" in this study as any self-perceived lack in any of the areas described above that cause an individual distress and/ or a negative sense of well-being. Importantly, this is considered a subjective and self-identified experience not limited to those with a biomedical diagnosis of mental illness. By focusing on those who self-identify with a broad term such as mental health struggles, rather than diagnosable mental disorders, I draw from varied student perspectives, including those who have been missed in the interdisciplinary literature on student mental health due to their lack of biomedical diagnoses (Condra et al., 2015;Kranke et al., 2013;Stein, 2013).\nIn the end, 13 of my 24 participants (54%) had a biomedical diagnosis for their struggles, while 11 did not (47%). Two of the 11 participants without a diagnosis were in the process of gaining a diagnosis, and a third had an \"unofficial\" diagnosis from a counselor (rather than a medical professional or psychologist). Similarly, two of the participants with a diagnosis were unsure of the validity or value of their diagnosis either because they were unsure of the qualifications of the individual who diagnosed them or because they did not believe 30 minutes was long enough for someone to make an appropriate evaluation of their struggles.\nRegarding demographics, 60.3% of UofG students are women (University of Guelph 2020), and it is anecdotally observed that the majority of students are White. It is important to note that this demographic data is reflected in this study; most of my participants were White (or White-passing) femininepresenting persons. 3 Nonetheless, the study also includes perspectives from people of colour, including men of colour, international students, and other masculine-presenting individuals. The age requirement for this study (18 to 24) was selected because it covers the age of onset for many diagnosable mental health disorders, allowing the study to capture students who may be struggling with their mental health for the first time (Anderson-Fye and Floersch, 2011), and because it controls for generational differences in beliefs about and experiences of mental health.\n---\nProductivity and Mental Health in the Canadian University Context\n\nStudent Understandings of Mental Health as Productivity I think right now there's this culture that promotes\u2026this glamorization of depression and anxiety and it's just become this very \"fashionable\" part of life. I think it kind of ties into people being like \"oh wow, I only got two hours of sleep yesterday I was so busy working, I was so busy grinding, the grind never stops,\" that kind of tagline. I think it's super unhealthy, but it's become very popularized as the way that we should be living our lives\u2026and then people are like \"oh wow I didn't eat that much today. I have only had one meal today.\" I heard that yesterday, and I was like why are you only eating one meal, that's so bad, please eat some food, drink some water, right? We have glamorized this not taking care of ourselves but still achieving things as this great feat when it's incredibly unsustainable and super terrible. (Rose) 4\nIn the context of the university students often understood their mental health as productivity, an understanding that frequently entrapped them in a cycle of poor mental health and low productivity, producing further negative effects on their subjective well-being. Above, Rose paints a rich image of the university social context in which students are experiencing mental health struggles such as feelings of depression and anxiety; this social context is one where making sure \"the grind never stops\" is prioritized over self-care, and not taking care of oneself is worn as a badge of honour. Rose's quotation suggests that subjective well-being is secondary to one's ability to be productive. When I asked students in this study how they knew when they were feeling mentally well or mentally unwell, many referred directly to the entangled concepts of \"productivity,\" \"energy,\" and \"motivation.\" Rose explains when she is mentally well, I just am generally more productive\u2026I have the motivation to do things, and accomplish things, and I also have the energy to do them\u2026 [Conversely,] when I don't feel mentally well, it's a lot of energy drainage, where I don't wanna get out of bed. I don't want to move. So, the opposite of that where it's like yeah, I have the energy, I have the motivation, I'm getting stuff done. And I also feel good about the things that I'm getting done.\nSimilarly, Marie defined her level of mental wellness by her ability to be \"productive,\" explaining I guess for me, it's kinda how much work I can get done. So, if I'm feeling really stressed out, then I'll sometimes just get this kind of block where I can't get any work done. If I'm productive then I'm not struggling to get stuff done, then that's how I know that we're having a good day.\nLily also refers directly to productivity when describing feeling mentally unwell, explaining that \"\u2026there are days where even if I don't know why I'm feeling a certain way, I know it'll still affect how I function that day and it'll affect my productivity\u2026I know I'm feeling mentally well when I'm not taking naps throughout the day. And\u2026when I'm productive.\" According to this construction of mental health, one is mentally healthy when one can get work done and maintain productivity throughout the day; phenomenologically these students feel motivated and energized when mentally well.\nFeeling energized was an important part of constructing mental health as productivity; for instance, Rose understands being \"mentally unhealthy\" as a state where \"\u2026you don't have energy.\" Equally, Ricky notes that mental wellness comes with energy: I tend to feel very energetic [when I'm mentally well], I'm down to do anything. I wanna go out, I wanna go, I don't know, play basketball. I'm always calling my friends like OK what's going on? I'm in a good mood, let's go. So, it's kinda that surge of energy that comes with being happy and being in a good mood.\nMatt provides a similar explanation when asked how he knew he was feeling mentally well: I think I definitely have more energy. That's one thing I noticed when I would be worrying about, whether it be an exam or anything, it took a lot of energy out of me. And the times where I was the most anxious, and did the most worrying, I slept the best\u2026thinking all day about this stuff does take energy out of you. You're not running laps or anything, but your mind is. And it was crazy, 'cause I've never slept that well, but then I wake up in the morning and I have a shitty day so obviously it wasn't [a good sleep].\nConcurrently with a lack of energy, students found they lacked motivation when they were struggling with mental health, tying back into the issue of \"productivity.\" Participant 1 5 explains \"[when] my mental health is a little lower I'm not quite as energetic, or necessarily talking as much. I may, when I look at tasks, think like, 'Ugh, I don't wanna do this,' or less motivated is a good word for that.\" This lack of motivation can lead students to struggle with procrastination, which is constructed as the opposite of productivity and is therefore associated with mental health struggles. Marie explains she knows she is struggling, \"If I can't seem to get any work done, if I'm procrastinating a lot because I am too anxious to do it \u2026 where just kind of looking at [the amount of work] makes you feel some kind of anxiety.\"\nWhile Marie makes note of struggles with motivation, she also highlights another part of this energy-motivation-productivity triad-perception of workload. Rose explains how her perception of tasks changes based on her mental health: I think definitely energy plays in. I'm less motivated to do things. Things start to feel a lot heavier\u2026Where I see a list of things I need to do and I'm like \"Wow, this is going to take me forever,\" I don't want to do any of these things. And I also tend to not know where to start, and I get overwhelmed very easily\u2026I often feel like I also can't kind of zoom out of a situation, I get very stuck. Yeah, I guess those words like stuck, unmotivated, tired, all kind of fit into that feeling.\nSimilarly, AB knows she is feeling mentally unwell \"when I have a lot of things to do in one day and I don't feel like doing it. Maybe I'm feeling low or maybe I don't want to. I procrastinate and stuff maybe I just don't want to do it, that is when I feel mentally pressured.\"\nLooking at the participants' descriptions together, a patterned cycle begins to emerge (see Figure 1). Emma explains \"I'll feel overwhelmed from the stress, and then for me those feelings of being overwhelmed kinda come out, emotionally. So, I'll get quite upset. Or I'll just, I won't want to do anything.\" Lily makes this pattern even clearer: I also felt like my energy levels decreased a lot when I was in university\u2026I would be able to do a full day [in high school], and then university I would go to two lectures and I'd be exhausted. So that definitely affected my mental health as well, 'cause it made me feel like I'm not doing enough. And I still feel that way, even though I've been at university for three years now. It still feels like I'm never getting enough done. And then, that makes me feel worse because I'm not doing enough, I'm not getting enough done, it's going to take me forever. And then when that happens, when I get super overwhelmed, I just do nothing, which makes me feel like shit again, so it's a cycle. This cycle begins when students attempt to maintain unsustainable levels of academic productivity at a time when they need to address other aspects of their wellness, such as getting appropriate levels of sleep and resting both mind and body. As productivity is conflated with wellness in student conceptualizations of mental health, the conceptual contrast between Student Wellness and Human Wellness in the university context begins to emerge.\n---\nConstructing Mental Health as Productivity in the University\n\nNot only is the concept of mental health as productivity present in students' accounts of mental health struggles, it is also present in mental health events/ workshops held at the university. UofG offers several mental health events/ workshops for students each semester. In these workshops the construct of mental health as productivity, encompassing energy and motivation, was apparent; wellness in this context is understood as the ability to maintain constant productivity. One series of events called \"Thriving in Action\" clearly emphasized the connection between well-being and productivity as demonstrated by academic success. The workshop series' stated purpose was to \"build academic and well-being skills.\" The first session on the topic of \"connection\" repeatedly reiterated that connection fosters both a sense of wellbeing and academic success. It was implied that when well-being is impacted by other areas of our lives such as being parents or volunteers, this impacts our academic success and potentially leads to decreased grades and drop-out. Similarly, the workshop on mindfulness focused on how mindfulness tools can improve productivity within the university context. It explained that multitasking is a myth and is inefficient; being single-minded or \"mindful\" was the more efficient route. Later, mindfulness was connected to the concept of attentiveness which was described as making one more productive and thus more academically successful. Relatedly, during the workshop on \"balance\" the host tried to \"sell\" wellness and balance as important because they improve our academic success, as opposed to for their own sake. Additionally, one host claimed, \"When we are satisfied with our lives our GPAs increase.\" Generally, the workshops expressed the implicit idea that productivity was the main goal of life, positioning increased academic success as the primary reason to work on improving one's mental health, with subjective well-being seen as a means to better grades rather than as a worthwhile goal in and of itself. This was also clear from the journal prompts given to participants in \"Be Well Be Safe Week\" events for university staff (including student staff members) such as \"What could you have done better to benefit from your break(s) more?\" and \"Ask if you felt more reenergized and focused when returning from breaks.\" These prompts ask students to optimize their breaks, not for their subjective enjoyment, but to increase productivity upon returning to work. Thus, while these workshops attempt to provide tools for addressing Student Wellness, Human Wellness is largely neglected, and few tools are provided to help students escape the mental health-productivity cycle described above.\nIt is important to note that the students attending these workshops often agreed with this construct. For example, the workshops typically began with an ice-breaker activity called \"What went well despite\" during which students would share what went well this week despite the pandemic context. During this activity, most participants talked about things going well in terms of productivity, such as finishing to-do lists or managing to get up early. Furthermore, university mental health resources often position students as personally responsible for their mental health, and thus at fault if they are not striving to increase their productivity. The term \"personal issues\" (sometimes \"personal problems\" or \"personal challenges\") is used throughout the university's wellness website as a euphemism for mental health struggles. For example, the website describes Counselling Services as offering \"\u2026individual counselling sessions to help support students through personal challenges.\" This phrasing has an internalized connotation, individualizing these problems by suggesting the issue resides within the students-similar to a \"disease\" model-rather than being a relationship between the student and their circumstances, as students seem to experience them. Positioning these challenges as \"personal\" suggests the university environment is not at fault and that students are personally responsible for resolving these challenges. While structural challenges, such as the expectation of unsustainable hyperproductivity, require institutional responsibility, personal challenges just require personal responsibility. This orientation to mental health struggles works to socialize students into neoliberal personhood, pressuring them to take responsibility for their own ability to maintain (or not) constant productivity.\nIn sum, university wellness events sell goal-oriented individualized \"self \"care as the gold standard for attaining and maintaining wellness. Wellness in this context is understood as the ability to meet the social expectation that one maintains constant productivity, with academic achievement seen as reflecting one's ability to meet this standard. The university focuses on Student Wellnessacademic achievement-rather than Human Wellness-subjective wellbeing-and sees maintaining wellness as the sole responsibility of the student.\n---\nGrinding the Wheel: Stigmatization of Inactivity in the University\n\nIn this context, where mental health is equated with one's ability to be productive, and constant academic productivity is expected of the \"normal\" university student, the stigmatization of low productivity becomes an important factor in the maintenance of the mental health-productivity cycle described above. The \"good\" student is the student who is hyper-productive and furthermore is able to translate this productivity into academic success, while students who are struggling with energy and motivation (and thus are unable to maintain this level of productivity) are stigmatized as \"lazy\"-and thus \"bad\"-students. This stigmatization, which might come from the student themself or others, such as parents and faculty, prevents students from circumventing the mental health-productivity cycle as it continually triggers the connecting self-criticism stage. This is an issue Daniella works on with her therapist, explaining that she has learned that \"lazy\" is just what we call struggling students when we want to stigmatize their lack of productivity: \u2026lazy is not really a thing. People aren't lazy, they're either burnt out or anxious or they're struggling with something, people aren't lazy just to be lazy \u2026that's a big thing that I talked about with my therapist. She's like you always tell me you feel lazy, but it's not really laziness it's just you are trying to protect yourself from those feelings of anxiety and whatever, sadness, whatever.\nTo avoid this stigma, students prioritize academic productivity over various forms of self-care. For instance, when Maanaav is feeling mentally unwell, and thus has limited energy to spend on daily tasks, he prioritizes his academics over other important aspects of human life. He explains he socially withdraws \"\u2026because at times I have been mentally unwell, and then I just focus on my academics, even though I should connect with other people to be mentally better.\" This prioritization is reinforced by the conflation of productivity and wellness discussed in the proceeding sections. If students are producing academic work, then they must be well, subjective well-being notwithstanding.\nWhen students continue trying to maintain this hyper-productivity, they no longer have energy even for daily self-care. Celine explains that she knows she is struggling with mental health when \"'basic' daily tasks start to feel more mentally exhausting.\" Similarly, Daniella identifies some signs that she is not doing well as including: I stop taking care of myself. I'll get up in the morning and not wanna do anything. I won't wanna brush my teeth, I don't wanna get up and make my bed, I don't wanna do anything. Just lacking basic self-care skills and basic ways to take care of myself too, right? I can kind of tell it's going downhill.\nThe examples these students describe correlate to Rose's opening quotation from the section on student definitions of mental health as productivity. This lack of participation in basic daily self-care, while amplifying subjective feelings of mental unwellness, is important in avoiding the stigmatization of being an unproductive, and thus \"bad,\" student. A lack of self-care can demonstrate that a student is putting all their energy into their academic success, while also placing responsibility for their subjective unwellness on their individual choice not to engage in appropriate self-care activities (eating, sleeping, socializing etcetera).\nMany participants recognized that continually doing things that are \"exhausting\" without \"refueling\" oneself, was harmful; however, this \"refueling\" was difficult to accomplish in the university context. As Sammie explains: I think especially for a science major there's just a lack of time to yourself to do things that are relaxing, and that make you feel just good, and have time to rest and really take a break and invest in therapy and stuff. I think that has been challenging for me, 'cause I'm definitely a bit of a workaholic, so, I don't wanna let my grades drop, but it's also really important to prioritize your mental health.\nAB had a similar recognition explaining that once she started paying more attention to both her physical and mental health, she would do things such as: \u2026going off with friends \u2026 or maybe having a movie night once a week or game nights and things like that\u2026basically, if you're overburdening yourself, take some time out to not burden yourself at all\u2026I\u2026decided that there should be a balance in your life so if you are doing things that exhaust you, you should also do things to refuel you up.\nWhile students wanted to take time off to have fun or take care of themselves when burnt out, their internalized expectation to maintain hyper-productivity often translated into an anxiety about \"not doing enough.\" Lily describes:\nIt feels like this place with the whole thing about \"I'm not doing enough\"\u2026and then it also feels like am I wasting time here? 'cause tuition is expensive too\u2026I was never one to take breaks and I feel like I just started recently and I realize how much I need it, like that's why I would feel burnt out all the time, but yeah, as a student, it just feels like I'm not doing enough.\nThese examples show how students feel personally responsible for maintaining constant academic productivity and how their inability to maintain hyper-productivity further impacts their mental health, reiterating the cycle of poor mental health, low productivity, and amplified negative self-talk leading to further mental health struggles. In this way, the stigmatization of struggling students as \"lazy\" and the university's orientation towards increasing Student Wellness by increasing student productivity can be understood as having a ratcheting effect on this cycle of poor mental health in the university context.\n---\nImplications of the Reproduction of Neoliberal Values of Productivity within Higher Education\n\nThe narratives above demonstrate how the university becomes a site for the reproduction of neoliberal values, where stigma around mental unwellness is implicitly used to identify unproductive members of neoliberal society, understandings of mental unwellness thrown in relief against the values of our time, which place primacy on productivity (Cohen 2017;Grinker 2020;Skultans 1991;Tseris 2017). Previous anthropological studies of mental health and disability have shown that constant, uninterrupted productivity is a normative expectation of the North American university student, as it is of the neoliberal citizen more generally (Dolmage 2017;Geert Van Hove et al. 2014;Gordon 2019;Nishida 2016). In this context, the expectations regarding productivity pattern both how students and the university as an institution understand what mental health consists of-namely that to be mentally well is to be constantly productive. This is illustrated by both student definitions of mental health and the orientation of university wellness events towards individual care, with achieving increased productivity-rather than increased subjective wellbeing-as a primary goal. We can understand how the university's attempts to improve wellness often fail to meaningfully help students by using the conceptual contrast between Student Wellness-academic success-and Human Wellnesssubjective well-being. This maps easily onto neoliberal structures which promote economic wellness-profit-over human social welfare.\nAs mental health struggles are equated with being unproductive within the university, we can see the usefulness of Grinker's theory of stigma (2020) for understanding mental health struggles and their meaning in a neoliberal society. Grinker (2020) suggests that \"mental illness\" labels allow society to identify and penalize those who are not economically productive. In the university context, this study suggests an analogous process occurs as students are socialized into an appropriate neoliberal personhood. When a student fails to achieve the version of wellness subscribed to by the university-being capable of constant academic productivity-the student is stigmatized as \"lazy\" or \"bad,\" and their academic failures are attributed to poor work ethic, even when they may be working at their utmost capacity. This creates feelings of guilt and often sparks students to engage in negative self-talk. The university then creates wellness events intended to address this \"unwellness\" by increasing student productivity rather than by taking responsibility for the ways the institution creates a toxic environment for students. In this way, students who do not meet the normative expectation of constant productivity are \"identified\" as mentally unwell and stigmatized as \"lazy\" or \"bad\" students who need to take personal responsibility for their health and education. However, as one participant-Daniella-suggests above, laziness is not a \"real\" thing but rather what we choose to call suffering when we wish to stigmatize it.\nGrinker's disability studies informed understanding of stigma is useful here because it places the onus for decreasing stigma on social structures, asking that we acknowledge the diversity of persons and build this understanding into the environment rather than asking students to accommodate themselves to fit a preconceived \"normal.\" Nonetheless, while Grinker concludes that capitalism does not create \"mental illness,\" it only gives new meanings to these experiences, my participants illustrate that in this meaning-making, the values embedded within neoliberal expectations do create and exacerbate phenomenological experiences of suffering. Meaning changes experiences, so to say capitalism does not cause suffering simply because \"mental illness\" meanings are a social construction needs to be nuanced to achieve awareness of the harm caused by these expectations.\nWe can achieve this nuance by turning to Scheper-Hughes and Lock's (1986) theory of the \"double illness.\" By understanding that social and cultural responses to an individual's suffering can create secondary suffering in the form of stigma (1986) we can better understand how students become entrapped in this cycle of mental unwellness, reduced productivity, and negative self-talk. The double illness of this stigma reinforces negative self-talk, making it increasingly difficult for students to extricate themselves from this cycle. While the original source of their \"dis-ease\" may come from beyond the university context, the expectation that students are responsible for maintaining hyperproductivity not only exacerbates existing suffering but is in itself disabling; students become increasingly less capable of academic productivity as their energy levels and motivation suffer due to lack of self-care, negative thoughts, and self-doubt (see also Nishida 2016).\nUnderstanding how mental health is constructed as productivity in the university context and how this construction is made more harmful by the stigmatization of struggling students as \"lazy\" can help us create effective wellness initiatives which take into consideration this understanding and the cycle it produces. We need to remove the onus for overcoming toxic environments and avoiding stigma from individual students and instead explore how we can ameliorate the social structures present in the university that uphold toxic expectations of hyper-productivity and create mental health policy and wellness programming accordingly.\nWhile productivity is not in and of itself a negative construct, and it is a condition necessary to produce quality academic work, the issue is the expectation that this productivity be constant and uninterrupted by restful activities, and the interpretation of those who do not meet this expectation as \"lazy\" and not taking appropriate responsibility for their health or education. As AB puts it, if one is going to burden oneself for half of the day, another half of the day ought to be spent in a way that balances this energy output; after all energy and motivation appear to be finite resources only renewable through rest and relaxation. I suggest that rather than trying to promote self-care and rest as values in and of themselves, which continues to individualize responsibility for wellness rather than placing the onus on institutions to improve toxic environments, a more culturally appropriate way to disrupt the mental health-productivity cycle would be to reframe how we think about and define productivity. Instead of taking for granted the idea that valuing productivity is inherently negative, we can ask the anthropological questions of \"Productive for whom\"?, \"What is being produced?\", \"Is it meaningful?\", \"Whom does it harm?\", and, \"Whom does it benefit?\" in order to reframe notions of what is \"productive.\" This practice could allow faculty to critique the purpose of hyper-productivity and design courses which alleviate the burden on students where it does not improve learning but rather serves to advance neoliberal agendas, causing harm. We might further understand this as an instance of communal care. Faculty are also under pressure to be hyperproductive and so may engage in micro-resistance by drawing on empathy and using their power within the institution to care for students through their daily teaching and mentoring practices (Nishida 2016).\nCurrently, university wellness programming implicitly reproduces toxic expectations of constant productivity by orienting wellness goals to focus on academic success rather than on subjective well-being and the creation of a meaningful university experience. Academic success is no doubt important within this context; however, it is imperative that we critique the extent to which it is meaningful when trying to balance it with other key aspects of students' lives, such as maintaining health, building relationships, and engaging in other meaningful activities/hobbies. I suggest both these issues might be addressed by reorienting wellness programming to focus first on Human Wellnesssubjective well-being-instead of on Student Wellness-academic success. Rather than understanding that rest must be productive of further academic productivity, we might instead understand subjective well-being and personal joy as \"things\" worthwhile of producing in their own right. In this way, we may begin to disrupt the mental health-productivity cycle by reducing stigma and eliminating the connecting self-criticism stage.\n---\nConclusion\n\nUniversity programming constructs academic success through enhanced productivity as the primary goal of self-care, and students understand mental unwellness as manifesting as decreased productivity due to low energy and motivation. Consequently, my participants find it difficult to prioritize self-care in an environment that expects them to be hyper-productive, especially since they identified self-care activities, such as sleep and rest, as \"unproductive.\" This is reinforced by the stigmatization of the less than hyper-productive as \"lazy\" and \"bad\" students and therefore uncompetitive neoliberal citizens. Essentially, while improved well-being was understood to ultimately make one more productive, a paradox arose in that students felt the expectation to be hyperproductive meant they did not have time to engage in self-care. Furthermore, since students who struggle with energy and motivation due to burnout and/or underlying mental health struggles are perceived as \"lazy\"-their lack of \"success\" blamed on their lack of productivity-they further prioritize academics over other wellness-promoting activities, such as socializing, in order to reverse this label. This paradox produces a cycle where students feel unwell and overwhelmed and are therefore unable to be as productive as usual, leading to negative self-talk and both external and internalized stigma where students understand themselves as \"not doing enough\" or being \"lazy\" and feel guilty for their lack of productivity. This is followed by a decrease in mental wellness\n---\n\n\nassociated with this stigma and negative self-image, which restarts the cycle. Ultimately, this pattern illustrates how normative expectations of constant productivity arising from neoliberal values both impact student mental health and limit students' ability to engage in care, including the care encouraged by the university. In this way, these expectations produce \"toxic\" environments within the university which prioritize Student Wellness rather than Human Wellness.\n---\nAdrianna N. Wiley\n\nUniversity of Toronto, adrianna.wiley@mail.utoronto.ca Notes 1 In Canada, \"post-secondary\" students consist of two subgroups, \"university\" students and \"college\" students. Unlike the United States where these terms might be used interchangeably, in Canada, post-secondary institutions deemed \"universities\" are degree-granting institutions which tend to focus on theoretical learning and analytical skills, whereas colleges grant diplomas and have a focus on hands-on learning and technical skills.\n2 Readers will note that while this study was conducted during the COVID-19 pandemic, I do not analyze the pandemic in this study as a specific theme or context as it relates to mental health. I do this because my participants do not implicate the pandemic as creating unique mental health struggles but rather as exacerbating and making more easily visible the challenges students were already facing. Emphasizing the pandemic context would downplay the historically deep structural issues this article is meant to highlight.\n4 All names are pseudonyms chosen by participants.\n5 The pseudonym \"Participant 1\" was selected by the participant herself as she wished to be identified by a number rather than a name. Note that Participant 1 requested I still use she/her rather than gender-neutral pronouns.",
        "Background\n\nIn 2008, the Centres for Disease Control and Prevention defined child maltreatment as \"Any act or series of acts of commission or omission by a parent or other caregiver that results in harm, potential for harm, or threat of harm to a child\" [1]. Commissions refer to child abuse including physical, sexual, and psychological (emotional) abuse, and omissions refer to neglect whether physical, emotional, medical, or educational [2].\nChild abuse is a global health concern [3][4][5], occurring in both developing and developed nations [6,7]. A systematic review on global prevalence of violence against children in 96 countries reported that over one billion of children had experienced physical, emotional or sexual violence during the year that preceded the survey [8]. Likewise, the United Nations International Children's Emergency Fund (UNICEF) reported that around 300 million children aged 2 to 4 years are subjected to violent disciplines around the world [9]. In the USA, 80% of the children were spanked by the time they were in the kindergarten [10]. A study conducted in 3 provinces in Turkey on 7540 children reported that 62.7%, and 46.0% of the children experienced psychological, and physical abuse, respectively [11]. A study on 154 children in Afghanistan concluded that 71% of the children reported exposure to physical violence [12].\nChild maltreatment affects morbidity, mortality, social behavior, wellbeing, and quality of life [11,13,14]. Child victimization has been shown to impair physical and mental health [15]. Researchers found a link between parental use of harsh verbal discipline and depressive symptoms, low self-esteem, and academic achievements [16,17]. Negative discipline practices such as corporal punishment, yelling, and demonstrating disappointment studied in 6 countries have been shown to be related to internalizing problems (such as anxiety and depression) and externalizing disorders such as aggression regardless of the culture or the country [18,19]. In addition, parental aggressiveness and negligence were shown to increase child misbehaviour and hostility [20]. Exposure to child abuse increases risk of smoking, immobility, obesity, diabetes, heart, lung and hepatic diseases [21][22][23][24][25]. A study conducted in Canada on child abuse in early life found a positive association between slapping and spanking children and psychiatric disorders, alcohol addiction, and dependence in later life [26].\nSeveral factors have been shown to influence the prevalence of child abuse. Those include demographics such as age, gender, education [27,28]. Family structure, parental conflicts, single mothers, number of children in the family, and parental warmth are important determinants of child maltreatment [29][30][31]. In addition, parents; economic status, family daily stressors, neighbourhood poverty, and place of residence play a major role in child abuse rates [11,32]. A systematic review of the literature indicated that children with disabilities are more likely to be abused compared to other children with no disability [33]. In 2010, Euser and colleagues reported that children of refugee families remain at high risk of being abused or maltreated [34]. Furthermore, cultural issues, religious considerations, and ethnicity affect prevalence of violence against children [35,36]. Indeed, research has shown that parental beliefs, attitude towards domestic violence, exposure of parents to maltreatment in their early childhood, parent's exposure to violence, civil conflicts, presence of armed groups, armed conflicts, and political stress are major determinants of child abuse [37][38][39][40].\nPolitical instability and war like conditions have been shown to result in the general deterioration of the quality of life of families including social, economic and developmental aspects [41,42]. Children are more susceptible and prone to have greater negative health consequences in such environments [43]. The Palestinian context is characterized by a protracted chronic political conflict extending for several decades [44]. This conflict adds more stressors and burdens on family and parents and as has been reported elsewhere is likely to increase the chance of child abuse [37].\nChildren living in such an environment are very vulnerable and research on prevention and development of policy guidelines is indispensable and must be considered a top priority [45].\nResearch on child exposure to violence is relatively rare and inconsistent in the developing countries [46]. A study conducted in 35 low and middle income countries showed that 75% of the children aged 2 to 14 years were exposed to some forms of violent disciplines [47]. In addition, the study showed that Arabic countries reported higher rates of physical punishment, with Palestine being the most disadvantaged. Indeed, the Palestinian Central Bureau of Statistics (PCBS) conducted a Multiple Indicator Cluster Survey in collaboration with the UNICEF in 2014 and assessed some of the commonly used forms of violence (8 practices were investigated, see Index 1, V 10, 11, 15, 18, 24, 29, 31, 36) and found that 92.2% of the Palestinian children experienced one or more incidents of physical or psychological abuse in the month that preceded the survey [48]. However, no study has included internationally recognized forms, methods, and practices of child abuse and uncovered major predictors in the Palestinian context.\nThe theoretical framework upon which this study was designed is based on an ecological model indicating an active role of the human being in the development process and a permanent mutual influence between humans and the environment where they inhabit. This model proposes parenting practices as dependent on the child, parents and the larger socioeconomic and cultural context that eventually affect the human development process and behavior [49]. Given such a framework, the aim of this study is to assess prevalence of child abuse in the West Bank (WB) of the occupied Palestinian territory (oPt) and to determine some of its associated factors. We hypothesize that (1) the prevalence of child abuse in the WB is high and (2) child abuse has social and political determinants.\n---\nMethods\n\n\n---\nData source\n\nThis study used secondary data extracted from a larger national cross sectional household survey conducted in Palestine and Qatar to compare child disciplinary methods used by caregivers in both countries using the ICAST-P tool in 2014/2015 [50]. The data is representative of the Palestinian children living in the WB and aged 0 to 12 years old. Stratified multistage cluster sampling based on the Palestinian Census Projections of 2007 was used -where the total population of the WB was 2.3 Million-taking size of the different communities of the WB in consideration. Nonresponse rate was 5.6% (with 3.6% refusal rate and 2% absence rate). One thousand five hundred ten mothers were interviewed. Questionnaires with missing data (403 cases) were not included in the final analysis. When we checked, the distribution of the missing cases was generally similar to the distribution of the sample. This was done to ensure that the missing responses were not clustered in a specific area or locality. Data analysis was performed on 1107 respondents.\n---\nInstrument\n\nThe International Society for Prevention of Child Abuse and Neglect (ISPCAN) side by side with the UNICEF has developed ISPCAN Child Abuse Screening Tools (ICASTs). Three questionnaires were developed to estimate incidence and prevalence of global violence against children at various stages of their life [51]. In this study, a questionnaire was administered to mothers (ICAST-P) to obtain information on a range of disciplinary (both nonviolent and violent methods including physical and psychological) methods used by parents to discipline their children. This tool which includes the most commonly used disciplinary methods around the globe was used to estimate the extent of abuse of children (aged 0 to 12 years old) at home as reported by their mothers. This tool does not only uncover disciplinary practices the caregivers utilise to discipline their children, but is also a valuable tool to understand and compare the practices across several contexts, cultures, and countries [52].\n---\nOutcome measured\n\nThe outcome investigated was child abuse assessed through exposure to physical (moderate or severe) and psychological violence. The mothers were asked about 28 most commonly used practices around the globe and which are included in the ICAST-P tool (Index 1). Internal consistency was checked and Chronbach's alfa was 0.82 indicating very good internal consistency. A six point Likert scale (1 = once or twice, 2 = 3-5 times, 3 = 6-10 times, 4 = more than 10 times, 5 = not in the past year, 6 = never) was used to rate answer of each item. Responses were recoded as 0 (never or not in the past year) and 1(one to 10 times). The scores were summed up to form a scale then multiplied by100/28 to get a 100 point scale. The scale was recoded as low to moderate levels versus high levels of disciplinary methods. 29.66% was used as the cutoff point based on the formula mean \u00b1 0.25 standard deviation [53]. Accordingly, those who experienced 8 or less incidents of violence during the past year were considered as with low to moderate levels of child abuse and those who experienced 9 or more incidents of violence were considered as with high levels of child abuse.\n---\nIndependent variables\n\nThe Independent variables investigated were the variables proposed by the ISPCAN questionnaire in addition to selected variables deemed necessary to cover in the Palestinian context. Study variables were child's age and gender, Parents age, size of the family, child position in the family, parents' levels of education, type of locality, region, refugee status (Palestinian refugees refer to those who were displaced in 1948 and 1967 and have become residents in West Bank camps since then), child disability, and dependency ratio (no. of children below 14 years old and/or elderly 65 years and above). Parental warmth measured by three questions about child being loved by his parents was also included (Index 2). Chronbach's alfa for this scale was very good (0.84) and the factor analysis showed that the three questions of warmth are loaded on a single variable. A six point Likert scale (1 = more than 10 times, 2 = six to 10 times, 3 = three to five times, 4 = once or twice, 5 = not in the last year, 6 = never) used to rate the answers on how many times the child was told he was loved was then recoded as 0 (no, never) and 4 (more than 10 times). Answers were summed up for the three variables to form a scale from 0 (minimum warmth) to 12 (maximum warmth). The scale was then converted to 100 by multiplying the scale by 100/12. Using the same formula mean \u00b1 0.25 standard deviation [53], the scale was categorized into low levels versus moderate to high levels of parental warmth. In addition, we have investigated exposure of the parents to political violence using the two variables: exposure to sound bombs and/or tear gas bombs during the last 12 months. A five point Likert scale (1 = yes, extremely, 2 = yes, a lot, 3 = yes, moderately, 4 = yes, a little, 5 = no, not at all) was used to rate the answers. The scale responses were recoded as 0 (no, not at all) or 1(yes little, moderate, a lot, or extreme). Answers were summed up for the two variables to form a scale with 0 (no. not at all) and 1 (yes). Chronbach's alfa for this scale was (0.84) indicating very good internal consistency.\n---\nStatistical analyses\n\nThe outcome was child abuse -physical and emotionalcategorized into two categories low to moderate levels versus high levels. All the independent variables were used as categorical variables in the analysis. Background characteristics were investigated in the univariate analysis. Bivariate analysis to assess the association between the outcome and the independent variables was completed using Chi square test. Multivariate binary logistic regression analysis was performed with all the variables that remained significant in the bivariate analysis using SPSS version 20.\n---\nInclusion and exclusion criteria\n\nChildren aged 0 to 12 years and mothers aged 16 years and above were included in the study. The study was specific to children of the WB only.\n---\nResults\n\nTable 1 shows background characteristics of the sample population. 48.1% of the children were females, 51.3% were less than 6 years old, 21.9% of the mothers were 16-29 years old, 9.7% of the children ranked as first born child in the family, 92.9% of the families were nuclear, 60.8% of fathers had less than secondary education, 8.9% of the mothers were employed, 10.7% of the children were considered to have one or more disabilities, 30.4% of the respondents were refugees, 22.0% of the children experienced low levels of parental warmth, and 41.3% of the families were exposed to political violence.\nOverall, 33.9% of the children were exposed to high levels of abuse as reported by their mothers (Table 2). Mean level of child abuse was 25.8% (SD \u00b115.4). 36.9% of the males experienced high levels of abuse compared to 30.6% of females (p < .05). 42.1% of the children whose mothers aged 16-29 years old were exposed to high levels of abuse compared to 12.2% of those whose mothers aged 50 years and above (p < .05). 37.4% of the children who were first or only children in the family were abused compared to 29.9% of those who were last in the family (p < .05). 33.0% of the non-disabled children experienced high levels of abuse compared to 41.2% of the disabled children (p < .05). 49.6% of the children with low levels of parental warmth were abused compared to 29.4% of the children with moderate to high levels of parental warmth (p < .05), and 30.3% of the children whose families were not exposed to political violence were abused compared to 38.9% of those who were exposed to political violence (p < .05).\nTable 3 represent results of multivariate binary logistic regression analyses for child abuse. Males were 1. \n---\nDiscussion\n\nChild abuse is prevalent among children in the WB of the Palestinian society. This high prevalence was reported in a previous study which indicated that 92.2% of the Palestinian children experienced one or more incidents of violence by their mothers during the month that preceded the survey [48]. Overall, 33.9% of the children in this study experienced high levels (9 or more incidents) of physical and psychological/emotional abuse during the past year as reported by their mothers. This finding is also comparable with other reported studies from Jordan, Syria, Iraq, and Turkey [11,27,54].\nOur study found that males were more likely to be abused compared to females. This finding is supported by other studies which reported that male children were more likely to be physically [55] and psychologically abused [56,57]. For instance, our data showed that 61.7% of the children were spanked (Index 1, V24) with males being more disadvantaged than females (65.6% for males versus 57.5% for females). This trend could be attributed to the patriarchal system prevalent in our society as has been reported elsewhere [27]. In 1991, Lytton and Romney in their meta-analysis reported that male children are usually more resistant and express more disobedient bahaviors compared to female children and hence more likely to be physically punished [58]. In addition, some researchers attributed this trend to biological factors and to the gender roles that children are socialized into where boys are given more freedom, autonomy, and power than girls who are treated more strictly and taught to be more obedient than boys early on [59,60]. Akmatov investigated child abuse in 28 developing countries and reported that the higher prevalence of child abuse among boys could be attributed to higher expectations from boys since they are seen as the future family's main source of income [61]. No statistically significant differences were detected by child's age in this study.\nYounger mothers were more likely to abuse their children compared to older mothers. This finding endorses findings of other studies which provided evidence on this trend [62,63]. Researchers attributed this to life experience such as lack of knowledge and experience in child rearing among younger mothers [64]. Indeed, Whitman and colleagues (as reported by Bert and colleagues) concluded that younger and adolescent mothers were more likely to be intolerant, insensitive, and impatient making them more eligible to involve negative discipline practices than older mothers [65]. In addition, as age advances, mothers develop more parenting skills which is likely to have positive impact on their behavior and attitudes [66].\nEducation of the father was an important predictor of child abuse. High levels of father's education were associated with less use of harsh disciplinary methods while no effect of mother education was observed. Similar findings were reported in Turkey in the recent years [27]. Indeed, levels of education are important predictors of socioeconomic status of the family [67]. Research has shown that lower family socioeconomic situation, resulting in poverty and economic hardships, is an important precursor of child maltreatment [68,69]. On the other hand higher education level with its role in improving socioeconomic status enables people to fulfil their daily needs and respond to demands, relieving stress, and stabilizing the family. In addition education protects parents from involvement in negative bahaviors such as cigarette smoking and alcohol consumption [69][70][71]. This could be an explanation of why highly educated fathers are less likely to commit child abuse.\nOur result showed that Low levels of parental warmth increase risk of child abuse. This finding endorses findings of Wade and Kendler who reported an inverse association between parental warmth and child abuse [72]. This finding is attributed to the fact that parents who show high levels of parental warmth are more likely to utilize peaceful communication and interaction with their children [73] which is well reported to be protective against child abuse [74,75]. Better social and emotional child development were reported among parents who showed love, support, and acceptance to their children. In contrast, inferior social and emotional child development were reported for children whose parents exercised punishment and rejection [76]. Exposure of parents to political violence was associated with increased chances of child abuse. Research has shown that exposure to contextual violence and existence of armed groups in communities were associated with increased violent disciplines such as corporal punishment [37]. In fact, exposure to political violence and conflicts have negative consequences on people's life such as suffering, distress, trauma, general and mental health, family stability and functioning, and wellbeing of the communities in general [77,78]. The resulting deterioration of social, economic, and political structures has been well reported as increasing the risk of child maltreatment [71,79]. In addition, Cuartas and colleagues in 2019 reported that exposure to violence and armed conflicts is likely to alter dynamics, social norms, and attitudes towards the use of violent discipline [37].\n---\nStudy limitations\n\nDespite being conducted on a regionally representative sample, our study has got some limitations. This is a cross sectional study that elucidates association but not causation. Important variables like parents' wellbeing, distress, and conflict were not investigated. Mothers may have underreported some of the violent practices to avoid pressure from the family and the community. This study did not include Palestinian children of the Gaza Strip.\n---\nStudy implications\n\nChildren have the right of protection from all forms of violence as has been stipulated by the Convention on the Rights of the Child (CRC) [47]. The findings of this study are very important and provide guidance to policy makers to identify high risk groups among children where preventive interventions are essential and constitute a top priority. Such interventions are necessary for creating an optimum environment for a healthy and safe development of children in the early stage of life so that they can achieve their maximum potential. These interventions should be implemented side by side with the principles of child rights illustrated in the CRC without compromising the capacity of families to exercise the desired harmless forms of child discipline. Providing young mothers with education and support in the use of appropriate rearing practices together with an understanding of the consequences of child abuse could be important steps in preventing violence against children. Those providing the education and support, such as health and social care providers will themselves require training and on-going support. Palestine has ratified the CRC recently and policy makers need to make concerted efforts to protect our future generations. Thus, laws and legislations should be set to ensure an optimum environment for the development and protection of the child. A just political solution to the Palestine question that ends the protracted conflict and exposure to violence is essential if we are to save our children and improve their lives and wellbeing.\n---\nConclusions\n\nOur results revealed an overall high prevalence of child abuse in the WB of the oPt. The most disadvantaged children were boys, children whose mothers were younger, children whose fathers were less educated, children with low levels of parental nurturing, and children whose parents were exposed to political violence. These findings identify and emphasize the place of social, economic, and political issues as major determinants of family wellbeing and population health. \n---\nAvailability of data and materials\n\nThis study used secondary data extracted from a larger national cross sectional household survey conducted in Palestine and Qatar to compare child disciplinary methods used by caregivers in both countries using the ICAST-P tool in 2014/2015. Two of the authors of this paper (Rita Giacaman and Margaret Lynch) were involved in the original study design. Thus, they have a legal access to the data and no permission or consent was required. The data is not available publicly. However, upon a reasonable request, the data can be obtained from the authors.\n---\nSupplementary information\n\nSupplementary information accompanies this paper at https://doi.org/10. 1186/s12889-020-09251-x.\nAdditional file 1: Index 1. Physical and emotional child abuse questions (adapted from ICAST-P questionnaire).\nAdditional file 2: Index 2. Parental nurturing questions (developed by Samia Halileh, a senior Palestinian paediatrician, from her local research and practice). Authors' contributions NH analyzed the data, prepared the methods, results, discussion, and wrote the manuscript. ML participated in data analysis, literature and results presentation, and paper drafting. RG played a major role in data analysis, paper conceptualization along with NH, and supervision of the entire work. All authors read and approved the final manuscript.\nEthics approval and consent to participate Not applicable.\n---\nConsent for publication Not applicable.\n\nCompeting interests \"The authors declare that they have no competing interests\".\n---\nPublisher's Note\n\nSpringer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.",
        "Introduction\n\nParenting can be difficult at any age, and parenting while still a teenager or a young adult can bring additional challenges. Despite significant reductions in teen births across the United States over the past two decades, 153,152 teens under age 20 and 383,388 young women between ages 20 and 24 became first-time mothers in 2018 (Martin et al. 2019). About one in six (15.8%) births to 15-to 19-yearolds and close to half (47%) of births to 20-to 24-yearolds were to females with one or more children (Martin et al. 2019). Early childbearing can make it more difficult for teens of both genders to finish high school and earn a college degree (Fletcher and Wolfe 2012;Hoffman 2008;Mollborn 2010;Prentice 2012;Shuger 2012). Early childbearing can also increase the likelihood of being poor and experiencing fewer employment opportunities (Hoffman 2008;Bunting 2004). Pregnant teens are also often victims of violence-prior to becoming pregnant, while pregnant, and after giving birth (Futures Without Violence 2010). Experiencing violence during and after pregnancy can increase the likelihood of preterm birth or low birth weight and of having a repeat pregnancy within 24 months (Futures Without Violence 2010; Hill et al. 2016).\nTo address these challenges, the Pregnancy Assistance Fund (PAF) program aims to improve the health, social, educational, and economic outcomes for expectant and parenting teens and young adults, their children, and their families (Patient Protection and Affordable Care Act 2010). The PAF program is administered by the Office of Population Affairs (OPA; formerly the Office of Adolescent Health) in the U.S. Department of Health and Human Services. Through the PAF program, states and tribes receive funding to provide expectant and parenting young mothers, fathers, and their children with the wide range of services and supports they need to be healthy, complete their education, and secure fulfilling employment.\nThis article introduces the Maternal and Child Health Journal supplement Supporting Expectant and Parenting Teens: The Pregnancy Assistance Fund by providing an overview of the PAF program, using performance measure data to describe the reach and success of the PAF program, and sharing implementation experiences and lessons learned from PAF grantees that were gathered through a standardized review of grantee applications and from interviews with grant administrators (Person et al. 2018). This introductory article, along with the other articles in this supplement, describes the diversity of PAF-funded programs across the country and highlights promising practices, strategies, tips, and tools that may be useful to others working to improve outcomes for expectant and parenting young families.\n---\nOverview of the Pregnancy Assistance Fund\n\nThe PAF program provides competitive grant funding to states and tribes to improve health, social, educational, and economic outcomes for expectant and parenting young families. To receive grants, states and tribes submit applications to funding opportunity announcements and submissions are reviewed against written criteria by federal staff and an independent review panel. OPA staff then make final award selections to fund states and tribes (referred to here as \"PAF grantees\"). PAF grantees take different approaches to service delivery, with the majority making subawards to other organizations to provide direct services to young families. States and tribes use PAF funds to establish, maintain, and/ or operate expectant and parenting student services in high schools, community service centers, and/or institutions of higher education (IHEs); improve services for pregnant women who are victims of violence; and increase awareness of available services and resources for expectant and parenting young families. The PAF program is unique in its focus on providing access to the wide range of services needed to improve outcomes for expectant and parenting young families. Since 2010, a total of 81 competitive grants have been awarded to 32 states, including the District of Columbia, and seven tribal organizations (Fig. 1) through five distinct cohorts of grants (Table 1). Each cohort of grantees submits a new application for funding, but many states and tribes have been awarded multiple, consecutive grants. Specifically, 13 states and five tribes received a grant for one cohort, 6 states and two tribes received grants for two cohorts, 8 states received a grant for three cohorts, 5 states received grants across four cohorts, no states or tribes received a grant for all five cohorts, and 19 states never received a PAF grant.\nThe PAF program allows grantees to implement services in high schools, community service centers, and/or IHEs. The choice of where to implement services is up to the funded states and tribes; however, implementing a program in IHEs is unique in that it requires a 25% match on behalf of the IHE. The settings for service delivery have varied over the years of the PAF program (Table 1). In grant cohorts 1, 2, and 3 (n = 40 grantees, with some grantees counted more than once), PAF grantees were asked to select the specific setting or settings where they would provide services. Most grantees selected to provide services in community service centers (31 out of 40 grantees) and high schools (29 out of 40), compared to IHEs (6 out of 40). To expand the reach of PAF services to more expectant and parenting students in IHEs, PAF grantees in cohorts 4 and 5 (n = 41) were asked to provide services across multiple settings, if possible. As a result, the number of grantees providing services in IHEs increased substantially (21 out of 41 grantees) while the number of grantees providing services in community service centers (36 out of 41) and high schools (23 out of 41) remained high. In addition to providing services in community settings, high schools, and IHEs, PAF grantees can also use their funding to improve services for pregnant women who are victims of violence. In grant cohorts 1, 2, and 3 (n = 40), six PAF grantees used their funding to partner with their state attorney general to provide comprehensive violence prevention and intervention services to pregnant women who were victims of violence. Beginning in cohort 4, all PAF grantees were asked to provide violence prevention and intervention services as a part of their PAF grant.\nPAF grantees provide access to a wide range of support services to meet the diverse needs of expectant and parenting young families across all implementation settings. Services are either provided directly or through referrals and are \n---\nKey Components of the Pregnancy Assistance Fund\n\nBeyond providing a wide range of supportive services across multiple implementation settings, four additional components of the PAF program are critical to achieving its mission:\n(1) an emphasis on serving both young mothers and young fathers; (2) the use of a thorough needs and resource assessment to determine what services to provide, where to provide them, and to whom to provide them; (3) reliance on well-coordinated, cross-sectoral partnerships to meet the multifaceted needs of expectant and parenting young families; and (4) the ongoing use of performance measure data to assess program implementation and identify areas for continuous quality improvement.\n---\nIncluding and Engaging Young Fathers is Critically Important\n\nPAF grantees emphasize serving both young mothers and young fathers. Often, services for expectant and parenting teens and young adults focus on the mother and the baby, and the same emphasis is not placed on engaging and serving young fathers. Given the critical importance of having fathers actively and positively engaged in the lives of their children (Howard et al. 2006), PAF grantees actively recruit, retain, and engage young fathers in all aspects of their projects. Although engaging young fathers has been difficult, PAF grantees have many lessons and promising practices to share about their work to engage young fathers (Niland and Selekman 2020;McGirr et al. forthcoming), and OPA developed a suite of technical assistance products to support grantees and others in their efforts to serve young fathers (OAH 2016).\n---\nA Thorough Needs and Resources Assessment Leads to Robust and Responsive Services\n\nAll PAF grantees are required to engage in a thorough needs and resource assessment in each community where services are provided. This assessment determines the needs of the expectant and parenting young families in the community and identifies the resources and services that already exist.\nThe results are used to ensure that services are provided to those persons most in need, tailor services to best meet the needs of participants served, address gaps in services, ensure no duplication of services, and improve access to existing services. Grantees can conduct needs and resource assessments according to their own design, but in their response to the funding opportunity announcements, they must describe how they will assess needs and resources on an ongoing basis to ensure programs continue to be aligned with changing community needs.\n---\nCross-Sectoral Partnerships Meet the Multifaceted Needs of Expectant and Parenting Young Families\n\nTo best address the multifaceted and diverse needs of expectant and parenting young families, PAF grantees establish and maintain numerous partnerships with public and private service providers across sectors (state/tribal/local government, businesses, child care, education, health care, housing, transportation, etc.) in the communities served. For example, some states and tribes required their local providers to develop a collaborative entity (such as a community coalition or local advisory board) as part of their grant. Grantees described that engaging cross-sectoral partners enabled them to provide holistic, wraparound services and ensure coordination across partners, reduce service duplication, and raise awareness and increase the use of available services (Person et al. 2018).\n---\nCollecting and Using Data Strengthens Sound Monitoring and Quality Improvement Practices\n\nBeginning in cohort 2, OPA established a uniform set of performance measures that all PAF grantees collect and report to OPA annually. The performance measures include data on participants served; services and referrals provided; partnerships; trainings; and outcomes focused on high school graduation, enrollment in postsecondary education, and repeat pregnancy. PAF grantees use performance measure data to continuously assess their progress; identify successes, challenges, and lessons learned; and make continuous quality improvements to the services they provide. PAF grantees have used the performance measure data to adjust their recruitment strategies (where and how they recruit participants); their implementation practices (duration and time of parenting classes, referral systems, modality of instruction); and their strategies for raising awareness about available services (social media marketing).\n---\nReach and Success of the PAF Program\n\nSince 2010, the PAF program has served 109,661 expectant and parenting teens, young adults, and their family members (Table 1); established more than 3400 partnerships; and trained more than 7500 professionals, according to performance data. On average, 54.4% of participants served were expectant and parenting mothers, 8.4% were expectant and parenting fathers, and 37.2% were children. Most expectant and parenting participants were age 18 and older (63.7%), 30.1% were 16-17 years, and 6.2% were age 15 or younger. Most of the PAF participants were white (47.7%) and black or African American (34.9%), and almost half were Hispanic (48%). On average, across all PAF cohorts, 49% of participants received PAF services in high schools, 38% in community service centers, and 9% in IHEs. In IHEs, 64% of those persons served were enrolled in community colleges, 26% were in four-year colleges or universities, 7% were in vocational or technical schools, and 2% were in other types of IHEs. The majority of participants served in high schools were seniors (29%) and juniors (19%), 12% were sophomores, and 6% were freshman.\nThe services most commonly provided to participants varied by year. On average, the most common services provided directly or through referrals between 2013 and 2016 were parenting skills, case management, concrete supports, education supports, healthy relationship education, and health care services (Table 2). The services most frequently provided directly by PAF grantees included parenting skills, case management, healthy relationship education, and home visitation services. Services most commonly provided through referrals to partner organizations included education support, concrete supports, health care services, child care, vocational services, and transportation.\nFederally required performance measure data collected by PAF grantees for fiscal year 2016 show that many expectant and parenting teens and young adults who participated in the PAF program stayed in high school, made plans to attend college, and had low rates of repeat pregnancy within a year (OAH 2017). Grantees collected data elements defined by OPA about the number and types of people served, the types of services provided, and key outcomes and reported them annually to OPA. Although these performance data have limitations, the data elements were defined consistently across grantees and demonstrated positive outcomes for many PAF participants. Pregnancy and parenting contribute to school dropout among teen girls: 30% of female dropouts report these as key reasons they left school (Shuger 2012). However, just 8% of PAF participants who were enrolled in high school dropped out of school during the 2016 reporting year. 1 Furthermore, although only 5% of teens who became mothers before age 17 complete two years of college by their late 20s (Hoffman 2006), 52% of PAF program participants who were high school seniors or those participants enrolled in GED programs were accepted into an IHE during the 2016 reporting year. 2 Finally, although 17% of births to teens nationally are a repeat birth (Martin et al. 2019), PAF The denominator (number of students in high school) underestimates the total number of participants at risk of dropping out because some grantees reported the number of students in high school at the end of the year and omitted participants who dropped out during the course of the year. 2 The denominator (number of participants who were either seniors or preparing for a GED) underestimates the total number of participants eligible to be accepted into an IHE because some grantees reported the number of participants who were seniors or preparing for a GED at the end of the year and omitted participants who had already graduated or completed the GED earlier in the year.\nprograms reported that only 6% of their participants had another pregnancy during the 2016 reporting year. 3In addition to documenting suggestive findings from grantee performance measure data, OPA contracted with Mathematica to conduct a rigorous impact study of New Heights, a PAF-funded project in Washington, D.C. New Heights is a school-based program that places a dedicated coordinator in each school who is responsible for providing advocacy services, targeted case management, weekly educational workshops, and concrete supports to the expectant and parenting students within the school. The evaluation found that New Heights increased school attendance and credits earned, reduced unexcused absences, and increased the graduation rate among the expectant and parenting teens and young adults who received the program (Asheer et al. 2017a). An article that describes the unique evaluation design, which relied on low-cost administrative data, is included in this supplement (Zief et al. forthcoming). OPA also contracted with Mathematica to conduct a randomized controlled trial of Healthy Families Healthy Futures enhanced with Steps to Success, a home visiting program for teen parents in Houston focused on improving parenting skills, preventing abuse, and reducing rapid repeat pregnancies. The evaluation found a statistically significant 20.8 percentage point increase in exposure to information on parenting and a 15.4 percentage point increase in exposure to information on methods of birth control as well as suggestive evidence of an 11 percentage point increase in use of long-acting reversible contraceptives and an 8.9 percentage point decrease in knowledge about birth control pills (Zief et al. forthcoming). An evaluation of the California-funded PAF project is also currently underway, with findings from its implementation reported in this supplement (Asheer et al. forthcoming).\n---\nInsights and Lessons from Implementing PAF Programs\n\nBeginning in 2013, OAH conducted interviews with PAF grantees to document their implementation experiences and lessons learned (Person et al. 2018). Program challenges and lessons learned from how grantees addressed them are included below and provide important insights for others interested in serving expectant and parenting young families.\nEstablishing and maintaining diverse partnerships is key for addressing the wide range of services needed to improve health, social, educational, and economic outcomes for young families. Grantees leveraged both formal and informal partnerships to reduce barriers to accessing services, link participants with specific services, and support program sustainability. PAF grantees stressed the importance of working with partners across different sectors (i.e., health, education, and social services) to be able to provide expectant and parenting young families with the full range of services they need.\nDifficulty coordinating services across many existing providers made it challenging for expectant and parenting young families to receive the full range of services they needed. Program administrators reported that prior to receiving PAF funding, expectant and parenting youth were not well served in their communities, but not because of a lack of available resources. Rather, the problem was a lack of services specifically for the expectant and parenting teen and young adult population; a lack of knowledge of and access to available programs and services; and a lack of coordination among service providers that prevented youth from getting the holistic, wraparound services they needed. Therefore, PAF funding was used to fill service gaps, enhance existing programs, and improve coordination. PAF grantees used their PAF funding to improve services for expectant and parenting young families by (1) providing new services or targeting specific underserved subpopulations to fill gaps in existing services; (2) enhancing existing programs by adding or refining services while also expanding programs to serve more youth; and (3) improving coordination across state or tribal agencies and among local program providers to make holistic, wraparound services more readily accessible to expectant and parenting young families, avoid the duplication of services, and support the sustainability of PAF programs.\nGrantees perceived a lack of evidence-based programs to address the unique needs of expectant and parenting young families. Nearly all grantees tried to find evidence-based or evidence-informed programs that had been shown to improve health, social, educational, and economic outcomes but expressed that they experienced challenges finding programs to meet the specific needs of expectant and parenting young families. One challenge was a lack of evidence for broader, more collaborative approaches used to address the multifaceted needs of expectant and parenting young families. Another challenge was that grantees perceived that many of the most common evidence-based programs designed to addresses relevant maternal and child health outcomes (e.g., home visiting programs, substance abuse programs, parenting programs) were not designed specifically for teens and young adults. For example, some home visiting programs require participation start early in the pregnancy, before some teens recognize their needs and seek services. Some were also too limited in focus and did not adequately address areas of importance for young families, including educational goals and access to services. Nonetheless, many grantees used evidence-based programs, such as the Parents as Teachers home visiting curriculum and Nurturing Parenting curriculum for preventing and treating child abuse and neglect. Using evidence-based programs was helpful for supporting sustainability because this could enhance grantees' eligibility for additional funding streams, such as the Maternal, Infant, and Early Childhood Home Visiting Program (Asheer et al. 2017b).\n---\nConclusion\n\nThe PAF program takes a comprehensive approach to addressing the needs of expectant and parenting young families. PAF grantees implemented programs across multiple settings including high schools, community service centers, and IHEs; provided a wide range of coordinated services to mothers, fathers, and their children; and served pregnant women who are victims of violence. Since its establishment in 2010, the PAF program has served 109,661 expectant and parenting teens, young adults, and their family members across 32 states, including the District of Columbia, and seven tribal organizations. Expectant and parenting teens and young adults in the PAF program demonstrated success in meeting their educational goals and preventing repeat unintended pregnancies. In addition, the staff who implemented the PAF programs learned many lessons for how to enhance programs and services to have the best outcomes for expectant and parenting young families, including creating partnerships to meet the multifaceted needs of teen parents and using evidence-based programs to promote program sustainability.\n---\n\n\nOpen Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creat iveco mmons .org/licen ses/by/4.0/.\nPublisher's Note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.",
        "Introduction\n\nAn association between socioeconomic status (SES) and lung cancer survival has been reported not only in countries without egalitarian health care system (1), but also in countries with tax-based health service (2,3) or public health insurance system (4,5). In Japan, all citizens are supposed to enroll in one of the public health insurance programs, which allow them to receive healthcare in return for paying insurance premiums and attend any medical institution they choose without a referral from primary care. Although public insurance system is one way to achieve universal health coverage philosophy, recent research from Japan has shown that cancer mortality was higher in the most deprived areas than in the most affluent areas, especially for lung cancer (6), suggesting that health care services are not covered equally in Japan. As it is difficult to obtain individuallevel SES information, such as income or education, the use of areal socioeconomic deprivation based on census indicators as an SES indicator has been widely adopted in the study of social inequality in cancer mortality (1,5,7). A previous study using the Japanese population-based Cancer Registry of Osaka Prefecture, which linked patient address to areal socioeconomic deprivation, found a survival gap between the most deprived and most affluent groups of the local area for lung cancer patients diagnosed between 1993 and 2004 (8).\nInequality in access to surgery due to SES is reported in research from various countries (9)(10)(11), and this may partially contribute to socioeconomic inequalities in survival (2,12). Patients who have undergone lung cancer surgery are a more homogeneous group than the total lung cancer population in terms of cancer stage as they are relatively early stage and have acceptable risks of surgical treatment. Nevertheless, inequality might remain in cancer survival due to SES when limited to patients who receive curative intent surgical treatment. According to a nationwide population-based study from a tax-funded health service country, Sweden (2008)(2009)(2010)(2011)(2012)(2013)(2014)(2015)(2016)(2017), there was an inverse association between individual household disposable income and all-cause mortality in patients who underwent surgery for lung cancer even after adjusting for demographics, tumor stage, performance status and comorbidities (13). On the other hand, in a report from England (2004England ( -2006)), another tax-funded health service country, there was no significant increase in mortality hazard when comparing patients with the lowest community-level income deprivation with those with the highest income deprivation among patients undergoing pulmonary resection for lung cancer (14). Surgery rates for primary lung cancer were reported to be 17.5% in Sweden (15) and 3-18% in England (14); this variation may be due to differences in histology distribution (16) or other factors, indicating that the survival trends of surgical patients may vary from country to country. Examination of the prognosis for lung cancer surgery patients by SES is extremely important for optimizing health services for cancer patients while reducing survival inequalities due to SES; however, as yet there are no relevant reports from Japan.\nIn the present study, we aimed to estimate net survival of up to 5 years for primary lung cancer patients undergoing curativeintent lung resection. Specifically, we used an excess hazard model to identify the survival gap by SES, as estimated by the area-based deprivation index in Japan.\n---\nPatients and methods\n\n\n---\nData source\n\nThe hospital-based cancer registry (HBCR) is a national, unified cancer registry system in Japan that records the incidence of newly diagnosed or treated cancer at each hospital (17) and includes information on patient demographics such as address, date of birth, sex, tumor characteristics such as tumor site, histology and disease stage, medical process such as the history of referral route and first-course treatment (i.e. surgery, chemotherapy, palliative care, radiation therapy), and vital status. As a designated cancer care hospital, Tokyo Medical and Dental University (TMDU) hospital plays a central role in providing cancer treatment and coordination in the Tokyo area. It is also a tertiary emergency university hospital. The data of patients who have undergone lung surgery at TMDU hospital are aggregated into one list (surgical dataset) through the routine work of general thoracic surgeons, which includes preoperative diagnostic workup results, surgical details, postoperative course and tumor pathological information. In this study, the HBCR data were linked to the surgical dataset of lung cancer surgeries at TMDU to supplement the HBCR data with detailed clinical information on treatment and risk factors from the surgical dataset (that is not routinely collected in the HBCR).\nThe smallest common area employed in both the HBCR and the census was 'Cho-Aza' level (average population of 3000). To construct the areal deprivation index (18), small area statistics from the National Census in 2015 were used.\n---\nPatient eligibility\n\nWe obtained data from the HBCR on all patients newly diagnosed with lung cancer (International Classification of Diseases for Oncology 3th edition [ICD-O-3] codes, C34.0-34.9) at TMDU hospital between 2010 and 2018. Linking the HBCR to the surgical dataset using patient identification number and date of the surgery was followed by anonymization of the data. Of a total of 2065 patients who were diagnosed between 2010 and 2018, 1026 patients were excluded for the following reasons: (i) one patient was later diagnosed with non-primary cancer, (ii) 892 patients did not receive surgery for primary lesion, 29 patients received surgery not as the first treatment or later than 6 months after cancer diagnosis and 1 patient received concurrent surgery for oesophageal cancer, (iii) 22 patients underwent surgery later than 2018, (iv) 25 patients received exploratory thoracotomy or biopsy only, (v) 53 patients had carcinoma in situ and (vi) 3 patients did not have linked patient address and deprivation index. Finally, 1039 patients were included in the analysis (Fig. 1). Data on sex, address of patients when diagnosed, process of cancer detection, vital status and date of birth, diagnosis with ICD-O-3 code, surgery and last follow-up were derived from the HBCR. Tumor stage, histology and postoperative survival factors such as body mass index (BMI), the Eastern Co-operative Oncology Group performance status (ECOG-PS), smoking status, comorbidities and other clinical information such as symptoms, treatment strategies, surgical procedures, operative time, bleeding, postoperative hospital stay length, complications and short-term outcomes were derived from the surgical dataset. \n---\nVital status\n\nThe TMDU HBCR assessed the vital status of patients from the residential registry database obtained by the government-commissioned National Cancer Centre (3 and 5 years after diagnosis) for patients with 2011-2015 diagnosed cases (19). For patients who continue to be followed up at TMDU hospital, the last follow-up date and vital status are updated at each visit. For the lost to follow-up cases, active follow-up was conducted by referring to medical records and contacting the patient's family or the hospitals where the patient was referred until the end of 2021. Finally, at least 5-year follow-ups were available for 97.0% of patients who received surgery between 2010 and 2015 (2010-2015 cohort), and at least 3-year follow-ups were available for 90.8% of patients who received surgery between 2016 and 2018 (2016-2018 cohort).\n---\nMeasurement of SES\n\nThe ecological socioeconomic deprivation index (18) was used to assess individual socioeconomic position. The deprivation index in Japan was originally intended as a weighted composite measure of census variables for each census area, to estimate the household poverty component which reflects the deprivation experience of the individual. Higher values indicate a higher component of households in poverty. More details of the Japanese area deprivation index can be found elsewhere (18).\nFor this study, the deprivation index was built using data from the 2015 census for each Cho-Aza and divided into tertiles (Q1-Q3; Q1 = least deprived) based on national distribution to obtain a relative socioeconomic position in Japan. Then, the index was linked to the patient's address at diagnosis. A similar approach using the areal deprivation index has been applied in other Japanese studies using Cancer Registry data at prefecture-level (8,20).\n---\nCovariates\n\nTumor stage identification was standardized to the 8th edition of the Union for International Cancer Control/American Joint Committee on Cancer Tumor-Node-Metastases (UICC/AJCC-TNM) staging system classification ( 21) by board-certified general thoracic surgeons, based on pathological report. For patients who had received some form of preoperative treatment, when the pathological stage determined for the surgical sample differed from the pre-treatment clinical stage, the higher stage was used. Lung cancer was classified as adenocarcinoma, squamous cell carcinoma, neuroendocrine carcinoma (including small cell and large cell neuroendocrine carcinoma) and other.\nBMI was stratified into low, middle and high according to the Asian-specific BMI cutoff value (22) as <18.5, 18.5-22.9 or 23+. A weighted comorbidity score, the Charlson comorbidity Index (CCI) (23) was categorized as 0, 1-2 or 3+ (24).\n---\nStatistical analysis\n\nThe baseline characteristics of the three groups were described as proportions (percentage) or median (interquartile range) as appropriate. To evaluate the differences, Pearson's Chi-squared test for categorical variables or the Kruskal-Wallis test for continuous variables were used. P values <0.05 were considered statistically significant.\nSurvival time was defined as the date of the last follow-up from the date of surgery. In the present study, since most mortality information was based on the HBCR, cause of death information was not available. As we were interested in the probability that patients will die because of their specific cancer, we applied a relative survival framework to obtain net survival. Net survival accounts for the expected mortality hazard (competing death from their specific cancer) and represents the survival of cancer patients under the hypothetical situation that the cancer patients cannot die from causes other than their specific cancer (8). Expected mortalities were referenced from lifetables for the general Japanese population by sex, single year of age, and calendar year, provided by the National Cancer Center Japan (25). From the 5-year follow-up data of the 2010-2015 cohort and 3-year follow-up data of the 2016-2018 cohort, we estimated cumulative 5-year net survival rates using a complete approach with the unbiased non-parametric approach described by Perme et al (26,27). For the time-to-event analysis in net survival within 5 years of surgery, we fitted excess hazard regression models which adopted a Poisson assumption for the observed number of death on each piecewise follow-up interval (27). The excess mortality Downloaded from https://academic.oup.com/jjco/article/53/4/287/6991227 by Hochschule Luzern user on 18 February 2024 hazard is the mortality hazard experienced by the cancer patients on top of their expected mortality hazard. The excess hazard ratios (EHRs) estimated from these models quantify the relative differences in excess mortality hazards between groups. Area deprivation, age, sex, risk factors (i.e. BMI, ECOG-PS, smoking history and CCI), tumor histology and disease stage were included in the univariate excess hazard models. To estimate the adjusted 5-year EHRs of death due to deprivation, demographic factors (age, sex) were first entered into a multivariate excess hazard model. Then, to investigate the mechanisms governing the survival gap, the model was adjusted sequentially for possible contributing factors such as risk factors, tumor histology and stage, according to previous studies (28,29).\nAll statistical analyses were carried out using Stata version 16.0 (Stata Corp. College Station, TX, USA).\n---\nEthical approval\n\nConsidering the retrospective study design and complete anonymization of the data, we used the opt-out approach to ethical approval for this study. This was obtained from the Ethics Committee of Osaka University (21 281(T9)-2).\n---\nResults\n\n\n---\nPatient characteristics\n\nOf 1039 patients (Q1: least deprived 450 patients [43.3%], Q2: moderately deprived 372 patients [35.8%], Q3: most deprived 217 patients [20.9%]), the median age was 71 (range 38-89, interquartile range: 65-76) years, and 67.9% of patients were male (Table 1). More than half of the sample were patients with cancer detected incidentally and about 30% of the cancers were detected through screening. Subjective symptoms associated with cancer were identified in 15.5% of patients. Approximately 7% of patients had a low BMI (<28.5) and 88.9% had a good ECOG-PS. About 77% were current or former smokers, and 70.5% had one or more CCI. The prevalence of CCIs (0, 1-2, 3+) was not different between the groups, but the most deprived patients tended to have at least one more comorbidity (75.1%) than the least deprived patients (67.8%). Adenocarcinoma was the most common subtype overall (65.4%), followed by squamous cell carcinoma (24.7%), neuroendocrine carcinoma (4.9%) and other. Early stage lung cancer (Stage IA/IB) was the most common stage overall (62.8%), followed by advanced-stage disease (Stage IIIA+, 19.7%) and Stage IIA/IIB (17.5%).\nBMI distribution was significantly different by deprivation group (P = 0.007), and the prevalence of low BMI was more common among the moderately deprived patients (9.9%) and less common among the least deprived (4.7%). Early stage cancers accounted for the majority of the least deprived group (65.6%), whereas advancedstage cancer was more prevalent in the most deprived group (28.1%) compared with the least deprived group with fewer early stage cancers (52.5%). There was no difference in the T-factor, but there was a difference in the N-factor of disease. Mediastinal or further distal lymph node metastasis (N2+) was more prevalent in the most deprived group (20.3%) than in the least deprived group (11.8%). There were no differences in the distribution of other mortality risk factors among the deprivation groups.\nPreoperative treatment preceded 3.8% of patients (Table 2). Lobectomy was performed in most patients (86.4%), and 72.1% of procedures were performed under video-assisted thoracic surgery. Based on the surgical record and pathological report, 98.1% were completely resected. There was no difference in the prevalence of incomplete resection between the deprivation groups. Median (IQR) operative time, bleeding, chest drainage duration and postoperative hospital stay were 172 (143-214) min, 131 (65-260) ml, 3 (2-4) days and 7 (6-10) days, respectively. Postoperative complications included bleeding requiring transfusion in 1.9% of cases, respiratory morbidities in 8.2% and 30-day mortality in 0.4%. There were no significant differences in the prevalence of preoperative therapy, surgical factors or postoperative course between the three groups.\n---\nNet survival\n\nThe 5-year net survival rates from the least to the most deprived tertiles were 82.1 [95% confidence interval (CI) 76.2-86.6], 77.6 (95% CI: 70.8-83.0) and 71.4 (95% CI: 62.7-78.4) %, respectively. There was a gradual trend between deprivation and up to 5-year net survival, which was only observed in males (Fig. 2). For males, the difference in 5-year net survival between the most deprived and least deprived groups of patients was 14.1%. For females, the difference in 5-year net survival between the most deprived and least deprived groups of patients was 2.1%.\n---\nExcess hazard for 5-year death\n\nTable 3 shows excess hazard ratio estimates of 5-year death by deprivation groups. The crude excess hazard of 5-year death increased with deprivation tertiles, and was significantly higher in the most deprived group than in the least deprived group (EHR: 1.67, 95%CI: 1.08-2.56), whereas there was no significant difference between the moderately deprived group and the least deprived group. Male sex, worse ECOG-PS (0 versus 1+) and histology other than adenocarcinoma were associated with increased excess hazard of 5-year death. For smoking history, CCI and stage of disease, there was an increase in the effect size of excess hazard of 5-year death with a higher index or more advanced disease. Lower BMI (<18.5) was associated with increased excess hazard of 5-year death, whereas higher BMI (23+) was not significantly associated. Compared with the under 65 years age group, the 65-74 age group was associated with increased excess hazard of 5-year death; however, there was no significant association with the over 75 group.\nFigure 3 shows the excess hazard ratio estimates of 5-year death by deprivation group. After accounting only for age and sex, the most deprived group had a significantly higher 5-year net survival than the least deprived group of 64% (EHR: 1.64, 95%CI: 1.09-2.47). To investigate the mechanism governing the survival gap, the model was adjusted sequentially for possible contributing factors such as risk factors, tumor histology and stage. The analysis revealed that the excess hazard ratio slightly reduced to 1.5 (95% CI: 1.00-2.25) after additionally accounting for risk factors (i.e. BMI, ECOG-PS, smoking history and CCI) and to 1.47 (95% CI: 0.98-2.21) after additionally accounting for tumor histology. However, the excess hazard ratio greatly reduced toward null (EHR: 1.13, 95%CI: 0.75-1.71) after additionally accounting for the stage of disease, meaning that stage of disease may have the greatest impact on the 5-year net survival difference between the most deprived patients and the least deprived.\n---\nDiscussion\n\nIn this study, we found that among patients who received curative intent surgery for primary lung cancer, the net survival of patients with low SES was worse than that of patients with high SES, and   accurate excess hazard estimates. The above strengths of data linkage enabled us to show that there were inequalities in the net survival between patients with low SES and high SES who underwent curative surgery for primary lung cancer.\nOne of the factors that may delay cancer diagnosis is the loss of screening opportunities. In Japan, lung cancer screening is provided by the local government, and some workplaces offer optional screening. SES and lung cancer screening participation rates were found to be positively associated in previous studies (30,31); however, there was no significant difference in the motivation for cancer detection in the present study. This appears to be because in the present study, patients had a median age of 71 years old and were already attending hospital for a disease or condition and were receiving regular medical checkups, or the inequalities in screening options decreased after retirement from the workplace (30).\nThere are several population-based studies on the association between SES and lung cancer stage at diagnosis in Japan. A report from the Osaka Cancer Registry (the midwest of the main island) on advanced lung cancer found that age-adjusted incidence rates were significantly higher in the most deprived area than in the least deprived area in cases diagnosed from 1993 to 1998. The difference narrowed in cases diagnosed from 1999 to 2004 but the difference was still significant (6). Another study from the Miyagi Cancer Registry (the northeast of the main island) of cases diagnosed from 2005 to 2010 also reported that the relative risk of age-adjusted advanced cancer incidence was significantly higher in the most deprived area than the least deprived area for lung cancer (20). On the other hand, a study from the Aomori Cancer Registry (northernmost point of the main island) from 2010 to 2012 found no significant differences in the prevalence of advanced lung cancer patients by areal income (32). The reason why the association between SES and advanced lung cancer incidence varied across studies may be that the Aomori Cancer Registry study used the average income of a larger geographic unit, a municipality, rather than Cho-Aza for linkage. The largest city in Aomori Prefecture has a population of just under 300 000, which is thought to have resulted in the smaller disparity.\nA meta-analysis including reports from Sweden, Denmark, the UK and New Zealand found no evidence of an association between SES and late-stage at diagnosis (33). In the UK and Denmark, which employ an egalitarian healthcare system, there is in principle no charge for medical services, and the financial barrier to an initial visit is relatively low. On the other hand, Japan, although known as an egalitarian healthcare system country, has a financial barrier to the first visit because of the requirement for a co-payment for each visit. In addition, unlike countries with a general practitioner system, in Japan, where patients have access to any hospital, they must gather information about the available medical services and decide where to visit themselves, according to their symptoms and abnormal findings noted by screening; this decision-making process is likely to be affected by the patient's education history. These differences in health care systems may have some impact on the association between the low SES and advanced cancer incidence rates in Japan.\nA delay from initial symptom onset or cancer detection to surgery can advance the cancer stage. A meta-analysis to investigate the association between SES and lung cancer care pathway divided the process from symptom onset to treatment into several intervals (i.e. patient, primary care, referral, diagnostic and treatment interval) and found that there was no treatment delay due to SES in each interval (33). However, the results of this meta-analysis study are not necessarily consistent with surgical treatment cases because of the inclusion of non-surgical treatment cases. Non-surgical treatment has a wider range of indications than surgery and is different in terms of cancer progression and the patient's general condition. The presence and type of comorbidities are a critical factor in the selection of surgical treatment (34) and can affect preoperative preparation. For example, if diabetes is not well controlled, preoperative blood glucose control is necessary (35), and if there is low pulmonary function due to chronic obstructive pulmonary disease, preoperative rehabilitation is introduced (36,37). Furthermore, if anticoagulants have been indicated, an interruption period is necessary to reduce the risk of perioperative bleeding (38). There were no significant differences in the prevalence of CCI categories (0, 1-2, 3+) for survival analysis among the three groups; however, the most deprived patients tended to have more multiple comorbidities than the least deprived patients (75 vs. 68%). In the HBCR used in this study, the date of diagnosis was either (1)   so the interval between the date of diagnosis and the date of surgery could not be used as an accurate indicator to reflect the delay in the diagnosis or treatment process. However, the complexity of chronic disease in patients with low SES (39,40) can be a factor that prolongs the preoperative preparation period. There are several limitations to the present study. First, the sample size was relatively small and was limited to a single center, thus further studies in other institutions are warranted to confirm the current study findings. Second, since there was no Japanese life table that reflects the gradient of mortality due to deprivation, we used the life table of the general population. Patients with low SES are a population expected to have many health risk factors, but if their background mortality is not reflected in the reference life tables, the survival gap due to SES will be overestimated. Third, we used a relative deprivation of the residential area, not aspects of the individual's SES, such as income or education. The small area used in this study is 'Cho-Aza' units, which is the smallest administrative unit, like Census tracts, Institute for International and Strategic Affairs (IRIS) or Super Output Areas (41). In this study, each 'Cho-Aza' unit includes a median of 3145 individuals. The areal population from which the deprivation index is derived is known to affect the deprivation gap (42), and the gap is smaller when using larger geographic units than smaller ones. As the population included in each of the smallest geographical units in Western Europe used in previous literature ranges from 170 (Census tracts in Italy) to 2000 (IRIS in France) (41), which was relatively less than that included in the 'Cho-Aza' unit, the survival gap in the present study could be underestimated. In addition, we used the patient's address at diagnosis for linking the areal deprivation index; therefore, we do not take into account patient movement before the diagnosis. However, another Japanese study, JAGES, which covers > 100 thousand over 65 years population without activities of daily living limitation found that >80% of the participants had lived in the same municipality for over 30 years (43). In the present study, about 77% of the participants were over 65 years old, and it can be estimated that the residential stability of this generation is comparable to the previous study. Fourth, we were not able to take postoperative treatment into account due to a lack of data. The 5-year survival benefit of postoperative adjuvant chemotherapy is reported to be 5-6% (44,45). Although we do not have figures for Japan, in the USA (46) and Canada (4), opportunity inequality for adjuvant treatment by SES has been reported. Finally, since vital status in the HBCR is not automatically linked to regional or national vital statistics, this study required active follow-up in 18.1% of cases to achieve adequate follow-up rates. In Japan, the HBCR used for cancer survival statistics requires at least 90% follow-up (19). Both the 3-and 5-year cohorts in the present study meet this criterion; however, incomplete tracking can lead to overestimation of survival. Notably, the prevalence of 3-and 5-year lostto-follow-up was not significantly different between the deprivation groups.\nDespite these limitations, the present study is the first to report that even in Tokyo, Japan, where health care in return for paying insurance premiums is provided, there is a survival gap by deprivation in postoperative net survival for lung cancer, and this is mainly because patients with a high deprivation index undergo surgery at a more advanced disease stage. This study implies that there is a reason why patients with low SES have more difficulty in accessing earlier lung cancer treatment opportunities than those with high SES. Thus, it is necessary to confirm which areas of first detection, diagnosis and treatment contribute to stage inequalities by deprivation in a population-based study.\n---\nConclusion\n\nThe net survival rate for patients who underwent curative intent surgery for primary lung cancer was worse in low SES patients than high SES, which was largely explained by advanced tumor progression at the time of surgery. The mechanism of this inequality needs to be identified in a population-based study.\n---\nAuthor contributions\n\nMariko Hanafusa: Conceptualization, Formal analysis, Investigation, Methodology, Resources. Yuri Ito: Formal analysis,\n---\nConflict of interest\n\nThe authors declare they have no conflicts of interest.\n---\nSubmission declaration and verification\n\nThe authors declare that there has been no prior publication or submission of this research.",
        "Introduction\n\nOnline Social Networks (OSN), such as Facebook or Friendster, can quickly become popular, but can also suddenly lose large amounts of users. The appearance of competing OSN, with different functionalities and designs, create unexpected shifts of users that abandon one community for another [13]. While the dynamics of growth in these online communities are an established research subject [4,18], there are still many open questions regarding the decline of online communities, in particular related to large OSN [30]. What are the reasons behind the decision of users to stop using an OSN? What is the role of the social network in keeping user engagement, David Garcia, Pavlin Mavrodiev, Frank Schweitzer: Social Resilience in Online Communities: The Autopsy of Friendster Submitted on February 22, 2013. http://www.sg.ethz.ch/research/publications/ or in the spreading of user dissatisfaction? Are there network structures that lead to higher risks of massive user departures? In this article, we assess the question of the relation between the topology of the user network, and the cascades of user departures that threaten the integrity of an online community. We build on previous theoretical work on network effects [5], providing the first empirical study of this phenomenon across successful, failed, and declining OSN.\nThe most successful OSN attract millions of users, whose interactions create emergent phenomena that cannot be reduced back to the behavior of individual users. The OSN is a communication medium that connects a large amount of people, which would stay together only if their interaction dynamics leads to the emergent entity that we call the community. The OSN and its users form a socio-technical system in which the persistence of the community depends on both the social interaction between users, and the implementation and design of the OSN. In this context, the social resilience [2] of an online community is defined as \"The ability of the community to withstand external stresses and disturbances as a result of environmental changes\". In particular, the technological component of the OSN can change the environment of the users, and create stress that threatens the cohesion of the community. As an example, changes in the user interface pose a general risk for user engagement in OSN.\nThe fast pace of the Internet society has already led to the total disappearance of some very large online communities. The most paradigmatic example is Friendster, one of the first and largest OSN, which suffered a massive exodus of users towards competing sites. This led to its closure in 2011, to reopen as an online gaming without its profile data. As a reaction, the Internet Archive1 crawled as much information as possible, creating a timeless snapshot of Friendster right before its closure. If, on the other hand, Friendster was still an alive and active community, this data would have been kept private and never made accessible at such scale. Before closure, users were warned and offered to delete their data from the site, leaving all the remaining data from this community as one of the largest publicly available datasets on social behavior.\nThe decay of Friendster is commented in a comedy video of the Onion News, in which a fictitious \"Internet archaelogist\" explains Friendster as an ancient civilization2 . While proposed as a satire of the speed of Internet culture, this video illustrates the opportunities that a failed OSN offers for research. The users of such a community leave traces that allow us to investigate its failure. In this sense, we can name our work as Internet Archeology, because we analyze non-written traces of a disappeared society, aiming at understanding the way it worked and the reasons for its demise.\nIn this paper, we provide a quantitative approach to the collective departure of users from OSN. We start from a theoretical perspective that, under the assumption of rational user behavior, allows us to define a new metric for the relation between network topology and massive user leaves. We apply this metric to high quality datasets from Friendster and Livejournal, comparing their social resilience with partial datasets from Facebook, Orkut, and Myspace. The research presented here is based on publicly available datasets, allowing the independent validation of our results, as well as the extension to further analyses [17]. In addition, we focus on the time evolution of Friendster, tracking the changes in its social resilience and investigating how it decayed to a complete collapse. We finish by commenting on the limitations and extensions of our approach, and outline possible future applications.\n---\nRelated work\n\nRecent research has focused on the question of growth and decay of activity or interest-based social groups [22]. This line of research analyzes social groups as subcommunities of a larger community, tied together due to underlying common features of their members. Such approach can be equally applied to scientific communities and online social networks [4,32], revealing patterns of diffusion and homophily that respectively spread group adoption, and increase internal connectivity. In particular, the big datasets provided by online communities allow the study of group creation and maintenance [18], as well as the patterns of their internal network structure across communities [24,31]. These results lead to applied techniques to predict the fate of interestbased groups, and to improve clustering analysis of social networks. Our work differs from these previous results in the scope of our analysis: Instead of looking at small to medium sized groups within larger communities, we look at the OSN as a whole. In our approach, users are not connected to each other due to certain common interest or affiliation, but through an online platform that maintains their social links and serves as communication medium.\nAnother research topic close to our work is the analysis of individual churn, defined as the decision of a user to stop using a service in favor of a competitor. This topic has received significant attention due to its business applications in the telephony sector [9], studying how the social environment of an individual can influence the switching to a competing company. In addition, further studies explored how individual users disconnect from P2P networks [16], and stop using massive multiplayer online games [20].\nRegarding social networks, the question of user departure and churn has special relevance [19]. As an emerging topic, a recent study shows the relation between social interaction and user departure in the online community Yahoo answers [10]. Furthermore, the same question has been addressed in a recent article [30], analyzing a mysterious online social network of which nor the name, size, nor purpose is explained. While these results are relevant for the question of user engagement, it is difficult to consider them in further research if we do not have information about the nature of the studied network. Social networks can have very different roles in online communities, requiring a differentiation between traditional social networking sites [24], and online communities with a social network component, but where social interaction is mediated through other channels. The results of [30] reveal that 65% of the users that have no friends still remain active after three months, indicating that such social network is not precisely necessary for a user use the site. As an example, a Youtube user does not need to create and maintain social contacts to interact with other users, which can be done through videos and comments independently of the social network.\nOur work complements the previous results on individual user departures mentioned above, as we analyze the social resilience of the online community at the collective level. We build on these empirically validated microscopic rules of churn, to focus on cascades of departures through large OSN. We analyze the macroscopic topology of the social network and its role in the survival of the community. This kind of macroscopic effects are relevant to study the emergence of social conventions [23], an dynamics of politically aligned communities [8,12], in addition to the case of OSN we address here.\nThe particular problem of enhancing resilience by fixing nodes of a social network has been proposed and theoretically analyzed [5], aiming to prevent the unraveling of a social network. This implies that social resilience can be analyzed through the k-core decomposition of the social network, as explained in Section 3.1 . In addition, k-core centrality is the current state-of-the art metric to find influential nodes in general networks [21], and information spreading in politically aligned communities [8]. Regarding social media in general, the k-core decomposition was applied for a global network of instant messaging [25], as well as for the Korean OSN Cyworld [3,6], motivated by user centrality analysis rather than social resilience. To our knowledge, this article introduces the first empirical analysis of social resilience, relating changes in user environment with cascades of departing users, through analysis based on the the k-core decomposition of different OSN.\n---\nSocial Resilience in OSN\n\n\n---\nQuantifying Social Resilience\n\nA characteristic property of any online social network is the presence of influence among friends. In particular, individual decisions regarding participating or leaving the network are, to a large extent, determined by the number of one's friends and their own engagement [4]. Therefore, users leaving a community have negative indirect effects on their friends [30]. This may trigger the latter to also leave, resulting in further cascades of departing users which may ultimately endanger the whole community. Social resilience acts to limit the spread of such cascades.\nOne approach to quantify social resilience is by natural removal of nodes based on some local property, for example degree [25]. By studying the network connectivity after such removals, one In this paper, we propose an extension based on the k-core decomposition [28]. A k-core of a network is a sub-network in which all nodes have a degree \u2265 k. The k-core decomposition is a procedure of finding all k-cores, \u2200k > 0, by repeatedly pruning nodes with degrees k. Therefore, it captures not only the direct, but also the indirect impact of users leaving the network. As an illustration consider Figure 1, which shows targeted removal of nodes with degrees < 3. On one On the other hand, starting again from A, and applying the k-core procedure, will repeatedly remove nodes until only those with degrees \u2265 3 remain. The first step, A \u2192 C, removes the same light-grey nodes as before. Continuing, C \u2192 D, removes those nodes that have been left with < 3 neighbours in C, and disconnects them as well. The final step, D \u2192 E, finishes the process by disconnecting the last white node in D that was left with < 3 friends. As a result, the final network is the fully connected network of the 4 white nodes.\nHence, supposing that users leave a community when they are left with less than 3 friends, the k-core decomposition captures the full cascading effect that departing users have on the network as a whole. We proceed by formalizing social resilience based on a generalized k-core decomposition. To this end, we present a theoretical model in which rational users decide simultaneously either to stay in the network or to leave it. These decisions are based on maximizing a utility function that weighs the benefits of membership against the associated costs. We show that the equilibrium network which maximizes the total payoff in the community, corresponds to a generalized k-core decomposition of the network.\n---\nGeneralized k-core decomposition\n\nFollowing [15], we extend the traditional k-core decomposition by recognizing that the pruning criterion need not be limited to degree only. Let us define a property function\nB i (H) that given a sub-network H \u2286 G associates a value, n i \u2208 R, to node i. A generalized k-core of a network G is, then, defined as a sub-network H \u2286 G, such that B i (H) \u2265 k, \u2200i \u2208 H and k \u2208 Z.\nThe general form of B i allows us to model different pruning mechanisms. For example, the traditional definition of the k-core can be recovered in the following way -for every node i take its immediate neighbourhood, N i , and fix B i (H) := |N i |, \u2200H \u2286 G. Other authors have also shown that considering weighted links in B i can more accurately reveal nodes with higher spreading potential in weighted networks [11].\nNote that by definition higher order cores are nested within lower order cores. We use this to define that a node i has coreness k s if it is contained in a core of order k, but not in a core of order k > k.\n---\nA rational model for OSN users\n\nHere, we model the cost-benefit trade-off of OSN users in the following way. Assume that users in a given network, G, incur a constant integer cost, c > 0, for the effort they must spend to remain engaged. Accordingly, they receive a benefit or payoff from their friends in the network. Let the benefit of player i be the property function B i (H) with i \u2208 H. Assume non-increasing marginal benefits with respect to the size of H, i.e. B i (H) \u2264 0, otherwise costs are irrelevant as any cost level could be trivially overcome by increasing the size of H. This assumption is also supported by other empirical investigations of large social networks which show that the probability of a user to leave is concave with the number of friends who left [4,30].\nPlayers can choose one of two possible movesstay or leave. The utility of player i, is U i = 0, if he played leave or U i = B i (H) -c, for stay. Finally, users are utility-maximizing, therefore they will choose stay as long as U i > 0.\nIt is easily seen that the equilibrium network, G * , which maximizes the total utility, U (G) = i U i , is composed of users who choose stay when c < k i s , and leave otherwise. In other words, David Garcia, Pavlin Mavrodiev, Frank Schweitzer: Social Resilience in Online Communities: The Autopsy of Friendster Submitted on February 22, 2013. http://www.sg.ethz.ch/research/publications/ node i should remain engaged in the network as long as the cost, c, does not exceed its generalized coreness, k s . In this sense, G * corresponds to the generalized k-core of G.\nTo illustrate that G * is indeed an equilibrium network, we need to show that no user has an incentive to unilaterally join it or leave it. Consider a node, j \u2208 G * who chooses stay. This node would belong to a generalized k-core, k j s , and by definition, B j (H)-k j s \u2265 0. Since, j stayed in the network, it must be that c < k j s , therefore B j (H) -c > 0. So, j will be forfeiting positive utility, had he decides to leave. In the same manner, consider another node l / \u2208 G * who chooses leave, thus his coreness k l s \u2264 c. All his friends with the same coreness would have left the network, therefore the only benefit that l could obtain from staying would come from his connections with nodes in higher cores. The benefit, B l , from such connections must not exceed k l s , otherwise l would have belonged to a higher core in the first place. Since k l s \u2264 c we have B l < c. This implies that l necessarily obtains negative utility from staying, so he has no incentives to do so. Moreover, G * is optimal, as we showed that any change from the equilibrium actions of any user inevitably lowers his utility and decreases the total utility in the network. We also argue that it is reasonable to expect this equilibrium network to be reached in an actual setting, since it maximizes the utility of all users simultaneously, as well as the welfare of the network provider.\nIn the rest of the paper, we approximate B i as proportional to the number of i's direct friends, N i , i.e. B i = bN i , for some b \u2208 Z. Taking k i s to be the coreness of i, by definition it holds that bN i \u2265 k i s . The maximum cost, c, that i would tolerate as a member of the community must be strictly smaller than its coreness, hence bN i > c and N i > c/b. The last result implies that the minimum number of friends that a node i needs to remain engaged must be strictly larger than c/b. Therefore, the coreness of a participating user i must be at least c/b + 1, i.e. k i s \u2264 K, where capital K = (c/b) + 1.\nBased on the above discussion, we see that an user will remain in a network with a high c/b ratio if its coreness k s is high. This is because, by definition i is part of a connected network of nodes with large minimum degrees and hence large benefits.\nIn contrast, simply having a large degree does not imply that an user will obtain large utility from staying. Note that a high-degree node may nevertheless have low coreness. This means that i would be part of a sub-network in which all nodes have low minimum degrees. As a result a lower c/b ratio would suffice to start a cascade of users departing, that can quickly leave i with no friends and thus drive it to leave too.\nFinally, we define social resilience of a community as the size of the K core. In other words, this is the size of the network that remains after all users with k s \u2264 c/b have been forced out. This definition allows us to quantify social resilience and reliably compare it across communities even for unknown c/b ratios, as shown in Section 6. \n---\nFriendster\n\nThe most recent dataset we take into account is the one retrieved by the Internet Archive, with the purpose of preserving Friendster's information before its discontinuation. This dataset provides a high-quality snapshot of the large amount of user information that was publicly available on the site, including friend lists and interest-based groups [31]. In this article, we provide the first analysis of the social network topology of Friendster as a whole.\nSince some user profiles in Friendster were private, this dataset does not include their connections. However, these private users would be listed as contacts in the list of their friends who were not private. We symmetrized the Friendster dataset by adding these additional links. Due to the large size of the Friendster dataset, we symmetrized the data by using Hadoop, which we distribute under a creative commons license3 .\n---\nLivejournal\n\nIn Livejournal, users keep personal blogs and define different types of friendship links. The information retrieval method for the creation of this dataset combined user id sampling with neighborhood exploration [26], covering more than 95% of the whole community. We choose this Livejournal dataset for its overall quality, as it provides a view of practically the whole OSN.\nNote that the desing of Livejournal as an OSN deviates from the other four communities analyzed here. First, Livejournal is a blog community, in which the social network functionality plays a secondary role. Second, Livejournal social links are directed, in the sense that one user can be friend of another without being friended back. In our analysis, we only take include reciprocal links, referring to previous research on its k-core decomposition [21]. By including \n---\nOrkut\n\nAmong declining social networking sites, we include a partial dataset on Orkut [26], which was estimated to cover 11.3% of the whole community. Far from the quality of the two previous datasets, we include Orkut in our analysis due to its platform design, as this dataset includes users that did not have a limit on their amount of friends. Furthermore, Orkut has a story of local success in Brazil, losing popularity against other sites at the time of writing of this article.\n---\nMySpace\n\nOne of the most famous OSN in decline is Myspace, which was the leading OSN before Facebook's success [13]. We include a relatively small dataset of 100000 users of MySpace [3], which was aimed to sample its degree distribution. This dataset was crawled through a Breadth-First Search method, providing a partial and possibly biased dataset of Myspace. We include this dataset as an exercise to study the influence of sampling biases in the analysis of social resilience.\n---\nFacebook\n\nWe want to complete the spectrum of success of OSN, from the collapse of Friendster to the big success of Facebook. The last dataset we include is a special crawl which aims at an unbiased, yet partial dataset as close as possible to the whole community [14]. This dataset was retrieved with a special technique based on random walks, keeping unvaried some network statistics, including Facebook's degree ditribution.\n---\nNot power-law degree distributions\n\nThe first step in our analysis explores the degree distributions of each OSN. The reason to do so is the epidemic properties of complex networks. Under the assumptions of epidemic models, networks with power-law degree distributions do not have an epidemic threshold [27], i.e. a \"sickness\" would survive within the network for an unbound amount of time and eventually infect most of the nodes. Such sickness could be a meme or a social norm, but could also be the decision of leaving the community. Therefore, we need to assess the posibility of a lower-law degree distribution, as it would pose an alternative explanation for the masive cascades of user departures.\nNumerous previous works have reported power-law degree distributions in social networks [3,6,25,26]. Nevertheless, most of these works rely on goodness of fit statistics, and do not provide a clear test of the power-law hypothesis. could be sufficient for some practical applications, the empirical test of the power-law hypothesis can only be tested, and eventually rejected, through the result of a statistical test, assuming a reasonable confidence level. We followed the state-of-the-art methodology to test power laws [7], which roughly involves the following steps. First, we created Maximum Likelihood (ML) estimators \u03b1 and deg min for p(d).\nSecond, we tested the empirical data above deg min against the power law hypothesis and we recorded the corresponding KS-statistics (D). Third, we repeated the KS test for 100 synthetic datasets that follow the fitted power law above deg min . The p-value is then the fraction of the synthetic D values that are larger than the empirical one. Thus, for each degree distribution, we have the ML estimates deg min and \u03b1, which define the best case in terms of the KS test, with an associated D value, and the p-value.\nUltimately, a power law hypothesis cannot be rejected if (i) the p-value of the KS-test is above a chosen significance level [7], and (ii) there is a sufficiently large amount of datapoints from  [29]. We found that the degree distributions of Facebook, Friendster, Orkut and Livejournal have p-values well below any reasonable significance threshold, showing an extremely reliable empirical support to reject the power-law hypothesis (Table 2).\nFor the case of Myspace, a KS test gives a p-value of 0.22, which can be considered high enough to not reject the power-law hypothesis [7]. Therefore Myspace satisfies the first criterion, but when looking at the range of values from deg min to deg max (roughly one order of magnitude), and the low amount of datapoints included, this KS-test composes a merely anecdotal evidence of the extreme tail of Myspace. If accepted, the power-law distribution would explain just 0.623% of the Myspace dataset. In addition, the unsupervised BFS crawling method used for this dataset has been shown to have a bias that creates artificial power-law tails [1]. This leads to the conclusion that, while we cannot fully reject the power-law hypothesis, we can safely state that the dataset does not support the hypothesis otherwise. Figure 2 shows the degree distributions and their CCDF. For each OSN, we show how the typical log-log plot of the PDF is misleading, as a simple eye inspection would suggest power-law distributions, but a robust statistical analysis disproves this possibility. 6 Empirics of OSN Resilience\n---\nK-core decomposition\n\nWe computed the k-core decomposition for each of the OSN datasets we introduced in Section 4. Among those datasets, Friendster and Livejournal cover the vast majority of their respective communities. Figure 3 shows a schematic representation of the k-cores of Friendster and Livejournal. Each layer of the circles corresponds to the nodes with coreness k s , with an area proportional to the amount of nodes with that coreness value. The color of each layer ranges from light blue for k s = 1, to red for k s = 304. The distribution of colors reveals a qualitative difference between both communities: Friendster has many more nodes of high coreness than Livejournal, which has a similar color range but a much larger fringe, i.e. the set of nodes with low k s . This difference indicates that, to keep together as a community, Livejournal needs to David Garcia, Pavlin Mavrodiev, Frank Schweitzer: Social Resilience in Online Communities: The Autopsy of Friendster Submitted on February 22, 2013. http://www.sg.ethz.ch/research/publications/ have a much lower c/b than Friendster. This scenario is rather realistic, as Livejournal is a blog community in which users create large amounts of original content. This leads to high benefits per social link as long as users have similar interests, which seems to be the key of Livejournal's relative success. Our theoretical argumentation, presented in Section 3.3, indicates that node coreness is a more reasonable estimator for resilience than node degree. A degree of at least k s is a necessary condition for a coreness of k s , but a high degree does not necessarily mean a high coreness. Taking Friendster an example, Figure 4 shows the boxplot for the distribution of k s versus node degree, indicating the spread of k s for nodes of similar degree. The empirical data shows that a high degree does not necessarily mean a high k s , even finding nodes with very low k s and very high degree. Nevertheless, it is clear that k s is likely to increase with degree, but mapping degree to coreness would misestimate the resilience of the community as a whole. By measuring coreness, whe can detect that some nodes belong to the fringe despite their high degree, as the coreness integrates global information about the centrality of the node.\n---\nResilience comparison\n\nExtending the above observations, we computed the k-core decomposition of the three additional OSN, aiming at comparing their relation between their environment, measured through c/b, and the amount of users expected to be active under such conditions.\nWe focus our analysis on the Complementary Cumulative Density Function (CCDF) of each network, defined as P (k s > K). As shown in Section 3.3, the cost-benefit-ratio c/b corresponds to a value K that determines the nodes that leave the network, which are those k s coreness below K. Under this conditions, the CCDF of k s measures the amount of nodes that will remain in  The right panel of Figure 4 shows the log-log CCDF of the five OSN. The first two communities to compare are Livejournal and Friendster, as the datasets on these two are the most reliable. First, the CCDF of Friendster is always above the CCDF of Livejournal. This is consistent with the structure shown in Figure 3, where it can be appreciated that Livejournal has many more nodes in the fringe than Friendster. Second, both CCDF reach comparable maximum values, regardless of the fact that Friendster was 20 times larger than Livejournal. Such skewness in the coreness of Livejournal can be interpreted as a result of a higher competition for attention, as expected from a blog community in comarison with a pure social networking site, like Friendster was.\nFocusing on the tails of the distributions, we can compare the patterns of resilience for environments with high K. The comparison between the resilience of these communities is heavily dependent of the value of K, as for example, Livejournal is less resilient than Facebook for values of K between 10 and 50, but more resilient below and above such interval. A similar case can be seen between Friendster and Orkut, as their CCDFS cross at 60 and 200. Thus, Friendster would be more resilient than Orkut if K lies in that interval, while Orkut would have a larger fraction of active nodes if K < 60 or K > 200.\nIt is important to note that these comparisons are made between the reliable datasets of Friendster and Livejournal, compared with partial datasets from the other communities.\nDavid Garcia, Pavlin Mavrodiev, Frank Schweitzer: Social Resilience in Online Communities: The Autopsy of Friendster Submitted on February 22, 2013. http://www.sg.ethz.ch/research/publications/ While our conclusions on the first two OSN can be seen as global findings on the community as a whole, the rest are limited to the size of the datasates available. A particularily clear example of the effect of the crawling bias is the distribution of coreness for Myspace, which shows an extreme resilience in comparison to all the other datasets, with the exception of Orkut for K < 50. As commented in Section 4 , the method used for Myspace was very biased towards nodes of high degree, leaving an unrealistic picture of the resilience of the whole community. Additionally, the method used for Facebook seems to have delivered a degree distribution close to a random sample of Facebook users, but its restarting of random walkers leaves tendrils of nodes that accumulate on the 1-core. Hence the low starting value of the CCDF of Facebook could be an artifact of this crawling method.\nRegardless of any crawling bias, we found that these networks have maximum coreness numbers much higher than previous results. The maximum k s found for the network of instant messaging was limited to 68 [25], and close to 100 for the OSN Cyworld [6]. Livejournal has a maximum k s of 213, Friendster of 304, Orkut of 253, and Myspace as a very deep core of k s = 414. The exception lies in the Facebook dataset, where we find a maximum k s of 74. This evidence shows that OSN can have much tigther cores than the ones found in previous research, revealing that they contain small communities with very high resilience.\nAs a final comparison, we focus on the values of K for the catastrophic case of the networks losing 80% of their nodes, i.e. where the CCDF has a value of 0.2. The data shows that both Facebook and Livejournal would lose 80% of their users under a value of K close to 10. For the case of the unsuccessful communities of Orkut and Friendster, it requires a much worse environment, with values of K above 60. This way, the emprical data supports the idea that, under the same environmental conditions, both successful communities is less resilient than the three unsuccessful ones. This means that the topology of their social network is not enough to explain their collapse, but indicates that bad decisions in design and interface changes can spread through the network and drive many users away.\n---\nThe Time Evolution of Friendster\n\nIn this section, we describe a post hoc case study of the way how Friendster rised and collapsed, using the avaliable timing information in the dataset.\n---\nSocial growth mechanism\n\nThe Friendster dataset does not provide the date of creation of user accounts or social links, but it includes a user id that increased sequentially since the creation of the site. We analyzed the time series of Friendster in an event time scale, where each timestamp corresponds to the David Garcia, Pavlin Mavrodiev, Frank Schweitzer: Social Resilience in Online Communities: The Autopsy of Friendster Submitted on February 22, 2013. http://www.sg.ethz.ch/research/publications/ id of each user. We measured the time distance of an edge e, which connects users u 1 and u 2 , as the difference between the ids of these users d(e(u 1 , u 2 )) = |id(u 1 ) -id(u 2 )|. In the following, we how early users connected to later users, making the network grow.\nWe divided the network in time slices of a width of 10 million user ids, with a last smaller slice of 7 million ids. Each of these 12 slices contains a set of nodes that have connections i) to nodes that joined the community before, ii) to nodes that joined the network afterwards, and iii) internally within the slice. This way, for the slice of time period t we can calculate its internal average degree 2|E in (t)|/|N (t)|, where E in (t) is the set of edges between nodes in the slice t, noted as N (t).\nAs an extension, we define E p (t) and E f (t) as the sets of edges towards nodes that joined the community before t (past nodes), and nodes that joined after t (future nodes). We measured the time range of connections to the past P (t) as the mean distance of the edges in E p (t), and the rage of connections to the future F (t) as the mean distance of their future counterpart E f (t). By definition, the amount of past nodes for the first slice is 0, equally to the amount of future nodes for the last slice. If the process of edge creation was purely random, the network would resemble an Erd\u00f6s-Renyi graph with an arbitrary sequence of node ids. In The time evolution of the range of connections to past and future is shown in Figure 5. Each circle represents a slice of the network, with growing t from top to bottom. Their vertical alignment represents the present with respect to the slice, and each circle is connected to a blue square on the left that represents past nodes, and a red square on the right that represents future nodes. Balls have a size proportional to |N (t)|, which keep approximately constant throughout time. The darkness of each circle is proportional to its internal connectivity |E in (t)|, and the width of the connections from circles to past and future squares are proportional to |E p (t)| and |E f (t)| respectively. Internal connectivities decrease through time, as early slices had significantly higher |E in (t)|. This indicates that the initial root of users of Friendster was much more tightly connected among themselves than towards other nodes, creating a denser subcommunity of old users. A possible explanation for this pattern is that Friendster started as an OSN for dating, and its design was later shifted towards generalized networking as it became popular. The squares of Figure 5 are positioned according to the mean past P (t) and future F (t) distances of each slice. As a comparison with random network construction, dashed lines show their expected values as explained above. For early slices, the mean future distance is significantly lower than its random counterpart, revealing a pattern of time cohesion that limits the range of future connections. This shows a decay in the diffussion process through the offline social network, where the potential of a user to bring new users decreases through time. This suggests a possible \"user expiration date\" after which a user of a OSN cannot be expected to bring new users.\n---\nResilience and decline of Friendster\n\nWe combined the sequence of user ids with the k-core decomposition of Friendster to study how its resilience changed over time. In particular, we explored the relation between the coreness of users and the time when they joined the community. We divided users along the median of the distribution of coreness values, ks = 6. This way, for each period of time, there is a set of users in the lower half of the distribution (k s < ks ), which are nodes at risk of leaving the OSN. We measure the resilience of these time-dependent parts of network as the ratio between users at risk of leaving, and the total amount of users in the slice.\nWe created slices of 100000 user ids, calculating a point sample estimate of P (k s < ks ). Inset of Figure 6 shows the time evolution of this ratio, with a dark area showing 99% confidence intervals. First, we notice that the skewness of k s does not affect our statistic, as the confidence intervals are sufficiently concentrated around the point estimates. Second, whe can identify certain time periods when the new users of Friendster only connected to its fringe, having larger ratios of nodes at risk. The first moment with a peak is at the very beginning, to drop to ratios around 0.3 soon after. This shows that the set of very early users did not fully exploit the social network, and it took a bit of time for the OSN to become more resilient. The second peak is shortly after having 22 million users, which coincides with the decay of popularity of Friendster in the US. Finally, the ratio of users at risk went above 0.5 before the community had 80 million accounts, showing a lack of cohesion as its shutdown approaches, as new users do not manage to connect to the rest.\nTo conclude our analysis, we explored how the spread of departures captured in the k-core decomposition (see Section 3.3) can describe the collapse of Friendster as an OSN. As we do not have access to the precise amount of active users of Friendster, we proxy its value through the Google search volume of www.friendster.com. The inset of Figure 6 shows the relative weekly search volume from 2004, where the increase of popularity of Friendster is evident. At some point in 2009, Friendster introduced changes in its user interface, coinciding with some technical problems, and the rise of popularity of Facebook4 . This led to the fast decrease of active users in the community, ending on its discontinuation in 2011. We scale the search volumes fixing 100% as the total amount of users with coreness above 0, 68 million. At the point when the collapse of Friendster started, the search volume indicates a popularity of 78% of its maximum. We take this point to start the simulation of a user departure cascade, with an initial amount of 58 million active users, i.e. users with coreness above 3. The second reference point we take is June 2010, when Friendster was reported to have 10 million David Garcia, Pavlin Mavrodiev, Frank Schweitzer: Social Resilience in Online Communities: The Autopsy of Friendster Submitted on February 22, 2013. http://www.sg.ethz.ch/research/publications/ active users5 , corresponding to 15% of the 68 million user reference explained above. The search volume on that date is 14%, showing the validity of the assumption that the maximum amount of active users corresponds to those with coreness above 0. Thus, these 10 million remaining users correspond to nodes with k s > 67.\nGiven these two reference points, we can approximate the collapse of the network through its \"unraveling\" per k-core. Our assumption is that a critical coreness K t starts at 3 and increases by 1 at a constant rate. Such K t is the result of an increasing cost-to-benefit ratio, and thus all the nodes with k s < K t would leave the community. Then, for each timestep, the amount of remaining users would correspond to the CCDF shown in Figure 4. In our analysis, K increases at a rate of 6 per month, i.e. from 3 to 67 between our two reference point.\nThe red line of Figure 6 shows the remaining users under this process, with dashed values after the second reference point of June 2010. We can observe that this process approximates well the decay of Friendster from the start of its decline, to its total shutdown in 2011. The R 2 value for this fit is 0.972, leaving some slight underfit through 2009. This fit show the match between two approximations: on one side the search volume as an estimation of the amount of active users, and on the other side the amount of remaining users when the c/b ratio increases constantly through time.\n---\nDiscussion\n\nIn this article, we have presented the first empirical analysis of social resilience in OSN. We approached this question using a theoretical model that relates the environment of the OSN with the cascades of user departures. We showed how a generalized version of the k-core decomposition allows the empirical measurement of resilience in OSN.\nWe provided an empirical study of social resilience across five influential OSN, including successful ones like Facebook and unsuccessful ones like Friendster. We have shown that the hypothesis of a power-law degree distribution cannot be accepted for any of these communities, discarding the epidemic properties of complex networks as a possible explanation for large-scale cascades of user departures. Our k-core analysis overcomes this limitation, quantifying social resilience as a collective phenomenon using the CCDF of node coreness. We found that the topologies of two successful sites, Livejournal and Facebook, are less resilient than the unsuccessful Friendster and Orkut. This indicates that the environmental condition of an OSN play a major role for its success. Thus, we conclude that the topology of the social network alone cannot explain the stories of success and failure of the studied OSN, and it is necessary to focus future empirical analysis in measuring these costs and benefits. Additionally, we found very high maximum coreness numbers David Garcia, Pavlin Mavrodiev, Frank Schweitzer: Social Resilience in Online Communities: The Autopsy of Friendster Submitted on February 22, 2013. http://www.sg.ethz.ch/research/publications/ for most of the OSN we studied. The existence of these superconnected cores indicates that information can be spread efficiently through these OSN [21].\nAs a case study, we provided a detailed post hoc analysis of the changes in Friendster through time. We detect that the range of connections towards future nodes is much lower than the expectation from a random process. Using the coreness of the nodes, we could track the time dependence of the risk of leaving for new users. We found shocks that indicate periods of lower resilience of the whole community. Finally, we apply all our findings to Friendster's collapse, fitting an approximated time series of active users through the spread of user departures predicted by the k-core decomposition. We estimated the amount of active users through search volumes, but other sources can provide more reliable data, like Alexa ranks, or last login times if provided by the site. Such datasets would allow further validations of the k-core decomposition as a measure of social resilience.\nOur analysis focused on the macroscopic resilience of OSN, but additional research is necessary to complete our findings. Microscopic data on user activity and churn can provide estimators for the benefits and costs of each network, to further validate the work presented here. Furthermore, the generalized k-core can be applied when user decisions are more complex than just staying or leaving the network, for example introducing heterogeneity of benefits or weights in the social links.\nAnother open question is the role of directionality in the social network, and how to measure resilience when asymmetric relations are allowed. The benefits of users of these networks would be multidimensional, representing both the reputation of a user and the amount of information it receives from its neighborhood. The work presented here is theoretically limited to the study of monotonously increasing, convex objective functions of benefit versus active neighborhood. While empirical studies support this assumption [4,30], it is possible to imagine a scenario where information overload decreases the net benefit of users with very large neighborhoods, creating nonlinearities where the generalized k-core is not a stable solution. We leave this questions open for further research, and the study of social resilience in other types of online communities.",
        "Background\n\nPrimary care is recognized internationally as the cornerstone of strong health systems [1] and strengthening primary care is a key element of many national health systems reform. Good primary care has been shown to mitigate socioeconomic disparities in healthcare utilization, is associated with better and more equitable health outcomes, and fosters greater patient satisfaction [1]. With increased patient participation in healthcare, patients' assessments of quality and satisfaction have become important outcomes in evaluating health services [2][3][4].\nMeasuring quality of healthcare is complex. However, the attributes of primary care have been used to develop measurement scales by researchers [5]. These attributes have been found to be associated with quality of care [6,7] and are the most relevant characteristics of effective primary care organization and delivery at the population level. They are: 1) First contact access to care; 2) ongoing care; 3) Comprehensiveness of care; and 4) Coordination of care [1,8].\nHong Kong is a high-income, special administrative region of China, with a highly developed, mixed healthcare system, and population health indicators comparable to those in high-income western nations. Rapid health expenditure growth, and a fast aging population have stimulated consultations on health systems reform in Hong Kong since 1993 [9]. Strengthening the role of primary care is a key element of the ongoing reform strategy [9]. Although Hong Kong's underlying ethical principle in health care is that \"no one shall be denied healthcare due to a lack of means\" [10], it is to all intents egalitarian on access to care not quality received. In practice, the private sector provides the largest proportion of primary care (70%) and operates under free market principles [11][12][13]. Physician reimbursement therein is usually fee for service (FFS), care is funded primarily by out-of-pocket (OOP) payments and charges are not subject to government regulation [14,15]. The exception to OOP payments is private insurance, which covers about 27% of the population [11] and is generally regressive as it is usually provided by employers and tied to staff seniority [13]. Insufficient regulation, has fostered marked variation in the quality available, poor coordination with other levels of care within the system, and substantial supplier-induced demand [15]. The other 30% of primary care is provided by 74 government-run general outpatient clinics (GOPC's). GOPCs provide service to all the population, but target the poor and the elderly population. GOPCs are highly subsidized (over 90%) by the government, and are usually overcrowded. However, patient reported lower quality of care on accessibility and communication compared with private care clients [16]. In general, affordability is still said to be a major impediment to receiving primary care, and doctor shopping is prevalent amongst patients [17]. Policy analysis documents identify that primary care in Hong Kong is provider-dominated, and fragmented [18].\nOur study is part of a larger research project called -\"Using a systematic approach to evaluate primary care development in Hong Kong, Shenzhen, Kunming and Shanghai\". The overall study aims to measure the impact of healthcare reform policies on structure, process, output and outcome of primary care in the four cities. Wong et al.'s 2010 study [19] on the quality of primary care compared quality between the public and private sector. No studies have explored the inequity in the quality of primary care utilized by different socioeconomic groups of patients yet. Consequently, our study examined the association between patients' socioeconomic status, and experiences of primary care. This should provide useful information for ongoing policy discussions about primary care organization and delivery.\n---\nMethods\n\n\n---\nMeasurement tool\n\nThe primary care assessment tool (PCAT)\nOur study used the Primary care Assessment Tool-Adult short version (PCAT-AS) for data collection. It was developed by Starfield and Shi at the John's Hopkins primary care policy center [6,7]. The patient tool assesses patients experience of primary care rather than their satisfaction and thus attempts to capture quality based on the characteristics outlined by the World Health Organization and Institute Of Medicine definitions [19]. Haggerty et al's 2011 study has showed that the PCAT have satisfactory validity and reliability [20]. It evaluates a wide breadth of attributes and its scales are specific to the core primary care attributes. Hence, it is easily interpreted from a policy standpoint. Our tool has been translated into Cantonese Chinese from English, validated [21] and used in Hong Kong in a previous study [19]. The primary care provider within this tool was defined using the 3 questions asked in the original PCAT survey-\"is there a doctor that you usually go to if you're sick and need advice about your health?\" \"Is there a doctor that knows you best as a person?\" and \"Is there a doctor or place that is most responsible for your health care?\" We examined data to see what proportion of respondents had the same provider for 2 or 3 of these questions to understand how clearly they defined their primary care provider. For subsequent interview questions, when all 3 providers were the same, that provider was used as the primary care provider. If the response to \"the usual doctor\" was the same as for either of the other 2 questions, then the \"the usual doctor\" provider was used. However, if the response for \"the usual doctor\" was different, but the responses to the 2 other questions were the same, then the provider where both are the same was used. Finally, if all 3 responses were different, then the provider identified for \"the usual doctor\" was used [22]. The PCAT measures the four core domains of primary care i.e., first contact, ongoing care, coordination, and comprehensiveness (see the Definition of the domains of the primary care assessment tool subsection), divided into 8 core subdomains [23] Specific definitions of each subdomain is published in Wong's study [19], which consists of 3-15 questions. Answers are ranked on a 4-point Likert-type scale with \"1\" indicating 'Definitely Not', \"2\" 'Probably Not', \"3\" 'Probably', and \"4\" 'Definitely'. The total score for each subdomain was calculated by summing the values of all items under it with reverse coding conducted where appropriate. Missing items or non-response was managed according to the scoring instructions provided with the tool. The overall primary care score (total PCAT score) of quality was calculated by adding the mean scores of each of the 8 core subdomains [23]. In this study, 4 out of the 8 subdomains reflecting the process of primary health care delivery and the total primary care assessment tool score were used as outcome variables. The four subdomains are first contact utilization, ongoing care, coordination and comprehensiveness provided.\nDefinition of the domains of the primary care assessment tool First contact access to care: the first contact with the health care system with services that are accessible and at a close proximity Continuity of care: interventions cover the patient's health needs throughout the course of their lives Comprehensiveness of care: providing care for common problems including providing curative, rehabilitative and supportive care, as well as health promotion and disease prevention Coordination of care: seamless care so that when patients are referred elsewhere the advice they receive is integrated into their care.\nThe patient tool collects additional individual information including household-income; insurance coverage; education; geographical districts; age; self-reported health status; and gender. These were adapted to suit Hong Kong's context.\n---\nData collection\n\nA stratified random telephone survey using the PCAT tool was conducted with Cantonese Chinese-speaking residents of Hong Kong aged 18 and above by trained interviewers from the Telephone Survey and Research Design Laboratory in Hong Kong. Survey interviews were conducted in the last quarter of 2011. Three major geographic regions were stratified and 16,662 telephone numbers were randomly selected from a telephone directory. No interviews were attempted in non-Cantonese households (about 1% in Hong Kong), commercial numbers, or fax numbers. We aimed to obtain 2000 completed surveys to detect a mean difference in the overall primary care score of 0.5 between the users of public and private providers, at an alpha level of 0.005, and power of 80% allowing for a 70% response rate. This was estimated based on a previous study in Hong Kong [19].\nThe details of participant selection are shown in Figure 1. A total of 2,932 respondents were eligible out of which 1994 completed the interview. The overall response rate (number of completed interviews divided by the total number of valid contacts) was 68%.\n---\nStatistical analysis\n\nSocioeconomic status of subjects was described using three indicators: household-income, education and type of housing. The median monthly domestic household income in Hong Kong in 2011 was HK$ 20,200 (USD2605). Monthly household income was categorized into 3 groups based on the 2011 Thematic Report regarding household income distribution in Hong Kong [24]: below HK$ 15,000 (USD1934), between HK$ 15,000 and 39,999 (USD1934-5158), and HK$ 40,000 (USD5159) and above. Source of health payments were divided into 4 groups: only out of pocket payments, only the medical waiver scheme, only private insurance, and any combination of sources which also included people using medical waivers. Scores of the four main domains of primary care and the total primary care assessment score were calculated.\nMultivariable ordinal logistic regression was conducted to examine the association between indicators of socioeconomic status and the primary care assessment score. Thereafter, we attempted to disentangle the components of the total primary care score by using the 4 subdomains representing the fulfillment of the attributes of primary care as outcome variables (first contact utilization, ongoing care, coordination information systems and comprehensiveness provided) [23]. In the multivariable model, all covariates were entered simultaneously. We controlled for type of healthcare provider, indicators of health need and demographic characteristics. All analyses were conducted using STATA version 12.1 (Statacorp).\n---\nResults\n\nThe demographic characteristics of our respondents were compared with the general population. The age distribution of our survey respondents did not differ significantly from the general population (2011 Hong Kong census data, p = 0.62). Our respondents were however more likely to have higher education than the general population (p < 0.001). Table 1 shows the socioeconomic, demographic and healthcare service characteristics of our respondents. Majority of respondents (74.5%) named a private provider as their primary care source, and financed their care in the past 12 months by only out of pocket payments (68.4%). Majority of the respondents (82%) had the same provider for at least 2 of the questions to define primary care while 79% had the same provider for all 3 questions.\nTable 2 presents the results of the comparison of PCAT scores among participants with different household incomes and sources of payments for healthcare in the past 12 months. It shows that the total PCAT score increases within each higher income category. Notably, respondents who have household-income lower than HK$ 15,000 (USD1934) reported significantly poorer experiences in all domains except for first contact utilization. Respondents who used only private insurance had significantly higher total PCAT score compared with other sources of health payments.\nTable 3 shows the adjusted odds ratios for our regression models where the total PCAT score is the dependent variable. Individuals with household-income between HK$ 15000-39999 (USD1934-5158) and HK$ 40000 (USD5159) and above are 47% (OR 1.47, 95% CI 1.10-1.96) and 200% (OR 2.07, 95% CI 1.38-3.09) more likely to have higher total PCAT score than the lowest income group respectively. Type of provider was also significant but in 2 different directions. Individuals using private care are 200% more likely to have higher a total PCAT score (OR 2.07, CI 1.53-2.82) compared with patients using government clinics. However, those using traditional Chinese Medicine are 63% less likely to have a good total PCAT score (OR 0.37, CI 0.19-0.70). There is a significant association between how health care was financed and primary care experience. Compared with individuals using only OOP, those who used only private insurance were 80% (OR 1.80, 95% CI 1.20-2.68) more likely to have a higher total PCAT score. Similar results were obtained when OLS linear regression was used as a sensitivity analysis.\nFurther analysis attempted to disentangle the overall experience of primary care quality by examining the association between patient's experiences of each core attribute of primary care and the explanatory variables. Table 3 shows the adjusted odds ratios from multivariable regressions where we fit the same model for 4 domains: first contact utilization, ongoing care, coordination information and comprehensiveness provided, as the dependent variables. Notably, household-income is the only indicator of SES to show an association with the primary care attributes. Respondents with household-income, respondents between HK$ 15000-39999 (USD1934-5158), and HK$ 40000 (USD5159) and above, are 33% (OR 1.33, 95% CI 1.00-1.79) and 84% (OR 1.84, 95% CI 1.21-2.78) respectively more likely to have higher first contact utilization compared with the lowest group. Similarly, respondents with household-income HK$ 40000 (USD5159) and above are 61% (OR 1.61, 95% CI 1.04-2.47) more likely to experience better coordination compared with the lowest income group. The association with comprehensiveness increases within each higher income category. As seen in Table 3, persons in HK$ 15000-39999 (USD1934-5158), and in HK$ 40000 (USD5159) and above are 46% (OR 1.46, 95% CI 1.07-1.93) and 90% (OR 1.90, 95% CI 1.26-2.87) respectively more likely to experience more comprehensiveness than those with householdincome below HK$ 15000 (USD1934).\nHealth payment source shows significant associations with first contact utilization, and comprehensiveness provided. Compared with respondents who pay only OOP, people who have private insurance are 67% more likely to have high first contact utilization (OR 1.67, 95% CI 1.09-2.56). However, they are 48% (OR 0.52, 95% CI 0.34-0.80) less likely to experience good comprehensiveness.\nThe type of healthcare provider shows a very significant relationship with ongoing care. Respondents who reported private providers and traditional Chinese practitioners had a 12 fold (OR 11.76, 95% CI 8.19-16.89),  \n---\nDiscussion\n\nWe examined socioeconomic differences in patients' evaluations of primary care quality, using a validated tool in Hong Kong. Consistent with other studies [9,19], majority of our respondents named a private provider as their primary care source, and financed their care in the past 12 months by only out of pocket payments. Our study shows that people with higher income experienced significantly better overall primary care quality compared with people with the lowest income. There appears to be a trend of higher quality rating as income increased across the three groups. Similarly compared with individuals who only paid OOP for care, patients who used only private insurance had significantly higher odds of good quality experience. People utilizing private care also had better quality experiences compared with those using government clinics.\nA US based study showed that people with low socioeconomic status had worse ratings of their healthcare provider [25], Conversely a 2001 UK study showed that socioeconomic differences accounted for a small amount of variability in patient evaluations of care [26], One previous study in Hong Kong suggested that the public specialist outpatient care was highly equitable [18]. However Wong et al's study suggested the quality experienced by patients differed based on the provider. Patients using private providers had significantly higher odds of better quality primary care than GOPC patients [19]. These findings are similar to those from developing countries with a large for-profit private sector and high out-of-pocket payments, wherein quality of service tends to vary by cost [27]. In most of Europe source of health payments doesn't typically account for variability in quality scores, because of the universal coverage, standardized quality of primary care and prepaid financing studies. However evidence from the US [28] and Hong Kong [19] has demonstrated an association between prepaid care and higher quality experiences. This contrast in how SES affects patient evaluations may be due to differences in   organization, financing and delivery of primary care in both contexts [29]. Similar to Hong Kong, the US has a largely provider driven, private and fragmented primary care sector in contrast to the UK which has publicly delivered primary care with a gate-keeping function and no payments demanded at point-of-care [26]. This study found a strong association between having a poorer quality primary care experience, and being in a lower-income group in Hong Kong. One possible explanation is that choice of provider is dictated by patients' ability to pay. Previous work has shown that the cheaper it is, the higher the chance it is of lower quality [30]. It may also be that providers tend to deliver inferior care to patients with lower SES due to reduced ability to pay [31]. Exploring the diversity within private care, which is the most frequent source of care, may provide more insight into the association between income and quality experienced. We also found an association between how patients paid for their healthcare and the quality they experienced. Private insurance is predominant amongst the higher cadre of the workforce [9], thus such people may well be able to access higher quality care generally. Alternatively, the coverage afforded by prepayment may allow for consistent receipt of more services or higher quality services, for people that have insurance even within the middle-income group.\nFurther analysis examined the association between quality experienced for each attribute of primary care and explanatory variables. Higher Household-income is significantly associated with higher first contact utilization and comprehensiveness scores. This is consistent with studies showing that people with higher incomes have higher utilization of primary care in Hong Kong controlling for differences in need [32,33]. In contrast, international experience from other high-income OECD countries except the US usually shows that primary care utilization is pro-poor [34,35]. Primary care utilization in Hong Kong is largely funded OOP thus cost may be a major deterrent for lower-income people [36]. While private insurance also showed significant association with higher first contact utilization compared to out-of-pocket payments, it was associated with lower comprehensiveness. This may be associated with available packages and requires further research.\nOur results also suggested that \"acceptability of health expenditure\", which was used as a measure of satisfaction, was significantly associated with better ongoing care but not with any other domain. This correlation is consistent other research showing that that price and patients satisfaction affects patient's decisions to continue with the same provider [37]. It is also consistent with the studies in Hong Kong showing that dissatisfaction with cost is the commonest cause of doctor shopping, which compromises the continuity of primary health care [17,38].\nOverall healthcare financing in Hong Kong is progressive since it is tax based [33]. However, out-of-pocket payments for primary care make access and thus utilization biased towards people with higher-incomes. In the short term the government should realign financial and incentives to help improve utilization by patients and comprehensiveness of services provided. Possible options include: encouraging private insurance providers to provide better coverage for primary care [36], and standardizing charges for private healthcare. Similarly scaling up the reform strategy to create primary care networks and facilitate access to electronic care records may greatly improve coordination [10]. With its health reform goals already in place, taking account of inequities should be a priority in monitoring plans, such that policy and program design is aligned to achieve the desired goal of an equitable and financially sustainable health system.\n---\nLimitations\n\nSeveral limitations were identified in our study. First, the use of a telephone survey may limit its representativeness. The response rate of 68% to the survey may limit the generalizability of results to only respondents, as their characteristics, health-seeking behavior and perceptions of quality may be different from those who refused to be interviewed. Second, the PCAT-AS tool algorithm for identifying the primary care provider has scope for people to speak about a usual source of care, which might be a specialist consulted frequently for a specific health problem rather than a primary care provider. These 2 categories of respondents e.g. those with a primary care provider versus those who speak about a usual source of care who might be a specialist may differ considerably, and comparing them may be misleading. Third, data from this study were cross-sectional, which does not allow for demonstration of causality. Additionally all explanatory variables were self-reported and unverified. They are thus subject to recall and misclassification bias. We were unable to adjust for the effect of household family size on income, marital status of the respondent and clustering in primary care practice, as data on specific providers was not available in the dataset. Additionally an ordinal logistic regression was fitted to the data instead of an ideal multi level model of patient within providers practice.\nDespite the above limitations, the findings of this study are consistent with the literature on Hong Kong and inequities in health systems in general. In particular, the association between income and sources of health payments with patient evaluations of quality cannot be attributed to bias or confounding. Additionally the  PCAT-AS is a validated tool regarding actual patient experiences.\n---\nConclusions\n\nIn summary, we have shown that quality of care in Hong Kong appears to be associated with household income and how patients pay. Unlike most high-income countries wherein primary care utilization is pro-poor, it appears to be pro-rich in Hong Kong, largely due to incoherent financing and a provider-dominated health system. This indicated that the inequality in primary care is likely to be related with the private dominated primary care system in Hong Kong. More public responsibility on primary health care should be advocated for in Hong Kong and similar contexts to reduce the inequality in primary care.\n---\nCompeting interests\n\nThe authors declare that they have no competing interests.\nAuthors' contributions XLW, SW, MW and SG designed the study. ZZZ, XLW, NY, HTL participated in data acquisition. OO and ZZZ are responsible for data analysis and interpretation. OO and ZZZ drafted the manuscript. XLW, SW, MW, WY and SG revised the draft for intellectual content. All authors helped in final approval of the completed article.",
        "Introduction\n\nChildren exposed to early childhood adversities, such as poor socioeconomic conditions, maltreatment and neglect or unhealthy family functioning, are at increased risk for poor physical and mental health and low educational success (1)(2)(3). A promising approach to reach and support socially disadvantaged families is home-based interventions in early childhood. Research trials show lasting effects of these interventions on mother and child health outcomes (4)(5)(6)(7)(8). However, less is known about the factors that contribute to the \"survival\" of such interventions in real-world settings after initial project funding has ended (9,10). Discrepancies between research settings and the context in which the interventions are implemented are a fundamental challenge for sustaining evidence-based public health interventions. Furthermore, over time interventions evolve due to factors such as changing populations, policies, available resources, and organizational structures, which may have positive (refinement of program delivery) or negative implications (loss of fidelity, discontinuation of program) (11). The identification, description, and understanding of internal and external factors, as well as how they interact to influence long-term implementation, is hence essential to maintain program continuation and effectiveness, further optimize the intervention benefits, and prolong program sustainability (6). By long-term implementation we mean the continuation of a public health program after the initial, project-based implementation that was supported by external (research) funding. Long-term implementation has also been described as program sustainability (10). Research on implementation processes and sustainability is needed to plan proactively for program continuation and to support programs in unfolding their full potential (12,13).\nThere is a wide range of terminologies for relevant constructs, and an abundance of frameworks and models identifying factors that are important for the implementation process of health interventions (14,15). Regarding the evaluation of early childhood interventions, previous research has revealed that contextual factors, as well as the dynamic interplay between the program and its environment, play a crucial role (16,17). Previous studies mostly investigated earlier stages of implementation, focusing on constructs such as fidelity, dosage and quality of early childhood interventions (18,19). However, for a comprehensive evaluation of the success of early childhood interventions, it is essential to understand the adoption, scale-up, and sustainability of interventions that have been in place within communities for some years (20). To date, only a few studies have investigated factors that are related specifically to long-term implementation of early childhood interventions, focusing mainly on settings in the US (21)(22)(23)(24). The factors identified in these studies include the consideration of the powerful role of context (e.g., community characteristics, addressing service context) as well as the impact of other factors such as program delivery (e.g., service dosage, staffing, program flexibility) (21,22). For instance, in the Nurse-Family Partnership program (NFP), a large home visiting program from the US, analyses of implementation and outcome data helped the identification of issues specific to certain contexts (23). The results of a mixed method analysis of participant attrition showed, for example, that home visitors in high retention sites adapted the program more completely to their clients' needs and used less directive and prescriptive approaches. Hence, a flexibilisation of the program led to adaptations of the program guidelines, nurse education, visit frequency, content, and location of visits (24).\nIn this study, we investigate the long-term implementation of the prenatal and infancy home visiting program Pro Kind (25,26). The program is based on the NFP program (27) and was adapted to the German context. The aims of the program, which focuses on psychosocially and economically disadvantaged families, are to enhance maternal and child health and to reduce the risk of child abuse and neglect. Professional home visitors (midwives or social workers) support first-time mothers from pregnancy to the child's second birthday. The home visits start during the second trimester of pregnancy and are generally then scheduled for every other week. The home visitors work with the families following a structured topic guide covering a wide range of issues including e.g., maternal health, healthy family routines, and life-course planning. In sum, the key features of the Pro Kind program are its tightly defined target group criteria (only first-time mothers, socially disadvantaged, start during second trimester of pregnancy), its thematically comprehensive and structured approach and its duration. These elements are essential for achieving the desired outcomes for children and families (27).\nThe development of the Pro Kind program is closely tied to changes that occurred at the national level at the time of its conception. A national early childhood intervention program (ECI) was initiated in 2006 (28,29). In this context, the Pro Kind program was one of several pilot projects to receive additional funding at the federal state level. It started in 2006 with a multicenter randomized controlled trial (RCT) in 15 sites located in three federal states (Bremen, Lower Saxony, Saxony) and ended in 2012. After this phase, the program materials were revised substantially in close cooperation with the National Center for Early Prevention. The key features mentioned before were, however, kept. The overall sustainability of the Pro Kind program was low across the sites, as it was continued in two of the original sites.\nAlongside the evaluation of program outcomes (26,(30)(31)(32)(33)(34)(35), the implementation of the Pro Kind program was closely monitored to examine implementation differences (36) and the association of participant characteristics and process variables with program attrition (37). However, investigations on the long-term program development that assess different implementation levels are still needed, considering the different natures of the local implementation settings.\nTherefore, we aim to explore factors that shape the long-term implementation of Pro Kind. The findings will enable us to illustrate and contrast factors contributing to the positive as well as negative program development and intervention performance.\n---\nMaterials and methods\n\n\n---\nStudy design\n\nWe conducted semi-structured interviews with program implementers (midwives, social workers, program managers) and key stakeholders (e.g., representatives of youth and welfare services, pediatricians). Qualitative methods were used to gain insights into participant's perspectives about the program development and its integration into local community structures over time. Ethical approval for the study was obtained from the ethics committee of the University of Bremen, Germany (reference number 2021-05). Participation in the interviews was voluntary, and all participants provided written informed consent. This study was conducted in line with the Consolidated Criteria for Reporting Qualitative Research recommendations (COREQ, Supplementary File 1) (38). The research team characteristics are presented in Supplementary File 2.\n---\nSelection of sites and site characteristics\n\nThe interviews were conducted at two German sites, Bremen and Brunswick. These cities were selected because they were the only sites, where the Pro Kind program was still being implemented since 2006. The city of Bremen has over 563.000 inhabitants and is located in Northern Germany. It is surrounded by the larger federal state of Lower Saxony, where the city of Brunswick, with about 248.500 inhabitants, is located.\nThe implementation conditions between the two sites differed already at program initiation. During the trial phase, 80 families in Bremen and 35 families in Brunswick took part in the Pro Kind program, reflecting the different sizes of the cities. At both sites, the program was delivered through established local social service organizations. However, in Brunswick the program was integrated into the structures and processes of the youth and welfare office to a greater extent than in Bremen. This affected in particular the procedures in recruiting families, laying with the youth and welfare office in Brunswick. With its three employees and a relatively small number of cases (about 10), the program in Brunswick has been scaled down over the past years, whereas in Bremen the number of cases has increased to 140.\n---\nSampling\n\nAt study onset, the research team (TB and MLS) presented the study aims and procedures to the Pro Kind staff from both sites at an annual network meeting. The program managers facilitated contact between the research team and the midwives and social workers who were implementing the program. The sampling of the Pro kind staff was purposive in that we wanted to prioritize interviews with staff who had been working with the program for several years. Potential key stakeholders in the field of early childhood interventions were initially identified by the program managers, the interviewed staff and the research team. Thereafter, snowball sampling was applied to identify further stakeholders, continuing until no additional interview participant could be identified or data saturation was achieved. In an effort to counterbalance the snowball approach, we conducted internet searches to try and identify further potential interview participants that were not mentioned by the program implementers.\nThe interviews were conducted between March and September 2021 and the interviewer (MLS) did not know any of the interviewees prior to the study.\n---\nInterview guide and data collection\n\nUsing an exploratory approach, the research team discussed the key domains of program implementation with the program implementers at their annual network meeting and developed topic guides for each target group (program implementers and stakeholders). The topic guides were designed to assess interviewees' perceptions and experiences on how the program has developed over time internally and in the interaction with contextual factors (see Supplementary File 3 for the original interview guides and the English translations).\nDepending on the COVID-19 regulations, the interviews were either conducted online (using the platform GoTo Meeting), by telephone or face-to-face. Where possible, the interviews took place face-to-face at the partner organization's workplace, in a closed room during normal operating hours. Regardless of the format, only the interviewee and the interviewer were present during the interview. Before the interview, all participants received the study information sheet and a consent form. The interviewees were interviewed once and did not receive any incentives for their participation. The interviews were conducted by the same researcher (MLS). The mean interview duration was 42 min (range 23-70 min). All interviews were conducted in German and were digitally audio-recorded and later transcribed verbatim. Samples of the transcripts were double-checked by reading the text while listening to the audio-recordings (MLS). Selected interview quotes were translated into English for this manuscript by MLS and CS, and TB cross-checked the translations (see Supplementary File 4 for the original quotes and the English translations).\n---\nData analysis\n\nInterview transcripts were coded in MAXQDA (version 2020). The analysis followed the phases of thematic analysis (39). To identify patterns in the data, we employed a hybrid inductivedeductive approach. Despite the exploratory nature of our data collection, the inductive analysis revealed certain themes and codes that increasingly aligned with a widely used implementation framework known as the Consolidated Framework for Implementation Research (CFIR). The CFIR offers a comprehensive typology categorizing barriers and facilitators associated with implementation (40). It comprises 39 constructs organized into five domains: Intervention Characteristics, Outer Setting, Inner Setting, Characteristics of Individuals, and Processes. Initially, all interviews were coded inductively by MLS. To obtain different perspectives on the coding scheme, two research assistants independently coded two interviews. Where differences occurred, MLS and the research assistants discussed the codes and the coding scheme was adapted accordingly. After the first round of coding, a second round was carried out by MLS to refine the codes. The codes were then collated and classified under the domains of the CFIRmodel. An example of the coding frame used to classify codes under the CFIR-domains is provided as Supplementary File 5. In the last step, underlying themes deemed to be of central meaning for the long-term implementation of the program were identified. The results were presented to the research team and the Pro Kind program managers several times to discuss major themes and key findings.\n---\nResults\n\n\n---\nSample characteristics\n\nA total of 25 persons, one man and 24 women, aged 29-68 years, took part in the interviews. Four were from Brunswick, and 21 were from Bremen. Eleven of the interviewees were program implementers (midwives, social workers, and program managers). Their experience of working with the Pro Kind program ranged from 5 to 16 years. The remaining 14 were stakeholders with a range of professional backgrounds, including social work, pediatrics and psychology, who were working for institutions related to early childhood interventions (e.g., child and youth welfare services, counseling centers, social security office, job centers).\n---\nFactors relating to long-term implementation organized under the CFIR-domains\n\nFactors related to long-term implementation were found in three of the five CFIR-domains: Intervention Characteristics, Inner Setting, and Outer Setting. The specific factors mentioned for each of the three domains are presented hereafter (in italics) using quotes from the raw interview data.\nFigure 1 depicts an overview of the three domains, the related factors (within the big bubbles) and subfactors (within the smaller bubbles) which are each highlighted as facilitating (+) or hindering (-) factors, or both (+/-). We did not identify factors in the data that could be assigned to the CFIR-domains Characteristics of Individuals and Processes.\n---\nIntervention characteristics\n\nThe main facilitating factors that emerged included the evidence of the effectiveness of the program and the relative advantage of the implementation of the program compared to other interventions in the field.\nBased on their experience during program delivery and the client's positive feedback, most program implementers were convinced that the families benefited from program participation. One stakeholder reported on the feedback from the families as follows:\n\"Especially families, who at first were somehow resistant, because they could not really grasp the program at first, said afterwards that it was really good.\" (stakeholder#1, Bremen) This view was shared by many other interview participants and reinforced the stakeholders' and program implementers' general belief in the program's approach. Targeting first-time mothers, starting early during pregnancy, and providing a relatively long program duration that allows a strong working alliance (between home visitor and mother) were viewed as obvious advantages compared to other local programs in the field of early childhood interventions by program implementers and stakeholders. In addition, the holistic and voluntary approach of the program was highlighted as particularly important. However, these core components of the program lead to relatively narrow target group criteria, and this also resulted in criticism of the program's lack of adaptability. For example, several stakeholders regret that the program is limited to first-time mothers only, thus withholding it from other mothers with needs: \"\u2026 but they already have a clearly defined target group. And many of my clients, for example, don't fit in at all. So it's not always first-time mothers who need this support. It is often second and third-time mothers (\u2026).\" (stakeholder#2, Bremen)\n---\nInner setting\n\nSeveral home visitors positively emphasized aspects related to the implementation climate within the organization. Permanent employment contracts for midwives, which are often not offered by comparable employers, were perceived as an appreciation of the program providers and their work fostering commitment to the program. Additionally, opportunities for further training, supervision, feedback, and case consultations on a regular basis were reported to contribute to a positive learning climate. An external stakeholder commented on the working conditions as follows:\n\"\u2026 the impulse of the professional support and (\u2026) the relatively conducive working conditions of the professionals, right? So professional advice, regular training, regular permanent employment of colleagues in contrast to the family midwives (\u2026) in all other states, family midwives are not employed, but work on a fee basis, which is a disaster for this work.\" (stakeholder#5, Bremen)\nThe implementation processes, both at the organizational level and individual level with clients, could thus be continuously reviewed and, if necessary, adjusted through the assistance and input of program providers. This ensured the sustainable quality assurance of intervention delivery, as well as the satisfaction and commitment of employees, fostering retention within the organization in the long-term.\nFurthermore, leadership engagement emerged as a strong facilitating factor for long-term program implementation. The active engagement of program managers in advertising the program personally, their involvement in the adaptation of intervention materials (also at national level), as well as their participation in various local events of early childhood interventions were reported and highlighted by various interview participants:\n\"And Pro Kind is actually also active in smaller projects. So, I have already experienced that the management participated in the designing of the flyer in simple language, or I mean that they were also there when these cards for smartphone use and childcare were somehow developed, that they were also present and actively contributed.\" (stakeholder#4, Bremen)\nThe commitment of the program managers to the program and beyond, to the promotion of early childhood interventions, was valued by stakeholders, leading to an increase in the program's visibility and fostering a trusting relationship between stakeholders and the program managers. Consequently, it promoted closer collaborations, especially regarding the referrals of families to the program.\nOne factor that links the inner and the outer setting is the critical role of the program's size at the two sites. It can be viewed as both, a facilitator and a barrier to long-term implementation, depending on the location. As it started with a larger team and more families, the Bremen site had a significant increase in funding and therefore in personnel. This enabled the program managers and the midwives to invest more time in networking at the city district level with the overall goal to establish collaborations with stakeholders who refer families to the program. Particularly the wide access to the target group is seen as an important prerequisite to survive in the long-term. According to stakeholders, the size of the program also played a  \"I experience Pro Kind as one of the big players. So, then I immediately think, okay, big organization, many colleagues, widely known too, and very established, in my choice of words.\" (stakeholder#6, Bremen) Due to a decreasing, smaller program size at the other site, in Brunswick, this facilitating process could not be initiated yet, with a negative consequence for its visibility. In this context, the program manager reported a shortage of staff, especially local midwives, and a different funding scheme hindering the program's growth at this site. Accordingly, the program size has played a significant role in its reach, ability to act and, thus, long-term implementation.\n---\nOuter setting\n\nRegarding the aspect clients' needs and resources, the program implementers emphasized the constant social change, mainly through immigration, which resulted in ongoing diversification, also of the target group. Accordingly, working materials were revised and provided in easy-to-understand language, and program implementers were trained in intercultural competencies. Despite these efforts, one stakeholder, for example, still saw a need to expand the language diversity in the team:\n\"\u2026 what I experience again and again (\u2026) is the language, the language barrier. So, in many families, the mother tongue is present, there is little German proficiency. And of course, not all midwives have these language skills. And I think we need to look again at the employees, can we also hire people who speak one language or another. (\u2026). I think that would probably also be Pro Kind`s wish.\" (stakeholder#3, Bremen) While acknowledging room for improvement, program implementers emphasized that directing attention and adapting to clients' needs aimed to enhance the working alliance and ensured the quality of program delivery-both are considered facilitators for long-term implementation.\nThere was a broad agreement that the degree of networking with other external stakeholders was essential for getting access to the hard-to-reach families, and to provide appropriate support. The program implementers rely on the cooperation with local stakeholders in order to integrate the families into the existing community structures, such as childcare, counseling services, or activities for mothers in similar situations and to promote their self-efficacy. One program manager summarized the importance of networking as follows:\n\"So, networking is very important. Pro Kind without networking wouldn't work at all (\u2026). Access is only possible through our stakeholders. And then there are specific issues. That means we see ourselves as guides for specific issues. Meaning we can tell the families that they can go there for the problem (\u2026) and that we work together to ensure that the families manage to receive the help and support they need.\" (program manager#1, Bremen) Further, in Bremen the versatile, extensive networking through participation in workshops, expansion of local working groups, in addition to low-threshold networking through personal contact in local city districts, facilitated a general expansion of the network. This was reflected in the consistent comments of the site's stakeholders, who perceived the program as being present, well-known and established in the local network. One stakeholder noted: \"\u2026 by being present as a program not only in individual districts, but throughout Bremen, it is well known and thus also an established partner in the municipal network.\" (stakeholder#4, Bremen) However, in Brunswick, networking was perceived to be a central challenge. A fixed recruitment procedure, organized by the youth and welfare services, limited the ability of the program implementers to get in contact with other stakeholders who could refer families to the program. It also provided limited flexibility in the way of advertising the program. Home visitors were concerned that recruitment through the youth and welfare services could lead to families being wary about participating:\n\"It is more difficult to motivate women to join the project. Because I tell you now, in the eyes of the young mothers, who may have already had experiences with the youth and welfare office as a child, the similarity with outpatient child protection service is too large.\" (home visitor#1, Brunswick)\nIn this context, the staff reported a stagnation and decline in the network, and thus, a decrease in the number of participating families. Besides the difficulties related to this recruitment regulation, further challenges in cooperation between the Pro Kind program and the local youth and welfare services were evident. Staff from both institutions reported difficulties in communication, lack of clarity about each other's roles and functions, and recurring tensions in the joint assessment of child protection. These difficulties were vested in the contrasting approaches and priorities of the two institutions. While both were interested in a constructive cooperation to ensure the best support for families, the Pro Kind staff placed greater value on the voluntary and preventive nature of the program. This also included a trustworthy relationship with mothers, respecting their privacy concerns. In contrast, the youth and welfare offices have a strong child protection mandate and emphasized the need for close exchange of information about critical cases.\nAs a part of external policy, the program managers, in particular, expressed uncertainty about the program's grant-based funding situation, which posed challenges to long-term planning:\n\"We have to re-apply every year, check again and that takes a lot of energy as well.\" (program manager#2, Bremen)\nOne key factor repeatedly mentioned by different participants in Bremen was the integration of the Pro Kind program in a community-wide approach to foster child health in disadvantaged families. This community-wide approach combined several preventive interventions and was accompanied by a large research project. The political decision to implement a community-wide approach secured extra funding for the Pro Kind program and led to an increasing number of families that could be served. At the same time, the pressure to relax the eligibility criteria increased.\n---\nDiscussion\n\nIn this study, we examined key factors related to long-term implementation of the home visiting program Pro Kind at two different sites in Germany.\nApplying the CFIR-model to the analysis, we found relevant factors related to three of the five CFIR-domains: Intervention Characteristics, Inner Setting and Outer Setting. Our findings also highlight the dynamic interplay between program factors (e.g., target group criteria), organizational factors (e.g., program size) and the context of implementation (e.g., degree of networking).\nLooking at the intervention characteristics, stakeholders and program implementers viewed the evidence of effectiveness and the relative advantage of the implementation of the program compared to similar interventions as contributors to long-term implementation. However, criticisms pointed to the lack of the program's adaptability as a constraining factor for growth, primarily because of the program's tight target group criteria. Concerning the inner setting, the implementation climate and the leadership engagement were perceived as relevant factors for staff qualification, continuity and the visibility and credibility of the program. In addition, the program's size emerged as an underlying factor that shaped the capacities for intensive networking, activities to increase visibility and access to the target group. Concerning the outer setting, next to the external policy and efforts to meet the clients' needs, the central importance of the degree of networking was highlighted. In particular, the program's relationship with the youth and welfare services emerged as challenging, mainly related to difficult access to families, tensions in communication, and different priorities.\nDrawing on research on the sustainability of public health interventions, the factors and subfactors we identified from the data largely align with the three primary influences on sustainability highlighted in numerous studies: Characteristics of the intervention, factors in the organizational setting, and factors in the community environment at each intervention site. Thus, the importance of shifting the primary focus away from funding sources when designing sustainability research is highlighted (10,41).\nTo some degree, there is an inherent tension between evidence of effectiveness, which relies on program integrity, and a program's adaptability and flexibility. In the field of implementation research on early childhood interventions, this challenge is well-recognized, as addressing this issue requires an understanding of theories, components, contextual influences (e.g., variation of risk exposures in families) that contribute to the effectiveness of a program (16). In the case of Pro Kind and NFP, the tightly defined eligibility criteria, the structured approach during the visits and the long program duration are hypothesized to be key elements for program effectiveness (27). Extending the target group criteria to include multiparous women has not been investigated within the RCT of the Pro Kind program, but has also been raised in other studies evaluating the NFP program (23,42). However, this adaptation could result in reduced or no effectiveness and would entail larger changes to the program's content. Current research from the NFP is therefore investigating whether the program can be adapted to meet the higher acuity and overlapping needs of multiparous mothers (43).\nOur results highlighted that a positive implementation climate, characterized by regular feedback, training, and supervision of staff, is crucial for successful implementation. This is because, as prior research shows, such a climate enhances the providers' abilities, readiness, and competencies to deliver early childhood interventions effectively (16,17,44,45). Consequently, these factors influence the quality of implementation of early childhood interventions.\nOur findings regarding the role of intensive networking in facilitating access to targeted families and addressing the families' needs by linking them to other resources in their communities, is in line with findings from other studies (46)(47)(48). These studies indicate that home visitors are likely to be more effective in retaining clients and in serving families with multiple needs when collaborating closely with those providing other relevant services in the local communities they serve. Moreover, continuously engaging stakeholders throughout the ongoing implementation processes might foster the fit between the intervention and the local context and the maintenance and improvement of interventions within care settings (9,11).\nOur findings also point to the critical program size, which enables or prevents program implementers to engage in intensive networking. This intensive networking is not only important for the practical work with families but also for the visibility in the stakeholder network and for political influence to sustain or increase funding. There is certainly no fixed rule for the critical size of a program and it would also be a limiting factor for a countrywide implementation if a program like Pro Kind could only be offered in larger cities to achieve an adequate size. Nevertheless, small-scale program sites may need specific strategies or extra support from other program sites for intensive networking.\nAs our findings confirm, the collaboration between early childhood interventions and youth and welfare services, particularly the child protection service is vital to maximize the benefits of the intervention (49). The issues reported at both sites are mostly in line with recent findings indicating the need to address misalignments of the priorities and working styles of the institutions involved (50) and the stigma associated with child protection services as well as to establish adequate communication channels between the programs to enhance collaboration and serve the same families adequately (42,51).\n---\nPractical implications\n\nFrom the themes that emerged from our analysis concerning the lack of adaptability, the program's size, and the degree of networking, several practical implications can be derived. These are particularly directed towards researchers and practitioners involved in program development and implementation, who must respond to continuous environmental changes to ensure the ongoing success of these programs. Adaptability is certainly a necessary trait of an intervention that tries to survive in a rapidly changing environment. As one approach for regular small-scale program adaptations, internal discussions about the appropriateness of the program materials and possibilities for further education could help an intervention remain relevant. With regard to alterations and changes that concern the whole intervention, implementation research has suggested that adding new components to an existing intervention can help to improve effectiveness (52,53). However, changing the core components, such as the eligibility criteria, may have consequences for the appropriateness of the intervention content and the effectiveness. Ideally, such an adaptation should be accompanied by a process and outcome evaluation (54). If program implementers decide to keep the integrity of the original model, then a strong emphasis on the program's effectiveness and relative advantage over other programs may counterbalance the lack of adaptability. While contextual factors, such as external policy-making or future austerity cuts, are rather out of control for program implementers, investment in local networking seems advisable because it may be a decisive factor for maintained funding. Regarding the critical size of the program, it may be specifically important for small-scale program sites to develop strategies for effective networking. For a preventive intervention that relies on voluntary participation and a trusting working alliance with the families, it may be important to keep a critical distance to the child protection service and to be viewed as working independent from it. Nevertheless, such intervention programs need to be reliable partners for the youth and welfare services when coordinated action is necessary. Proactive role clarification and clear process descriptions for coordinated action may help to resolve this tension.\n---\nLimitations\n\nThe findings of this study should be interpreted considering the following limitations. Firstly, the results reported here draw upon only two sites in Germany, which may limit their generalizability. Using the CFIR-model as a theoretical framework in our analysis however helped us to present our findings on a conceptual level, thereby adding to the transferability of the findings. Secondly, although the CFIR-model is comprehensive in scope, it does not pre-specify the importance or relationship between the individual factors. Consequently, while we highlighted the factors that came through as the most relevant ones according to our analysis, we cannot claim any causal relationships between them. Due to our exploratory data collection approach, we used the CFIR-model for guiding coding, data analysis, and reporting our results. It might have been however advantageous to incorporate the CFIRmodel into the data collection process earlier for capturing the factors more comprehensively. Thirdly, the number of interviews was not balanced between the two sites since we recruited only a small number of interview participants at the site where the program size decreased over time. Further insights into potential challenges of long-term program implementation from the stakeholders' perspective would have been beneficial for our analysis, but we did not identify any additional stakeholders who felt competent to discuss the program. However, considering the qualitative nature of our study, we gained fruitful information about hindering factors by including additional participants from a different context. Furthermore, following an exploratory approach, we did not collect data at the sites where the program ended after the initial study phase in 2012. This limits the generalizability of our findings. Lastly, it is possible that the snowball sampling may have resulted in a selection bias. Starting with the program implementers led us to interview stakeholders that were in close collaboration with the program. Despite additional internet searches conducted to counterbalance the snowball approach, we may have missed other stakeholders at the outskirts of the network who may have had different or more critical views on the program.\n---\nConclusion\n\nIn this qualitative study, we identified factors of particular importance for the long-term implementation of the Pro Kind program. We highlighted issues about the program's adaptability and the critical role of intensive local networking under consideration of different program developments at two German sites. Presenting our results on a conceptual level by using the CFIR-model as a theoretical framework and giving practical implications on the program, organizational and context level may inform future adaptations, enhancements and design of early childhood interventions for socially disadvantaged families.\n---\nData availability statement\n\nThe datasets presented in this article are not readily available because the informed consent signed by the participants did not include their agreeing to their qualitative data being shared publicly. Requests to access the datasets should be directed to Tilman Brand, brand@leibniz-bips.de.\n---\nEthics statement\n\nWritten informed consent was obtained from the individual(s) for the publication of any potentially identifiable images or data included in this article.\n---\nAuthor contributions\n\nSK, MS and TB conceptualized the study. MLS conducted, coded and analyzed the interviews. MLS drafted the manuscript with contributions from TJ and CS. All authors contributed to the article and approved the submitted version.\n---\nConflict of interest\n\nThe authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.\n---\nPublisher's note\n\nAll claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers. Any product that may be evaluated in this article, or claim that may be made by its manufacturer, is not guaranteed or endorsed by the publisher.\n---\nSupplementary material\n\nThe Supplementary Material for this article can be found online at: https://www.frontiersin.org/articles/10.3389/frhs.2023. 1159976/full#supplementary-material",
        "INTRODUCTION\n\nT hroughout the coronavirus disease 2019 (COVID-19) pandemic, family and friend caregivers have played an especially critical role in ensuring the health and well-being of a substantial population of chronically ill and vulnerable people. 1 In 2020, 21% of US adults were providing unpaid care to at least 1 family member or friend with health conditions or functional limitations-a substantial increase from 18% in 2015. 2 Two-thirds of care recipients are aged 65 years or older, 63% have a long-term physical disability, 32% have cognitive difficulties, and most fall into 1 or more categories of high risk 2 for COVID-19 infection and associated poor outcomes. To keep care recipients safe in the ever-changing pandemic environment, the work of many caregivers intensified-more than one-half reported an increase in caregiving intensity and burden following the start of the pandemic. 3 In a national sample of caregivers, 83% reported increased caregivingrelated stress following the start of the pandemic; increased stress was significantly higher among women caregivers (87%) than men (74%). 2 Before the COVID-19 pandemic, caregivers had significantly poorer physical and mental health than non-caregivers. [4][5][6][7][8][9][10][11] In addition, caregiving has been associated with health-related socioeconomic vulnerabilities (HRSVs)-including financial strain, food insecurity, transportation difficulties, and others-that are modifiable factors compounding the negative effects of caregiving on physical and mental health. 2 Pre-pandemic, 1 in 10 caregivers struggled to pay for food and other necessities for themselves and their care recipient, and women (61% of all US caregivers) were at higher risk for HRSVs than men. [12][13][14][15] The National Women's COVID- 19 Health Study was designed to capture the experiences of US women early in the pandemic. 16 A prior analysis found that nearly one-half of all women experienced incident or worsening HRSVs in the early pandemic period and those who did had 2 to 4 times higher odds of depression, anxiety, and traumatic stress symptoms. 16 The present analysis examines differences in prevalence of HRSVs before the pandemic and early in the pandemic for caregivers compared with noncaregivers. We hypothesized that caregivers would be significantly more likely to experience pandemic-related incident and worsening HRSVs compared with noncaregivers. Data are needed that describe HRSVs among caregivers, especially pandemic-related incident or worsening HRSVs, to facilitate caregiver policy and intervention development and inform COVID-19 response and recovery efforts.\n---\nMETHODS\n\n\n---\nDesign\n\nThe National Women's COVID-19 Health Study, conducted April 10-20, 2020, used a cross-sectional survey design that has been described in depth. 16 The University of Chicago Institutional Review Board approved the study protocol and all participants provided digital documentation of informed consent.\n---\nSetting\n\nThe study was conducted in the United States.\n---\nParticipants\n\nEnglish-speaking women aged 18 or more years were recruited from a research panel created by Opinions 4 Good. This research panel uses a non-probability, convenience sampling strategy to recruit participants online and through partnering organizations. Panelists were recruited to The National Women's COVID-19 Health Study via e-mail. Each e-mail included a unique, 1-time use survey link. Panelists' sociodemographic data, maintained by Opinions 4 Good, facilitated targeted recruitment to fulfill a nested quota sample of 3,200 women. The sample quotas matched the distribution of age and education of the 2018 US population of adult women and oversampled East/Southeast Asian women (Chinese, Filipino, Japanese, Korean, and/or Vietnamese) to achieve the goals of the primary study, which included subanalyses among race/ethnic groups, including East/Southeast Asian women.\n---\nMeasures\n\nSelf-administered, web-based surveys captured: (1) sociodemographic and self-rated health characteristics, (2) caregiving status, (3) main health condition of the care recipient, (4) 6 pre-pandemic HRSVs including financial strain, food insecurity, housing insecurity, interpersonal violence, transportation difficulties, and utilities difficulties, and (5) change in each of the 6 HRSVs \"since the start of the pandemic.\"\nCaregiving status was assessed by a yes/no item from the 2018 Behavioral Risk Factor Surveillance Study questionnaire: \"During the past 30 days, did you provide regular care or assistance to a friend or family member who has a health problem or disability?\" Participants who indicated \"Yes,\" were asked \"What is the main health problem, long-term illness or disability that the person you care for has?\" and provided with a list of 14 conditions.\nPre-pandemic HRSVs were assessed using the Centers for Medicare & Medicaid Services Accountable Health Communities (AHC) screening tool 17 and categorized as present or absent using the AHC instructions. Changes in HRSVs \"since the start of the pandemic\" were assessed using an adaptation of the AHC screening tool questions that used a 5-point Likert response scale (Supplemental Table 1). Change in each HRSV was categorized as: secure (absent pre-pandemic and early pandemic), incident (absent pre-pandemic and present early pandemic), persistent or improved (present prepandemic and unchanged or improved early pandemic), and worsening (present pre-pandemic and worse early pandemic). An HRSV was considered incident if the HRSV was absent pre-pandemic based on AHC instructions and the participant indicated a negative change in the early pandemic (eg, food secure pre-pandemic and \"a lot more worried\" about running out of food in the early pandemic; housing secure pre-pandemic and indicating \"I have a place to live today, but I am worried about losing it in the future\" in the early pandemic).\n---\nStatistical Analysis\n\nOf 3,634 eligible persons contacted, 3,200 were surveyed (88% cooperation rate, calculated as the number of participants who completed the survey divided by the number of eligible persons contacted). 18 Of 3,200 participants, 3,167 (99%) were eligible for this analysis (complete data for variables needed for weights and caregiving status). Of the 33 excluded, 24 were excluded due to missing income data required for weights and 9 were excluded due to missing caregiver status.\nPost-stratification sample weights were generated using the raking-ratio method with marginal distributions matched to 2018 US population estimates. All analyses were weighted. Differences in sociodemographic and health characteristics by caregiving status were examined using \u03c7 2 tests. Prevalence of pre-pandemic HRSVs and change in HRSVs were described by caregiving status. Multivariate logistic regression was used to model the odds of (1) each pre-pandemic HRSV, (2) early pandemic incidence of at least 1 HRSV, (3)  incidence of each HRSV among those without that HRSV pre-pandemic, and (4) worsening of each HRSV among those with that HRSV pre-pandemic. All models were adjusted for the following baseline covariates: age, race and ethnicity, marital status, education, income, number of people in household, number of children in household, self-reported physical health and mental health, and number of comorbidities. Covariates were selected for inclusion if known to be associated with HRSVs or caregiving status. No model covariate selection procedures were used. Due to small or null sample size, odds of incident utilities difficulties and worsening housing insecurity could not be calculated. All analyses were performed using Stata/ SE version 15.1 (StataCorp LLC). Results from unadjusted logistic regression models are reported in Supplemental Table 2 and Supplemental Table 3.\n---\nRESULTS\n\nNearly 1 in 3 US women (30%, n = 950) in this study were caregivers in the early pandemic phase. The most common conditions among recipients were old age/ infirmity/frailty (14%), mental illness (11%), heart disease (9%), and diabetes (9%). Of care recipients, 8% had Alzheimer's disease or a related dementia. Differences in sociodemographic and health characteristics between caregivers and non-caregivers are shown in Table 1.\n---\nPre-Pandemic Health-Related Socioeconomic Vulnerabilities\n\nMost caregivers (63%) had 1 or more prepandemic HRSVs, but rates were also high among non-caregivers (47%, P <.001). Nearly one-half (42%) of caregivers had 2 or more HRSVs compared with 28% of non-caregivers (P <.01). The most prevalent pre-pandemic HRSVs for caregivers and non-caregivers were food insecurity (48% vs 33%) and financial strain (42% vs 31%) (Figure 1A). When comparing prevalence of HRSVs by main condition of the care recipient, caregivers of people with arthritis/rheumatism had the highest prevalence (87%) of at least 1 pre-pandemic HRSV (66% were food insecure and 63% were experiencing financial strain). Caregivers of people with dementia (47%) and old age/infirmity/frailty (45%) had the lowest prevalence of at least 1 prepandemic HRSV (Table 2). After adjusting for sociodemographic and health characteristics, caregivers had higher adjusted odds of having at least 1 pre-pandemic socioeconomic vulnerability compared with non-caregivers (adjusted odds ratio [AOR] = 1.6; 95% CI, 1.3-2.0) (Figure 2A). Specifically, caregivers had significantly higher adjusted odds of pre-pandemic financial strain (AOR = 1.3; 95% CI, 1.1-1.6), food insecurity Note: Calibration weights were utilized and were generated based on the following variables: age group, race, education, income, and region. Change in each HRSV was categorized as: secure (absent pre-pandemic and early pandemic), incident (absent pre-pandemic and present early pandemic), persistent or improved (present pre-pandemic and unchanged or improved early pandemic), and worsening (present pre-pandemic and worse early pandemic). (AOR = 1.6; 95% CI, 1.3-2.0), and transportation difficulties (AOR = 1.9; 95% CI, 1.5-2.4) compared with non-caregivers.\n---\nEarly Pandemic Changes in Health-Related Socioeconomic Vulnerabilities\n\nIn the early pandemic, 54% of caregivers reported at least 1 incident HRSV compared with 38% of non-caregivers; incidence was higher for every HRSV. Due to higher incidence of individual HRSVs, the difference in prevalence of HRSVs between caregivers and non-caregivers widened for all HRSVs (Supplemental Table 4). The most common incident HRSVs for caregivers and non-caregivers were financial strain (31% vs 22% ) and food insecurity (17% vs 15%) (Figure 1B). More caregivers also experienced at least 1 worsening HRSV compared with non-caregivers (50% vs 32%). The most common worsening HRSVs for caregivers and non-caregivers were financial strain (34% vs 22%) and food insecurity (34% vs 20%) (Figure 1B). In the early pandemic, caregivers had higher adjusted odds of experiencing at least 1 incident HRSV (AOR = 1.8; 95% CI, 1.5-2.1) compared with non-caregivers (Figure 2B). Specifically, caregivers had higher odds of incident financial strain (AOR = 2.1; 95% CI, 1.6-2.7), interpersonal violence (AOR = 2.0; 95% CI, 1.5-2.7), food insecurity (AOR =1.6; 95% CI, 1.2-2.1), housing insecurity (AOR = 1.6; 95% CI, 1.1-2.3), and transportation difficulties (AOR = 1.9; 95% CI, 1.3-2.6).\nIn the early days of the pandemic, caregivers had higher adjusted odds of experiencing at least 1 worsening HRSV (AOR = 1.8; 95% CI, 1.4-2.3) compared with non-caregivers (see Figure 2C). Specifically, caregivers had higher adjusted odds of worsening financial strain (AOR = 2.0; 95% CI, 1.4-2.8). See Supplemental Appendix.\n---\nDISCUSSION\n\nConsistent with prior US population estimates, one-third of women in this national, early COVID-19 pandemic sample identified as a caregiver. Caregiving in the early pandemic was associated with significantly higher odds of incident HRSVs for every type examined (financial strain, food insecurity, housing insecurity, interpersonal violence, and transportation difficulties), and disparities in prevalence between caregivers and non-caregivers widened for each type, increasing by as much as 9 percentage points (financial strain). In prior studies (not specific to caregivers), these socioeconomic disadvantages were associated with poorer physical and mental health [19][20][21][22][23][24][25] in both pre-and early pandemic 16 phases. Widening HRSV disparities could increase these health disparities and negatively affect the health of care recipients.\nOur findings, using data from a national, cross-sectional study 16 conducted in April 2020, corroborate those of Beach et al, 26 both describing increased worry about food and finances since the start of the pandemic among unpaid family and friend caregivers compared with non-caregivers. That study 26 did not assess pre-pandemic HRSVs and, therefore, could not differentiate between new and worsening pandemic-related conditions among caregivers. Importantly, we found that caregivers experienced more new-onset and worsening of prevalent HRSVs in the early pandemic than non-caregivers. HRSVs are potentially modifiable through provision of community resources (eg, food pantries for food insecurity), but early in the pandemic, the Centers for Disease Control and Prevention guidelines (ie, social distancing and masks) and state mandates (ie, stay at-home orders and business closures) limited access to these resources. COVID-19 response and recovery efforts should formally recognize and work to remediate the disproportionate impact of the pandemic on HSRVs among caregivers. The high rates of financial strain among caregivers found in this study are noteworthy: caregivers of loved ones with arthritis, asthma, and diabetes had the highest prevalence of financial strain during the pandemic onset. Though early pandemic employment rates were higher among caregivers than non-caregivers (56% vs 43%), fewer caregivers were in the highest household income bracket (\u2265$100,000, 25% vs 31%). Caregiver's household size was larger (59% vs 46% with 3 or more) and included more children (26% vs 20% with 2 or more) who were now at home due to school closures. Though we did not examine changes in employment or income in this study, a 2019 national study of caregivers reported 61% were employed (similar to the 56% in this study), and that 6 in 10 caregivers endorsed caregiving-related negative impacts on their work. 27 Such impacts may have been exacerbated during the pandemic due to state-mandated closures of resources relied upon by employed caregivers-adult daycare, home help, and respite centers. 28 Historically, friend and family caregivers have been an invisible, mostly unpaid health care workforce whose efforts represent substantial cost savings to the health care system. In 2017 (the latest available data), the services of unpaid caregivers were valued at more than $470 billion dollars-a value that steadily rose from prior years and exceeded the total value of paid long-term care services as well as out-of-pocket health care spending. 29 During the pandemic, caregivers' responsibilities have grown, with increased needs for emotional support, medical support tasks, and assistance with everyday errands and daily food preparation. 30 The value of caregivers' efforts-both in terms of health care cost savings and lives saved-was likely far higher during the pandemic than before. The increased responsibilities and effects could be seen as a socioecological component of burden that is not captured in existing measures and suggest a need to broaden burden assessments to consider how circumstances affecting the general population may have greater impacts on caregivers.\nAn especially concerning finding of our study is the incidence of interpersonal violence associated with the early pandemic. More than 1 in 4 caregivers reported experiencing interpersonal violence in the early pandemic-and for more than one-half (15% overall), this was new. We found no prior studies examining interpersonal violence or related constructs (ie, domestic violence, intimate partner violence) among caregivers in the early pandemic. However, a rate of 25% is more than 3 times the pre-pandemic rate of interpersonal violence among women (7%) reported in a national, 10 site clinical study using the same measure. 31 Additionally, physician and public health scientists have indicated that pandemic-related mandates (ie, stay-at home orders) and other stressors (eg, loss of employment or income) created conditions likely to increase rates of intimate partner violence, especially among women. [32][33][34][35] These concerns are supported by hyperlocal Notes: Calibration weights were utilized and were generated based on the following variables: age group, race, education, income, and region. Sample sizes for participants with incident utilities (B) and participants with worsening housing (C) were too small to include in the models. Models adjusted for age, race/ethnicity, marital status, education, income, number of people in household, number of children in household, self-reported physical health and mental health, and number of comorbidities. Change in each HRSV was categorized as: secure (absent prepandemic and early pandemic), incident (absent pre-pandemic and present early pandemic), persistent or improved (present pre-pandemic and unchanged or improved early pandemic), and worsening (present pre-pandemic and worse early pandemic). data from police reports of rising rates of domestic violence calls. 31,32 Prior studies of violence by care recipients toward family caregivers highlight the need for clinicians to sensitively address this violence given the unique relationship between the caregiver and care recipient. Clinical guidelines and interventions specific to caregivers who are abused are needed to implement this call-to-action, especially given the pandemic-related increase in interpersonal violence among caregivers.\n---\nAdjusted odds ratio\n\n\n---\nAny socioeconomic vulnerability\n\n\n---\nLimitations\n\nThis study should be interpreted in light of several limitations. Participants were enrolled from a non-probability research panel with a very high response rate, which may limit generalizability. Using a previously enrolled research panel, however, allowed rapid enrollment of a large sample to study early pandemic effects. In addition, use of poststratification sample weights for the variables of age group, race, education, income, and region in all analyses forces the sample to match the marginal distributions of 2018 US population estimates on these factors, increasing likelihood of generalizability. Notably, the proportion of caregivers and the sociodemographic and health characteristics of the study participants were similar to other studies, including a 2020 nationally representative probability sample of caregivers. 2,27 For example, the proportion of non-Hispanic White caregivers was 61% in our study and in this national sample. 2 Also, caregiving experience differed depending on the care recipients' condition(s). Though we were able to qualitatively compare rates of HRSVs by care recipient's main condition, sample size limited these analyses. Lastly, this survey was done at a single time point and some responses, including those assessing pre-pandemic states, may be subject to recall or other bias. This study did not directly assess changes in caregiving demand or habits, including caregiving intensity.\n---\nCONCLUSIONS\n\nAs of 2020, more than 53 million Americans provided care for a friend or family member-a number projected to grow as the population ages and fertility rates decline. Additionally, people having long-term COVID-19 complications may rely on friend and family caregivers. [36][37][38][39]  Funding support: Research reported in this publication was supported by 5R01AG064949 (K.B., S.T.L., J.A.M., V.W), 5R01MD012630 (K.B., S.T.L., J.A.M., V.W), R21CA226726 (S.T.L.), and 1R01DK127961 (S.T.L., J.A.M., V.W). The content is solely the responsibility of the authors and does not necessarily represent the official views of the National Institutes of Health.\nPrevious presentation: Portions of the research reported in this publication were presented as a poster at the Interdisciplinary Association of Population Health Sciences; October 18, 2021; Baltimore, Maryland.\n---\nSupplemental materials\n\n",
        "Introduction\n\nAdults play a crucial role in supporting or limiting children's play activities [1,2]. They take on the roles of teachers and social referents throughout an individual's childhood and adolescence, and are central in leading, organizing, and participating in children's outdoor play experiences. Adults, especially parents, can positively promote children's participation in specific activities by acting as play supporters, enrolling children in sports, driving them to events, or using their own behavior to encourage physical activity [3]. However, parental controls can also significantly restrict children's play experiences, for example, due to fear of traffic dangers [4], social apprehension [5], or physical hazards [6]. Therefore, when overseeing children's outdoor play, adults influence children's play behavior in essentially two ways: firstly, by assuming the role of playmates and engaging frequently with the children, they can enhance children's play experience. Secondly, adults can restrict children's play behavior by prioritizing their safety. Notably, these two approaches can be combined and employed interchangeably on the playground. However, the multifaceted nature of adult supervision, encompassing both these roles, has yet to be thoroughly examined in previous research.\nPrevious research has examined phenomena related to parenting styles and parental supervision modes in Western society [3,7,8]. These phenomena, which greatly influence children's use of outdoor environments and their childhood outdoor play experiences, are widely discussed, including over-parenting [9,10], over-supervision [10,11], and intensive mothering [12][13][14]. To be more precise, the prevailing trends of over-supervision and maternal oversight have substantially restricted children's independent mobility and reduce their interactions with the surrounding environment on a global scale. Research on these phenomena also concludes that parenting styles are largely influenced by the dominant cultural background of a society. However, faced with the overwhelming parenting trends, each parent can make their own choice about whether to follow or counter their culture [2]. Apart from over-parenting, the phenomenon of intensive mothering also stems from the cultural tradition that associates mothers with the roles of nurturing and caregiving, not only in Western society, but also worldwide. The ideology of intensive mothering encompasses a set of beliefs about appropriate caregiving for children and corresponding maternal behavior [12,15]. This emphasis on maternal guidance and support is deeply ingrained in the prevailing ideology of Confucianism, which underscores the importance of filial piety, harmonious family relationships, and the transmission of ethical values across generations [14]. Significantly, even in contemporary times, this ideology continues to serve as a foundational cornerstone of social values within Chinese society.\nFurthermore, the phenomenon of grandparents parenting is widely discussed in both Western and Chinese societies. In the early years of the twentieth century in the USA, extended family members, particularly grandparents, held authoritative positions within the family structure and made substantial contributions to crucial family functions, including shared child-rearing. However, more recent research in the USA and Western Europe shows that the majority of grandparents adopt a non-interference style in intergenerational relationships and do not assume a central role in caring for or rearing grandchildren [16][17][18]. In contrast, in China, traditional cultural values underscore the importance of harmonious extended families, resulting in a well-established tradition where grandparents play an active role in childcare responsibilities [18][19][20][21]. In recent times, collaborative caregiving by both parents and grandparents has accentuated the role of grandparents in assisting working parents with childcare. According to the Shanghai Municipal Population and Family Planning Commission, a striking 90 percent of the city's young children receive care from at least one grandparent, and of these, half exclusively rely on their grandparents for care. This trend is particularly pronounced within urban families.\nPrevious research conducted in Western societies has highlighted that because different groups of caregivers have distinct perceptions of outdoor environments and expectations of children's outdoor behaviors, they tend to adopt diverse approaches to supervising children's outdoor play [9,[22][23][24]. These distinct modes of supervision can either encourage or curtail specific play activities among children. For instance, research indicates substantial differences in the supervisory methods employed by mothers and fathers, with maternal supervision being more attuned to risk awareness, potentially leading to reduced levels of physical activity among children [6]. However, in the context of China, where there are emphases on maternal guidance and grandparents frequently participate in the daily care of grandchildren, the predominant modes of supervision adopted by various groups of adults and their impact on children's activities remain an unexplored area of study.\nTherefore, building upon the prevalent trend of the domestic division of labor in urban families, this study focuses on a case study conducted in a representative local community. This community, established in the 1980s and currently undergoing an urban renewal project, is situated in the inner city of Shanghai. The principal aim of this research is to assess the impact of varied supervision methods, implemented by diverse caregivers, on the outdoor activities of preschool-aged children (1-6 years old) within communal outdoor spaces. In the pursuit of this goal, the study segregates caregivers into parental and grandparental generations and, based on the emphasis on protection or interaction, identifies four distinct accompanying modes. Additionally, this research also investigates the reasons behind adults' choices of different supervision modes.\nThe research results will help fill the knowledge gap regarding intergenerational interactions in Chinese families, on the one hand. On the other hand, by gaining more insight into the mechanisms of supervision modes on children's outdoor activities, it will be possible to promote more positive intergenerational interactions and build more sustainable all-age-friendly communities in Chinese cities.\n---\nMethods\n\n\n---\nDefinition of Generational Accompany Mode for Children's Outdoor Activity\n\nAdults who accompany children's outdoor play are distinguished based on both generational differences and family relations. From the perspective of generational differences, caregivers in the parental generation include fathers, mothers, babysitters, teachers, and relatives or neighbors of a similar age to the children's parents. In the grandparental generation, there are grandfathers, grandmothers, and relatives or neighbors of a similar age to the children's grandparents. From the perspective of family relations, caregivers can be categorized as adults within the family or adults outside the family. Based on the phenomenon of co-parenting within Chinese families, this research pays more attention to the accompany modes of parents and grandparents when caring for children in outdoor environments.\nIn addition, previous studies from the perspective of environmental behavior have concluded that the roles adults assume when accompanying children in outdoor play can be divided into two basic types: partners and guardians [2,3]. When acting as play partners, adults participate in children's play and interact intensely with them to promote various types of play. When acting as guardians, adults do not frequently communicate with children, but limit their extraordinary activities to prevent harm. It can be observed that the role of play partners emphasizes interaction, while guardians focus more on protection. Therefore, based on the different emphases of adults' accompanying behavior during children's outdoor play, four basic modes can be identified: PSIS, PSIW, PWIS, and PWIW. The division and description of each mode are shown in Figure 1. possible to promote more positive intergenerational interactions and build more sustainable all-age-friendly communities in Chinese cities.\n---\nMethods\n\n\n---\nDefinition of Generational Accompany Mode for Children's Outdoor Activity\n\nAdults who accompany children's outdoor play are distinguished based on both generational differences and family relations. From the perspective of generational differences, caregivers in the parental generation include fathers, mothers, babysitters, teachers, and relatives or neighbors of a similar age to the children's parents. In the grandparental generation, there are grandfathers, grandmothers, and relatives or neighbors of a similar age to the children's grandparents. From the perspective of family relations, caregivers can be categorized as adults within the family or adults outside the family. Based on the phenomenon of co-parenting within Chinese families, this research pays more attention to the accompany modes of parents and grandparents when caring for children in outdoor environments.\nIn addition, previous studies from the perspective of environmental behavior have concluded that the roles adults assume when accompanying children in outdoor play can be divided into two basic types: partners and guardians [2,3]. When acting as play partners, adults participate in children's play and interact intensely with them to promote various types of play. When acting as guardians, adults do not frequently communicate with children, but limit their extraordinary activities to prevent harm. It can be observed that the role of play partners emphasizes interaction, while guardians focus more on protection. Therefore, based on the different emphases of adults' accompanying behavior during children's outdoor play, four basic modes can be identified: PSIS, PSIW, PWIS, and PWIW. The division and description of each mode are shown in Figure 1. Given the complexity and diversity inherent in children's outdoor activities, there arises a need for a succinct and practical categorization system that caregivers can readily comprehend. Among the various classification frameworks for children's play, the system proposed by Bishop and Curtis has been both applied and refined within the scope of this study. This classification system places a distinct emphasis on the nature of children's play behavior, stratifying it into three foundational types: those with high verbal, imaginative, Given the complexity and diversity inherent in children's outdoor activities, there arises a need for a succinct and practical categorization system that caregivers can readily comprehend. Among the various classification frameworks for children's play, the system proposed by Bishop and Curtis has been both applied and refined within the scope of this study. This classification system places a distinct emphasis on the nature of children's play behavior, stratifying it into three foundational types: those with high verbal, imaginative, or physical content [25]. As part of the pilot study, conducted prior to the main research, this classification was further developed. This development included a heightened focus on physical play, discerning whether play activities were organized and whether they necessitated specific facilities. This adaptation was due to the observation that physical activities were the dominant type, but verbal or imaginative play activities were rarely witnessed in public open spaces during the pilot study. Ultimately, within the formal research phase, this categorization framework was expanded to encompass a total of six distinct types of play activities. Detailed descriptions of these play activity categories can be found in Table 1. \n---\nData Collection\n\nDue to the specific focus on the behaviors of distinct groups, this study adopted sociological research methods. From 2020 to 2022, data collection was conducted in Changhai Sub-district, Yangpu District, Shanghai. Initially, a quantitative research approach was employed, involving a questionnaire survey to collect data concerning the current state of children's outdoor activities and the methods caregivers use when accompanying them. Given that preschool children often lack the capacity to independently report their outdoor activities, parents or other adult guardians were entrusted with recording the children's outdoor play. The questionnaire on this subject comprised 8 questions: 3 about basic information, 2 about children's activities, and 3 about caregivers' activities. However, the questionnaires were designed to document children's activities, whereas a deeper understanding of the motivations behind these activities could only be attained through interviews. Subsequently, qualitative research was conducted via semi-structured interviews, centering on the behavioral motivations of different caregiver groups during children's outdoor activities. Furthermore, insights into the community's social and spatial environments were gleaned from conversations with local families.\nIt is noteworthy that the entire field research was crafted to comprehensively capture children's outdoor play activities, especially from four dimensions: space, people, activity, and time. This structure was rooted in the SPIT (Spaces, People, Interventions, and Time) model for understanding children's outdoor experiences, initially proposed by Woolley and subsequently refined by Tang [26] into the SPAT model (Space, People, Activity, and Time). This paper exclusively presents data and findings relevant to participants and activities within children's outdoor play.\n---\nData Analysis\n\nThe overall research employs a mixed methods approach, combining both quantitative and qualitative research [27]. The questionnaire data were analyzed using SPSS 13.0. The analysis commenced by employing Cochran's Q test to identify differences among the options. Once the significant differences among the choice of options were confirmed, correspondence analysis (CA) was applied to further explore the correlations between the variables. CA is a multivariate statistical technique proposed by Hirschfeld in 1935, designed to check correlations between categorical data rather than continuous data [28]. The results of CA are presented in an interaction summary table composed of qualitative variables and through visualizations that illustrate the degree of association between the variables (see Figures 234). This comprehensive approach facilitated a deeper exploration of how differences in outdoor companions impact children's outdoor activities.\nvariables. CA is a multivariate statistical technique proposed by Hirschfeld in 1935, designed to check correlations between categorical data rather than continuous data [28]. The results of CA are presented in an interaction summary table composed of qualitative variables and through visualizations that illustrate the degree of association between the variables (see Figures 234). This comprehensive approach facilitated a deeper exploration of how differences in outdoor companions impact children's outdoor activities.   variables. CA is a multivariate statistical technique proposed by Hirschfeld in 1935, designed to check correlations between categorical data rather than continuous data [28]. The results of CA are presented in an interaction summary table composed of qualitative variables and through visualizations that illustrate the degree of association between the variables (see Figures 234). This comprehensive approach facilitated a deeper exploration of how differences in outdoor companions impact children's outdoor activities.   Furthermore, the data collected through interviews underwent transcription and, subsequently, ROST Content Mining System (version 6.0), a content mining system software specifically designed for Mandarin text, was utilized for coding. By leveraging this software, the text data were organized and categorized based on the themes of \"play activities\" and \"caregivers\". The next step involved employing the semantic network analysis function to delve into the underlying connections and structural relationships among words related to children's outdoor activities and caregivers. The aim of this semantic network analysis on the interview transcriptions was to investigate the factors influencing how family members care for children during outdoor activities and to explore the impact Furthermore, the data collected through interviews underwent transcription and, subsequently, ROST Content Mining System (version 6.0), a content mining system software specifically designed for Mandarin text, was utilized for coding. By leveraging this software, the text data were organized and categorized based on the themes of \"play activities\" and \"caregivers\". The next step involved employing the semantic network analysis function to delve into the underlying connections and structural relationships among words related to children's outdoor activities and caregivers. The aim of this semantic network analysis on the interview transcriptions was to investigate the factors influencing how family members care for children during outdoor activities and to explore the impact of differences in accompanying caregivers on children's outdoor experiences.\n---\nParticipants\n\nAccording to statistics collected for the local government, there were over 6400 children aged 1 to 6 years living in this area in 2020 [29]. With a confidence level of 95% and a confidence interval of 4, the sample size should be more than 549. Consequently, employing cluster sampling, children were recruited with assistance from three kindergartens within the Changhai sub-district. Parents or grandparents, who registered as the principal guardian of children, were asked to complete online questionnaires, capturing detailed records of their children's outdoor activities, encompassing information about play spaces, participants, activities, and duration. Over these two years, over 600 families participated in the data collection. After excluding questionnaires with missing or incomplete answers to fundamental questions within each section, and those containing conflicting or contradictory responses, a total of 553 children's outdoor activities were subjected to analysis. Additionally, within the questionnaire, families were invited to partake in interviews. Then, 74 families expressed their willingness to engage in interviews. Subsequently, the researchers established contact with each of them. After receiving comprehensive information about the interview's content and procedure, and confirming the interview schedule, semi-structured interviews were successfully conducted with a total of 23 households, without any dropouts during the process. These interviews took place within the children's homes and focused on aspects related to the reasons for choices made in the questionnaires. Each of the interviews took around 30 min.\n---\nEthical Considerations\n\nThe research conducted in this study complies with the requirements and regulations in China. Ethical approval for all data collection procedures was granted by the Institutional Review Board at Tongji University. Furthermore, all participants were provided with in-detail information about the research procedure before granting their consent to participate. Consent forms were signed before initiating the online questionnaire responses, indicating permission for the authors to share the findings through publications and presentations. In addition, verbal consent was secured through audio recording before commencing the formal interviews. At no point during the data collection process were children and their families required to provide their full names. Additionally, all contact information, including phone numbers, was removed prior to the data analysis phase. Prior to commencing data collection, researchers underwent necessary training; a total of five researchers received training and conducted data collection over the span of two years.\n---\nResults\n\n\n---\nChildren's Gender and Age\n\nIn the survey, a total of 553 children's outdoor activities were reported. There were more boys (58.77%) than girls (41.23%) (Table 2). Furthermore, the dominant age group among the surveyed children was 4 to 6 years old (Table 3). \n---\nChildren's Outdoor Activities and Accompany Adults\n\nThe results demonstrated that all p-values were less than 0.01, indicating significant differences in the proportions of children's outdoor activity methods (Table 4) and outdoor activity companions (Table 5). The main ways for preschool children to carry out outdoor activities in the community are unorganized activities, with or without facilities. These activities include running, hanging around, and riding bicycles, scooters, or roller skates. In terms of caregivers, showing in Table 5, mothers assume the primary caregiving role for preschool children during outdoor activities. Grandmothers also take on more responsibility for children's outdoor play compared to fathers and grandfathers. However, babysitters, teachers, and other adults rarely accompany children during their outdoor play.\n---\nCorrespondence Relations between Caregivers and Accompanying Modes\n\nCorrespondence analysis unveils the relationship between the accompanying adults and their methods of accompaniment. As shown in Table 6, correlations exist between these two variables. These relationships are graphically represented in the form of a correspondence graph in Figure 2. From Figure 2, it becomes apparent that grandparents, encompassing both grandfathers and grandmothers, exhibit a strong correlation with the ISPS and IWPS modes. Conversely, the ISPW mode is associated with parents, including both fathers and mothers. Furthermore, the IWPW mode displays certain correlations with babysitters and teachers, but has a weaker correlation when compared to parents or grandparents. \n---\nCorrespondence Relations between Accompanying Modes and Children's Activities\n\nThe connection between accompanying modes and children's outdoor activities is also explored through correspondence analysis (as shown in Table 7). As depicted in Figure 3, it becomes evident that organized activities and community activities are closely associated with the accompanying modes of ISPW and IWPW. Conversely, unorganized activities, whether they involve facilities or not, as well as activities that rely on facilities, exhibit a strong relationship with the accompanying modes of ISPS and IWPS. This suggests that when children engage in organized ball games or community activities, caregivers pay less attention to ensuring their safety. However, when children participate in less structured outdoor activities, caregivers tend to prioritize their protection to a greater extent. ** p < 0.01.\n---\nCorrespondence Relations between Caregivers and Children's Activities\n\nThe results further indicate that caregivers within the same generation display a more consistent preference for guiding children's outdoor activities (refer to Table 8). Parents tend to promote unorganized activities for children, regardless of whether these activities involve facilities or not. Conversely, grandparents are more inclined to encourage children to use fixed facilities on site. Babysitters and teachers show greater similarity, as they tend to support community activities (refer to Figure 4). \n---\nPhysical and Social Environments of Children's Outdoor Activities\n\nBased on the correlation analysis conducted above, the study clarified the special relationship between the accompanying mode and children's outdoor activities. To further understand these relations, the study explores how family relationships and the social environment affect intergenerational care, based on the content of in-depth interviews. The interview data were analyzed thematically, categorizing the content of transcribed conversations into themes and topics.\nWithin the theme of \"caregivers\", specifically in the sub-topic of 'family roles', the data reveal that families in Shanghai generally believe that parents should be the most important caregivers and play the most important role in educating and accompanying children, while grandparents usually undertake auxiliary work. This distinction is especially prominent in the contemporary prevailing culture, which emphasizes parental responsibility on communication, education, and guidance through entertaining activities during children's outdoor activities. At the same time, grandparents, in their supporting role, focus more on \"carefully watching\" children and ensuring their safety. Additionally, parents and grandparents believe that they should not \"intervene\" in the care of neighborhood children, considering it \"other family's business\".\nContinuing within the theme of 'caregivers', in the sub-topic of 'perceptions of environments,' the interviews reveal that parents and grandparents primarily consider \"neighborhood relationships\" and \"same-age playmates\" as determining factors of their accompany mode. Regarding neighborhood relations, caregivers commonly mention factors such as \"unfamiliar neighbors\", \"unknown faces\", and \"strangers in the community\", which lead to increased guarding and watching of children. Moreover, due to a lack of same-age playmates in the community, accompanying caregivers have to assume the role of playmates, especially during activities such as running or riding bicycles and scooters. Many times, children still want to compete with others, but if there are no children of the same age or unknown children, the caregiver has to \"play with the children\". However, compared to parents, the elderly has a relatively limited ability to act as children's playmates. Therefore, grandparents prefer to take children to places with fixed equipment and watch them play.\nMoreover, transitioning to the theme of \"caregivers' perceptions\" within the other theme of 'play activities', the data underscore that because public spaces are constrained in urban central areas, children play in both formal playgrounds, such as plazas and playgrounds with fixed equipment, and informal play spaces, such as roads, lawns, and parking areas. Throughout the interviews, many parents express their reluctance to take their children to playgrounds with fixed facilities because they are \"outdated\", \"poorly maintained\", and \"unsuitable for children's play\". Therefore, more parents choose to let their children play in open spaces between buildings and on roads because they are more spacious and allow for bicycle or scooter riding. However, playing on the road presents more potential safety hazards, requiring caregivers to closely follow and firmly watch their children.\n---\nDiscussion\n\nThis study provides a comprehensive understanding of intergenerational accompanying methods and their influence on children's outdoor activities, as well as the reasons behind adults' choices among these modes, within the communities in the inner-city area of Shanghai.\nRegarding caregivers, the results highlight that intensive mothering and co-parenting are common phenomena in Chinese urban society. The phenomenon of intensive mothering, or a maternal-dominated parenting style, is not uncommon in both Eastern and Western societies [24]. Based on the study findings, it could be assumed that in contemporary Chinese cities, the emphasis on maternal rearing and care remains the dominant parenting style. However, unlike in traditional society, women in contemporary Chinese society participate in various types of work. Statistics indicate that in 2017, the female labor force participation rate in China was 61% (compared to 76% for men), which is among the highest in Asia and higher than the European average [12]. Consequently, balancing work and family duties can be challenging for women in contemporary Chinese society. In this context, joint child-rearing by both parents and grandparents becomes more of a necessity rather than a choice for families; though, joint child-rearing by both parents and grandparents has a long history in Chinese society [20]. For the elderly, taking care of grandchildren not only enriches their daily lives, but also provides an opportunity to experience family happiness [20]. Now, with more working mothers in urban families, co-parenting has become a necessity. There is a clearer division of responsibilities in joint child-rearing, which makes taking care of grandchildren more of a task than a choice for enjoying life for grandparents.\nFurthermore, this research examines the differences in accompanying modes applied by different generations of adults when taking care of children's outdoor activities. Considering the evidence presented, a tentative conclusion might be drawn that grandparents pay more attention to protection, while parents prefer to spend more time playing and talking with their children. A reasonable interpretation could revolve around the distribution of responsibilities within urban families, which likely plays a pivotal role in shaping this distinction. In modern urban families, parents play a more important role in leading and supporting various play activities, and even educating children through play activities, while grandparents provide more assistance and often act as substitutes in the absence of parents. Therefore, the purpose of grandparents' accompanying is more basic: to prevent children from safety risks and potential harm. This research finding supports previous studies that concluded that different caregivers would adopt different modes of accompanying children during outdoor play [11,20,24]. This finding also further develops previous research indicating that maternal caregiving is more sensitive to safety risks [6,24], with the emphasis that the parenting methods of grandparents are even more influenced by potential hazards in the context of co-parenting in urban families in Chinese cities.\nAdditionally, this research reveals that different accompanying modes are related to different types of play. When children participate in organized ball games or community activities, caregivers tend to pay less attention to their safety. However, when children engage in activities such as running, chasing, cycling, or skating freely on the road, caregivers closely protect them from potential traffic dangers or stranger risks. On the other hand, when children play on fixed facilities in the playground, the potential harm posed by outdated fixed facilities becomes the main concern. Therefore, it is reasonable to consider that the primary concerns of caregivers during children's outdoor play are still traffic dangers [4,30], stranger risks [31], and the potential harm posed by outdated fixed facilities. The concerns change with different activities, prompting caregivers to adopt different accompanying modes. Compared to research conducted in other parts of the world, the concerns of the potential harm posed by outdated fixed facilities are initially reported in Chinese cities, whereas, consistent with previous research, Asian and Indian parents consider traffic danger the most common reason for their concern regarding their children's outdoor activities [4,32].\nBeyond these results, this research provides a practical model for comprehending the rationale behind adults' involvement in children's outdoor activities. This model categorizes common accompanying behaviors into four modes based on the leading motive of interaction or protection. Serving as an intermediary, this model helps to understand the varied approaches adults adopt while accompanying children in outdoor play. Furthermore, it facilitates the exploration of the mechanisms underlying the different groups of adults' accompanying modes on children's outdoor play activities. Based on these four modes, the differences in the way parents and grandparents accompany children during outdoor activities could be described in detail and interpreted in-depth. Remarkably, this model's relevance resonates with the prior research that examined the comparative supervision styles among fathers [6], mothers [33], and siblings [9], showcasing its adaptability to a wide array of caregiver groups. This flexibility underscores the model's practicality and underscores its potential utility in understanding the intricate dynamics of caregiver involvement across diverse demographic contexts.\nBased on these findings, for better environments in old communities in Shanghai or other similar high-density cities, it can be predicted that if the physical and social environments of these communities built decades ago are improved to be more childfriendly, children's outdoor play could become more independent from caregivers. As a result, caregivers would feel more relaxed and engage more in play with their children. Furthermore, practical suggestions for building more child-friendly cities can be further developed with more consideration of improving the inclusivity of the community. This could also be considered as a limitation of this research, which can be a further developed topic in future research.\n---\nConclusions\n\nThis study selected several communities in downtown areas of Shanghai as examples to explore the characteristics of children's outdoor activities and intergenerational care within these communities. Based on this exploration, the study discussed the impact of intergenerational accompanying modes on children's outdoor activities and examined the factors that influence the various accompanying modes adopted by different groups of caregivers. The results demonstrate that the division of labor within families, as well as the social and physical environment of communities, all influence the accompanying modes of caregivers. These research findings provide valuable insights from the perspective of Chinese cities for understanding the methods and characteristics of intergenerational care within the context of the \"co-parenting\" phenomenon in Chinese urban families. \n---\nInstitutional Review Board Statement:\n\nThe study was conducted in accordance with the Declaration of Helsinki, and approved by the Institutional Review Board of Tongji University (protocol code 2020-0817 and date of approval: 18 June 2020).\nInformed Consent Statement: Written informed consent has been obtained from the patient(s) to publish this paper.\n---\nData Availability Statement:\n\nThe data that support the findings of this study are available from the first author, Pai Tang, upon reasonable request.\n---\nConflicts of Interest:\n\nThe authors declare no conflict of interest.",
        "Background\n\nThe theory of sense of coherence (SOC) was developed in the 1970's as the initiator of the theory, Aaron Antonovsky, examined factors that promote and maintain good health. By focusing on people's resources and capacities to create health, Antonovsky shifted the approach from the pathogenic to the salutogenic. His theory of SOC is the core of that salutogenic model. SOC is a health promoting psychological resource which strengthens one's capacity to deal with environmental strain [1,2]. Strong SOC is regarded as a characteristic that helps people to identify, use and reuse available resources and therefore minimizes the feelings of tension and stress in life further improving the prospects of staying health. The development of SOC begins in early childhood, and therefore the living circumstances during that time are essential regarding the level of SOC in adulthood. In addition to childhood circumstances, also many other factors called as generalized resistance resources, most importantly education, socio-economic position, social support and workrelated factors promote SOC [1,2].\nIn both cross-sectional and longitudinal study settings, SOC has been found to be positively associated with subjective state of health [3], and negatively with sickness absences [4], circulatory problems [5] and coronary heart disease morbidity and mortality [6][7][8]. Furthermore, a positive association between SOC and emotional coping and quality of life among people with chronic illness [9,10], health behaviour [11], and self-esteem [12] has been found. In addition earlier research has demonstrated that SOC has a strong negative correlation with anxiety and depression and a strong positive correlation with optimism [13].\n---\nRetirement as a process\n\nThe Finnish population is ageing more rapidly than in other OECD countries. At the same time the employment-population ratio for people aged 55-64 is below the EU average and is lower than in Finland's Nordic neighbours: Around 20% of those aged between 55 and 59 are early retired and around 50% of people aged 60-64. The main cause for early exit from the labour market is early disability pension: in 2007 of people aged 60-64 nearly around 24% were receiving disability pension. Additionally, around 14% of pensioners aged 60-64 were receiving unemployment pension. The effective average retirement age in Finland is somewhat below 60 years, although the official retirement age is 63-68 [14].\nIn the Finnish pension system, the granting of a disability pension requires a medically confirmed illness, disease or injury that essentially restricts or prevents working. Thus medical reasons are evident predictors of disability retirement. However, also other factors, e.g. labour market policy and social protection, affect the relationship between impairment of health and grant of disability pension. Regarding early retirement, an important factor associated with mental and somatic health status is the nature of work since physically and psychosocially demanding and unrewarding working environment is associated with high incidence of early retirement. Therefore alongside age, health status and working conditions are the main reasons for early retirement.\nAccording to Beehr [15], retirement is a process that occurs over a period of time preceding both intention and the decision to retire before the actual retirement age. Both personal factors (e.g. personality attributes, health, skill obsolescence, personal financial states) and environmental factors (e.g. attainment of occupational goals, job characteristics, marital and family situation and leisure pursuits) affect intentions to retire early [15], which further predicts the actual early retirement [16].\nSince the intentions to retire early and the actual retirement are different phases of the same process, also the factors associated with them are largely the same. However, also some differences have been found: Especially the characteristics of present work have been highlighted as factors affecting the intentions to retire early [17]. According to a comparative study including 10 European countries, the quality of work and factors presenting well-being (self-perceived health, depressive symptoms, quality of life and number of bodily symptoms) were independently associated with intentions to retire as early as possible [18]. In a Finnish study [19], poor mental health functioning showed a strong and independent association with intentions to retire early. Adjustments for physical health, socio-economic status and spouse's employment status did not essentially affect this association. The same researchers showed in another study [19] that after adjusting for several work and family related psychosocial factors, the association between poor mental health and intentions to retire early halved, yet a clear association remained. Further, the studied psychosocial factors, i.e., high job demands and low job control, low procedural and relational justice, and conflicts between paid work and family life, showed independent associations with intentions to retire early.\n---\nSense of coherence and intention to retire early\n\nPrevious studies, as well as the above mentioned, have shown that several socio-economic and psychosocial factors affect the intentions to retire early, or the associations between health and work related complaints and intentions to retire early, in complicated causal associations [20,17]. With regard to the retirement process, it is noticeable that under the same conditions some people continue working while others begin to consider early retirement. Therefore also people's attitudes and psychological processes and resources have an influence whether a health problem is associated with decreased ability to work or not [21]. Yet, studies on individual differences in terms of personality are sparse. According to a Finnish study [20], the impact of health concerns and aspects of job stress on intentions to retire early varied depending upon occupation and sense of life control: the higher the belief that one can influence one's own life and is responsible for him/herself, and the more favorable the self-image, the less thoughts of early retirement.\nTo our knowledge only one study has explored the association between SOC and disability retirement, i.e., actual early retirement. A prospective cohort study [21] showed that independently of initial health, a weak SOC of people aged 50 years or less was associated with an increased incidence of disability pension. A few studies have focused on the association between SOC and intentions to retire early. According to Huuhtanen and Tuomi [17] the high level of comprehensibility and manageability components of SOC decreased the intentions to retire early. Rasku [22] and Rasku and Kinnunen [23] showed that weak SOC was associated with intentions to retire early as well as highlighted the push factors as a reason of early intentions. According to Janatuinen [24], strong SOC was associated with the will to continue working until old-age pension. However, these studies addressed only few occupational groups and were based on relatively small (N= 1012-1823) cross-sectional surveys.\n---\nPurpose of the study\n\nIn the present study we explore whether SOC is associated with intentions to retire early, and whether socioeconomic, psychosocial and work and health behaviour related factors influence this association.\n---\nMethods\n\n\n---\nData\n\nThe data were derived from the survey of the 15-year Health and Social Support (HeSSup) study initiated in 1998. The study sample was stratified by gender and age (20-24, 30-34, 40-44 and 50-54 years), comprising three sub samples: Finnish speaking people living in surroundings of the city of Turku, Finland (N = 10 000), other Finnish speaking people in Finland (N = 46 797) and Swedish speaking people in Finland (N = 8 000). The baseline measurements were initiated in 1998 yielding 25 898 valid responses. After one reminder, the response rate was 40%. According to a non-response analysis, late respondents smoked and used psychopharmaceutical drugs more than early respondents, suggesting similar features in the non-respondents [25]. The mail survey was repeated after five years in 2003 with few alterations. It was sent to those who had responded at baseline in 1998, altogether 24 385 individuals. The response rate of the follow-up was 80%. For the purposes of this study, we excluded respondents of the youngest age group (20-24 years at baseline), those who had retired at baseline, those who had strong intentions to retire early at baseline, and those who had already applied for the retirement either at baseline in 1998 or during the follow-up 2003. Thus, the final sample consisted of 12 275 respondents (7409 women, 4866 men, 10882 Finnish speaking, 1393 Swedish speaking Finns).\n---\nStudy variables\n\nThe outcome variable was intentions to retire early in 2003 and it was asked as follows: 'Have you considered applying for a disability pension, an individual early pension or some other pension?' There were four preset response options: 1) No, I have not considered applying for a pension; 2) Applying for a pension has occurred to my mind; 3) I have strongly considered applying for a pension; 4) I have already applied for a pension. To study the intentions to retire early, those who reported that they had already applied for a pension were excluded. The exclusion of those who have already applied for pension will presumably attenuate the results of the present study since early pension will not be allocated to everyone who apply for it. The dependent variable was thus divided into three categories: no intentions (N = 9778), weak intentions (N = 1824) and strong intentions (N = 673). For further statistical analyses, we combined weak and strong intentions.\nSOC and all background variables were measured in 1998. SOC was assessed using Antonovsky's short 13item scale derived from the original 29 item Orientation to Life Questionnaire (OLQ) covering the three main subcomponents of SOC: comprehensibility, manageability and meaningfulness. The items scored 1 to 7 were randomly ordered in the questionnaire. If missing values existed, they were replaced based on values in other items if at least two-thirds of the questions were answered. The distribution of SOC ranged from 16 to 91. SOC was normally distributed (mean SOC 65.2, standard deviation 11.2). The Cronbach alpha coefficient was 0.85 both in 1998 and 2003 suggesting acceptable internal consistency of the used SOC scale [26].\nThe quality of the relationship with mother and father in childhood and adolescence was assessed using three categories: 1) Very close and warm or good to both; 2) Very close and warm or good to one, neutral, bad or very bad to the other; and 3) Neutral, bad or very bad to both. Childhood living conditions were measured by six different types of difficulties: Divorce/separation of the parents, long-term financial difficulties in the family, serious conflicts in the family, frequent fear of a family member, alcohol problems, or long-term illness of a family member. All these six questions were assessed as a sum variable with no or at least one difficulty in the childhood home.\nEducation was classified into four categories: Higher education, higher secondary education, lower secondary education, and primary education only.\nWorking conditions of the employed were assessed by Karasek's Job Demand-Control Model [27]. Job demands were assessed by a sum variable drawing on five, and skill utilization drawing on six separate questions. Three categories were used: high, average, and low skill utilization [26]. Those currently non-employed, i.e., unemployed, students, home makers and others (n = 3125) were included in the statistical analyses as the fourth category called \"others\".\nSocial life included having a partner (yes or no) and perceived social support which was assessed using the Brief Social Support Questionnaire by Sarason et al. [28] with six items. The seven alternative sources of perceived social support were spouse/partner, other close relative, close friend, close friend from work, close neighbour, someone else close to you, no one, and in six different types of situations. The four categories included more than 18 (maximum being 36), 12-17, 6-11, and 0-5 support-givers.\nHealth-related risk behaviour was measured by alcohol intoxication (drunken once a week or more) and by smoking (non-smoker, former smoker, smoking less than five cigarettes per day and smoking more than five cigarettes per day).\nHealth status was measured by self-reported somatic disease diagnosed by a physician as follows: Has your physician ever told you that you have or that you have had: Bronchitis, chronic obstructive pulmonary disease, allergic rhinitis, hypertension, diabetes, myocardial infarction, angina pectoris, atricular flimmer/flutter, stroke, other cerebrovascular disorder, peptic/duodenal ulcer, liver disease, renal disease, rheumatoid arthritis, arthrosis, sciatica, glaucoma, epilepsy, cerebral injury, neurological disorder, cancer, some other long-lasting/ difficult disease? (Yes/no). Here, two categories included no and yes (has or has had at least one of the diseases).\nThe Beck Depression Inventory included four classes: No depression (BDI 0-9), minor depression (BDI 10-18), moderate depression (BDI [19][20][21][22][23][24][25][26][27][28][29][30][31][32][33][34][35][36] and major depression (>36).\nFor further analyses the data were divided into somatically and/or mentally unhealthy respondents and healthy respondents, i.e., no self-reported somatic disease diagnosed by physician or no reported depression (Beck Depression Inventory).\n---\nStatistical methods\n\nLogistic regression analysis was used to examine the associations of background variables with intentions to retire early. Finnish and Swedish speaking Finns were pooled together since only one statistically significant interaction effect (between language and education with p-value of 0.001) was found. Additionally, weak and strong intentions to retire early were combined. The analyses were made separately among somatically or mentally unhealthy respondents (at least one of the selfreported somatic disease diagnosed by a physician or minor, moderate or major depression), and somatically and mentally healthy respondents (no self-reported somatic disease diagnosed by a physician or no depression). The results are presented as odds ratios (OR) and their 95% confidence intervals (CI). The age-adjusted effects of each variable were first fitted. In addition to age, SOC, language, childhood circumstances, education, working circumstances, social life aspects and health related behaviour factors were adjusted for at the same time. The reference category for the dependent variable, i.e., intentions to retire early, was the first category equalling no intentions. We then divided SOC into quartiles and made the same analysis as in model 2 with background factors adjusted for. Additionally we checked the interactions of SOC with language and gender. Analyses were carried out using the STATA statistical package [29].\n---\nResults\n\nTable 1 shows the prevalences for intentions to retire early by the background variables as well as means of SOC for all intention groups. The mean SOC was strongest among respondents with no intentions to retire early being 65.4 among women and 66.9 among men. Women scored lowest in the weak intention group (mean SOC 63.6) and men in the strong intention group (mean SOC 63.8). Among respondents aged 50-54 years, 42% of women and 53% of men reported some degree intentions to retire early. Finnish speaking women and men reported more often intentions to retire early (weak and strong) compared to Swedish speaking Finns. In both genders, the prevalence of both weak and strong intentions was highest if difficulties in childhood home existed. The prevalence of strong intentions was highest among those women and men who got drunk once a week or more. Among men the effect of smoking on both weak and strong intentions was clear. In both genders having or having had at least one somatic disease diagnosed by a physician and depression strongly increased intentions to retire early. Over 80% of women and men reported no depression, whereas 63% of women and 60% of men reported a previous or present disease diagnosed by physician.\nIn the further analyses we combined weak and strong intentions. Since both somatic and mental health problems strongly increased the risk of reporting intention to retire early, further analyses were stratified by health status. Thus analyses were made separately among those somatically or mentally unhealthy women (N = 5882) and men (N = 3784), and healthy women (N = 1527) and men (N = 1082). Of the unhealthy women, 4772 (81%) reported no intention to retire, and 1110 (19%) reported intentions to retire. Among men the corresponding figures were 2715 (72%) and 1069 (28%). Of the somatically and mentally healthy women, 1364 (89%) reported no intentions to retire early, and 163 (10%) reported intentions to retire early. Among men the corresponding figures were 972 (86%) and 155 (14%) (data not shown).\nTable 2 presents results of logistic regression models for intentions to retire early (weak and strong intentions combined) among somatically or mentally unhealthy and somatically and mentally healthy women. Among ill women, SOC had a statistically significant association with intention to retire early both in Model 1 ageadjusted and Model 2 with all background factors adjusted for. Regarding model 2 with all the background factors adjusted for, the OR of SOC was 0.97, i.e., each additional SOC score reduced the risk of intentions to retire early by 3% in somatically and/or mentally ill women. Further, socio-economic, psychosocial and work and health behaviour related factors did not influence the association between SOC and intentions to retire early among somatically or mentally unhealthy women; adding the background factors in the model 2 had no influence to the odds ratios of SOC in intentions to retire early. As shown in Table 2, among somatically or mentally unhealthy women, after adjustment for the background variables (model 2), in addition to SOC (OR 0.97, 95% CI 0.96-0.98) higher secondary, lower secondary and primary education (OR 1.42, 95% CI 1.07-1.87; OR 1.41, 95% CI 1.02-1.95; OR 1.46, 95% CI 1.08-1.99, correspondingly) remained associated with intentions to retire early. Among somatically and mentally healthy women the association between SOC and intention to retire early was weaker and non-significant, as were the rest of the variables in model 2.\nAs for women, also among somatically or mentally unhealthy men SOC was associated with intentions to retire early both in Model 1 (age-adjusted) and Model 2 (with all background factors adjusted for) (Table 3). Regarding model 2 with all the background factors adjusted for, the OR of SOC was 0.97 (95% CI 0.96-0.98) i.e., each additional SOC score reduced the risk of intentions to retire early by 3% among somatically and/ or mentally unhealthy men. Further, socio-economic, psychosocial and work and health behaviour related factors had almost no influence on the association between SOC and intentions to retire early (the OR of SOC increased from 0.96 to 0.97). However, unlike among somatically and mentally healthy women, among healthy  Odd ratios (OR) and their 95% confidence intervals compared with the \"no intentions\" class (OR = 1.00). Women. P < 0.05 men the association between SOC and intention to retire early remained in both models 1 and 2. Among somatically or mentally unhealthy men (Table 3) in addition to SOC, language, difficulties at childhood, education, high job demands and heavy smoking remained associated in Model 2 with all the background factors adjusted for. Somatically or mentally unhealthy Swedish speakers reported less intentions to retire early compared to Finnish speakers (OR 0.67, 95% CI 0.48-0.94). Further, among unhealthy men, having had difficulties at childhood increased the risk of intention to retire early compared to the group of no difficulties (OR 1.26, 95% CI 1.01-1.57). Also higher secondary, lower secondary and primary education increased the risk of intentions to retire early among unhealthy men compared to highest education group (OR 1.56, 95% CI 1.14-2.13; OR1.82, 95% CI 1.31-2.54; OR 1.80, 95% CI 1.29-2.51, correspondingly). Finally, high job demands showed an association with intentions to retire early (OR 1.46, 95% CI 1.05-2.04), as well as smoking over five cigarettes per day (OR 1.33, 95% CI 1.00-1.76) in Model 2 with all background variables adjusted for.\nAmong healthy men, in addition to SOC, relationship with parents, difficulties at childhood, primary education and heavy smoking had statistically significant association with intention to retire early in model 1. In model 2 with all background variables adjusted for in addition to SOC, heavy smoking remained associated\nWe did the same analyses as in model 2 (with all the variables adjusted for) having the SOC scale divided into quartiles to test the linearity of the association. The reference category for the intentions to retire early was the highest quartile. The OR of intentions increased gradually toward the lowest quartile among somatically or mentally unhealthy women: 1.2 (CI 95% 0.96-1.63), 1.5 (CI 95% 1.13-1.98) and 2.0 (CI 95% 1.49-2.66) and among somatically or mentally unhealthy men: 1.0 (CI 95% 0.76-1.35), 1.3 (CI 95% 0.94-1.72) and 1.9 (CI 95% 1.36-2.60), respectively. Further, the parameters of SOC increased gradually also among somatically and mentally healthy men (OR 1.60, 95% CI 0.87-2.94; OR 1.9, 95% CI 0.99-3.8; OR 2.4, 95% CI 0.96-6.03) suggesting a dose-response relationship between SOC and intention to retire early. Among mentally or somatically unhealthy women the two lowest SOC quartiles clearly represented a risk of intention to retire early whereas among men only the lowest SOC quartile had a similar effect.\nThere was no interaction neither between language and SOC, nor between gender and SOC when predicting early retirement.\n---\nDiscussion\n\nSOC was clearly associated with intentions to retire early among somatically or mentally unhealthy women and men, and this association remained even after adjusting for a range of socio-economic, psychosocial and work and health behaviour related background factors. Further, among healthy men, in addition to smoking SOC was the only significant predictor of intention to retire early. Among healthy women no significant association between SOC and intention to retire early was found.\nAmong unhealthy women, in addition to SOC, age and education were associated with intentions to retire early when all the background factors were adjusted for, i.e. the effects of these factors on intentions to retire early were independent. Among unhealthy men, in addition to these, also language, difficulties at childhood, education, high job demands and heavy smoking were independently associated with intentions to retire early. The effects of education and job demands are evident: Important factors associated with health status are working conditions [20], which often are socioeconomically patterned.\nHowever, due to differences in occupational status, occupational requirements and work and family related psychosocial environment and life situations among employees, the same disease and impairment in a medical sense may lead to different degrees of disability [19]. Further, subjective evaluation of one's health, well-being and resources may affect one's attitudes toward retirement [20]. The strong and independent association between SOC and intentions to retire early is noticeable since after adjusting for several factors known to affect the retirement process, the association between SOC and intentions to retire early did remain constant.\nConsidering the nature of SOC and salutogenesis, it seems plausible that the level of SOC may determine people's decisions regarding the continuation of work. A person with strong SOC stands the life and its challenges in health promoting manner: without \"unnecessary\" feeling of stress arising from the confidence that adequate resources will be at hand when needed. It is presumable that this confidential attitude -strong SOC -enhances people's trust that they have the resources needed to overcome the risk factors and challenges posed by their work, and that the work they do is meaningful and important. Whereas a person with a poor SOC may stand his/her work as life in general, it is chaotic, unmanageable, and meaningless, and hence be reluctant to continue working (until old-age retirement).\nAdditionally research has shown that among people with different kind of chronic illnesses, those having strong SOC deal better with the strain arising from illness [30]. Therefore among people with health complaints, SOC may be a factor preventing early retirement. Additionally, SOC is associated with perceived health [3], self-esteem [13] and attitudes such as optimism and hostility [14]. Optimism/pessimism and Odd ratios (OR) and their 95% confidence intervals compared with the \"no intentions\" class (OR = 1.00). Men. P < 0.05 hostility have also been associated with the retirement process [31]. Additionally, earlier studies have shown that early rehabilitation is associated with higher average retirement age [32,33]. According to Kaiser et al. [33], SOC is related to coping style and thus is a prerequisite for engaging in rehabilitation. It has also been shown that the climate at work and working circumstances of people who still are working are highlighted as factors affecting intentions to retire early. In a Finnish study [34] low job control, poor teamwork and unjust supervision were associated with retirement thoughts and retirement preferences among hospital physicians. The association remained after control for indicators of health and social circumstances. In another Finnish study [35] the association between low job control and intentions to retire early was stronger if job demand were high. The present study partly confirmed the role of psychosocial working conditions on retirement process: high job demands at work remained associated with intentions to retire early after full adjustment for other confounding factors among men. According to our earlier research, SOC has been associated positively with ability to use skills, decision authority and skill utilization at work, and negatively with high psychological job demands [26,36]. Moreover, the main effect of SOC on well-being at work and some support for a moderating role of SOC on the relationship between perceived work characteristics and wellbeing has been reported [37]. To sum up, SOC may influence the retirement process in several ways, i.e. directly determining the process of consideration regarding the continuation of work, or indirectly through better adaptability to ill-health, working circumstances, and engagement in rehabilitation.\nAnother way to discuss the role of SOC in the retirement process is its role in the retirement developmental transition, especially among people with ambiguous attitudes toward retirement. According to Antonovsky, people with a strong SOC are more prone to substitute work activity with other meaningful activities instead of ending up feeling worthless. Antonovsky et al. [38] showed in their study that a strong SOC is an important factor in coping with the retirement developmental transition for it helps adopting functional attitudes toward the retirement. Further, Smith [39] examined the role of family worldview (incl. SOC) on retirement process and showed that the way family views the world is important for a positive level of individual adaptation as well as family unit adaptation during retirement. It seems that SOC is an important factor not only preventing early retirement process but also promoting the readjustment to the old-age pension.\nSince earlier studies have shown that Swedish speaking Finns retire later than their Finnish speaking counterparts [40], the language was included in the present study. Compared to Swedish speaking men, Finnish speaking men reported more often intentions to retire early. This association remained even after full adjustment for the background variables. Hence it seems that Finnish and Swedish speaking men show to some extent different attitudes already in the first phase of the retirement process.\nThe weakness of the present study is that the response rate of the baseline survey was only 40%. However, according to the non-response analysis, only minor differences in socio-demographic and health-related issues were found between the respondents and non-respondents [25]. Further studies would also benefit from a wider range of family and work related psychosocial factors as well as social support compared to the present study. An advantage of the present study is a large nationwide population sample with prospective study design. Also the response rate of the follow-up was good being 80%.\n---\nConclusion\n\nAccording to the present paper SOC has an independent effect on intentions to retire early and therefore a strong SOC may have a potential to prevent early retirement in groups otherwise at risk. In the present study unhealthy people with low SOC and low education were in greatest risk to have reported intention to retire early. Determining SOC would help to identify individuals at risk for early retirement and focus supportive measures more effectively. Moreover, several recent studies [41][42][43][44] have supported Antonovsky's original hypotheses that intentional modification of SOC through an intervention is possible. An important challenge would be to target the resources of SOC, including working circumstances and social support, to the most vulnerable and design appropriate interventions in order to strengthen the level of SOC and hence prolong working years of the aging employees. More research on individual choices based on personality and attitudes is needed in order to better understand the multi-faceted mechanisms between SOC and retirement decisions.\n---\n\n\nAuthors' contributions S-MV participated in the design of the study, performed the statistical analyses and wrote the first draft of the manuscript. SS, EL, KK, MK and KS participated in the design of the study, commented the draft versions of the manuscript and helped to improve it. All authors read and approved the final manuscript.\n---\nCompeting interests\n\nThe authors declare that they have no competing interests.\n---\nPre-publication history\n\nThe pre-publication history for this paper can be accessed here:http://www. ",
        "Introduction\n\nRecent shifts in child welfare policies, increases in child welfare caseloads, declines in availability of traditional foster care homes, and case workers' favorable view of kin as foster parents have made placement with relatives the most common type of foster care for children with relatives willing and able to assume care (Dolan, Casanueva, Smith, & Bradley, 2009;Park & Greenberg, 2007). Grandparents are among the first to take on this responsibility, and an estimated 937,784 grandparent householders provide care to a grandchild (GC) under age 18 without birth parents present (2011 American Community Survey). Families of this composition are known as either \"custodial\" or \"skipped generation\" families in which custodial grandmothers (CGMs) almost always serve as the primary caregiver (Park & Greenberg, 2007).\n---\nWhy Research on Assessing the Parenting Practices of Custodial Grandmothers is Needed\n\nBecause the need for surrogate parenting by CGMs is largely due to a crisis or tragedy in the birth parent generation (e.g., drug use, teen pregnancy, divorce, mental and physical illness, AIDS, death, crime, child abuse and neglect, incarceration), most CGMs end up in a longterm commitment with about 40% having this role for five or more years (Simmons & Dye, 2003). Even though parenting is now recognized as the major responsibility of CGMs (Dolbin-MacNab, 2006;Dunifon, 2013), little is known about their parenting practices and how to best assess them.\nNumerous stressors are faced by CGMs in relation to their child care responsibilities (e.g., financial strain, insufficient social support, life style disruption, disregard by service providers, social stigma, role ambiguity, health adversity, conflict with birth parents) that may increase their psychological distress and diminish their ability to parent effectively (Dunifon, 2013;Hayslip & Kaminski, 2005;Kelley, Whitley, & Campos, 2011). This is salient because there is consensus in the parenting literature that caregiver distress is related to poor parenting, that poor parenting is related to child adjustment, and that parenting mediates the association between caregiver distress and child adjustment (Deater-Deckard, 1998;Downey & Coyne, 1990;Shelton & Gordon, 2008). Children of distressed caregivers are at risk for adjustment problems even when caregivers face daily hassles only (Elgar, Mills, McGrath, Waschbusch, & Brownridge, 2007).\nExamining the parenting practices of CGMs is also necessary because the GCs in their care are at higher risk for psychological difficulties than age peers in the general population (Smith & Palmieri, 2007). This is due primarily to exposure to prenatal toxins in the womb and to such early adversities as abuse, abandonment, and neglect by birth parents. Because some parenting behaviors (e.g., nurturance and apt discipline) can lessen the mental health problems of children previously exposed to major stressors (Sandler et al., 2003), it is critical to identify adequate measures of these parenting practices for CGMs.\nIt is also possible that parenting by CGMs is different than it is for young parents, which raises the question of whether parenting measures developed for birth parents are applicable to CGMs. Kaminski, Hayslip, Wilson, and Casto (2008) found that grandparent caregivers were less sensitive to GCs' needs and were less clear about proper parent-child role responsibilities than birth parents. An intergenerational transmission of poor parenting may also exist given that many CGMs did poorly at raising their own dysfunctional children, and might thus be even less competent in raising a GC (Climo, Patterson, & Lay, 2002;Gibson, 2005;Smith, Beltran, Butts, & Kingson, 2001). In fact, CGMs often feel guilty about how their offspring have fared and question their own parenting competency (Baird, 2003;Edwards, 2003;Glass, 2002;Smith & Dannison, 2001;Williamson, Softas-Nall, & Miller, 2003).\nThese caregivers have also been found to express difficulty in disciplining and setting limits due to the conflicting nature of their roles as grandparents and caregivers (Bratton et al., 1998). Shifting from being a traditional grandparent to caregiver may also yield role confusion and internal conflict given that the authoritative parent role means letting go of traditional grandparenting which sanctions fun, indulgence, and unconditional love (Glass, 2002;Landry-Meyer & Newman, 2004;Weber & Waldrop, 2000). Age-related factors may further influence the parenting practices of CGMs. Many worry about their ability to parent a GC due to their advanced age and corresponding health problems (Berrick, 1997;Landry-Meyer & Newman, 2004;Park & Greenberg, 2007). It is further claimed that CGMs have outdated knowledge of childrearing and would benefit from training and educational activities on effective parenting (Bratton, et al. 1998;Glass, 2002;Williamson et al., 2003).\n---\nPrior Studies on the Parenting Practices of Custodial Grandmothers\n\nOnly a handful of published studies have examined the parenting practices of CGM to date, and it is unknown how suitable the parenting measures used in those studies are for CGMs. Rodgers-Farmer (1999) hypothesized that CGMs experiencing stress related to their parenting role would report being depressed and use inconsistent discipline, punishment, and rejection-oriented techniques. She measured these parenting practices with the Management of Children's Behavior Scale (MCBS; Rodgers, 1988), a 20-item questionnaire rated on a five-point scale. Alpha coefficients for these subscales were found to be acceptable (punishment = .77; inconsistent = .71; rejection-oriented behaviors = .77). However, a factor analysis was not conducted to examine the purported dimensionality of these three subscales. Nevertheless, hierarchical multiple regression analyses revealed that parenting stress had a significant effect on depression, and depression had a significant effect on inconsistent parenting practices (but not on the use of either punishment or rejectionoriented behaviors). Smith and Richardson (2008) examined the psychometric properties of an adaptation of the Parenting Practice Interview (PPI; Webster-Stratton, Reid, & Hammond, 2001) in a study with 733 CGMs. The PPI was selected over other potential measures for two reasons: (a) Unlike the MCBS, the PPI measures both \"effective\" (Appropriate Discipline, Monitoring and Positive Parenting) and \"ineffective\" (Inconsistent and Harsh Discipline) parenting practices; and (b) these specific practices are related to behavioral outcomes of at-risk children (Perepletchikova & Kazdin, 2004;Rubin & Burgess, 2002;Webster-Stratton et al., 2001). Factor analytic methods revealed that each PPI item assessed uniquely the respective parenting practice it was originally intended to measure. Consistent with the general parenting literature, zero order correlations revealed that the ineffective practice subscales were associated with greater CGM psychological distress and more behavioral problems in CGs while the opposite was true for the effective subscales. A major shortcoming, however, is that each subscale was assessed by only three items which comprised the internal consistencies of these scales. Smith, Palmieri, Hancock, and Richardson (2008) examined the relevance of the Family Stress Model (FSM), which considers causal links among family stressors, caregivers' psychological distress, dysfunctional parenting practices, and children's adjustment (Conger et al., 2002), to parenting by CGM. They modeled dysfunctional parenting as a higher-order factor encompassing two first-order factors (ineffective discipline and low nurturance). The indicators of ineffective discipline within their structural model were the three-item harsh and inconsistent disciple scales derived previously from the PPI by Smith and Richardson (2008). The indicators of low nurturance, both comprising subscales from the Parenting Stress Index (Abidin, 1995), were the Reinforces Parent subscale (six items measuring how much a caregiver projects negative responses onto a child) and the Attachment subscale (seven items assessing emotional closeness and the caregiver's real or perceived ability to observe and understand the child's needs and feelings). Smith et al. (2008) found both the measurement and structural components of their model to fit the data well for their sample. Findings regarding the structural model supported the basic FSM tenet that the effects of caregivers' psychological distress on children's adjustment are mediated through poor parenting. In turn, both the measurement and structural components of their model were largely invariant by CGM race (White and Black), CGM age (\u2264 55 and > 55), CG age (4-7; 8-11; and 12-17), and CG gender. A key limitation, however, was that several of the parenting measures had low alphas (Attachment = .60; Reinforces Parent = . 69; Harsh Discipline = .66; and Inconsistent Discipline =.54). Thus, uncertainty remained regarding the measurement properties of these instruments.\n---\nThe Present Study\n\nThe present study encompassed three specific aims designed to assess the overall reliability and validity of several extant measures of parental nurturance and discipline, that were originally with birth parents, for their use with CGM. The first aim was to examine the underlying factor structure of these measures by using a confirmatory factor analytic (CFA) approach to test the four nested models, shown in Figure 1. After identifying the best fitting of these models, the second aim was to examine if that model would be invariant by GC age. Parenting differs according to the age of the child which has implications for the applicability of measures of parenting (Smith, 2011). The third aim was to investigate the construct validity of the final set of parenting scales identified under Aim 1. From the theoretical perspective of the FSM (Conger, Reuter, & Conger, 2000), which postulates the impact of parental psychological distress on children's outcomes is mediated by parenting practices, we hypothesized that those parenting practices indicative of higher nurturance and effective discipline would be associated with lower psychological distress for CGMs as well as fewer psychological difficulties among CGs. In contrast, ineffective discipline (i.e., in the form of both punitive and inconsistent) was hypothesized to be associated with greater CGM distress and more difficulties for CGs.\n---\nMethod Participants\n\nThe participants were 343 CGMs enrolled in a RCT designed to compare two evidencedbased interventions (behavioral parent training and cognitive behavioral skills training) to each other and to a theoretically inert control condition. Inclusion criteria were that CGMs had provided care to a CG between ages 4-12 for at least three months at her home in complete absence of the CG's birth parents; were fluent in the English language; were willing and able to attend 10 two-hour long group sessions at a community site; and selfidentified as being of either White, Black, or Hispanic origin. Recruitment occurred across four states (California, Maryland, Ohio, and Texas) and included diverse methods (e.g., mass media announcements; contacts through schools, social service and health agencies, courts, libraries, faith communities, and support groups; appearances at community events; brochures; and letters mailed to randomly selected households). The RCT was described to potential participants as providing \"information that can help grandmothers get through the difficult job of caring for grandchildren in changing times\". If a CGM was caring for multiple CG who met study eligibility criteria then a target CG was selected based on asking the CGM which child was the most difficult to provide care to. The target CG was then used as the reference for all measures reported on here. Key sociodemographic and background characteristics for these 343 CGMs and their target CGs are presented in Table 1. The mean age of the grandmothers was 58.45 (SD = 8.22, and 7.8 (SD=2.6) for the GCs. The age range for CGMs was 40 -89 years. Most CGMs were either Caucasian (44%) or African American (43%), followed by Hispanic/Latino (11%), and then \"other\" (1%). Only 38% of CGMs were married, with 51% being divorced or widowed. Most (44%) of the CGMs had completed some college, 19% earned their GED or high school diploma, 13% did not complete high school, 13% received bachelor's degrees, and 6% had graduate or professional degrees. At the time of this survey, 44% of CGMs were working part-time, 19% were retired, 13% were unemployed/looking for work, and 13% were working full-time.\n---\nProcedure\n\nAt RCT baseline each CGM completed a telephone interview during which all of the measures for this study were obtained via self-report. Both verbal and written consent were obtained.\n---\nMeasures and Data Analyses\n\nOnly measures of the broader constructs of discipline and nurturance were considered because these two constructs comprise the most influential parenting mechanisms known to affect adjustment problems in children (Cummings & Davies, 1994;Locke & Prinz, 2002;Lovejoy et al., 2000;Rubin & Burgess, 2002). We also selected measures that are brief (to reduce respondent burden), have shown good psychometric properties with birth parents, and include items with content that appears relevant to caregivers of children ages 4-12. We also favored scales that have been used in parenting intervention research. The specific items associated with each of these scales and their response alternatives are presented in Table 2.\nNurturance-Measures of this construct were selected in line with the view that parental nurturance is concerned with providing a positive atmosphere for the parent-child relationship and the child's emotional development through both emotional (e.g., communication of acceptance) expressions and instrumental (e.g., playing a game together) acts (Locke & Prinz, 2002).\nThe more instrumental side of nurturance was assessed by the 10-item SEB subscale of the PBI (Lovejoy et al., 1999), which corresponds closely to the construct of parental warmth and involves behaviors demonstrating acceptance of the child through affection, shared activities, and emotional and instrumental support (Lovejoy et al., 1999). With a sample of parents of preschool age and young school age children, the developers found support for the test-retest, internal consistency, inter-observer reliability, and construct validity of the SEB scale. In the present study, CGMs were asked to rate each of the 10 SEB items (Table 1) on a scale ranging from 0 \"not at all\" to 5 \"very true\". Items were then summed to yield a total SEB score with a potential range of 0 to 50 with higher scores indicating more supportive and engaged behavior reported by the CGM (\u03b1 = .88).\nThe more emotional form of nurturance was assessed with the 10-item PAI which measures the degree to which caretakers (e.g., parents and grandparents) report trust, fairness, respect, affection, and understanding between themselves and their child, as well as their perception of how the child feels about them in terms of these same core relationship dimensions (Bengtson & Schrader, 1982). The items on the PAI were found by its developers to load highly on a single factor, with relatively uniform loadings. (Bengtson & Schrader, 1982). In the present study, CGMs rated each item (Table 1) along a scale from 0 \"none\" to 4 \"a great amount\". Total scores were computed by summing all items, with a potential range of 0 to 40 (\u03b1 = .87). We chose this measure because it was designed to measure relationship quality irrespective of the target child's age, and thereby averts the concern that specific acts of emotional expressiveness vary considerably by child age (Locke & Prinz, 2002).\nDiscipline-Three specific types of discipline (Effective, Inconsistent, and Punitive) were measured by means of the PPI, a 17-item likert style instrument designed to assess these particular disciplinary styles that are known to be related to children's adjustment outcomes (Lochman & Conduct Problem Prevention Research Group, 2010). These three PPI sub scales, as established via exploratory factor analysis (Lochman et al., 2010), reflect the view that discipline includes both parenting techniques deemed to be more effective and less effective (Locke & Prinz, 2002). In the present study, CGMs rated each PPI item (Table 1) on a four-point scale ranging from 1 \"never\" to 4 \"often\". The 6 items tapping Effective discipline were summed to yield a total core with a potential range of 6 to 24 (\u03b1 = .83). The 6 items tapping Consistent discipline were summed to yield a total score with a potential range of 6 to 24 (\u03b1 = .79). The 5 items tapping Punitive discipline were summed to yield a total score with a potential range of 5 to 20 (\u03b1 = .73). Higher scores for each scale reflect greater frequency of the parenting behavior being assessed. An important caveat, however, is that the PPI Consistent scale was actually constructed to reflect parenting behaviors that are inconsistent in nature (See Table 1). Thus, higher scores on this scale are indicative of more frequent use of inconsistent disciplinary approaches.\nTG Adjustment-The broadband internalizing and externalizing subscales from the parent-informant version of the Strengths and Difficulties Questionnaire (SDQ; Goodman, 2001) were used. The SDQ has shown good psychometric properties in use with custodial grandfamilies (Palmieri & Smith, 2007) and generally correlates highly with other indices of childhood maladjustment (Goodman, 2001). Externalizing problems were assessed by summing the Hyperactivity-Inattention and Conduct Problems scales (potential range = 0 to 20; \u03b1 = .75), while Internalizing problems were assessed by summing the Emotional Symptoms and Peer Problems scales (potential range = 0 to 20, \u03b1 = .74) Each scale contained five items that were rated by grandmothers regarding the target GC behavior on a 3-point scale from 0 (not true) to 2 (certainly true). Higher scores indicate greater levels of the measured behavior.\nGCM Psychological Distress-This included self-report measures of both depression and anxiety. Depressive symptoms were measured with the 20-item Center for Epidemiological Studies -Depression scale (CES-D;Rodloff, 1977). For each item, participants endorsed the response that best described how often they had felt a particular way in the past week on a 4-point Likert-type scale from 0 (rarely or none of the time -less than 1 day) to 3 (most or all of the time -5 to 7 days). Potential CES-D scores ranged from 0 to 60 (\u03b1 = .91).\nAnxiety was assessed using the 5-item Overall Anxiety Severity and Intensity Scale (OASIS; Norman, Hami, Means-Christianson, & Stein, 2006) where items (e.g., \"In the past week, how often have you felt anxious?\" are asked on a five point Likert scale ranging from 0 \"never\" to 4 \"all the time\". Potential scores range from 0 and 20 (\u03b1 = .86), with higher scores representing greater anxiety.\nFrom the 343 CGMs who qualified for placement into the RCT, there were 14 (4.1%) who did not complete all measures. For these cases, multiple imputations was performed using Mplus 7.1 (Muthen & Muthen, 2014). Multiple imputation is a process by which missing values within a variable are estimated and added to the data set based on patterns and relationships existing between other values.\nThe four measurement models depicted in Figure 1 were tested respectively via CFA using EQS 6.0. Bentler, 1995). Model 1 is a five-factor, higher order solution, which corresponds to claims in the parenting literature that discipline and nurturance comprise the most important parenting mechanisms to affect adjustment problems in children (Cummings & Davies, 1994;Locke & Prinz, 2002;Lovejoy et al. 2000;Rubin & Burgess, 2002). It also represents the most parsimonious model. Model 2 differs only by excluding the effective discipline first-order factor from loading onto the second order discipline factor as proposed in Model 2. The rationale for this exclusion is that item content on the effective discipline factor emphasizes how a child behaves as a results of disciplinary tactics, whereas the other two discipline factors focus on what a parent actually does to discourage unwanted behavior (see items in Table 1). Moreover, measuring discipline effectiveness is generally thought to be much less clear than assessing ineffective discipline (Locke & Prinz, 2002). Model 3 differs from Model 1 by excluding the second-order nurturance factor. The rationale for this difference is that items on the PAI emphasize perceptions of the parent-child relationship, whereas the SEB scale measures nurturing behaviors (see Table 1). Moreover, as Locke and Prinz (2012) have noted, \"it is an open question whether nurturance is more useful as a unitary global construct or as a set of related subconstructs\" (p. 922). Model 4 is a lower order version of Models 1-3, in which the relations among the five parenting measures are explained by their own unstructured covariation rather than by any overarching second-order factors. This slightly less parsimonious model emphasizes the importance of viewing each parenting practice in its own right independently from other measures of similar constructs. It also recognizes the measurement particularities considered within Models 2 and 3.\nBecause initial screening of items revealed skewed distributions the Maximum Likelihood Robust (MLR) estimation was chosen, which has the ability to more accurately estimate data outside a normal distribution (Savalei, 2010). The model exhibiting the best overall fit was chosen for further analysis and further model building with young and old GC subgroups as described below. Four indices were used to evaluate model fit (Schumacker & Lomax, 2010): Satorra-Bentler chi-square, which functions as an adjusted chi-square estimation for use with MLR (Muth\u00e9n & Muth\u00e9n, 2007); the root mean square error of approximation (RMSEA, with values equal to or lower than .05 considered to be an indication of good model fit; the standardized root squared residual (SRMR), a measure of absolute model fit, with values less than .08 indicating good fit.; These four fit indices provide the variety of model examination sufficient to support or refute the structure of the data as similar enough to the proposed models (Schumacker & Lomax, 2010).\nAfter the best fitting of the four proposed models was identified it was then examined for invariance by CG age groups of 4 to < 7 years old versus \u2265 7 to 12 years old. This age group distinction was made in light of the shift from Piaget's preoperational stage of cognitive development to the concrete stage at about age seven, when children begin to reorganize mental images and symbols to create logically formed thoughts which are then modified and reinforced by parental behavior (Shaffer, 2008;Slavin 1988).\nFirst, the best fitting theoretical model was applied to each of the two GC age groups separately and evaluated by CFA to generate model fit statistics. Next, modifications to model structure in regards to cross-loadings and item error covariances were examined and incorporated sequentially within each age group. After no more theoretically meaningful modifications could be applied to each sample model, the age-specific modifications were then combined into a single model that was applied to both groups during the test of invariance. This was accomplished by constraining the factor loadings of each groups' measurement model to be equal, and then comparing its Satorra-Bentler chi-square value to that of a model in which factor loadings we estimated freely across groups. A nonsignificant Satorra-Bentler chi-square difference would indicate that the hypothesis of loading appeared reasonable.\nLastly, zero order correlational analyses were performed to demonstrate the concurrent validity of the parenting scales. We specifically examined their hypothesized relations with indices of CGM psychological distress (symptoms of depression and anxiety) and GC behavioral and emotional difficulties (internalizing and externalizing symptoms). The FSM served as the conceptual framework for this correlational analysis.\n---\nResults\n\nEstimates of the fit statistics for each the four proposed models are summarized in Table 3, where it is shown that Model 4 yielded the best overall fit to the observed data. Model 4 also yielded statistically significant loadings for each item on its respective factors and no modifications were indicated to the specification of the items outside of the suggestions of the scales' original authors. Although Model 3 did show similar indices of fit to the observed data in comparison to Model 4, the better values on all fit statistics suggests that Model 4 was the superior of the two. Additionally a difference test was conducted finding both models to be significantly different. With model 4 also being more parsimonious, it was preferred. In contrast, neither Model 1 nor 2 were able to converge within the EQS 6.0 analysis. Further investigation indicated that this was due to estimation within the PAI first order factor disrupting its second order variance.\nGiven the superior performance of Model 4 found above, it was then examined separately for each GC age group with unacceptable model fit indices resulting for both the younger (\u03c72 = 933.39 df= 619; RMSEA = 0.055; CFI =0.817; SRMR=0.089) and older (\u03c72 = 1003.07 df = 619; RMSEA = 0.059; CFI = 0.801; SRMR = .083) age groups. To improve model fit, a series of appropriate modifications suggested by the Lagrange Multiplier tests were made separately to Model 4 for both age groups as depicted in Table 3. Lagrange Multiplier modification indices were used to identify items whose errors covaried with other items' errors, or that cross-loaded on secondary factors. Regarding the former, suggested error covariances were allowed as supported by theory. As for the latter, as the goal of the measurement model was to obtain a relatively simple factor structure, severely cross-loading items were removed. Specific changes to Model 4 identified for the younger group were the removal of two cross loading items and the addition of one error covariance parameter. Changes identified for the older group included the removal of one cross loading item and the addition of six error covariance parameters. The specifics of these modifications can be viewed in table 3. After making these changes, model fit improved for the samples of both younger (\u03c72 = 769.02, df = 582, p < .01; RMSEA = .049; CFI = .86; SRMR = .08) and older GC (\u03c72 = 948.99, df = 578, p < .01; RMSEA = .051; CFI = .87; SRMR = .08). According to Hu and Bentler (1998), relying on RMSEA and SRMR as evidence of fit is adequate when the CFI is low due not to poor absolute (SRMR) and parsimonious (RMSEA) fit, but because of relatively low relations within the data.\nAfter the establishment of the best fitting models for young and old age groups, modifications from each procedure were combined to create an overall model that was then tested for invariance across the two CG age groups. The invariance test compared the null model, in which no constraints were applied, with the alternative model in which the factor loadings of the two age groups were constrained to be equal. This test provided a nonsignificant scaled chi-square (\u03c7 2 = 39.35, df = 27, p < .05), supporting measurement invariance across these two groups. Additionally, all item factor loadings remained statistically significant and no additional modifications were suggested by the Lagrange As shown in Table 4, significant correlations were observed between the majority of the parenting practice measures from the five factor model that was used in the previous invariance test and the constructs representative of the Family Stress Model. Three of the five scales (Punitive, Effective, and PAI) correlated exactly as hypothesized with all indices of GC adjustment and CGM psychological distress at statically significant levels. The Consistency scale correlated significantly as expected with both indices of CGM distress, but only correlated with one indicator of CG adjustment (i.e., Externalizing Symptoms). Although the SEB scale correlated significantly with both indices of CG Adjustment, it did not correlate with either indicator of CGM distress. In sum, 16 (80%) of the 20 hypothesized correlations were statistically significant.\n---\nDiscussion\n\nThe overall goal of this study was to examine whether or not five scales measuring nurturance and discipline originally developed for use with birth parents demonstrate adequate psychometric properties among a non-traditional parenting sample of CGM. We focused on five scales measuring particular facets of parental nurturance (Positive Affect and Supportive/Engaged Behavior, and discipline (Effective, Punitive, and Consistent discipline) because these two constructs are linked to children's adjustment problems and are common targets of clinical intervention (Cummings & Davies, 1994;Locke & Prinz, 2002;Lovejoy et al., 2000;Rubin & Burgess, 2002).\nOur first aim was to compare four nested CFA models in order to confirm the proposed factor structures for these five scales and to determine if their interrelations encompass the potential higher order factors depicted in Figure 1. Of the four models tested, Model 4 yielded the best fit to the observed data. This finding, along with the acceptable internal consistency values observed for each scale, suggests that they are measuring distinct interrelated first-order constructs. Thus, it appears that each scale could be used by itself to assess the underlying construct it purports to measure. This is also suggested by the fact that shared variance between any two of the five subscales did not exceed 46% as revealed by the squared zero-order correlations found across these five scales (Table 4).\nIt is important to consider, however, that the observed difference in fit between Models 3 and 4 is small albeit statistically significant. Unlike Model 4, Model 3 includes a secondorder discipline factor along with supportive engagement (SEB) and positive affect (PAI) within the CGM-GC relationship as covarying first order factors. In contrast, Models 1 and 2 failed to converge, which may be due to the fact that both contain a second order nurturance factor. Put differently, only those models without a second order nurturance factor converged properly and demonstrated acceptable fit to the observed data.\nThe apparent inability of the SEB and PAI to form a second-order nurturance factor may be due to differences in the manifest content of the items in these respective scales. Specifically, all of the items in the SEB refer to actual parenting behaviors whereas the PAI contains a mix of behavioral and cognitive items. Another key difference is that the PAI also requires the respondent to infer the feelings of the other person within the dyadic relationship. It is reasonable to conclude then that Models 1 and 2 may have shown better fit if different scales indicative of parental nurturance had been used in the present study. In turn, this illustrates the need for future studies where parenting measures beyond those investigated here are examined for potential use with CGM. Because parenting is a multidimensional construct (Smith 2011), it is important to examine measures of other parenting dimensions (e.g., attitudes, styles, satisfaction, stress, competence/self-efficacy) and how they relate to one another in future studies with CGM.\nIt is also noteworthy that, unlike nurturance, the proposed second-order discipline factor appeared to be more stable within Models 1, 2, and 3. Although it is tempting to conclude from this that a broader discipline higher order construct might actually exist, it is important to note that all three of the discipline scales were taken from the PPI (Lochman & Conduct Problem Prevention Research Group, 2010) which share the same response alternative format (Table 1). In contrast, the two nurturance scales not only had response alternatives that differed from the PPI but their response formats also differed from one another. Thus, the CFA outcomes observed for Models 1-4 may partially reflect these similarities and differences in response alternatives across the five scales examined here.\nOur second aim was to determine if the best fitting model of the four examined via CFA would show measurement invariance by GC age. This was particularly important because the scales that we examined were from different sources and developed for use with parents of children from varying age groups. For example, the PAI has been used mostly with adult children and adolescents (Bengtson, & Schrader, 1982;Orsmond, Seltzer, Greenberg, & Krauss, 2006), whereas the PPI subscales (Punitive, Consistency, Effective) were initially tested on preschool age children (Strayhorn, & Weidman, 1988) and the SEB with students in a Head Start program (Webster-Stratton, Reid, & Hammond, 2001). Moreover, Locke and Prinz (2002) have asserted that \"across all discipline and nurturance measures, better developmental mapping is needed\" (p. 922).\nBefore performing the invariance test, we first examined Model 4 separately for the younger and older GC age groups. These analyses revealed only a handful of modest changes to Model 4 were required within each age group to yield acceptable fit. For younger GC, these changes involved the removal of two cross loading items and the addition of an error covariance term shared by the older child group. For the older group modifications included the removal of one cross loading item and adding six error covariances. In retrospect, these changes are conceptually sensible given that the suggested error covariance modifications all shared manifest content related to the same material (affectionate physical contact). The items identified as cross-loaders contained material that was related to both punitive and effective disciplinary measures thereby confounding these two constructs. In addition, the error covariances that added to improve model fit for both age groups do not alter the overall meaningfulness or interpretation of the involved subscales.\nThese changes were then combined into a final version of Model 4 in which measurement invariance was examined between the two GC age groups by constraining the loadings for each factor to be equal, and this model showed complete invariance by age. Not only were all factor loadings high and statistically significant for both age groups across all factors within the best fitting version of model (Model 4), there were also no differences in either the magnitude of these loadings or in the patterns of covariance observed between the five first order factors. These findings indicate that each of the five parenting measures examined in the present study can be used acceptably with CGM providing care to CG of ages 4-12 after making the modest changes described above. Specifically, item 7 from the consistency scale should be removed for use with CGM of both young GC, and item 5 from the punitive scale removed for use with CGM of younger GC. Future research should be conducted, however, to determine if these five scales (as well other parent measures) are appropriate for use with the CGM of very young children and adolescents.\nOur final aim was to examine the concurrent validity of the five parenting scales identified by our final best fitting model. Specifically, from the conceptual framework of the FSM (Reuter & Conger, 2002), we hypothesized that those scales measuring parental nurturance (SEB and PAI) and use of effective discipline would be significantly and inversely related to internal and externalizing symptoms of the CG as well as to indices of CGM psychological distress. In contrast, also based upon the FSM, we hypothesized that our measures of ineffective discipline (Punitive and Consistency) would show the exact opposite patterns. These hypotheses are also consistent with prior findings in the general parenting literature (Downey & Coyne,1990;Shelton & Gordon, 2008) and with the rare parenting studies on CGM to date (Rodgers-Farmer,1999;Smith et al., 2008).\nAlmost without exception, our hypotheses were supported and a few noteworthy trends emerged as well. For example, among all five scales, the Effective scale demonstrated correlations of the highest magnitude with indices of GC adjustment and CGM psychological distress. As noted earlier, the item content of this scale is unique in focusing upon how well CG respond to parenting practices (e.g., \"When you ask NAME to stop doing something, how often will s(he) stop?\"), rather than upon parenting practices themselves. This is in contrast other measures of effective discipline that assess discipline practices deemed more effective such as use of clear rules and requests, direct reinforcement of appropriate behavior, application of reasoning and induction (Locke & Prinz, 2002). Thus, it is sensible that correlations will be greater when CG outcomes are directly stated rather than being inferred as is true of the other four scales (e.g., Consistency, Punitive, SEB, PAI). In general, however, the measurement of effective discipline is thought to be knotty given that that there is no agreed-upon standard for discipline practices in terms of lesser or greater effectiveness; the concept itself embodies social desirability, and it is difficult to separate the effects of discipline from a larger constellation of parenting practices (Locke & Prinz, 2002).\nAnother interesting trend in our validity findings is that the Punitive scale demonstrated across the board higher correlations with CG outcomes and CGM psychological distress than did Consistency scale. In fact, the correlation between Consistency and CG internalizing symptoms failed to reach statistical significance. Likewise, the PAI demonstrated across the board higher correlations with CG outcomes and CGM psychological distress than did the SEB with the latter not correlating significantly with CGM anxiety or depression scores. As a whole, this differential pattern of correlations reinforces our CFA findings which suggest that each of these scales is best viewed as a distinct factor in its own right.\nOur concurrent validity findings are also in line with the results of earlier studies with CGM which found significant relations among their parenting practices, their personal distress, and CG adjustment (Rodgers-Farmer, 1999;Smith et al., 2008). The fact that these prior studies used different measures of parenting practices than the ones investigated here lends credence to the collective generalizability of these studies. The present study, however, is unique in regards to its much fuller and more rigorous psychometric focus.\nThe present study is not without limitations. One drawback is that all of the measures used here, including our indices of concurrent validity, were self-reported by CGMs. In turn, our findings could have been much different if the five parenting practices had been assessed by other methods such as observations, interviews, rating scales or reports by other informants (e.g., GC; spouse). A chief advantage of caregiver self-reports of parenting is that they are more likely than others to have a comprehensive and wide-ranging knowledge of their parenting practices across differing contexts, whereas methods such as observations or querying independent informants do not provide this same breadth (Smith, 2011). On the other hand, self-reports may be subject to biases such as social desirability and are necessarily retrospective in nature. Nevertheless, given that self-reports are generally less costly, easier to obtain; and can be sensitive to intervention change (Perpletchikova & Kazdin, 2004), the present study is important in demonstrating the potential usefulness of the self-report scales we examined with CGMs.\nAnother limitation is that we did not consider other types of validity apart from concurrent validity. Because the data reported on here are entirely cross-sectional we are unable to comment on the predictive validity of these five scales in terms of whether they are associated with child outcomes over time. We also did not examine the incremental validity of these scales in terms of whether or not they contribute to the prediction of CG outcomes above and beyond other factors (e.g., socioeconomic disadvantage, family adversity, child abuse) associated with children's adjustment (Perpletchikova & Kazdin, 2004). Although these five scales are being used in our RCT we have yet to determine their sensitivity as measures of treatment change.\nAnother limitation worth noting is that our sample was restricted to CGMs who volunteered to enroll in a RCT and were recruited primarily by convenience. Thus, our sample may not be representative of CGMs on the whole. We also did not include grandfathers in the present study, which is regrettably typical of parenting research in general (Locke & Prinz, 2002). However, our sample was from diverse regions across the U.S. and included CGMs from the three ethnic/racial categories (Latinas, African-Americans, and Whites) that comprise the vast majority of the overall CGM population (2011 American Community Survey).\nDespite the limitations of this study, it is the first attempt to comprehensively examine the psychometric properties of existing scales that assess the key parenting practices of nurturance and discipline which are known to be associated with child adjustment outcomes within a sample of CGM. Both our CFA and validity analyses provide preliminary evidence that each of the measures examined here can be used meaningfully with this target population either separately or in combination with only a handful of modest changes from their original design.  \n---\nSEB -Supportive Engagement Behaviors\n\nResponses range from 0 to 5; not at all = 0, a little true = 1, somewhat true = 2, moderately true = 3, quite a bit true = 4, very true = 5.  \n---\nAuthor Manuscript\n\nSmith et al.   \n---\nAuthor Manuscript\n\n",
        "Introduction\n\nIn recent years and during the COVID-19 pandemic, substance problems of pregnant women have become more severe and involve more psychiatric conditions, difficulties, and challenges in everyday life management [1]. At the same time the treatment options may be understaffed or lacking, with access to care for co-occurring disorders remaining limited [2]. Unfortunately, it seems that current interventions do not reach the most vulnerable families with SUD, e.g. those with small children, or parents suffer from mental ill health or come from minority groups [2][3][4][5]. This article focuses on the outreach and engagement of women in interdisciplinary substance use treatment, as early interventions for women with perinatal substance problems are underresearched contexts [6,7]. Research on co-occurring and complex disorders recommends approaches to create involvement in treatment using specific clinical attitudes and welcoming ways to meet with clients [8]. Within the systems of care for pregnant women, enhanced support, endorsing positive aspects of treatment, a focus on gender-relevant topics, and the use of motivational enhancement have been associated with improved attendance among pregnant women with SUDs [5,6,9,10]. Pregnancy and motherhood are motivating forces to increase interest in help-seeking and making changes. They can, however, also function as barriers against disclosing mental health and substance use problems as mothers may be afraid of children being taken into custody [2,5]. Therefore, women with multiple vulnerabilities will need extra support in accessing treatment. The latest gendersensitive service development contains integrated and coordinated approaches, where substance treatment is coordinated with Child Protection Service (CPS) programs, childcare, mental health treatment, mother-infant attachment and interaction. Services have had good experiences with relational-based approaches, particularly when the women enroll during pregnancy or immediately after childbirth [2]. Strengthening the personal skills, resources and self-efficacy of the mothers can help to improve the health of both the mother and the newborn [10], while punitive policies and practices discourage access to SUD treatment during pregnancy [2,5,10,11]. Stigma in society towards substance-using people is pervasive, as policies and practices blaming people for their problems exist, undermining both scientific evidence and the personal resources and motivation of pregnant women [10,11]. Societal disapproval of women's use of substances [7,12,13] or failing the societal expectation of being a \"good mother\" can cause women to feel shame, regret, and guilt [2,9,[14][15][16][17], and make them feel that they do not have a voice. Women perceive more barriers to treatment than men [12,18], such as lack of childcare [2,6], and they are less likely than men to enter treatment [2,12,[19][20][21][22][23][24]. There is also a scarcity of tailored treatment for women during pregnancy, as most SUD treatment focus on men. Men outnumber women in substance treatment [6,22], as women with alcohol disorders tend to underutilize treatment. Only 8.7% of pregnant women receive specialized substance treatment, even though buprenorphine and methadone treatments prevent relapse and treatment attrition [6]. The genderbased stigma may not recognize the probability of women suffering physical or sexual abuse, gender-based violence, trauma histories, social exclusion, or social role problems [7,16,25]. It may be particularly difficult for women with problematic substance use to leave an abusive relationship because of contextual and complicating dynamics within relationships to protect the partner or children [16,17]. Economic constraints, the need for shelter, and the fear of retaliation challenge the option of leaving a violent partner, even in situations of extreme emotional abuse, and many women choose to maintain secrecy even in front of available and helpful resources [16].\nBarriers to treatment may follow from choices within the health and social care system delivery level, as affordable and available services for substance use in pregnancy may be scarce in many areas, such as in the US [10,11]. We will study an outreach and low threshold specialized substance treatment within primary care in Finland, where the well-baby clinics are available for free for every pregnant woman and the family members. In this context the services were arranged by the local municipalities on the basis of national legislation until 1.1.2023 [26][27][28], when the new well-being areas were established in the new social welfare and health reform. Family support policies in Finland encourage individualized interpretations of family problems, where adults learn to use their own capabilities [29]. The policy aims to detect and screen the needs of children and their parents as early as possible, and the family may be guided to special support services. The primary screen for substance use is the AUDIT, and for perinatal depression, it is the Edinburgh Postnatal Depression Scale (EPDS) [30]. Early intervention universal parenting support focuses on strengthening the parent-child relationship and improving parenting skills [29,31]; Child protection services (CPS) and social work for families also support families in challenging life situations and may utilize home service, family work, support persons, support families, and other primary care services. Well-baby clinics provide counseling and support for child development and prevention of mental ill health in families. All pregnant families have free access to professional support for parenting, building healthy home environments, and promoting healthy habits. The coverage of well-baby clinics is 99.7% of pregnancies [30]. Currently, a national network of Huumeet, alkoholi, l\u00e4\u00e4kkeet (HAL)-outpatient clinics for women focusing on drugs, alcohol, and medication are located in connection with maternity outpatient clinics or gynecological outpatient clinics in several areas of Finland [1,30]. In 2017, 94% of the referrals were from well-baby clinics [30]. Between 2016 and 2020, approximately 1000 women visited this specialized service yearly [1]. Drug use is illegal in Finland, and opioid substitution treatment was approved in Finland in the late 1990s, with the first needle exchange program opening in 1997 [32]. There is no compulsory treatment during perinatal time due to substance use. Yearly, approximately 600 families in Finland attend specialized substance rehabilitation programs for pregnant families with substance issues. In 2021, there were 13 rehabilitation units, where 40% of the clients were women, 40% were children and 20% were fathers or partners [1]. These specialized programs can be considered women-centred, which is a difference to the general client profiles within SUD treatment. The National Holding Tight Treatment System (HTTS), administered by the Federation of Mother and Child Homes and Shelters, provides a special level of social service for pregnant mothers or for fathers with their children under three years and their siblings.\nThere is also a gap in the research literature in regard to help-seeking during the perinatal period among women with problematic substance use and co-occurring psychiatric disorders [6]. Specific comprehensive services have been developed that simultaneously tackle several problem areas and focus on relationship building, reciprocity, and involving whole families in treatment [9]. However, these principles are not yet known in treatment initiations within a comprehensive and continuous framework. Early first-line interventions are necessary in society to promote family mental health [33], as parental mental health and child well-being are interconnected. Solutions for the issues of mental ill health and problematic substance use have been extensively sought within preventive mental health care by earlier recognition of illness [34]. Screening is used widely, even though screening during pregnancy within SUD has not been recommended as a universal tool, as it may in itself be stigmatizing [10].\nTreatment of SUD during pregnancy is challenging, because mothers with SUD often have histories of trauma from their early interpersonal childhood experiences, resulting in trauma as a precursor of substance use and impacting parenthood and child development [5,35,36]. Trauma exposure affects an individual's capacities for emotional understanding and processing. Trauma-informed integrated treatment contains the cooccurring substance and mental health issues. Positive results in incorporating a trauma-informed approach arise from emphasizing treatment engagement and building relationships via therapeutic alliances [9,37]. The trauma-informed approach is especially important at the start of help-seeking and in engaging with the services, since early dropout is frequent, and past insecurities and parental uncertainty need to be expressed within caring and nonjudgmental clinical environments [36,38]. Relational continuity of care has been shown to be important, especially for women suffering from depression [39]. Relational models of parenting interventions involve the recognition of attachment and building relationships because the internal working models of women contain complex mental representations of the self, the caregiver, and the quality of relationships [36]. Women may perceive and interpret their distress as a personal failure, and they may fear disapproval from their social network and therefore do not seek help. If a mother is insecurely attached herself, she may have negative preconceptions of others, experience high levels of avoiding attachment, have low levels of trust in others, and be less willing to seek professional help [40].\nThere is a need to develop accessible, gender-sensitive services and approaches that challenge the barriers and stigma, and even coercion women are facing when seeking help in the perinatal period [2,7,10,11,19,22,41]. A personalized approach that promotes a feeling of being valued may enhance active engagement [42]. Engagement can mean commitment to using services [42,43], or it can be defined as a process where an inner experienced level of engagement emerges [44]. This study will focus on describing and analyzing a specialized outreach and low threshold service practice for women. This approach could add to the current understanding of low threshold services, assertive outreach programs, the promotion of gender-sensitive and trauma-informed services. Accessing safe spaces and building trust are important, and intertwined with engagement, as these are associated with greater service use [38]. Our study within the outreach and low threshold services for pregnant women with SUD aims to describe building relationships and engagement within the outreach and low threshold service encounter tailored for pregnant women.\n---\nMethods\n\n\n---\nData collection\n\nThe data collection occurred at the Holding Tight Treatment System (HTTS) administered by The Federation of Mother and Child Homes and Shelters. The first Mother and Child Home for women with problematic substance use was founded in 1990, and the national HTTS was founded in 1998. Staff has since been trained in community care, reflective functioning (mentalizing), early interventions and intergenerational trauma [15]. The theoretical perspectives are attachment, trauma and mentalization. HTTS has seven Mother and Child Homes specializing in substance treatment and eight open ward units. Rehabilitation includes supporting parenting and early interaction and special baby-oriented substance rehabilitation for pregnancy and the first three years. It is especially important to provide treatment as early as possible during pregnancy. The goal is to secure the healthy growth of the infant and to motivate the parent into intensive rehabilitation. Rehabilitation for lasting changes requires sufficient time and planning between the families and multidisciplinary professionals. The service is focused on women-centred care, as in a profile of the service users in 2015-2018, most clients were women (62%), and most were in the age group of 25-29 years (29%) (Table 1). In 2015-2018 in HTTS, the rehabilitation phases lasted most often over two months (Table 2).\nHTTS started to implement and develop a low threshold outreach program (ETMA) in 2018, which aims to increase the accessibility of services and reduce stigma. The program utilizes the same theoretical framework as HTTS; it supports abstinence and promotes parenting and early interaction. Families consist of the mother, baby, siblings of the baby, and a partner, whether living in the same home or elsewhere. ETMA is active in social media work; they run peer support groups and chat discussions online, aiming at early support. The work contains harm reduction, such as promoting discussion about the substances and stigma related to it. They also collaborate with CPSs, well-baby clinics, and different low threshold sites, where families can join anonymously. The program participation can be anonymous and take place flexibly at several locations. In 2021, most participants in the low threshold program were women (67%) and in the 18-to 29-year age group (Table 3). There are no similar statistics available on outreach work, including social media, chat discussions or support groups.\nOf all visits in 2021, the pregnant woman participated in 42% of the meetings, 20% were nonpregnant women, and 29% were partners/fathers. The most common substance in 2021 was alcohol (27%), and illicit drugs (17%), mixed usage of substances or medications (38%), and the substance was not known (18%).\nThe data consist of staff and client data. All staff members (n = 14) within ETMA were approached in September 2018 with an e-mail from within the organization, which led the staff members to an online query. The eleven participants all came from a tailored ETMA program specifically addressing the needs of women with problem substance use and small children. Additionally, data contain feedback collected in HTTS from 504 families in 2015-2018, after the families with small children had ended their rehabilitation phase (Table 1 and Table 2 describe the families).\nThe eleven female workers come from six different units in different locations in Finland. Their ages were 28-52 years, with an average of 43 years. They had worked at HTTS for 4-25 years, on average 13 years. Three of the workers had nursing, and eight workers had social work training backgrounds. The staff participated via online written narratives. They were asked to describe their outreach and low threshold work, what knowledge and skills they need and the specificities of the ETMA   Whether there was something else to add.\nThe responses and data from staff consist of approximately 7000 words.\nResearching the area of treatment initiation and engagement within SUD services is very difficult because women and families may not recall what happens when help-seeking occurs, as many arrive while experiencing traumatic life events. The choice of this study is to ask clients their feedback afterwards in regard to the early phases and the relationship with workers. Many clients arrive with referrals from well-baby clinics or CPSs and may be strongly recommended to participate. All women may not feel they attend voluntarily. Therefore, an appropriate timing for interviewing the women is after the recovery phases have been completed. HTTS families give both numerical and open-ended feedback when their recovery phase ends. Workers connected with the parents and wrote down the oral feedback. Families were asked:\n1) Open feed-back about the rehabilitation phase, and 2) To express development needs.\nAltogether, 504 persons (ca 70%) out of the 719 persons in total had given feedback in 2015-2018. Families gave feed-back in the end of rehabilitation (n = 306), half year after the recovery phase (n = 124), and one year afterwards (n = 74). The respondents were mothers (n = 204), fathers (n = 74) or the respondent was not known (n = 226). Feedback was given during four years: 2015 (n = 97), 2016 (n = 95), 2017 (n = 180) and 2018 (n = 132). All feedback is included as data for the study. The family feedback from HTTS was available as paper prints, and all texts consisting of approximately 8000 words were digitized to enable verbal analysis with software. The feedback contained 228 open answers related to early help-seeking and encounters with staff, which forms the basis of the qualitative data set of client and family feed-back.\n---\nData analysis\n\nThe special interdisciplinary perspective of the ETMA system might be of interest to other service providers and function as an asset for developing general services. We are placing the substance use of women in a cultural and practical context, deriving from previous theoretical frameworks. The selected services have a unique contribution to the research question [45]. The qualitative data are analyzed with a thematic analysis, which is especially well suited for an underresearched area [46]. The data analysis process was as follows: 1) thorough reading; 2) sentences responding to the research question were inductively coded from both data sets using QSR NVivo to facilitate the process; 3) the themes were described; 4) the two data sets were combined; and 5) the themes were contested in discussions within the study group.\nRigor in the study was established via several tools [47]. We have described how we obtained the two data sets and completed the data analysis. We wanted the themes to be descriptive and as authentic as possible so that the report could be useful and utilized further, e.g., for theory development. The themes reached depth and contained contrasting viewpoints. The background section of the article describes the standpoint from which we have approached the data. The validity questions regarding thematic analysis relate to whether the data are an accurate reflection of the whole data set [46]. Therefore, all themes include excerpts from the original data. Confirmation occurs in relation to previous research, and the original excerpts assist in highlighting new perspectives [45] within engagement in SUD treatment for women and during pregnancy. We have also contextualized the study findings within a specific national context. Additionally, we incorporated the COREQ evaluation of the study process [48].\n---\nEthical questions of the study\n\nThe ethical aspects of the study were reviewed and approved by the management of the Federation of Mother and Child Homes and Shelters. Staff gave their consent to participate prior to participation. The mothers and families responded voluntarily to a regular feedback by the time they ended their rehabilitation. No names of sites are utilized during data analysis or reporting the findings to protect the respondents. The families' feedback was anonymous by start. The data from staff were collected with an online tool administered by HTTS, and after this, only the 1st author (MS) worked with the data to ensure confidentiality and anonymity. The data from mothers and families were collected by HTTS anonymously with the register files containing solely year and site of feedback.\n---\nResults\n\nThis study will focus on describing and analyzing specialized outreach and low threshold program practices for women with problematic substance use. The research question is as follows: How are relationships and engagement formed within an outreach and low threshold service for pregnant women with SUD? The themes for enhancing relationships and engagement within outreach and the low threshold program will then be described (see Fig. 1). The women's backgrounds are very diverse when they connect with the services; since the women are at a different stage of pregnancy, they may be first-time mothers or already have older children. They use different substances, drugs, or alcohol or misuse medications. Some suffer from psychological distress or depression, and others endure chronic physical illnesses. Women may be active users of substances when meeting with them the first time. Not everybody is motivated to participate in the ETMA program.\nWhen attending the program, the clients carry with them their memories and the stigma they experienced during encounters with previous service providers or within society. The health care professionals' attitudes at childbirth may be memorized and feel painful, despite inner knowledge of their right to equal access to supportive services and their right to proper treatment (hyv\u00e4 kohtelu), \"It's about human values, \" not undermining or being arrogant about the women's requests for help:  If previous requests for help have been negative, many women prefer not to disclose in the future. It may be surprising for the woman to find out that people and staff at the ETMA program can be friendly, and in fact, kindness may in itself function positively in regard to the women's previous experiences within the substance world involving threatening events and experiences.\n---\nAcceptance and attitude: a sensitive approach of approval\n\nThe staff has a specific attitude when connecting with women experiencing this special life situation with problematic substance use and motherhood. Staff members have in-depth knowledge about substances within the contexts of addiction, illness, and lifestyle that are threatened to be challenged when women seek help. Because their backgrounds are versatile and their life situations are diverse, meeting with professionals entails individualization, flexibility, and adjustments to the staff 's perspectives and the working models employed. Staff members within ETMA need skills to assess the help-seeker within a larger societal and cultural frame or phenomenonthe very many diverse levels and forms of substances have an impact on women arriving at the services. The personal agency of a woman is contested when she seeks help for her pregnancy or with motherhood.\nAccording to families and staff, the best results occur if staff can begin collaborating with families during pregnancy. Staff aim to create an atmosphere of acceptance to contrast the stigma the women may have experienced prior to ETMA. The women must feel as if they are welcomed; the saying \"come as you are\" is applicable in the overall theme of acceptance, and a tailored approach is favored. Furthermore, a sensitive approach of approval can create connection: \"A suitable environment and workers..important to understand the human irrespective of their starting point. \" (Client,442).\nThe first encounter is described as especially important from the staff perspective. In the beginning, the mother, child, and healthcare professional need opportunities to become closer to each other. Trust can be built in an open atmosphere, and it benefits the woman and the staff in the event that a relapse occurs later, so that additional challenges can more easily be disclosed. Women want to be supported and encouraged in the process. They need positive feedback. The workers can have a positive impact by demonstrating that they believe in the mothers: \"It has been important that somebody believes in me, even though I do not always believe in myself. Even though we only chat a little bit, I might later feel that the discussion was useful. \" (Client,210).\nMany staff members were perceived as warm. Women noted that after their rehabilitation phase, they had also received help within other areas that they had not thought of in advance. In situations when they feel tired, this fatigue may restrict their options to ask for and receive help. From a woman's perspective, helpseeking requires honesty and openness, and emotions arising during staff encounters may feel good, bad, irritating, or even anxiety-provoking. When the women felt physically ill, they sometimes felt they had to repeatedly repeat themselves and explain their situation to be taken seriously:\n\"As a tip to other clients, openness is the most important. You cannot be helped if you do not share anything about yourself. \" (Client,384).\nThe clients admitted that their own attitudes could impact their help-seeking efforts. Moreover, the timing of their access could impact their ability to adhere to treatment or how they interpreted staff attitudes. Treatment also felt like an ambiguous period because they were living and learning about their emotions.\nThe clients noted the importance of not blaming them. The clients considered that it was most important to find their motivation themselves and to pursue progress even though it did not always feel useful. Many women come from well-baby clinics after a positive substance use screening or if they shared information on their close family's substance use. The staff perspective is decidedly oriented toward the well-being of the fetus and developing \"baby in the mind\" thinking:\n\"Working throughout the pregnancy to ensure the growth and development of the fetus, so the baby would be born healthy, despite exposure. Fathers, if known, and if they want to be involved in the family, should have access to rehabilitation at the same time as the mother. \" (Worker).\nStaff considers the baby, pregnancy, and the effects related to expectancy every time they are able to meet with the woman or soon-to-be mother. Staff try to answer any questions the mothers have, and they discuss how the baby is doing. The pregnancy period gives a woman the chance to focus on her own life because self-centeredness will be more difficult after the child has been born. She might have difficult experiences in her life that have not yet been addressed and require attention. It may take a long time until motivation emerges. During the phase when a connection is being formed with staff, many women may experience difficulty accepting care. To make a trusting relationship work, staff need their own personality while still maintaining a professional distance.\n---\nFlexibility within strictness to allow for diversity and individuality\n\nWomen may feel afraid or ambivalent by the time they arrive at the service encounter, partly because of the risk of their children being taken into care unless their substance abuse issues are resolved:\n\"Child protection feels scary, but it is important they are strict by start\u2026staff wants to support, provide feedback that it is going well, and that keeping their child is possible. \" (Client,476).\nThe openness in communication includes explaining why urine tests are taken and using screens with the aim of assessing for possible continuing substance use. Staff also discuss the child's wellbeing and the impact of substances on the child's development.\nWhen working with these women, enhancing the development of trust and attempting to enhance the collaborative relationship between staff and clients is continuous. From the staff perspective, supporting and encouraging the parenting agency of women in situations while also setting necessary boundaries requires a combination of flexibility and strictness. Flexibility helps when a woman forgets to tell staff that she cannot make it to her scheduled meeting, cancellations occur, and meeting hours or the number of meetings are changed. The situation can be reviewed and re-evaluated even quickly, such as in the event of physical health conditions. The reason for this flexibility stems from the raison d'\u00eatre of HTTS:\n---\n\"To hold tight to the clients and try to make them engage in some form of support\u2026 if a mother cancels a meeting, I try to reach out to her and clarify, why the meeting did not occur; if I cannot reach her, I'll go and visit the mother at her house if necessary. I only make home visits to clients I already know. On the other hand, the need to approve, if the mother does not want to accept support. \" (Worker).\n\nFlexibility and alertness to the possibility of making changes is needed in the event of a relapse. If a collaborative relationship has commenced, a relapse may mean that the woman will need additional support for a while. More testing and screening are occasionally administered. The respondents believed that an individualized flexible approach was possible if a worker was more knowledgeable about the special family questions.\nRespecting the woman's choice and agency means that it remains the mother's right to decide how to set boundaries in the event of challenges. During these times, it is not possible to strictly follow plans, and flexibility may be needed:\n\"It is important to go through failures to have an experience that overall, it is possible to move forward, and failures do not ruin it all. \" (Client, 369).\nThe process also involves disappointment if a relapse causes a client to stop attending her meetings or a relationship cannot be formed with a specific client. For workers, understanding that a relationship cannot be formed with every client may be a frustrating emotional journey to process. Workers know that noninterrupted intensive help has been shown to be the most effective. Beginning collaboration is a long-term effort, and it is important to also hold tight during difficult moments. Clients assert that individuality gives them the opportunity to recover. As humane people, staff challenge the stereotypes of addicts; the individuality that is necessary to connect with the mothers involves respect and is woman-centred. The clients emphasize the individuality of each event and of each woman and mother in relation to their own personal goals, as these need to be the women's goals, not the workers' .\nBecause trust was developed in person-to-person connections, a contrast can be seen in whether it is possible for the women to speak out in groups. This may not be possible due to sensitive questions and issues related to the culture residing in communities within addictions, such as interpersonal violence in the drug world. If a close person such as the baby's father continues using substances, it will be challenging for the mother to continue toward her goal of recovery.\nA family-centered approach supports the individualized perspective. Under Finnish law, workers must report living conditions that would challenge or harm the child's well-being to social care. On the other hand, work at HTTS is baby-centred. Therefore, staff utilize an honest, direct, and friendly approach to cope with such situations while developing a relationship. While there is a risk that the woman will leave and not commit to treatment in the outreach and low threshold phase, many clients giving feedback were in favor of coercion:\n---\n\"Perhaps when I hid things and in a certain way you knew, you could have faced me with the truth and asked, 'What the shit are you doing?'\" (Client, 185981).\n\nBecause other clients were against coercion and felt that the focus on the baby was oppressive, many questions required negotiations between the clients. Staff needed to confirm the mother, since the aim at ETMA is to make the mother understand the viewpoint of her child. In the beginning in the outreach and low threshold program, there is more flexibility and less confrontation than in future recovery steps. According to clients and workers, strict boundaries do not work in outreach. The challenge is how individualized boundaries are perceived by peers, who may insist upon the exact same rules for everyone in the rehabilitation communes. Because speaking openly is important, workers choose the point of confrontation with sensitivity, so as not to interfere with the development of trust.\n---\n\"You should be able to talk openly about things without sanctions or fear. Speaking honestly is very important. It is difficult to be honest-for example, about stealing or other problems-because of fear of children being taken into custody. \" (Client, 362).\n\nInsensitive confrontations may lead the individual to choose to be silent or be offended if trust has not yet been developed.\n---\nAvailability and space to ensure a trustful atmosphere\n\nLow threshold staff are available in easily accessible locations. Outreach services can meet women anywhere, and when some level of connection has been established, home-based low threshold care can also become an option. Being available to the woman's needs utilizes calmness within a specific space:\n---\n\"Meeting, hearing the other human, respect, calmness in the situation-you must not hurry the other-and verbalizing and bringing up difficult issues with respect. \" (Worker).\n\nThe women may feel vulnerable (rikki) if they have not yet attended preventive care. Their diverse background and experiences impact the manner in which workers can connect with them. Workers and clients emphasize the need for easily accessible care: \"It is important that it would be easy to seek care and to be given the chance and understanding of the enabling change involved in motherhood. \" (Client,134).\nAvailability is a space that contains a sense of freedom because the person seeking help is fully entitled to speak within a trustful atmosphere. Again, staff utilize sensitivity to create the feeling of non-pressure and availability. Since each human has a unique background, they also require unique solutions. A woman's specific situation may necessitate changes in their personal manner of connecting with other humans within the services or when creating trust. Developing trust, starting to discuss her own problems, and asking questions about her life situation emerge in stepwise co-creative processes. Staffing changes may interrupt building a connection and may lead to setbacks. Situations where the child's well-being requires action may not be pleasant experiences for the woman.\nMany families positively mentioned home-based outreach, their everyday lives, and rhythm with children as possibilities for learning mothering and fathering in the context of their own home. The service can develop into a trusted safe space that women can contact by choice. Clients said they could trust the ETMA-built trust, which ensured that someone would always connect with them. Home-based support in their everyday lives was favored as easy access to care for families with children or when a client had a bad day. The goal of recovery and everyday situations could function as a frame for discussion and availability.\n---\nNegotiating via doing to build connections\n\nThe experiences of being heard and receiving positive feedback can strengthen a woman's ability to function. Because meetings between humans occur in specific times and spaces, several negotiations are needed regarding different common rules of collaboration. After connection between the woman and workers has been co-created, a question may arise as to who is responsible for the recovery. The woman's agency may grow when she makes decisions and has the freedom of choice, such as what to do within the spaces where connections occur:\n---\n\"A family-cafe would be great, so that parents could meet freely, exchange views, discuss bothering matters and children could play. There could be workers, if something acute and worrying would occur and the workers role would mainly be an enabling bystander, she should no way intervene, unless the clients want. \" (Client, 225).\n\nClients said that meeting with peers was considered an ideal empowering option that would allow for the exchange of views from the inside and would shift the workers from an advisory role to consulting when they were needed. While some women required more encouragement when participating in peer groups, other clients refused participation because not everyone has the energy to be involved. If a woman was sensitive to criticism, workers negotiated with them with the aim of respectful counseling or feedback: \"Sometimes, the workers had too much advice and hints in motherhood and less would have been sufficient, considering that a new mother may be sensitive to too much counselling. I missed receiving more positive feedback. \" (Client, 168584).\nNegotiations and interactions between clients and workers are intertwined with details and impressions and involve taking directions when communicating during different shared actions. Many interactions in the outreach and low threshold programs occur by doing: sitting together to have coffee, visiting different places such as parks, going for walks or engaging in sports activities, relaxing, going out on day excursions, engaging in the arts or photography work, baking or beginning a new hobby, and finding additional recreation activities. Supporting motherhood by engaging in activities may further enhance the creation of a connection and participating in the program. Because verbal expression may be difficult for some women, doing provides more opportunities for building connections.\nDoing together and commencing activities may lead to the development of interest in living everyday life and in devising solutions such as learning how to manage money. Furthermore, the positive emotional component of engagement yields new experiences. It was suggested that the women need to connect with others, have fun together, and find everyday activities that give them a sense of meaningfulness. These activities can be seen as empowering and help them enjoy their everyday life. One of the clients suggested that connecting with peers might be helped by participating in activities: \"These would be easier at the start, to be with others and meet them. \" (Client,137).\nAiming to establish a rhythm in their everyday lives also benefitted by doing together with a worker. Concrete everyday skills and seeking solutions could be elaborated by using a pregnancy diary:\n---\n\"In every meeting, we go through the mother's pregnancy diary together, and we reflect on how the baby is doing and what the mother should know and remember in relation to the pregnancy. The mother has a task to ponder during every week of the pregnancy. I use cards and assignments. \" (Worker).\n\nWhile there were suggestions that the whole family would be included in treatment, other formats were also suggested: women-only peer support groups, groups for single mothers, father groups, and groups for both parents if they are together. Many felt that the relationship between the mother and father was also an issue that needed attention.\n---\nEveryday life changes: imagining the recovery\n\nIn early help-seeking efforts, clients connect with visions of what they perceive can become their future. During the first steps, it becomes necessary to view the future and discover potential goals. For recovery to occur, connections within ETMA and HTTS are co-created. This process creates the option of stronger personal agency and yields insights into a woman's everyday life:\n\"Accepting oneself the way you are, searching for your own strengths. \" (Client,501).\nThe workers stated that women must be able to define their goals themselves so that goals from outside the individual do not become overwhelming and impossible to implement at a practical level. A woman needs to elaborate upon her substance problems, her everyday life, and parenting at her own pace. Engagement and involvement may take a long time. Several clients expressed a need to hear the long recovery perspective and the possible phases of full recovery from the start, at the stage after the client connected with professionals: \"I would have wished more information about the whole process from start. Everything came so quickly that it was sometimes difficult to relate to things.\n---\nE.g. information on how long the rehabilitation takes\n\nand what things help it to end. In the beginning it would be good to go through all details of rehabilitation. In the end, rehabilitation has felt really good, and I have learnt much about myself, and I have strengthened myself, even though it felt difficult at start. \" (Client,483).\nIncreased knowledge about recovery phases may enhance the vision of a future experience of being helped. Committing to and engaging with the forthcoming recovery process requires a certain level of preparedness for the necessary steps thereof: \"Rehabilitation should be long enough-at least a year-because changes do not occur quickly. \" (Client,338).\nFrom the client viewpoint, the first steps involve being informed about their rights and the manner in which the overall helping system functions. The women need to know that self-directed reflections are part of the forthcoming recovery process. Women needed information about details such as the importance of reviewing their life histories as a whole, digging into problems with selfesteem, and how to maintain their individual point of view.\nTiming is of interest, since coming for a visit to ETMA and increasing interest in recovery from problematic substance use during pregnancy is an opportunity for the mother to focus on her own care. The earlier a woman shifts the focus to herself and the baby about to be born, \"the baby in the mind\" knowledge promotes the family's future steps. After the baby is born, learning to cope with the everyday routines in the family's life will require significant energy. As such, the focus on the child involves understanding the child and becoming prepared to live with an infant. In the next step, children are involved in the recovery steps of the mother and her partner. New skills can be learned in everyday situations:\n\"The child at the center: children accompanying them in rehabilitation. The children's joy and being together without drugs was rewarding and important. \" (Client,98).\nThe women received concrete guidance in childcare and knowledge about child growth and development. Rehabilitation focuses on the interaction between the child and the parent in the context of addictions, everyday skills, presence and engagement. The skills emerge stepwise. The knowledge of hardships ahead and perseverance is therefore helpful at the start. Workers considered it important that the parents' own skills would awaken so they could see their own resources:\n\"The individual meetings usually include a discussion on the mother's situation and how she is doing, and for every meeting, I have a theme related to the baby/motherhood/parenting planned. \" (Worker).\nAn orientation to the future also exists in the knowledge that any future rehabilitation could be temporary, and the main aim for the women and their families would be to eventually live their everyday lives without using the service. If a bond has already formed between the mother and baby during pregnancy, the family can commit earlier to recovery-oriented communes and will start to hear about eg schedules and can imagine their way forward. They will learn basic skills according to their needs, such as nurturing and eating habits within the family, maintaining a daily rhythm, and basic interactive and parenting skills.\nA question about an orientation to the future emerged from the client responses, which suggested the need for increased visibility and the presence of the program within other services and in well-baby clinics: \"More visibility perhaps, survival stories, etc\u2026 Communicating more. \" (Client,259).\nClients use Google to create a pre-understanding and learn about early interventions, \"points of entry, \" and the different options available to them prior to accessing these services. In the current Finnish context, families can learn about suitable and available services, and they use peer recommendations.\nWorkers felt that awareness of HTTS and ETMA work with families in the early years was scarce. Other workers within health and social care fields may not know the client group, which may cause misunderstandings, misconceptions, and even stigma. As such, the workers networked among professionals that serve the same client\u00e8le, and some provided open groups in well-baby clinics. They regularly meet with the network and release information in the form of leaflets and as digital materials, and they are also aware of online content strategies and a presence on the Internet on social media platforms such as Twitter, Instagram and Facebook, where social media is accessible to the professional network and to clients. Staff marketed their work; in this context, outreach utilizes networking and virtual channels.\n---\nDiscussion\n\nThe results of our study characterize professional practices in relation to perinatal substance use and demonstrate how outreach and low threshold services are administered in gender-sensitive, trauma-informed and non-coercive ways. Because current interventions do not reach the most vulnerable families with small children [2][3][4][5], it is necessary to develop programs within outreach and low threshold services for women and families during the perinatal period. The barriers earlier described regarding social stigma, coercion and punitive practices, socially restrictive relationships, partner substance use, and feelings of shame and guilt [2, 6, 7, 9-12, 14, 15, 19, 21-25, 37, 40, 41, 49, 50] can be counteracted with the comprehensive, integrated solutions of the HTTS and ETMA programs. It is noteworthy, that even in the presence of early interventionist systems of care, most women do not seek help or they have problems identifying the most relevant ways to access care [1,41].\nIn contrast to punitive and coercive practices [10,11], our data were collected at a service focusing on inclusion and engagement from the theoretical perspectives of intergenerational trauma, attachment, early interaction and mentalization [9,15]. The themes covered in our study capture how outreach and low threshold interventions benefit from the approach. For example, the theme \"Acceptance and attitude: a sensitive approach of approval\" strongly emphasizes building relationships, which stand at the core of attachment-based and traumainformed interventions [9,15,35,40,51,52]. The need for a welcoming approach identified in relation to comorbidity involving substance use and mental health conditions [8] was also addressed in our study. According to our results, skilled staff need to demonstrate greater sensitivity in approaching women with problematic substance use or SUD-for instance, need to avoid confrontation-and instead cultivate trust. The women in our study also benefited from a gender-sensitive approach [2,7,19,22,41], which helped to create connections and engagement [42,44].\nThe theme of \"Flexibility within strictness to allow for diversity and individuality\" captured how the work had to be focused on children's well-being, while the gendersensitive approach emphasized the women's agency to make their own decisions in any situation regarding their own lives. In that process, staff needed to be strict when CPSs needed information, and sometimes extra screens had to be administered. In general, CPSs are responsible for ensuring that each child's right to a safe, stimulating environment for growth is protected. If an outreach or low threshold program emphasizes individuality and diversity within a family-focused frame, it may be easier to find solutions in which women feel approved and accepted as themselves, not as \"addicts\". In Finland in general, society and the community take responsibility for substance use problems [17,32]; however, our data also clarify that the women themselves need to start becoming active in their lives in order for recovery to proceed [29]. In turn, that undertaking can allow them to identify their strengths and develop agency.\nThe ETMA program, intended for all members of the family, focuses on the mother as the first step to potentially increasing self-awareness [24] because the period of pregnancy may afford women time to reflect on their lives and possible sources of trauma. Such self-reflection can help women develop skills in reflective functioning [9,15,51] before the birth of their children because after childbirth, they are required to devote attention to their babies and nurture relationships with them. The practice used in outreach and low threshold services contains a strong future orientation and targets changes in everyday life and a commitment to recovery. Along those lines, the theme \"Everyday life changes: Imagining the recovery\" included the recognition of some possible future steps. As captured by the theme \"Availability and space to ensure a trustful atmosphere\", the ETMA program took place within the community, as well as virtually with the aim of creating safe spaces where it would be appropriate to address challenging questions stemming from their everyday life. This is why the theme \"Negotiating via doing to build connections\" is relevant, as change is based on everyday life activities.\nThe biggest difference between the outreach and low threshold services and rehabilitation were the acceptance of the slowness of improvements and the ability to endure uncertainty until a connection with the women had been created. After a collaborative relationship had been established, it made it possible for the workers to explain their worries concerning the clients and the treatment process to clients in greater detail. To overcome stigma, staff members need to recognize that help-seeking may be challenging for clients given their background and that developing trust with them will take time.\nBoth workers and clients indicated the ideal of equality. Whereas staff expressed that every human shares the same reality irrespective of their life situation, clients wished that professionals would take a gentler approach. To imagine their recovery, clients suggested making more information about the whole process available online and appreciated the possibility of investigating the process before committing to treatment or seeking help. Promoting the well-being of such clients will require raising public awareness of the existing ETMA and HTTS services and marketing them. By informing and messaging about substance treatment, for example, on social media and other platforms, knowledge about such services will reach those in need. Providing the general public with information about services and SUD treatment may mitigate the stigma related to substance treatment. The possibility of attending anonymously is important, and first-line services should be free of cost.\nThe societal context in which women use substances during pregnancy imposes certain requirements and gender roles, including being a good mother, and judgements of the women's character can emerge among health care professionals as well [2,7,10,53]. Professionals who blame women for using substances during pregnancy may not recognize the women's suffering due to, for example, physical and/or sexual abuse, genderbased violence, and/or trauma, all of which have often been associated with substance use [5,7,16,17,52]. The flexibility and availability of services, including the possibility of at-home visits where women can feel safe and be candid, can be developed in parallel. Beyond that, connections could be built outside official relationships with staff while engaging in a wide range of activities, particularly by using resources of one's own choosing and interacting with peers. To build connections, many negotiations were required in such activities, which corroborates past findings showing individuals' willingness to participate [12,49] and well-being in everyday life despite challenges [41,54]. Women need nonjudgmental access to care, and they may benefit from promoting involvement in their communities as well [40,41]. It is noteworthy that the complex vulnerabilities women may experience should not cause clients to be treated as deficient or lacking [10,53]. Many problems may be longstanding and unlikely to be quickly resolved, as clients may have a number of competing demands that they are juggling and balancing [53].\n---\nLimitations\n\nOur study was contextualized within a country that provides free or low-cost mental health and social services. The outreach and low threshold services were developed in the context of perinatal substance use. We performed a COREQ evaluation for reporting the results [48]. The anonymous feedback was collected by the HTTS organization, and feedback was not available from all clients, which is a restriction of the study. The anonymous client feed-back represented 493 families and 719 parents, yet we do not know the reasons for the 215 clients (30%) out of the 719 persons for not responding. The findings are skewed towards those who completed the program, and who most likely had a positive experience with it and with their recovery.\nThe staff participated via online surveys, which was feasible as they could respond at any time from different parts of Finland. The narratives were saved online and were easily accessible for the analysis, we can recommend this option of online written narratives. The responses were thorough, and several workers expressed, that it required them quite long to reply in detail. They could write their narratives during their working hours of ETMA.\nIn the analysis we focused solely on engagement and did not have an intention to find correlations, but to describe building relationships and engagement, and use the results as a 1st step in theory building. Using one coder in this predefined and focused context produced descriptive themes, which were discussed and verified in the research group, where three persons worked within HTTS, and one especially within ETMA. The themes are presented in detail to explicate the complexity of the encounters, and we presented quotations to illustrate the themes.\n---\nConclusion\n\nThe outreach work was characterized by flexibility and the creation of a connection. The low-threshold work aimed at establishing trust, building relationships and engagement, and can use existing interventions such as motivational interviewing. Building a connection, relationships and engagement occurred via physical presence and psychological availability (Fig. 1). The four cornerstones we identified were professional attitudes enhancing engagement: acceptance, attitude, strictness and flexibility. Availability occurs in a specific space, where the everyday life changes require constant and even challenging negotiations and much of the work within outreach and low threshold is carried out by doing things together. Building relationships during pregnancy were characterized by connecting within everyday life situations and supporting the development of an attachment relationship between the baby and the parents. To promote recovery, a comprehensive approach in which substance-related issues and mental health conditions are interconnected is favored. Soon-to-be mothers might be more effectively encouraged to test and visit early preventive intervention programs by well-baby clinics. To help women with SUD during pregnancy, the special perspective of the HTTS and ETMA programs might be of interest to other service providers and function as an asset for developing general services and connections with CPSs. Building connections in a gender-sensitive manner and within trauma-informed approaches could add to the understanding of SUD outreach and low threshold work during pregnancy. The elements described in this study need further theoretical development, research and critical assessment. Engaging early on during pregnancy might enhance success during future rehabilitation.\n---\nAvailability of data and materials\n\nThe data cannot be openly available in data repositories because the data contain sensitive issues. The data to support the findings are available upon request from the corresponding author.\n---\nAbbreviations\n\n\n---\nSUD\n\nSubstance Use Disorder HTTS Holding Tight Treatment System ETMA Low threshold outreach program (Etsiv\u00e4n ja matalan ty\u00f6n palvelut)\nAuthors' contributions M.S., M.H. and H.S. planned the study. M.S. completed the analysis, and M.H., M.P., H.S. and K.P. contributed to the analysis and interpretation. M.S. wrote the main manuscript text; M.S. prepared Fig. 1; and M.P., M.S. and K.P. prepared Tables 123. All authors reviewed and approved the manuscript.\n---\nDeclarations Ethics approval and consent to participate\n\nEthical review and approval were not required for the study on human participants in accordance with the local legislation and institutional requirements. The ethical aspects of the study were reviewed and approved by the management of the Federation of Mother and Child Homes and Shelters. Staff gave their consent to participate prior to participation. The mothers and families responded voluntarily to an anonymous feedback by the time they ended their rehabilitation.\n---\nConsent for publication\n\nNot applicable.\n---\nCompeting interests\n\nThe authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest. Maria Hohenthal, Miia Pikulinsky and Hanna Sellergren work at The Federation of Mother and Child Homes and Shelters.\n\u2022 fast, convenient online submission\n\u2022 thorough peer review by experienced researchers in your field\n\u2022 rapid publication on acceptance\n\u2022 support for research data, including large and complex data types\n\u2022 gold Open Access which fosters wider collaboration and increased citations maximum visibility for your research: over 100M website views per year\n---\n\u2022\n\nAt BMC, research is always in progress.\n---\nLearn more biomedcentral.com/submissions\n\nReady to submit your research Ready to submit your research ? Choose BMC and benefit from: ? Choose BMC and benefit from:\n---\nPublisher's Note\n\nSpringer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.",
        "Introduction\n\nDue to higher life expectancy, lower fertility rates, improved healthcare systems, and longevity, the aged population is increasing globally [1]. About one billion people aged 60 years or above will increase to 1.4 billion and 2.1 billion by 2030 and 2050, respectively [2]. In low-and middle-income countries (LMICs), the expected population growth rates remain higher than in high-income countries [3], and 80% of the global older adults will be in LMICs by the year 2050 [2]. However, the increasing older adult population faces physical and mental health challenges, including older persons/elder abuse [4]. For instance, in Uganda, older persons face multiple challenges, including poverty, poor health, unemployment, chronic ill-health, HIV/ AIDS, lack of social security systems, low land productivity, political instability, low agricultural returns, and functional inability [5][6][7][8]. Culturally, in Uganda, older people were highly respected; younger people and children would even kneel to greet them, and were considered a symbol of knowledge and power, but all has changed in recent years mainly due to cultural dilution [5]. The country has also been affected by high rates of domestic violence, especially towards women in rural settings, and the law has poorly handled them, thus perpetrating these unhealthy behaviors against many individuals, including older persons [9][10][11][12]. These challenges impact older person's quality of life and make them susceptible to neglect and abuse by their peers and family members [5,13].\nElder abuse can be in different forms, such as physical, sexual, emotional/psychological, financial, abandonment, neglect, and institutional maltreatment [5,13]. A recent systematic review of older persons from rural settings by Zhang and colleagues reported the pooled prevalence of elderly abuse at approximately 33% [14]. Among these older persons from a rural setting, 5%, 7%, 17%, and 26% experienced financial, physical, and emotional abuse, and neglect, respectively [14]. Evidence from a systematic review of past-year abuse showed abuse among 15.7% of older persons [15]; emotional abuse was dominant at 11.6%, followed by financial (6.8%), then neglect at 4.2%, and sexual abuse least experienced at 0.9% [15]. In Europe, prevalence varies from 2.2% (Ireland) and 61.1% (Croatia), whereas, in Asia, the highest prevalence was reported in China (36.2%), and the lowest was in India at 14.0%, as per a global systematic review [16]. In South Africa, men's and women's prevalence rates were 64.3% and 60.3%, respectively. Physical abuse was more common among men, while emotional, financial, and sexual abuse was more frequent among women [17].\nAbused older adults present to the hospital with different signs and symptoms, including inadequately or usually unexplained locations of abrasions or bruises, lacerations, and burns [18]. Other signs are fractures in non-alcoholics, malnutrition and dehydration, pressure ulcers, sexually transmitted infections, and vaginal and rectal bleeding without a reasonable explanation [18].\nEvidence has highlighted some of the factors associated with elder abuse across the globe, including (i) perpetrator factors such as mental illness, abuser dependency, and alcohol dependence; (ii) host factors such as dementia, physical dependency, living alone, or with spouse; and (iii) environmental factors such as social isolation and negative societal perceptions towards aging [19]. In addition, psychiatric illness among older adults is an essential cause of vulnerability to abuse, especially when it is comorbid with other risk factors such as physical frailty, sensory impairment, social isolation, and physical dependency [13]. Other prominent factors associated with elder abuse are caregiver burnout, stress problems, coping with perpetrator childhood abuse, and relationship problems such as intimate partner violence [20].\nElder abuse is also associated with various adverse health outcomes, including physical and psychological effects that have long-term consequences such as depression, anxiety, and posttraumatic disorder [21]. It also can pose significant complications, such as premature death. For example, community-dwelling middle-aged and older women who reported prior physical, verbal, or both types of abuse had significantly higher adjusted mortality risk than women who did not report abuse [22]. In addition, reported and corroborated elder mistreatment is associated with shorter survival in both women and men [23]. Abused elders also face complications such as post-traumatic stress disorder, poorly controlled chronic diseases such as hypertension, diabetes, and heart disease, decreased quality of life, and loss of trust or quality of relationships [24]. Furthermore, elder abuse poses significant complications to society, like the cost of care for victims, the use of community legal and law enforcement resources, and the burden on nursing facilities [24].\nDespite the dangers of elder abuse, information on the prevalence, types, and factors associated with elder abuse in Africa is sparse. Therefore, the present study aimed to determine the prevalence of elder abuse, the various types of abuse, and their associated factors in a Ugandan older person attending tertiary hospital outpatient clinics.\n---\nMethods\n\n\n---\nStudy design, setting, and population\n\nThis cross-sectional descriptive study was conducted in February 2021 among older adults aged 60 years and above attending a referral hospital outpatient clinic in Uganda that attends approximately 600 older adults every month [6,25]. After receiving care for various ailments, elders were recruited by convenience sampling after obtaining written consent from the outpatient departments. Based on the Mini-Mental Status Examination, we excluded elders with severe neurocognitive impairment (a score of 17 and below). We thus recruited 363 participants.\n---\nData collection\n\nUsing the list of all individuals who were attending the outpatient department, the research assistants identified participants who met the inclusion criteria of not being severely sensory impaired (deaf, dumb, or blind), ensured they got medical care first, and then obtained informed consent from them to participate in the study. Then, they collected data and interviewed each participant for a minimum of 40 minutes using translated questionnaires. The tools used in this study were translated into the local language and back-translated to ensure the meaning was maintained. However, we faced the challenges of some older adults following instructions due to hearing or sight problems. Nonetheless, the research assistants overcame this challenge by giving more time to such older adults and speaking louder.\n---\nStudy variables and measures\n\nBasic information. This study collected basic sociodemographic information including gender, age, address (rural vs. urban), occupation (previous employment, formal vs. informal; currently inactive or active at the employment), marital status, level of education (none, primary, secondary, and post-secondary education), and type of housing they are currently staying in (public/government-owned, rental, personally owned)-these show the amount of autonomy one has on their property. In addition, we asked about chronic medical illnesses such as cancer, HIV, diabetes, hypertension, etc., and physical impairments such as blindness and being orthopedically handicapped.\nElder abuse. The Hwalek-Sengstock Elder Abuse Screening Test (HS-EAST) was used to screen for elder abuse severity [26]. The scale consists of 15 items and three conceptual categories. These three conceptual categories include \"overt violation of personal rights and direct abuse\" (items 4,9,10,11,15) and \"characteristics of elder that make him or her vulnerable to abuse\" (items 1, 3, and 6) and characteristics of potentially abusive situations (items 2, 5, 7, 8, 12, 13 and 14) [27]. It is scored by summing responses from each item (Yes/No), where 'Yes'scored one and 'No'-scored 0. Possible scores range from 0 to 15, where a higher score represents higher exposure to elder abuse. Based on previous studies, the HS-EAST had a cutoff of 3 and above with a sensitivity and specificity of 82.8% and 84.5%, respectively, for elder abuse; and a reported Cronbach alpha of 0.741 [28]. In this study, the Cronbach alpha was 0.78. The HS-EAST was translated into the local language and back-translated to ensure the meaning was maintained.\nIn addition to overall elderly abuse, different types of abuse experienced in the past six months (such as physical, emotional, and financial abuse) were assessed using questions adapted from the National Research Council on elder mistreatment monograph [19], S1 Table . Lastly, the older adults who preferred to report also gave information about the perpetrators.\nFunctional impairment. The level of functionality was tested using the Barthel Index (BI). The BI comprises ten items with varying weights [29]. The tool has two items assessing personal care (wash face, comb hair, shave, and clean teeth) and bathing evaluated with a 2-score scale (0 and 5 points); 6 items regarding feeding, getting onto and off the toilet, ascending and descending stairs, dressing, controlling bowels, and controlling bladder are evaluated with a 3-score t scale (0, 5, and 10 points); and two items regarding moving from wheelchair to bed and returning, and walking on a level surface are evaluated with a 4-score scale (0, 5, 10, and 15 points). The BI is a cumulative score calculated by summing each item's score. The BI scores are multiples of 5 with a range of 0 (completely dependent) to 100 (independent in basic activities of daily living (ADL)). Higher scores represent a higher degree of independence [29]. The score is categorized into five groups 0-20 = total dependency, 21-60 = severe dependency, 61-90 = moderate dependency, 91-99 = slight dependency, and 100 = complete independence. The Cronbach alpha for BI is 0.81 [29]. For this study, the Cronbach alpha was 0.88.\n---\nEthical considerations\n\nMbarara University of Science and Technology's research ethics committee approved the study (#05/11-,20), and administrative approval was obtained from the tertiary hospital director. Participants who agreed to participate in this study appended their signature or thumbprint (for those who could not read or write) on the consent form. The consent form translated to the local language (Runyankole) was read aloud to individuals who could not read and write and signed in the presence of a trusted witness (fluent in reading and writing) who countersigned. Participants diagnosed with severe neurocognitive impairment were referred to the psychiatry department for investigation and further management. All participants were interviewed in a private room away from their caretakers and other patients to maintain privacy. Counseling was provided to all participants who experienced any form of abuse by a trained study psychologist with experience in dealing with domestic violence. Individuals who experienced abuse were referred to the district probation office-an office responsible for managing cases of violence and abuse at the district level, for further management of the situation.\n---\nData analysis\n\nData were entered into an excel sheet and then exported to STATA 16.0 for analysis. Chisquare tests were performed to determine significant differences between individuals who experienced various forms of abuse and those who did not. For elder abuse severity based on the HS-EAST, we ran a t-test and ANOVA for the elderly abuse severity. Linear regression analysis determined the factors associated with elder abuse severity. A back stepwise multivariable linear regression was built after testing for collinearity to adjust for confounders. In addition, logistic regression was used to determine the factors associated with the different types of elderly abuse (neglect, physical, sexual, emotional, and financial abuse). The significant level was set at less than 5% for a 95% confidence interval.\n---\nResults\n\n\n---\nCharacteristics of the participants\n\nA total of 363 participants were included in this study, 57% (n = 208) females. The mean age was 67.08 (SD = 6.66), and most belonged to 60-69 years (69.0%). In addition, most participants came from a rural setting (71%, n = 259), 72.3% had a comorbid medical condition, and 44.4% had a physical impairment (Table 1).\n---\nElder abuse\n\nMean elder abuse severity was 6.0 (SD = 2.67) out of 15 at the HS-EAST, whereas 89.0% of the elderly had met the cutoff point 3 and above the HS-EAST. In addition, these participants experienced more characteristics of potentially abusive situations, mean of 2.99 (1.47; out of 7), followed by experiencing an overt violation of personal rights and direct abuse, 2.05 (1.41; out of 5).\nParticipants with physical impairment had higher elder abuse severity mean scores than those without physical impairments (6.56% vs. 5.90%, t = -3.71, p<0.001). Also, elders' who stayed in government-owned housing had higher elderly abuse severity mean scores (F = 4.15, p = 0.016), and those whose highest level of education was post-secondary had the lowest elder abuse severity score as per the education level (F = 5.03, p = 0.002) (Table 2).\nFactors associated with elder abuse severity. At bivariate analysis, the factors that increased elder abuse severity were: being divorced or separated, previously informally employed and currently still active, having a secondary level of education, staying in a government-owned house, and having a physical impairment. These factors were tested for collinearity and had Variance Inflation Factors (VIFs) below 3; the mean VIF was 1.14. They were included in the final model using backwards stepwise modeling. There was evidence of homoskedasticity with a Cook-Weisberg test p-value of 0.119. The final model could explain 9.7% of elderly abuse severity. At multiple variate analyses, having a secondary level of education statistically significantly increased elder abuse severity by 0.97, and having a physical impairment increased the elder abuse severity by 0.83 (Table 2).\n---\nPrevalence of different types of abuse\n\nThe prevalence of the different types of abuse was 86.3%, 49.0%, 46.8%, 22.2%, and 6.51, for neglect, emotional, financial, physical, and sexual abuse, respectively. About 30.4% of the participants experienced any two types of abuse (Fig 1), and a majority (66.6%, n = 243) experienced both physical mistreatment and neglect (Table 3).\nExperiencing physical mistreatment and emotional abuse had a low significant correlation (r 2 = 0.35). The elderly abuse severity significantly correlated negligibly with emotional abuse and physical mistreatment. However, the other combinations with significant correlations also had negligible correlations. For details, see Table 4.\nFactors associated with emotional abuse. Previously informally employed, currently retired, and unemployed participants were more emotionally abused (\u03c7 2 = 9.08, p = 0.028). Emotional abuse was less among individuals with a chronic medical condition than those with no chronic medical disease, and the difference was statistically significant (45.8% vs. 57.4%, \u03c7 2 = 3.93, p = 0.047). Moderately functionally dependent elders were more emotionally abused (65.2%) than those who were more independent (43.8% and 46.0% for a slight/little dependent and completely independent, respectively) or severely functionally dependent (36.4%) (\u03c7 2 = 9.35, p = 0.025) (S2 Table ).\nIn bivariate analysis (S3 Table ), the factors associated with the increase in the likelihood of experiencing emotional mistreatment were being moderately functional dependent, having to report the perpetrator to the police, or acknowledging the presence of a perpetrator. However, elders with chronic illnesses were less likely to experience emotional abuse. The factors were tested for collinearity; all had individual VIFs of less than three, and the mean VIF was 1.04. At multivariate logistic regression, the factors associated with experiencing emotional mistreatment were having reported having a perpetrator (aOR = 7.94, 95% CI: 3.71-17.06, p<0.001), having reported the perpetrator to the police (aOR = 3.33, 95% CI: 1.25-8.86, p = 0.016), and being moderately functionally dependent (aOR = 1.85, 95% CI: 1.01-3.78, p = 0.047) (Table 5). This final model had a specificity of 88.71%, a sensitivity of 41.90%, a positive predictive value (PPV) of 78.13%, a negative predictive value (NPV) of 61.34%, and was correctly classified as 65.75% of individuals who experienced emotional elder abuse. The goodness of fit p-value was 0.838 for the included four variables.\nFactors associated with physical abuse. The largest statistical difference was between individuals who reported perpetrators and those who did not, with those who reported experiencing more physical abuse than their counterparts (50.0% vs. 16.0%; \u03c7 2 = 26.8, p<0.001). Older adults aged 80 years and above experienced more physical abuse than younger (43.5% vs. 22.6% and 15.6% for 60-69 years and 70-79 years, respectively) (\u03c7 2 = 8.36, p = 0.015). All individuals who never married experienced physical abuse, whereas 21.7%, 22.4%, and 20.7% of individuals who were either cohabiting/married, divorced/separated, or widowed, respectively, experienced physical abuse (\u03c7 2 = 10.67, p = 0.014). Older adults having a chronic medical illness were statistically less physically abused (17.4%) compared to those with no chronic medical condition (34.6%) (\u03c7 2 = 12.6, p-value <0.001). Older adults with moderate functional dependence were the main culprit for physical abuse (37.7%) compared with individuals who were more functionally independent (13.6% and 19.8% for slightly dependent and completely independent, respectively) (\u03c7 2 = 13.05, p = 0.005) (S2 Table ).\nAt bivariate analysis, individuals with chronic medical illnesses were less likely to experience physical abuse (S3 Table). However, those above 80 years had reported the perpetrator to the police or acknowledged their presence, and being moderately functionally dependent increased the likelihood of experiencing physical mistreatment at bivariate analysis. These factors were tested for collinearity, and they all had VIFs below 3, with a mean VIF of 1.02, and consequently, they were included in the final model. The model had a sensitivity of 28.40%, specificity of 95.07%, a PPV of 62.16%, and NPV of 82.32%, and correctly classified 80.27% of experiencing physical mistreatment. The goodness-of-fit p-value was 0.639 and included five variables. In the multivariable logistic regression, the factors associated with experiencing abuse were having reported the presence of a perpetrator (aOR = 5.12, 95% CI: 2.79-9.44, p<0.001) and an elderly who has moderate functional dependence (aOR = 2.44, 95% CI: 1.13-4.08, p = 0.019). However, chronic physical medical conditions reduced the likelihood of experiencing physical mistreatment (aOR = 0.40, 95% CI: 0.23-0.71, p = 0.002) (Table 5).\n---\nFactors associated with neglect.\n\nThere was a statistical difference between neglect and functional dependence (\u03c7 2 = 18.78, p<0.001); neglect was more among elders with higher levels of dependence. Participants with physical impairment were statistically less neglected than those without physical impairment (79.0% vs. 92.1%, \u03c7 2 = 13.09, p<0.001). Elders who reported having a perpetrator (95.4%) were statistically neglected more compared to those who declined (84.3%) (\u03c7 2 = 5.71, p = 0.017) (S2 Table ).\nIn bivariate analysis (S3 Table ), neglect was associated with having the reported presence of a perpetrator and an increasing level of functional dependence; that is, those with moderate functional dependence have a higher likelihood of experiencing neglect than those with slight functional dependence. Staying in publicly owned/government housing and having a physical impairment significantly reduced the likelihood of experiencing neglect. These factors had a mean VIF of 1.01, and all their VIFs were below three at testing for collinearity. In multivariate analysis (Table 5), having physical impairment reduced the likelihood of neglect (aOR = 0.27, 95% CI: 0.14-0.52, p<0.001). However, individuals who reported the presence of a perpetrator and those with slight or moderate functional dependence were likely to experience neglect. The likelihood of experiencing neglect was highest in individuals with moderate functional dependence (aOR = 9.33, 95% CI: 2.15-40.42, p = 0.003). This final model had a sensitivity of 100%, specificity of 0%, PPV of 85.88%, and correctly classified 85.88% of neglect experienced by older adults.\nFactors associated with financial abuse. There was a statistical difference between financial abuse and the following variables: age, gender, dwelling area, employment status, level of education, presence of chronic illness, presence of physical impairment, history of a police report, and functional dependence. Participants aged 80 and above were statistically more financially abused (91.3%) compared with younger participants (43.2% and 45.6% for 60-69 and 70-79, respectively) (\u03c7 2 = 19.6 p<0.001). Male participants suffered more financial abuse than females (54.8% vs. 40.9%; \u03c7 2 = 6.95, p = 0.008). Urban dwellers statistically suffered more elderly abuse than rural dwellers (55.7% vs. 43.2%; \u03c7 2 = 4.66, p = 0.031). For details, see S2 Table. In bivariate analysis (S3 Table ), having experienced financial abuse was associated with staying in an urban setting, male gender, age above 80 years, level of education (primary and tertiary), and severe functional dependence. The likelihood was reduced among individuals with chronic physical medical conditions, those with physical impairment, and those who reported their perpetrators to the police. The factors significant at bivariate analysis were tested   5).\n---\nFactors associated with sexual abuse.\n\nThere was no gender difference between the level of sexual abuse experienced (S2 Table ). No factor was associated with sexual abuse (S3 Table ).\n---\nDiscussion\n\nIn this survey of older adults aged 60 years and above attending an outpatient department of a tertiary hospital in Southwestern Uganda, the prevalence of elder abuse (cutoff of 3 out of 15 at the HS-EAST) was high (89.0%). The factors associated with increased elder abuse severity were having a secondary level of education and physical impairment. However, the prevalence of the various types of elder abuse was highest with neglect (86.0%) and lowest with sexual abuse (6.8%), and about 30.4% of the abused experienced at least two forms of abuse, especially physical mistreatment and neglect. Moderate to severe functional dependence was associated with all forms of abuse apart from sexual abuse. Individuals who reported a perpetrator were likely to experience neglect, emotional abuse, and physical mistreatment. However, those who reported their perpetrators to the police had a higher likelihood of experiencing emotional abuse but were less likely to experience financial abuse. Emotional abuse was also associated with age above 80 years and attaining education (primary and secondary). Physical impairment and chronic medical conditions reduced the likelihood of experiencing neglect and financial abuse, and physical abuse, respectively.\n---\nElder abuse\n\nThe prevalence of elderly abuse (89.0%) was much higher than that estimated from India (50.2%) [30], the USA (35.0%) [31], China (36.2%) [32,33], and Iran (38.5%) [34] that used a similar tool-the HS-EAST. In addition it is higher than that estimated in other rural areas [14]. The high prevalence in this study may be because we recruited a community sample of older persons attending a hospital outpatient clinic and some of the victims visited the hospital  to treat complications of elder abuse such as injuries, illness, and mental health challenges [35]. In addition, the current study was in a country with limited laws, reporting, and followup of elderly abuse, on top of a cultural system diluted to the extent of the young or their children disrespecting the elderly [5].\nIn this study, having a secondary level of education (more than eight years of study in Uganda) increased the elder abuse severity. This is contradictory to other study findings, which report that a history of education for more than eight years reduced elder abuse severity [36]. Older adults with higher education levels can claim their rights, easily report abuse to responsible bodies, and even get more social respect [36]. However, recently older adults in Uganda have been disrespected by the young due to rapid cultural change [5], which puts many older adults at risk of abuse. In addition, the country is a low-income country with challenges such as unemployment, poverty, and a high fertility rate-many of the younger individuals financially exploit the weak and vulnerable elderly, including the educated. In this country, the educated are more likely to be financially stable, having accumulated some assets over time or are receiving a pension, and therefore become prey to the younger generation (their dependents) for their assets and money [5]. No wonder these older adults with a higher level of education were at a higher likelihood of being financially abused in our study. Furthermore, older adults with higher levels of education are most likely to have had formal jobs in the city and lost contact with extended family ties. Therefore, they have poor relationships with their relatives when they return to the villages after retirement, yet these ties could have been a buffer from abuse. Also, older adults with physical impairment had more severe elder abuse in this study, a finding similar to that reported in the previous studies [17,32]. This finding is not surprising as people with disabilities depend on others, especially in fulfilling their daily activities [37]. In addition, older adults with physical impairment may not defend themselves or report their perpetrators to the police due to their inability to fight for themselves and fear of being neglected.\n---\nFinancial abuse\n\nAlmost half of the participants experienced financial abuse, which was higher than reported in other African countries, including Nigeria (13.1%) [38], Egypt (27%) [39], and South Africa (24.4%) [17]. Despite all these African countries belonging to the LMICs, Uganda has one of the lowest Gross Domestic Product [40]; thus, its citizens experience a higher poverty level, leading them to abuse financially stable or dependent older adults. Financially stable or dependent older adults become a primary target by some poor youth due to their vulnerabilities due to old age. Financial abuse in this study was associated with increasing age above 80 years because individuals above 80 years are more vulnerable to abuse. After all, they are at higher risk of neurocognitive disorders and functional dependence, which makes them unable to manage their finances or forget to request financial assistance [6]. They may also be more likely to make poor financial decisions and fall victim to fraudulent deals [41]. An increase in functional dependency with age is a major factor that increases the likelihood of almost all types of abuse except sexual abuse due to increased vulnerability and need for help in most of their daily activities. An elderly with a high level of functional dependence causes significant care burden stress, leading to an increased likelihood of abuse [42].\nThe likelihood of being financially abused was reduced when the older adult reported the known victim to the police. Reporting the perpetrators to the police, a method suggested by the CDC to reduce abuse, was effective in this study [4]. A perpetrator reported to the police may be punished for their acts, for instance, by being taken away from the community or the victims' premises, thus, reducing further abuse. In addition, reporting a perpetrator to the police will make the community aware of the perpetrating persons, which will protect the potential victims. This community support may provide emotional and social support, thus reducing the likelihood of experiencing emotional abuse and other forms of elder abuse. In Uganda, the community considers abuse of an older adult taboo [5]. Disrespecting or abusing an older adult with a disability is considered a worse offense by the community. Due to this respect and possible fear of the community reaction, many perpetrators do not abuse older adults with physical disabilities. The respect they give individuals with a disability may be responsible for them being less likely to experience financial abuse and neglect since every community member expects one to take good care of them.\n---\nPhysical abuse and mistreatment\n\nThe prevalence of physical abuse (22.2%) among the older adults in this study was higher than that reported by most countries in a systematic review of 20 studies (0.2%-4.9%) [43]. However, studies included in the review by Pillemer et al. (2016) measured physical abuse using the conflict tactic scale that predominantly looks at intimate partner violence, excluding abuse from other possible perpetrators such as children, neighbors, and others; considered in our study. Thus, a large difference between the reported prevalence rates. On the other hand, the current study's prevalence of physical abuse was less than that reported by a Nigerian study (47.0%) [44]. The difference may be due to cultural belief differences between the two countries and associating older adults with witchcraft due to their appearance (wrinkled skin, gnarled hands, and yellow eyes) [45]. In addition, the Nigerian study classified physical neglect as part of physical abuse, which could have led to a higher prevalence since neglect is a commonly reported type of elder abuse [46].\nLike other types of elder abuse, experiencing physical abuse was associated with increased functional dependence. The individuals who reported the presence of perpetrators had a higher likelihood of physical abuse; this may be due to the demeaning nature of physical abuse and the desire to get help from others (i.e., reporting is an act of asking for help). However, individuals with chronic physical medical conditions were less likely to experience physical abuse since they looked too sick. Their perpetrators may not abuse them due to fear of them possibly killing them. It contradicted other studies where chronic diseases were associated with physical abuse [47].\n---\nEmotional abuse\n\nNearly half of all older adults in this study experienced emotional abuse, comparable to findings in South Africa, Nigeria, and Egypt [17,39,48]. These findings could be because there are ways of addressing older adults in traditional African society, and a deviation from this norm is easily noticeable and considered disrespectful [38]. However, the prevalence of emotional abuse was much higher than that reported by Cadmus and Owoaje (2012) in Nigeria among women without psychiatric illnesses [38]. Despite psychiatric illnesses such as depression being associated with or a complication of emotional abuse [21]. The psychological suffering associated with emotional abuse was associated with reporting the presence of a perpetrator to the research team and police in an attempt to seek emotional relief.\n---\nNeglect\n\nAs reported by other researchers in Africa [30,39], neglect was the most dominant type of abuse. This high prevalence could be attributed to the loss of caregivers due to the HIV scourge in Africa or the migration of family members to urban areas compounded by the loss of extended family ties leaving the elders to fend for themselves and hence feel neglected [49]. Poverty is another factor that could explain high-rate neglect, as caregivers cannot take care of their own families, let alone older adults [50]. In addition, this study recruited older adults seeking care in hospitals with a high probability of having chronic illnesses, which put them at an increased likelihood of neglect due to limitations in activities of daily living and their associated high financial demands from the caregivers [20,50,51]. On the other hand, participants with physical impairment had a reduced likelihood of being neglected. Since a physical impairment limits an older adult's ability to manage daily life activities, putting them at risk of self-neglect [52,53], this vulnerability makes caregivers present. The caregivers take care of them since not caring for the physically impaired is not culturally accepted [5].\n---\nSexual abuse\n\nSexual abuse was the least prevalent type, similar to other studies [15,17,38,46,54,55]. In the current study, there was no factor associated with sexual abuse. However, studies in South Africa and Canada have reported that women are at a higher risk of sexual abuse than males [17,55]. In Africa, sexual abuse of older persons seldom occurs, and when it occurs, it may be a ritual related to getting rich or getting special spiritual powers [56,57]. In addition, sexual abuse of older adult women is not expected as ageist perceptions produce a taboo around considering sexual relations with older people [58]. Furthermore, older persons who are sexually abused may not report sexual abuse due to feelings of shame [59].\n---\nLimitations\n\nFirst, the study was cross-sectional in design, so the causality between elder abuse and its predictors cannot be determined. Second, the study involved older persons attending a hospital outpatient for the management of their illnesses; therefore, elders at higher risk of abuse were sampled, leading to overestimation the prevalence rates. Second, elders with severe cognitive impairment were not sampled and yet are at higher risk of abuse. This sample may have difficulty with the recall of abuse and may end up inflating the number of cases of abuse reported, thus, excluded in the present study. However, we recommend future studies use prospective methodology involving individuals with severe cognitive impairment and examine the participants routinely for abuse; to enable accurate estimation of abuse in this population. In addition, neurocognitive scores were not considered in the analysis despite decreased scores being associated with abuse even without severe impairment. Thirdly, some variables, such as family income or socioeconomic factors, were not explored in detail despite their role in elderly abuse based on previous studies. Older people with lower socioeconomic independence were reported to have more abuse [60]. We recommend future studies add variables to explore the socioeconomic status of the elder, such as family income, saving used by the older person, income sources, and the number of dependents, among others. Lastly, the tool used to determine abuse was not adopted in Uganda; therefore, it could have been estimated as higher than the actual prevalence.\n---\nConclusions\n\nThis study highlights a high prevalence of elder abuse in Uganda, with the most common forms of abuse being neglect, emotional abuse, and financial abuse. In addition, our study showed that having at least a secondary level of education or physical impairment was associated with elder abuse. Older persons with physical impairment were less likely to be neglected, and those who reported perpetrators were more likely to be abused emotionally. Older persons above 80 years were more likely to be abused financially, but those who reported perpetrators to the police or had a physical disability were less likely to face financial abuse. Furthermore, older persons with increased functional dependence were more likely to be abused physically, but those with chronic illness were less likely to be abused. This study, therefore, is evidence of the need to implement more effective means of raising awareness about elder abuse. There is also a need to design interventions to prevent vice in these vulnerable groups, such as routine screening for abuse at health facilities where older persons have access to care. However, this may be difficult due to few personnel in health centers to screen older adults in addition to treating their health conditions in rural areas. Policies that address older person abuse should be put in place. Lastly, we recommend further studies on elder abuse in this setting, especially on its psychological impact and among the neurocognitively impaired, since these aspects were not exploited in this study.",
        "\n\nShaw University (Shaw), the oldest historically black college in the southern USA, was founded in 1865 and is a private, coeducational, liberal arts institution. It currently enrolls approximately 2,800 undergraduate and graduate students. Since 2000, Shaw has moved from a campus with a limited number of external grant awards by a few investigators who had little assistance with grant preparation to a fully staffed Office of Research and Sponsored Programs (ORSP) with grant funds totaling over $20 million in 2006. The increase in funded awards over a relatively short period of time prompted Shaw to review the need for establishing a mechanism on campus to review and approve research involving human subjects.\nShaw opted to form an IRB to review not only federally sponsored research, but all research conducted on campus. The goal was supported by a grant obtained through a partnership formed by researchers at Shaw and the University of North Carolina at Chapel Hill (UNC-CH). A Center of Excellence in Partnerships for Community Outreach, Research on Health Disparities and Training (Project EXPORT) was funded in 2002 by the National Institutes of Health (NIH) National Center on Minority Health and Health Disparities (NCMHD). The aims of the Project EXPORT grant were multifaceted and included developing a partnership that would rely on the extensive research experience of UNC-CH faculty to mentor and develop a new pool of health disparities researchers at Shaw. A research infrastructure core was established through the Project EXPORT grant at Shaw, and was given the tasks of enhancing the university's existing research infrastructure by upgrading its grants management office and creating an independently functioning IRB by drawing on a comprehensive training plan by our UNC-CH partners. The following description of the activities involved in developing the IRB may benefit other institutions of higher learning that wish to develop IRBs of their own.\n---\nEstablishing the Partnership\n\nFor Shaw, establishing an IRB required the guidance of well-trained and experienced personnel working in the field of human research ethics. The partnership with UNC-CH provided the opportunity. Since 2000, Shaw and UNC-CH have collaborated on multiple small contracts, research projects, and academic endeavors that helped to establish a sense of trust and mutual understanding between the two institutions (Carey et al., 2005). With the added advantage of geographic proximity (approximately 30 miles), Shaw's investigators and staff could interact face-to-face as needed with researchers from UNC-CH.\nThere were multiple meetings between the Shaw IRB development staff and the Director of the UNC-CH Office of Human Research Ethics (OHRE) early in the development process. Early discussions concerned government regulations of human research, other documents relevant to human research ethics, and information relevant to establishing an IRB. The Director of UNC-CH OHRE was available for questions on a continuous basis.\nThe President of Shaw entered into a Federalwide Assurance (FWA) for the Protection of Human Subjects for Domestic (U.S.) Institutions. This assurance indicates that all research at Shaw will be held to U.S. federal standards for the protection of human subjects. Next, Shaw and UNC-CH executed an IRB Authorization Agreement, i.e., an agreement that allows one institution to rely on another institution for review, approval, and oversight of research. In this case, UNC-CH agreed to provide review of all Shaw's research projects until Shaw was able to establish its own IRB. Shaw's investigators were given the opportunity to submit IRB applications to any of the eight UNC-CH IRBs. The agreement also laid out a plan for transition after Shaw's IRB members were well-trained and prepared to review research independently.\nAccording to a June 2000 requirement issued by NIH, all investigators submitting NIH grant applications or receiving awards for research involving human subjects must receive education on the protection of human research participants (National Institutes of Health, 2000). Many institutions, including UNC-CH, broadened these requirements to all human subjects, regardless of funding. To meet its initial training needs, UNC-CH relied on the Collaborative IRB Training Initiative (CITI) hosted online by the University of Miami (https://www.citiprogram.org/Default.asp?). This web-based course includes a series of comprehensive modules on the protection of human research subjects. Shaw's investigators were allowed access to CITI through the partnership established with UNC-CH.\nShaw's preparation also included regular meetings with UNC-CH staff to observe their entire review and administrative procedures. Shaw's staff attended UNC-CH administrative meetings as well as several IRB committee meetings at UNC-CH to observe experienced IRBs in practice. Also valuable was the interaction with IRB administrators and chairpersons to discuss specific IRB applications by Shaw investigators.\n---\nForming the Committee\n\nOne staff member at Shaw was selected as the interim IRB administrator to manage the dayto-day operation of the OHSP, ensure that all Shaw faculty and staff involved in research completed the human subjects protection training certification, present IRB procedural workshops for Shaw employees, and recruit IRB committee members. There were many challenges inherent in the task of building this infrastructure from the ground up. The IRB administrator introduced the development of the IRB at a Shaw campus-wide faculty meeting at the start of the fall 2003 semester. Subsequent to the meeting, three e-mails were sent to all faculty and staff over a one-month period describing the IRB, the time commitment necessary to participate, and the role of the IRB committee members. The most challenging facet in constructing the IRB committee was recruiting IRB members because of their heavy teaching load; release time had to be approved by each member's department chairperson so their schedules would permit for them to participate in meetings without conflicting with their classes. Interested individuals agreed to attend an informational session. It soon became apparent that the IRB needed to have the status of an \"official\" campus committee (a committee approved by the Executive Vice President of Shaw) so that serving on the IRB would fulfill the faculty's contract requirement of serving on at least one such committee. The timeline for implementation of a fully functioning IRB started September 2003 and ended November 2005.\nNine IRB members representing a range of backgrounds, expertise, and cultural sensitivities were recruited from Shaw's faculty and staff to satisfy regulatory requirements regarding varying backgrounds to support complete and adequate review of research activities commonly conducted at Shaw. The initial members of the committee ranged from professors in Sociology, English, Accounting, and Business to the Assistant Dean of the graduate education program. In addition to these University employees, an unaffiliated committee member was recruited, a recently retired assistant principal from a local high school.\nRecruiting an IRB Chair was a challenge as well. The Chair needed to be a respected member of the campus community who had significant leadership qualities to guide and defend the decisions made by the IRB. Between the time when Shaw's IRB became an independent entity in 2005 and the identification of the IRB Chair, the IRB Administrator served as interim chair and passed the administrative duties to another IRB staff member. In order to attract a strong person to the position, 25% release time from other academic duties was requested for that individual. Despite this selling point, none of the initial IRB members accepted the offer; consequently, OHSP staff contacted Shaw's Faculty Development Coordinator for assistance in identifying a Chair. The Coordinator identified a faculty member who had prior experience on an IRB and who was enthusiastic about the position. The faculty member met with OHSP staff on several occasions, observed some IRB meetings, and assumed the role of Chair in the fall of 2006.\n---\nTraining the Committee\n\nThe Shaw University IRB Administrator and committee members needed to be educated in the ethical principles, regulations, and guidelines that govern human subjects research. These educational materials included international documents (the Nuremburg Code and the Declaration of Helsinki), national (U.S.) documents (Belmont Report, 45 Code of Federal Regulations 46), and educational and reference materials for IRB decision-making and management. The DHHS OHRP and the UNC-CH OHRE websites were reviewed. Based on these printed materials and websites, policies and procedures to guide the day-to-day activities of the Shaw IRB were written. In addition, the IRB Administrator attended the national conference sponsored by Public Responsibility in Medicine and Research (PRIM&R), a national organization committed to advancing ethical standards in research.\nThe Shaw IRB committee members continued their training by attending a summer course on Responsible Conduct of Research in 2004 sponsored by the General Clinical Research Center at UNC-CH. Shaw IRB members rotated through the IRBs at UNC-CH to observe convened meetings of active experienced committees. Some also attended IRB meetings at another nearby major institution. They attended events hosted by the North Carolina Consortium for Protection of Human Subjects, which was developed from an NIH program enhancement grant to UNC-CH. In 2005, some of the Shaw IRB staff and committee members attended two of the Consortium's Regional Conferences held at two HBCUs across the state of North Carolina.\nAs an additional means of continuing education, articles from the journal IRB: Ethics and Human Research were distributed to IRB members at regular IRB meetings.\n---\nAdministrative Structure\n\nAt the outset of Project EXPORT, it was recognized that a stand-alone IRB committee would not function efficiently without a supporting office to provide information, field questions, document IRB meeting minutes, distribute letters from the IRB to investigators, handle the intake and processing of applications, and other organizational matters. Since such an office did not exist at Shaw, Project EXPORT developed a document at Shaw titled, \"Establishment of the Office of Human Subjects Protection (OHSP),\" and disseminated it to Shaw's administration, outlining the importance of the Federalwide Assurance, the IRB Authorization Agreement between UNC-CH and Shaw, educating the Shaw community on the necessity of protecting human research subjects, and the process for submitting proposals for IRB review.\nIn 2004, the Shaw administration moved to approve its Office of Human Subjects Protection (OHSP). A temporary plan needed to be developed for investigators who required IRB reviews while IRB members were training and before Shaw's IRB became an independent entity. A brochure was created, \"HBCU IRB Submission Instructions for Investigators,\" which described the interim process for submitting IRB applications to UNC-CH's IRBs through the Shaw OHSP and was distributed to all faculty members. Shaw's Office of Research and Sponsored Programs (ORSP) was utilized to further inform investigators of the new processes and to inform the OHSP when new grant awards had been issued that would have human subject involvement.\nPrior to initiating review of research from Shaw's investigators, the OHSP developed standard operating procedures (SOPs) essential to the effective management of the IRB, modeled after parallel procedures at UNC-CH. A Research Investigator's Guide to the IRB, an OHSP Administrative Procedures manual, and a website (http://www.shawu.edu/IRB/new/Review%20Process.html) were developed containing all the essential information for investigators. The goal was to make the OHSP and IRB two entities that everyone on campus-faculty, staff, and students-could access with ease. Efforts were made to identify all protocols and IRB applications from previous and current studies to get an accounting of activity on campus that required IRB review. OHSP staff also wanted to ensure that all investigators on campus and their key personnel were trained in human research protections. One of the first databases developed in the OHSP tracked the names of individuals and the type, name, and date of the training they received. OHSP developed an additional database to track and store all the information. The purpose was to build a history of each application and the resulting research. In addition, the OHSP had the task of developing job descriptions for key players, such as the IRB chairperson, IRB administrator, and IRB committee member. Other essential tasks in the new office included creating files for each application, and developing forms required for submission of an IRB application.\nAccording to Gunsalus (1993), \"The single most influential component in an institutional culture of research integrity is institutional leadership committed to ethical conduct.\" She also asserts that the most effective plan is to create policy that fits the institution in question. This institutional support or leadership should be apparent by its commitment to (1) creating an ethics infrastructure, (2) assuring adequate resources, (3) supporting IRB members, and (4) educating all involved (Sugarman, 2000). Shaw's commitment to research integrity was expressed in a number of ways, including the following: On March 24, 2004, the president of Shaw formally announced that the University would accept the responsibility of assuring that all activities related to federally and non-federally supported human subject research would comply with the terms of the Federalwide Assurance (FWA) for the Protection of Human Subjects for Domestic (U.S.) Institutions. In addition, the UNC-CH Office of Human Research Ethics and the Co-Director for the EXPORT Partnership IRB section and his staff provided ongoing consultation for IRB development. The Center Director, Co-Director, and Core Directors were responsible for scientific leadership, effective communication, making the IRB visible to Shaw faculty and students, monitoring partnership outcomes, ensuring oversight in ethical conduct, monitoring all budgetary matters, and maintaining records and reports. The leadership team met monthly to discuss progress and steps to move forward with the development of the IRB. Support for the IRB was displayed by the Shaw administration's provision of release time for the chairperson and faculty credit for participating in a campus committee as IRB members. The OHSP also consulted with the university attorney about liability insurance for all IRB members, and the IRB was added to a Shaw insurance policy. It was also important to establish this contact with Shaw's counsel in the event that legal issues arose in the future. Shaw's commitment to educate the campus community in the ethics of research has been displayed in numerous ways. Early in the process, OHSP staff attended a university-wide faculty meeting and presented information pertaining to the IRB and the new procedures on campus; they distributed a letter explaining the new direction, brochures with instructions for submitting IRB applications, and a list of seminars geared toward training. Currently, the OHSP has a mandatory educational requirement for investigators prior to the commencement of any research involving humans. Investigators and key research personnel must complete on-line training in the ethics of research involving human subjects every two years. The OHSP hosted a campus community discussion based on the video, \"We All Have Our Reasons: Community Perceptions of HIV Vaccine Research.\" The video examined the perceptions of government-sponsored research from the perspective of three different communities. A discussion followed in which participants, including IRB members, were allowed the opportunity to discuss their opinions of research in general and the varying opinions of people who have participated in research or have heard about research taking place in their communities, as well as the factors involved in making the decision to participate in research.\n---\nChallenges and Conclusions\n\nThe Office of Human Subjects Protection and IRB were developed to supplement the Shaw research infrastructure by mentoring and training faculty and staff who would become the IRB committee. Shaw investigators and faculty members have embraced health disparities and minority health research, conducting pilot-research studies in HIV/AIDS, diabetes, cardiovascular disease, and quality-of-care issues in African Americans. UNC-CH mentors have continued to provide scientific leadership and direction to the Shaw IRB as well as the researchers who utilize the IRB for protocol approval. The OHSP and IRB have been a catalyst for heightened interest in expanded research activity at Shaw with more faculty members considering involvement in research activities. In 2005, the Shaw IRB registered with the federal Office for Human Research Protections (OHRP), recognizing its readiness to function independently. The FWA was initially linked to all of the UNC-CH IRBs but is now able to cite Shaw's own IRB. Since Shaw's IRB registration as an independent entity, eighty-six IRB applications have been submitted to the Office of Human Subjects Protection and the amount of funding for health research has greatly increased from $1.1 million in 2005 (12 grants submitted) to $3.9 million in 2008 (30 grants submitted).\nShaw has a low-volume IRB compared to the IRBs at UNC-CH in terms of the number of IRB protocols submitted and reviewed each year. However, it requires considerable resources to establish and sustain an effective independent IRB, to maintain federal and university compliance, and to protect human subjects. These resources include the following: training and education (conferences, CITI on-line course, newsletters), time commitments of IRB faculty and staff (IRB administrator, IRB Chair, committee members), and supplies (travel, paper, phones, photocopying). It clearly would be more cost-effective to continue the IRB Authorization Agreement with UNC-CH, however, a strict cost-benefit analysis would ignore the intangible benefits of an independent IRB at a minority-serving institution such as Shaw. The benefits include the Shaw community being viewed as having a decisive voice in the research process, the increased involvement in the research process by principal investigators and the IRB committee members, and the opportunity to train students to participate in research. When these intangible benefits are taken into account, it is likely that many teaching institutions seeking to develop research programs would opt to develop their own IRBs.\nOne of the greatest challenges in the process of developing Shaw University's IRB and human research protection program was the general lack of investigator experience with completing IRB applications and navigating the review process. The University's move toward developing new research investigators coincided with the development of the IRB. Many new investigators were not familiar with the entire process of developing, submitting, and responding to requests from the IRB. This inexperience, coupled with the new investigators' substantial teaching responsibilities and other campus commitments, made it a challenge to prepare adequate IRB applications. Initially, when IRB applications were under review by UNC-CH, Shaw OHSP staff had numerous contacts with research investigators. There were many follow-up e-mails and phone calls to investigators to ensure that forms were filled out satisfactorily. Investigators also received assistance with amendment preparation; review of individual protocols; consent form preparation; submission deadlines; and calls to UNC-CH IRBs to clarify questions.\nShaw's research infrastructure developed at a fast pace with the Project EXPORT grant serving as the catalyst. The infrastructure now includes an IRB; the Center for Biostatistics and Data Management; the Center for Survey Research; the Institute for Health, Social, and Community Research; a health sciences library; and a research building. Shaw and UNC-CH have collaborated on additional research endeavors, such as the Shaw UNC-CH Center for Prostate Cancer Research where a Shaw faculty member served as the primary investigator. The universities have been awarded a second Project EXPORT grant that focuses on primary research and designates primary roles for Shaw's Center for Biostatistics and Data Management and Center for Survey Research.\n---\nBest Practices\n\nTo successfully establish a Human Research Protection Program and an IRB at a university similar to Shaw, an important first step would be to conduct a cost analysis to determine the overall cost of operating an IRB and whether it is financially feasible for the institution to support. These costs may include the following: (1) personnel; (2) training and education;\n(3) equipment and supplies; and (4) space. If it is estimated that the resources are available to establish and sustain an IRB, the next recommended step would be to partner with an experienced major research institution for guidance and mentorship. An IRB Authorization Agreement with the partnering research institution allows all protocols submitted by the developing university's principal investigators to be reviewed by the mentoring institution until the developing institution's IRB is in operation. A partnership with the research institution also allows the developing university's IRB staff to observe and learn the research institution's procedures, including the administration of its IRB, and provides an opportunity for the members of the developing university's team to observe actual IRB meetings at the research institution. Another important component in the development of an IRB is garnering buy-in from the university president, administration, department chairpersons, faculty, and staff. Educating them about the significance of human research oversight is vital to gaining their cooperation as the IRB moves from the stage of development to functionality. At universities similar to Shaw where there is limited research infrastructure and a lack of investigator experience in completing IRB applications and understanding of the sensitive nature of human subjects protection, there must be an effort to educate investigators.\n---\nResearch Agenda\n\nOnce an academic institution has developed an IRB, a campus-wide evaluation is appropriate for evaluating the various facets of its operation from the perspective of its staff and users. Receiving feedback from investigators and students about their experiences, satisfactions, and dissatisfactions with the IRB allows the institution's research administration to learn what works and what needs to be improved. Such evaluation could also assess the change in research done by faculty, staff, and students and the change in grant funding.\n---\nEducational Implications\n\nThe purpose of this article is to detail the steps taken by Shaw University to develop an effective infrastructure for protecting human subjects of research. The particular educational materials and experiences that enabled Shaw to achieve its goal may differ somewhat from those appropriate to other developing institutions, particularly those in other countries. However, the general educational needs of other developing institutions would be similar and an appropriate parallel set of materials and experiences might be modeled on those described herein.\nresearch interests focus on a spectrum of population-based investigations of prostate cancer and ethnic and racial health disparities.",
        "Introduction\n\nSix figures of debt is a scary but self-inflicted reality for many law school graduates these days. 1 Mainstream media has chronicled the growing dissonance among the country's young lawyers, showing sympathy for struggling graduates as they attempt to pay back their loans. 2 Tuition at American law schools has skyrocketed over the last twenty years and nothing so far suggests this trend will change. 3 The recent recession and corresponding reduction of traditionally high-paying, entry-level jobs has exacerbated the distance between the cost of a U.S. legal education and expected earnings as a lawyer. 4 This may be a good thing-if it leads to change. 5 This Article takes the approach that law school is an investment and that prospective law students are best viewed as potential investors in their legal education. As such, they both demand and deserve access to important information about the relative risks of attending each law school. As Justice Brandeis said in his historical call for transparency in the banking profession, disclosure is necessary to \"aid the investor in judging the safety of the investment.\" 6 Increased public scrutiny may stir reform in legal education to a degree that was not conceivable when the job market was more stable top to bottom. At the very least, the current market could cause prospective law students to more carefully assess the risks associated with getting a law degree in today's market, rather than taking a degree's inherent value as a foregone conclusion.\nRather than focus on the current trend of devaluing the worth of a legal education, this Article focuses on a systemic problem that affects prospective law students (\"prospectives\").\nHow can anybody accurately measure the value of a law degree or understand actual job opportunities when law schools have little incentive to exceed the reporting standard required by the American Bar Association (\"ABA\")? Although the case for reform may be stronger in today's current market, these issues date back well before the current recession. Without more attention and some meaningful action, these issues are likely to continue well into the future.\nAccording to FinAid.org, a graduate should make $138,000 annually to repay $100,000 without enduring financial hardship, or $92,000 annually to repay the debt with financial difficulty. 7 Law school graduates are not entitled to high paying jobs to repay this debt, but jobs that allow repayment of such high debt are unavailable to the vast majority of newly minted law 6 LOUIS D. BRANDEIS, OTHER PEOPLE'S MONEY AND HOW THE BANKERS USE IT 102 (Melvin I. Urofsky, ed., Bedford Books of St. Martin's Press 1995) (1914) available at http://www.law.louisville.edu/library/collections/brandeis/node/196. 7 Loan Calculator, http://www.finaid.org/calculators/loanpayments.phtml (last visited Nov. 10, 2009). We calculated these figures using a loan balance of $100,000, an interest rate of 6.8%, no loan fees, and a 10 year loan term. If we change the loan term to 30 years, the borrower needs to make $78,000 annually to repay without financial hardship and $52,000 annually to repay with financial difficulty. Federal Stafford loans have a fixed interest rate of 6.8%, but are limited to $20,500 per year. Student Loans, http://www.finaid.org/loans/studentloan.phtml (last visited Nov. 10,  2009). Federal PLUS loans are available for additional needs, but bear a fixed rate of 8.5%. Id.  school graduates. 8 This is true regardless of the economic climate, and loan repayment begins 6 months after graduation. 9 Each year, nearly 50,000 law students begin investing in their legal education 10 expecting to derive value from both the experience and the degree itself. 11 It is under this assumption that many prospective law students finance their degrees and forego other opportunities.\nNevertheless, thousands of these students will graduate in three years bearing massive debt. 12 To varying degrees, prospectives consider assorted factors to select which law school, if any, to attend. Prospectives ask themselves questions during this process that fall into two nonexclusive categories. In the long and short term, (1) What do I give up by going to X law school? and (2) What do I get by going to law school? Everything matters, from employment opportunities and prestige, to location and feelings of fit. 13 Prospectives consider not just the positive cost of attendance, 14 but also the opportunity costs and savings of forgone alternatives. 15 Career objectives and salary goals certainly vary among prospectives, but job opportunities are usually the primary motivator in deciding where to go to school. 16 The decision process, in other words, is personal. During this process a prospective's internal cost calculus and penchant for risk inform the measurement and significance of each selection factor.\nProspectives actively and sensibly seek information to assess their factors. 17 Without meaningful information, factors may go unanalyzed, under-analyzed, or wrongly analyzed. It can be extremely difficult to determine how one school's offerings compare to another; and comparing every factor across schools is both time-consuming and costly. 18 Consequently, prospectives often look to tools to facilitate comparisons. Most famously, the U.S. News & World Report (\"U.S. News\") provides composite law school rankings, as well as program specialty and judicial clerkship placement rankings. 19 But the U.S. News rankings are 13 This list is not exhaustive. Other \"selection factors\" include degree programs and specialties, school resources, education quality, faculty quality, student-body quality, cost, reputation among friends, and alternative careers. 14 This includes tuition, fees, books, living expenses, travel, mandatory insurance, etc. 15 Herwig J. Schlunk, Mamas Don't Let Your Babies Group Up To Be\u2026Lawyers (Vanderbilt Law and Economics Working Paper Group, Paper No. 09-29), available at http://papers.ssrn.com/sol3/papers.cfm?abstract_id=1497044. 16 See generally, e.g., Top Law Schools (\"TLS\"), http://www.top-law-schools.com/. 17 Id. 18 Richard Posner discusses rankings as cheap method that is appropriate for unimportant decisions. Richard A.\nPosner, Law School Rankings, 81 IND. L.J. 13, 13 (2006). He expects rational students to invest the time researching school characteristics, rather than relying on rankings. Id. Yet, he recognizes that there is a significant number of schools, so this task is difficult and rough measures to determine which schools to research further could be useful. Id. 19 U.S. News Article III Clerkship Rankings, http://grad-schools.usnews.rankingsandreviews.com/best-graduateschools/top-law-schools/article_iii_clerks; U.S. News Law School Rankings, http://gradschools.usnews.rankingsandreviews.com/best-graduate-schools/top-law-schools/rankings; U.S. News Specialty Rankings, U.S. News & World Report Article III Speciality Rankings, http://grad-not the only tools available. Faculty quality 20 and student body 21 rankings each aim to compare law school offerings. While these tools drive down transaction costs for prospectives seeking to acquire and explain information, they tend not to function dispositively, leaving room for personalized calibration. Calibration appears particularly necessary in light of the many criticisms facing law school rankings like the U.S. News rankings. 22 This Article concerns the tools prospectives use to answer questions about employment outcomes immediately 23 following graduation (\"post-graduation outcomes\" 24 ). We argue that these tools inadequately serve prospective students striving to take a detailed, holistic look at the diverse employment opportunities at different law schools. While these outcomes are only a single factor in the prospective's analysis, and other factors may lack adequate information, 25 the significance of post-graduation outcomes and ease of measurement make it a good place to start. schools.usnews.rankingsandreviews.com/best-graduate-schools/top-law-schools (index that includes all specialty rankings). 20 Brian Leiter's Faculty Quality Rankings, http://www.leiterrankings.com/faculty/index.shtml (last visited Feb. 2, 2010). 21 Professor Leiter measures \"schools in terms of student quality as measured by the average of the 75th and 25th percentile LSAT scores.\" Brian Leiter's Ranking of Top 40 Law Schools by Student (Numerical) Quality 2009, http://www.leiterrankings.com/students/2009student_quality.shtml (last visited Nov. 1, 2009). A larger school (including full-time and part-time) with the same LSAT average is ranked higher. Id. If two schools are within 100 students in class size, and have the same average LSAT (or are .5 apart), a school with a GPA .1 higher is ranked higher. Id. Although Professor Leiter ranks the top 40, carrying this out across all law schools would be easy using the ABA Official Guide.  REV. 403 (1998). 23 This means outcomes at graduation and outcomes at 9 months after graduation. This is consistent with the current post-graduation outcome reporting standards of NALP, U.S. News, and the ABA. See infra Part I.C.1. 24 Professor Morriss and Professor Henderson use this term similarly. Morriss, supra note 22 at 795. 25 Nima Monebbi, Informational Asymmetries, the Emperor's New Clothes and More Cries For Value, THE BLACKBOOK LEGAL BLOG, Mar. 5, 2010, http://blackbooklegal.blogspot.com/2010/03/more-cries-for-valueinformational.html (\"law school seems (oddly enough) to present a sort of transparent information asymmetry . . . We will not spend time in this Article normatively grounding our premise that investments should be made on an informed basis. It is reasonable to infer that law school trustees, administrators, and faculty would all agree that informed decisions are especially important for investments like a legal education. 26 Nobody seriously questions educators' desire to make students better off. 27 There are few situations more detrimental to this mission than idly watching students undertake an enormous investment without a meaningful opportunity to determine material characteristics of the legal education and degree they seek. Of course, this is law schools fail to meet the demands and expectations students have upon entering and that employers have when hiring. Yet, it seems like we all know a little bit of what we are getting at the outset; the sales pitch is just all too compelling.\"). 26 Professor Morriss and Professor Henderson claim that \"reliable school-level [employment] information . . . would permit [prospectives] to make more informed choices,\" and that \"[s]ince our collective enterprise is made possible by [prospectives'] ability to borrow money against their future earnings, the legal academy has an obligation to ensure fair and accurate disclosure to prospective students.\" Morriss, supra note 22 at 831-32. In 1992, the MacCrate Report -the product of an ABA-commissioned task force for improving legal education -underscored \"the need for informed choice.\" A.B.A. SEC. LEGAL EDUC. & ADMISSIONS B., REPORT OF THE TASK FORCE ON LAW SCHOOLS AND THE PROFESSION: NARROWING THE GAP, at xi, 227-32 (1992) [hereinafter MacCrate Report]. While the \" [o]pinions expressed in this Report are not to be deemed to represent the views of the [ABA] or the Section [of Legal Education and Admissions to the Bar] unless and until adopted pursuant to their Bylaws,\" \"[i]nterpretation 509-1 to Standard 509, which pertains to Basic Consumer Information, was adopted in August 1996 to prescribe expressly that 'placement rates and bar passage data' are to be published by every accredited law school.\" Id. at ii; Robert MacCrate, Yesterday, Today and Tomorrow: Building the Contiuum of Legal Education and Professional Development, 10 CLINICAL L. REV. 805,819 (2004). On this topic, the MacCrate Report discusses \"the perceived lack of adequate information,\" MacCrate Report, supra at 229, and that \"prospective law students generally are not knowledgeable about the profession, [including] . . . different paths for entry into the profession.\" Id. at 228. The Report prescribes responsibility to the ABA and individual law schools. See id. at 229; 2009-2010 ABA Standards and Rules of Procedure for Approval of Law Schools, http://www.abanet.org/legaled/standards/2009-2010%20StandardsWebContent/Chapter5.pdf, at Standard 509 (last visited Mar. 9, 2010) [hereinafter ABA Standard 509]. Per ABA Standard 509, the ABA recognizes law schools' obligation to provide \"basic consumer information\" to prospective law students. ABA Standard 509, supra. It does not matter to whom the law schools report this information; it must be both \"fair and accurate.\" Id. at Interpretation 509-4. Accordingly, one debate centers on whether implementation of Interpretation 509-1 is correct. 27 See, e.g., Kenneth D. Dean, Information Sharing with Law Schools -One Dean's Perspective, THE BAR EXAMINER, Feb. 2002, http://ncbex.org/uploads/user_docrepos/710102_Dean.pdf. Dean Dean argued that law schools need adequate information to perform analyses in order to improve student performance on the bar exam. Id. Fittingly, this example of a law school administrator seeking to help students parallels our argument for more employment information. not a paternal call to arms. Our concern is whether prospectives have a meaningful opportunity to make an informed risk-assessment. 28 The debate accordingly centers on how much information prospectives have, how much information prospectives need to be adequately informed, and how to best achieve that level of disclosure. Part I analyzes the barriers to an informed decision, where prospectives may go wrong on their journey towards an informed decision, and whether prospectives should believe they have made an informed decision. Part II introduces a prospective's ideal tool, along with a discussion about why compromise is appropriate in light of stakeholder criticisms about the tool's components. Finally, in Part III, we outline a way forward. This specific compromise balances prospectives' interest in making an informed decision with the interests of other stakeholders. In other words, after finding that prospectives need help, we propose a new standard in employment reporting to improve transparency at American Law Schools.\n---\nI. Do Prospectives Make Informed Decisions?\n\nChoosing where to attend law school screams for in-depth analysis. But when does a prospective's due diligence move her decision from mere choice to informed choice? When is the analysis good enough for her to act? Prospective law students care about this question, and they are not alone. Legal academics, journalists, and lawyers have spoken out for prospectives to Administration (\"FDA\"), and Consumers Union. See About the SEC, http://www.sec.gov/about/whatwedo.shtml (\"The laws and rules that govern the securities industry in the United States derive from a simple and straightforward concept: all investors, whether large institutions or private individuals, should have access to certain basic facts about an investment prior to buying it, and so long as they hold it.\") (last visited Mar. 6, 2010); About the FDA, http://www.fda.gov/Cosmetics/CosmeticLabelingLabelClaims/default.htm (\"These laws and their related regulations are intended to protect consumers from health hazards and deceptive practices and to help consumers make informed decisions regarding product purchase.\") (last visited Mar. 6, 2010); About Consumers Union, http://www.consumersunion.org/about/ (\"Consumers Union (CU) is an expert, independent, nonprofit organization whose mission is to work for a fair, just, and safe marketplace for all consumers and to empower consumers to protect themselves.\") (last visited Mar. 6, 2010). take care in their decision-making process, tacitly recognizing the importance of making an informed decision. 29 But why are so many worried that prospectives are not making informed decisions? Nearly 50,000 people make this decision each year, 30 and presumably, almost all of them would claim to have made an informed decision. Is the intuition correct that many of these law students should not consider themselves well informed? Or are people merely reacting to one of the worst legal markets in history?\nIn order to make sense of these concerns, we build a model that describes the personalized decision process, and use it to point out where in their process prospectives may go wrong. This takes our descriptive model to its descriptive limits. We cannot broadly show that prospective law students are inadequately informed. Adequacy, at least as we intend to use it, is a subjective or personal evaluation. What we can do, however, is show where prospectives can run into problems with the available information, and conclude with a judgment that few reasonable prospectives could look at our analysis and believe they are adequately informed about postgraduation outcomes. Prospectives, recognizing that past prospectives were not as informed as they should have thought, may react by retooling their analyses, reconciling the common intuition with informed behavior. For convenience we build this model around \"P\", a prospective law student seeking to make an informed decision as to which law school to attend (if one at all).\n---\nA. What Is an Informed Decision?\n\nAdequate information refers to a point (\"A\") on an information scale, where P's need for information overcomes P's tolerance for a lack of perfect information. As such, this point is a moving target that depends on P's internal cost calculus and penchant for risk. P may still want 29 Id.; William D. Henderson & Andrew P. Morriss, What Law School Rankings Don't Say About Costly Choices, NAT'L LAW J., Apr. 16, 2008; Posner, supra note 18, at 13. 30 Enrollment Chart, supra note 10. more information, but an increase only results in a more-informed decision. The information scale ranges from -on the far left -total information deficit to -on the far right -perfect information. An \"L\" on the scale represents the total information P acquired to answer the question. The more useful that P believes the included information is to the decision, the further right on the scale L belongs. If P places L at A or further right on the scale, P has adequate information. If L is to the left of A, P does not have adequate information. Consequently, whether P makes an informed decision about which law school to attend (or to attend one at all) depends on the information P acquires about her selection factors. P's ability to parse, understand, and organize the acquired information, with consideration to P's objectives and risk averseness, ultimately affects what P decides. But for this analysis to happen, P must determine what information, and how much information, is available about the selection factors.\nP will ask questions requesting information that is valuable to understanding each choice's offerings for a specific factor. The amount of information that P obtains will determine how informed P is as to that factor. However, this analysis is useful beyond whether P crosses the A threshold. How far above or below A, and how important to P it is to reach A, will be pertinent to P's analysis. Once P analyzes the selection factors, P can determine whether she is in a position to make an informed decision. This determination, unlike consideration to the inevitable information deficit of each factor, resolves to a yes or no. Either P determines that she is sufficiently informed to make a decision, or she does not. And then -for better or worse -P decides.\n---\nB. How May Prospectives Go Wrong?\n\nVarious epistemic breakdowns cause information deficits during P's information acquisition process. However, whether P is aware of these deficits and correspondingly adjusts L depends on the individual. Where a question is a request for information, an answer is the presentation of acquired information. Accordingly, problems may arise with both the questions P asks and the information P uses to answer. In this Article, we analyze the problems as they affect one specific selection factor: post-graduation outcomes. Our analysis concerns whether the available post-graduation outcome information can adequately instruct P as to the likely postgraduation consequences of P's decision.\nFirst, the questions P asks about post-graduation outcomes may not be the correct questions to ask. While each prospective should ask questions he or she believes are valuable, the available information may affect which questions that P believes are relevant, causing P to place L further right on the scale than she should. Notably, the U.S. News ranks law schools ordinally from 1 to 100, plus two additional tiers. 31 Should prospectives seek the rank of each school they consider? Many Deans and scholars argue they should not. 32 But in reality students 31 U.S. News Law School Rankings, supra note 19. 32 Deans from a number of ABA-Approved law schools specifically warn prospectives to minimize the importance rankings when deciding where to go. Letter to Prospective Students, \"Deans Speak out on Rankings,\" available at http://www.lsac.org/choosing/deans-speak-out-rankings.asp (last visited Nov. 1, 2009). But cf., Morriss, supra note 22, at 791 (concluding that \"the best solution to law schools' complaints about the impact of U.S. News rankings is greater data availability and transparency, particularly on post-graduation outcomes and other factors affecting students' eventual employment prospects\"). do ask this question, and they ask it because rankings are pervasive. Even the information schools provide on their websites shape -for better or worse -what information prospectives believe is important. 33 If P asks too many wrong questions, P may unwittingly be confused as to the probable consequences of deciding to attend School X.\nFour major hang-ups may limit P's progress while she seeks information to answer questions. First, there may be a problem with statements used as information to answer questions. 34 While information can be useful regardless of its truth-value, 35 a statement's utility depends on P's ability to determine that it is either true or false. Discovering what the statement attempts to convey about the world will pose problems if it is incoherent or ambiguous. That is, the statement 'this house is on fire' conveys information if and only if the truth or falsity of the statement is evaluable. P cannot evaluate an incoherent or ambiguous statement without clarification. Second, if P has uncomfortable doubt about the information's truth or falsity, P may not trust the statement, and thus be unsure about what exactly it is that she has discovered. In either case, what the statement means to P's answer is unsettled, hindering P's information acquisition progress because the statement is currently unusable (or less usable).\nThe third problem is with how P uses the information. The key here is that, while P may determine that some information is true, the questions P asks limit the information's qualified uses. This is a test as to the quality of the connection between the answer sought (whatever it is) 33 Id. Schools that publish \"median private sector starting salaries\" on their websites are often great examples of how provided information shapes beliefs about that information's importance, despite low response rates and accordingly unrepresentative salaries. New York Law School Class of 2008 Employment Statistics, http://www.nyls.edu/prospective_students/student_life/employment_stats (providing that 25% of its graduates reported salary information) (last visited Feb. 9, 2010). 34 For example, the statement 'this house is on fire' conveys information if and only if the truth or falsity of the statement is evaluable. 35 Statements like 'the capital of Italy is Rome' can be either true or false. In either case, determining the truth-value is useful. If the statement is false, then we reduce the capital candidate cities by one (this is equivalent to 'it is true that the capital of Italy is not Rome'); if the statement is true, then we know which city is Italy's capital. and the information used to support that answer (whatever P uses). Among other things, information may be incomplete, unrepresentative, or statistically insignificant. Saying that, for example, 'information is incomplete' does not say something about the information content. This would misunderstand the distinction between information as 'some statement that can be true or false' and P's use of the information. The former application wrongly judges the character of the information content. The latter application, however, emphasizes that how well information serves P depends on what P asked. Saying that 'information is incomplete' points to some measure of inadequacy for P's project of answering a question. Information is only incomplete because some question determined that a certain use was not enough.\n---\nWhile it may be worthwhile to determine exactly what makes information relevant, this\n\nArticle will be more fruitful by containing our analysis to P. That is, information is relevant when P believes the information answers the question. However, this leads us to the fourth problem: P is not perfect. P may wrongly determine the truth-value of any of the three abovementioned problems, which may lead to undesirable outcomes. She may wrongly believe that (1) a statement is information, when it is not; (2) a source is trustworthy, when it is not; or (3) she used the information correctly, when she did not. When we say she 'wrongly believes that X', we assume that P, as a rational decision-maker, would change her belief or the reasons for believing her belief when presented with more facts.\nEven when P has determined that the statement is information coming from a reliable source, it matters that the determination about the information is true. Consider the question, \"what is the capital city of Italy?\" If P determines after visiting Florence that 'the capital city of Italy is Florence' is true, that information is useful because P could determine whether the statement is true or false and P trusts her savvy for finding capital cities. Moreover, naming a city in Italy is a qualified use because P is seeking a city in Italy for her answer.\nYet, there is still a problem because P has unwittingly determined something false. The problem is not that the statement is false, but that it is false that 'the capital city of Italy is Florence', while P believes it to be true. Although P has no clue that Rome is the capital of Italy, unless we include a prong for P's connection of facts to information, we must consider P informed. The results run counter to the normal intuition about what it means to make an informed decision. This problem creeps up both when P is simply mistaken, and when P lacks the ability to use the available information rationally.\nIn summary, P has numerous opportunities to go wrong under our decision model. In each case, P may be inclined to move L further right on the scale than she should. If P had L to the right of A, she would have considered herself informed. But it is plausible that after considering these generic problems as applied to her situation, she would realize that L belongs to the left of A. This is what the common intuition predicts would happen when presented with information intuitively known to be unsatisfactory. The next section will consider the available information to test whether the intuition is right and whether prospectives should be convinced that their L is misplaced.\n---\nC. How Do Prospectives Go Wrong?\n\nMany of the problems prospectives face while acquiring information concern P's use of information and P's potentially mistaken beliefs. We built a model in Part I.A that captures how prospectives do a multi-factor balancing test to decide which law school to attend, showing how a decision moves from mere choice to informed choice. In this Section we consider some common questions that prospectives ask (or should ask) about post-graduation outcomes, as well as how prospectives may try to answer these questions using existing tools. But first, what motivates P to care so much about job prospects? Numerous reasons could shape P's desire for certain professional opportunities. Expected debt, expected pay, practice area interests, exit opportunities, desire to help people, and prestige are all relevant to P's inquiry. 36 These reasons cause prospectives to ask questions about what jobs are open to graduates from different law schools. To help answer these questions, a number of reputable sources offer tools, and each provides considerable information about job prospects.\nNevertheless, the realm of questions that these tools should be used to answer could cause P to reevaluate her placement of L.\nBefore we discuss common problems prospectives face deciphering and using the available information, we should highlight a persistent issue with the data this information represents. Post-graduation outcomes are necessarily single data points 37 that reflect the conclusive end to a complicated process. This process is special for each law student, and the end reflects unknown choices students make along the way. The Part I.A decision model applies, and predictably, the results may be largely irregular because more goes into deciding which job to take than what would commonly be the best available. Plausibly, a graduate from a top law school at the top of the class may want and choose to work for a parent's small private practice, where the name of the law school, grades, and other factors are irrelevant. While this appears like the graduate could not \"do better,\" the outcome was still the most desirable to the graduate.\nSimply put, outcomes may not reflect the opportunities available to a particular graduate due to 36 \"[I]n addition to determining an attorney's clients and strategies, [the choice of a particular legal job] can also dictate income, hours, and overall job satisfaction\" Andrew M. Perlman, A Career Choice Critique of Legal Ethics Theory, 31 SETON HALL L. REV. 829, 858 (2001). 37 This applies to people who take multiple jobs too because there is still just one graduate. self-selection away from or towards certain jobs. 38 Private, often hidden narratives accompany each outcome, precluding a genuine understanding of each graduate's decision. With access to each graduate's private narrative P could predict her own results, but no tool provides her with widespread access.\nAny tool that provides information about post-graduation outcomes has to use employment data and information from somewhere. In this Section, we first discuss the collection processes for all employment data and information that law schools report to the ABA, NALP, and U.S. News. We also discuss how attorneys, employers, and others publicly provide employment data on the internet. Once the data and information sources are clear, we can examine how the tools that rely on this data and information answer prospectives' questions.\n---\nData and Information Sharing a. ABA Information\n\nAs a \"critical part of the accreditation process,\" and as a \"requirement of the Department of Education,\" the American Bar Association requires that \"every approved law school, provisionally approved law school, and law [school] seeking provisional approval\" answer the ABA's annual questionnaire. 39 The ABA publishes the results of each school's questionnaire in the Official Guide. 40 38 Among other things, this includes to whom law students choose to send resumes and bid on during OCI. It does not simply include job offers a student chose among. 39 Annual Questionnaire Training Presentation, http://www.abanet.org/legaled/questionnaire/2009/AQTraining%202009.ppt, at slide 4 (last visited Nov. 1, 2009). 40 Searching the 2010 Official Guide to ABA-Approved Law Schools, http://officialguide.lsac.org/ONLG_Default.aspx (\"The data collected by the ABA and published on this website.\") (last visited Feb. 9, 2010).\nConcerning post-graduation outcomes, the questionnaire requires that schools report the second most recent graduating class' placement rates. 41 For the report due on October 31, 2009, the ABA specified that \"All data . . . relate to the class of 2008 (graduates from September 1, 2007 to August 31, 2008) as of February 15, 2009 as reported to NALP.\" 42 There are four question categories under the placement rate section: graduate status, type of employment, type of job, and geographical location. 43 For each placement rate question, the ABA requests both percentages and total numbers for the category related to the question. 44 First, the questionnaire requests employment status (known or unknown) of the school's graduates for the relevant period. 45 Next, the questionnaire requests a breakdown of those graduates whose status is known: total known to be employed; total enrolled in a full-time degree program; total unemployed and seeking work; and total unemployed and not seeking work. 46 Finally, the questionnaire requires three different breakdowns about every graduate known to be employed, each denoting different information about those graduates: type of employment, 47 type of job, 48 and geographical location. 49 For type of employment, the only consideration is the 41 44 Id. at 5. 45 Id. at 17. 46 Id. Both part-time and full-time employees are included on the \"total known to be employed\" category. Id. at 5. 47 The categories are law firms, business and industry, government, public interest, judicial clerkships, academia, and unknown employers. Id. 48 The categories are Bar admission required/anticipated, J.D preferred/enhances the position, professional other, non-professional other, and unknown job type. Id. Each category is further broken down into the total number in full-time and part-time positions, with the total being 100% (i.e. a lawyer is not J.D. preferred). Id. 49 The categories are jobs located in the law school's state, out of that state, out of the United States, and in unknown locations. Id. at 18. Also total number of states where graduates are employed, not including the law school's state. Id.  kind of employer, rather than the type of job. 50 The type of job breakdown attempts to broadly capture that information. 51\n---\nb. U.S. News Information\n\nThe U.S. News, via survey, collects information in the same categories as the ABA questionnaire, for the same period. 52 After compiling the supplied information, U.S. News publishes the latest information in April, about 23 months after graduation. 53 But U.S. News goes further than the ABA's requirements, also asking schools to provide information about placement rates at graduation. 54 This provides a picture of how graduates fare before school ends, at OCI for instance, and after school, where employers may require bar passage before offering employment. Supplementing the placement percentages, U.S. News also requests starting salary quartiles for graduates employed full-time in private sector jobs as reported to NALP. 55 To determine how useful the salary figures are, U.S. News requests the percent of all graduates employed full-time in private sector jobs that reported salary data. 56 Additionally, U.S.\nNews requests the median salary for graduates employed in full-time, public service jobs, including any branch of government, judicial clerkships, academic posts, and non-profit 50 Id. at 5. 51 59 In order to do this, law schools survey their graduates, using NALP's model survey at their option. 60 Schools then compile the surveys into a single document, including other data from reliable sources that they collect to fill in the gaps, 61 and NALP creates report summaries from all of the schools' documents. 62 Although NALP periodically publishes the aggregate and average information from all law schools, each graduate data point and each school's comparative performance are confidential. 63 NALP's reports are about the legal profession, rather than individual schools. 64 NALP collects much more data about graduates than either the U.S. News or ABA. In addition to the ABA categories, NALP requests race/ethnicity, gender, age, disability status, 57 U.S. News Survey, supra note 52, at Question 168. 58 63 Id. The survey provides: Your law school and NALP respect your expectations concerning confidentiality of these data. The responses provided on the enclosed survey will not be submitted directly to NALP. Data submitted to NALP will be recoded by your school and will not include any information identifying you as an individual. Moreover, you can be certain that NALP treats all information in a highly confidential manner. No information that could be associated with a specific individual or school is released -only aggregates and averages are published. Id (bold text omitted; underlined text preserved). 64 See NALP Research and Statistics for the Class of 2008, http://www.nalp.org/classof2008 (providing studies and reports that only discuss trends in aggregated data); supra note 62.\nprogram type at graduation (full-time or part-time), special job funding, timing of job offer, source of job, search status of employed graduates, job duration (temporary, permanent, etc.), annual starting salary, the size of the law firm, and the type of law firm job. 65 Since these are data requests for each graduate, NALP is able to create summaries across categories, such as median salary for a minority woman. 66 If desired, NALP could create a summary of salary quartiles for nonminority male attorneys working in the main office of private law firms with 100+ attorneys in the same state as their law school. The possibilities are practically endless, though as we will see in Part II still fall well short of what an ideal tool might offer.\n---\nd. External Reports\n\nIn addition to law schools providing information about graduates to external bodies, other parties publicly release graduate employment data and information. Some law firms list their first-year associates with school attended, journal status, and graduation year on their websites. 67 Many firms also release employment outcomes to the National Law Journal (\"NLJ\") in a survey each year. 68 Meanwhile, graduates voluntarily provide data points on websites like Martindale 69 and LinkedIn, 70 where they self-identify with their employer, school, and graduation year. Law Clerk Addict -via chambers, law school administrators, and anonymous tipsters 71 -provides federal clerkship placement information about each Article III court, by school, though not by 65 Class of 2008 National Summary Report, supra note 8, at 1. 66 Id. 67 E.g., Skadden, Arps, Slate, Meagher & Flom LLP, http://skadden.com/Index.cfm?contentID=3 (last visited Feb. 9, 2010); Paul, Hastings, Janofsky & Walker LLP, http://paulhastings.com/ProfessionalSearch.aspx (last visited Feb. 9, 2010). 68 NLJ 250 Methodology, NAT'L LAW J., Nov. 10, 2008, http://www.law.com/jsp/article.jsp?id=1202425892561 With this data, the NLJ releases some school-specific employment information. Infra note 104 and accompanying text. 69 Martindale, www.martindale.com (last visited Feb. 9, 2010). 70 LinkedIn, http://www.linkedin.com (last visited Feb. 9, 2010). 71 Law Clerk Addict, http://www.lawclerkaddict.com/hiring (last visited Feb. 9, 2010). graduating class year. 72 Finally, anecdotes from graduates, friends or family, and media outlets provide data, either formally or informally, that prospectives can use to supplement other acquired information.\n---\nA Journey for Predicting Outcomes\n\nTo illustrate the manner in which prospectives seek information about job prospects, it helps to go back to our hypothetical prospective and follow her on her journey. Like many prospectives, P has some idea about where she wants to attend law school and where she wants to work. Let's say that P's journey begins with law schools in the Northeast. P seeks to find what happened to the graduates at these schools because she values the ability to find work, as well as work of a certain kind. If she cannot find a school that reasonably enables her to find a desirable job and pay back her inevitable loans, then she may reevaluate her options, including whether it is in her best interests to attend law school at all.\n---\na. What Do Graduates of School X Do? (\"Q1\")\n\nQ1 is a very basic question to ask, even though the answer and process for answering it are not basic. One cause of difficulty is figuring out which sources to use and what information to rely upon. Another is that P hopes to use this information to predict the future. While the past is not necessarily indicative of the future, examining the outcomes of recent graduating classes should give her some idea of what to expect, barring any major changes to the entry-level legal market. 73 Even where major changes do disrupt prospectives' predictions, prospectives can hypothesize about market effect to enhance their predictions. One theory is that the market will retract somewhat proportionally across schools. Another theory posits that there will be fewer 72 E.g., Law Clerk Addict S.D.N.Y., http://www.lawclerkaddict.com/district/sdny (last visited Feb. 9, 2010). 73 The ABA only requires that schools provide information about past graduating classes, supra Part I.C.1.a, thus the ABA seems to recognize that previous outcomes are useful for making a basic consumer decision. ABA Standard 509, supra note 26. reductions at the most competitive jobs at the top of the law school hierarchy than at all other schools, with the top of the other schools diminishing their mark on those jobs. A different theory is that smaller schools, and schools that do no feed into a primary (and more competitive) market, will fare better than average. In each case, the prospective's goal is to find some value in older information in light of new challenges.\nBack to our hypothetical P. If one of P's options in the northeast is New York Law School (\"NYLS\"), P might start with the 2010 Official Guide to Law Schools to answer Q1. 74 It reveals that NYLS tracked down the employment status of 96% (363/378) of their graduates 9 months after graduation for the class of 2007. 75 The following These numbers do not account for eight graduates for whom NYLS knew the employment status. It is unclear why NYLS declined to categorize these graduates as employed, 74 If she wanted to know that certain schools' placement followed certain trends, she might also consult previous Official Guides to identify the trends. Additionally, the U.S. News provides employment summaries, albeit for a fee. U.S. News Law School Rankings, supra note 19. While the U.S. News summaries provide a more information than the ABA summaries, supra Part I.C.1, much of the U.S. News's information is redundant. U.S. News Law School Rankings, supra note 19 (although particular percentages vary, the \"Areas of Legal Practice\" rely on the same request from law schools that the ABA makes, supra note 41 and accompanying text). 75 Official ABA Data for NYLS, http://officialguide.lsac.org/SearchResults/SchoolPage_PDFs/ABA_LawSchoolData/ABA2552.pdf, at 2 (last visited Feb. 2, 2010) 76 Id. 77 Seeking, not seeking, or studying for the bar. Id.\npursuing a graduate degree, or unemployed. The force of defining a graduate's status as known is that the school can fit a graduate into one of these categories. This problem may be the result of the ABA changing its reporting procedures on the 2008 Questionnaire. 78 Under the new reporting procedures, the ABA counted 'unemployed, but seeking employment' and 'unemployed, but not seeking employment' in the same category, rather than separately. 79 Under the old reporting procedures, some schools would avoid a higher unemployment rate by prematurely counting graduates as individuals who were not seeking employment. 80 One explanation is that NYLS used a similar practice under the new reporting procedures, declining to include as unemployed those who were not seeking employment. Another explanation may be that NYLS made a mathematical mistake. In either case, NYLS is not alone. A preliminary survey of 10 other law schools provided seven instances where the data did not account for all graduates for whom the school knew the employment status. 81 The ABA and Law School Admissions Council (\"LSAC\") disclaim any warranty as to the accuracy of the information submitted by law schools, so it is unlikely that anybody corrects even basic errors. These numbers do not account for 21 graduates who NYLS determined to be employed. What happened to these individuals? The ABA Questionnaire provides a clear answer that P will not find in the Official Guide. It allows schools to utilize an 'unknown' category for graduates that \"did not indicate type of employment.\" 85 Yet, unless P knows that she should inquire further, she will not seek out the instructions necessary to understand the data the Official Guide presents to 83 The American Association of Law Schools (\"AALS\") criticizes U.S. News for using employment status, both at graduation and 9 months after graduation, as part of their rankings. Stephen P. Klein & Laura Hamilton, The Validity of the U.S. News and World Report Ranking of ABA Law Schools, http://www.aals.org/reports/validity.html. They found that:\nThe failure to distinguish between legal and non-legal jobs raises serious questions about the validity of this index. The placement rate also included graduates who kept jobs that they had before beginning school. By including jobs that were not acquired as a result of a student's law degree, this measure may artificially inflate values for certain schools, especially those where large numbers of students work to pay their tuition. Id. This criticism functionally criticizes the ABA too because the ABA does not distinguish between legal and nonlegal jobs. Supra Part I.C.1.a and Part I.C.1.b. We discuss this failure more infra Part I.C.2.b. 84 Official ABA Data for NYLS, supra note 75, at 2. 85 ABA Questionnaire, supra note 41, at 5. prospectives because it appears self-explanatory. At this point, P can determine the employment types for 83.1% of the class. 86 While these employment summaries are easily digestible and readily available to prospectives (though sometimes at a cost), they provide many other opportunities for misuse. By asking Q1, P intimates value on the job she can expect to secure, especially as it pertains to the price she would pay to attend a school. The ABA summaries' qualified uses with respect to this question are very basic, painting only a vague picture of what her job prospects would look like if she attended NYLS. P has some additional information on graduates' employment statuses and types. Yet, P's ability to use this information is limited because the percentages, even accompanied by their category title, lack specificity. Consequently, her examination of the ABA summaries should raise two clarifying questions. First, what do these employment type categories mean? Second, are some jobs available to only some of the students in the graduating class?\n---\nb. What Do These Employment Type Categories Mean? (\"Q2\")\n\nThough on the surface the categories seem to do a good job of sorting past outcomes for P and her future classmates, closer inspection reveals a number of issues. 87 The ABA Questionnaire specifies that employment type categories refer only to the kind of employer, and not the type of job. 88 When NYLS reports 48.4% in \"law firms\", this means 48.4% of their employed graduates work as an attorney, law clerk, paralegal, contract attorney, or administrator.\nWithout access to the underlying data, P cannot evaluate which jobs graduates take in law firms, 86 P can consult U.S. News Article III Clerkship rankings, supra note 19, and the NLJ 250 Chart (2007), infra note 105, as well as NYLS's website, http://www.nyls.edu/prospective_students/student_life/employment_stats, to learn more about these graduates' employers. We discuss these later, infra Part I.C.3.c. 87 NALP appears to agree because, in order to understand the legal profession, they require more data from law schools than the schools provide to the ABA. See supra Part I.C.1. 88 ABA Questionnaire, supra note 41, at 5; U.S. News Survey, supra note 52. and she risks improperly using the available information. 89 This matters especially to prospectives who want a law school that is committed to developing public interest lawyers.\nProspectives may treat the public interest percentage as indicative of the school's outward and inward attitude towards legal aid, as well as their achievement with fostering connections and funneling graduates to these jobs. 90 But if those who go off to do public interest work turn out to be community organizers, or some other non-attorney position, reliance on the public interest percentage would be unwarranted, though not necessarily inconsistent with the school's mission.\nStill, the prospectives cannot make this determination.\nWhile these categories seem harmless enough, they do not intuitively describe a considerable number of law school graduates. 6.9% of all law school graduates from the class of 2008 listed as working at law firms actually work in non-attorney positions. 91 When P learns that 48.4% of law school graduates work for a law firm, it is reasonable to think these graduates use their degrees. The percentage may seem small and insignificant, but it is not spread evenly throughout all ABA-approved law schools. For example, the University of Michigan Law School had zero graduates in the class of 2007 who worked in positions determined to be nonprofessional/other for the U.S. News survey -the job type which describes non-attorneys at law firms. 92 On the contrary, NYLS had 15 graduates working as non-professional/others. 93 This 89 The particular problem assumes either that (i) those who attend law school do not do so to graduate and work, for example, as a paralegal or legal secretary, (ii) or at least that P would not consider these jobs in the same way. 90 E.g., Posting of rolark to TLS, http://www.top-law-schools.com/forums/viewtopic.php?f=1&t=109531#p2647558 (Mar. 6, 2010, 6:11 pm); Posting of najumobi to TLS, http://www.top-lawschools.com/forums/viewtopic.php?f=1&t=101763#p2360569 (Dec. 31, 2009, 1:01 pm). 91 Class of 2008 National Summary Report, supra note 7, at 1. 92 US News, Michigan Law School Career Prospects, http://premium.usnews.com/best-graduate-schools/top-lawschools/items/03082/@@career-prospects.html (subscription required) (last visited Feb. 9, 2010). 93 U.S. News, NYLS Career Prospects, http://premium.usnews.com/best-graduate-schools/top-lawschools/items/03109/@@career-prospects.html (4% x 378 = 15 total graduates in 2007) (subscription required) (last visited Feb. 9, 2010). means that between 0% and 9.3% of NYLS's graduates employed by law firms were nonattorneys. 94 While this is more information than when she started asking questions, P cannot determine a more precise value from the available information. This feature of the ABA summaries is not evident, even to the careful reader. The NYLS non-attorney range requires information from two separate sources, including one that requires payment. 95 At best, these categories are confusing. The categories do not appropriately group like-jobs. Working as an in-house counsel is much more like working as a junior associate than a paralegal, even if the junior associate and paralegal work in the same office. Nevertheless, both the ABA and U.S. News group in-house counsel with short-order cooks at Waffle House. 96 c. Are Some Jobs Available to Only Some of the Students in the Graduating Class? (\"Q3\")\nAn enormous number of different opportunities are available to law school graduates, and placement summaries that contextualize the entire class by aggregating individual graduateseven when accurate -fail to convey the nuances of these opportunities. Percentages do not tell P about the individual stories. Even knowing that 42.9% of NYLS's 2007 graduates worked as attorneys at private law firms after graduation fails to answer P's first question; she is concerned with how she will fare by attending NYLS, not the graduating class as a whole. Knowledge of prior classes facilitates P's journey towards predicting her own outcome -or at least her chances of achieving certain outcomes. The more information P has about the underlying data, the more informed she will be. 94 This is derived from law firms employing between 0 and 15 of the 163 graduates as non-attorneys. Official ABA Data for NYLS, supra note 75, at 2. 95 Although discovering this range only requires access to U.S. News premium information and the ABA summaries, P only knew to consider this question because the NALP National Summary Report demonstrates the considerable percentage of graduates working non-attorney jobs. This is neither obvious nor trivial. 96 See ABA Questionnaire, supra note 41, at 5 (\"business and industry (all jobs, legal and non-legal)\"); U.S. News Survey, supra note 52 (same).\nOne nuance lost by using employment summaries is the amount of competition associated with attaining particular jobs within the categories. Q3 draws out P's appropriate concern that some jobs are only available to some graduates. If a prospective's opportunities are limited relative to her peers at School X, then it will be important to know which jobs are available to which graduates. When P has more specific categories that describe the outcomes, P will have a better handle as to the opportunities available throughout the class, provided that the new categories actually serve an additional function. Two available tools impart additional information about some of the most competitive jobs by isolating parts of the employment summaries.\nArticle III clerkships are among the most competitive legal positions available. 97 The U.S. News provides a table of clerkship placement percentages for every ABA-approved school, including Article III clerkships. 98 This table distills the \"employed as judicial clerks\" category into two subcategories. 99 The first is the percentage of the entire graduating class who obtained a judicial clerkship. 100 The second is the percentage of the entire graduating class who obtained an Article III clerkship. 101 Notably, the U.S.  99 Id. 100 Id. 101 Id. 102 Id.\nan important step for resolving the meaning of two ABA employment type categories because each increases the amount of specific, useful information about law school graduates. 109 Despite the promise of these two tools, they are not very useful for answering Q3 for most prospectives, except for showing which jobs certain schools' graduates do not attain year to year. 110 The Article III summer, 30% of whom will apply for and receive clerkships. Each received an offer for permanent employment and each decided to accept, declining the clerkship opportunity. Law firms anticipate that some 2L offerees will not accept the offer to begin work shortly after graduation for many reasons, including their 2L offerees accepting clerkships. While it does not follow that each firm that offered one of these 2Ls a job overextended offers, it is plausible that some firms did because the firms accounted for less than 100% yield while making 2L summer offers. Although firms may make room for the competitive graduates who would have otherwise clerked, it is a safe assumption that NLJ 250 firm placement is closer to a zero sum game than not. 128 It is unclear whether this means that School X's graduates without clerkship offers, for whatever reason, are forced out by their more competitive classmates who would have otherwise clerked, or if those would-be clerks instead force other schools' graduates back into the legal marketplace. What is clear, however, is that some shuffling will have to take place. Unless a finite group of graduates is aggregately labeled \"competitive\", School X's 2Ls affect the market.\nFor the Aggregate Tool, this means aggregate percentages, while indicative of a minimum 128 Assuming firms attempt to hire as many associates as they project a need for. Some schools provide graduate profiles so that prospectives can see the successes of past graduates. 133 Others provide broad data about entire graduating classes to their prospective students, including employer name and office location. 134 Indeed, schools that provide this sort of data are the exception. Usually if schools provide lists of employers, they are either lists of firms that interview on campus 135 or lists of firms that have hired from the school in the recent past. 136 The main issue here is that it does not follow from employers interviewing students that those students work for those employers, especially when the school is in a major market where the firms attend at minimal expense to interview the very top of the class. 137 Another issue is that listing employers that have hired from the school in the past does not tell prospectives how often\nor over what period those names have accumulated. 138 Moving away from school-generated information, P may consider websites like LinkedIn and Martindale to gather post-graduation outcome data. Each datum, so long as the graduate obtained the job within 9 months and reported it to the school, explains one individual outcome that underlies the school-reported percentages. For example, if P learns that a 2007 graduate works as an attorney for a litigation boutique in San Francisco, P can determine that one of the outcomes in 'employed in a law firm' category required a J.D. for a particular kind of job. The more data P acquires like this, the better the picture she can paint. But this data is difficult to parse. On LinkedIn, 139 P must contend with private profiles and graduates choosing to provide their graduation years. Though prospectives cannot overcome private profiles, an indefinite graduation year may be easier to overcome.\nFirst, clues -like undergrad graduation year -may narrow the possible graduation years.\nSecond, the exact graduation year only matters for some questions. While Q1 only asks about past graduates, rather than past graduates from the class of 2007, the school-provided information comes prepackaged by class. Data points that do not track to a specific class are less useful for shedding light on the class-specific information because the datum is out of contextand context matters for any anecdote. Without context, prospectives are more likely to misuse particular positive or negative outcomes to fulfill preconceived notions.\nBeyond relying on anecdotal data to patch together what happened to past graduates, P may also try to find proxies for desirable post-graduation outcomes. While the NLJ charts and Article III clerkship rankings tend to provide a more direct measure of desirable outcomes, prospectives also use indirect proxies to fill the gaps in available information. Most pervasive are the U.S. News composite rankings. 140 The intuition goes something like this. The higher a school ranks, the better that school's graduates fare in the job market compared to schools ranked lower. Empirical research suggests that the students who attend roughly the bottom 75% of the law school hierarchy engage \"in a calculation that asks whether a marginally higher U.S. News ranking is worth higher tuition,\"\nwhere worth has to do with the \"wide array of employment opportunities.\" 141 This is not to say that prospectives function indiscriminately, but conversations with current and prospective law students demonstrate how important the rankings are to their final decisions because they think it says something about post-graduation outcomes.\nIt is not clear why anybody should think there is a relationship between a school's rank and their post-graduation opportunities -at least one that is strong enough to warrant choosing the #46 school over #56 because #46 is more highly rated by U.S. Compared to other industries, first-year salary information is relatively accessible. Many firms that belong to NALP annually provide salary information. 152 If P can identify graduates working for NALP employers, she stands a good chance of identifying their salaries. The issue is still identifying those employers, but at least some information is available about employers.\nEach year, U.S. News provides private and public sector salary information for subscribers about a relatively recent graduating class. 153 The 25 th , 50 of the entire class to show how the salaries break down by firm size; for example, 15% of reporting graduates worked for huge firms making between $135,000 and $160,000 -or a maximum of 13 graduates (3.3%). 167 While not every school provides information in this manner, this is the sort of thing with which prospectives must contend. An information deficit here may be particularly damning for risk averse prospectives who manage to appreciate the deficit.\n---\nD. Should P Consider Herself Informed About Post-Graduation Outcomes?\n\nAt this point, P has examined numerous tools to answer questions about post-graduation outcomes. Under our decision model, P's next step is to evaluate the usefulness of the information she gathered. That is, she must conclude whether she has adequate information about this selection factor, as well as how close the total information she gathered is to adequate.\nOf course, prospectives do not think this through scientifically. The decision model is only supposed to basically capture the tacit, multifactor decision to matriculate at a law school. So instead we expect this conclusion to look something like \"I have plenty of information\", \"I do not know nearly enough\", or \"close enough.\"\nDespite a broad account of career placement information, these tools present prospectives with many opportunities to misuse information because the tools do not meaningfully answer common questions like Q1 -Q5. Temptation to use available information beyond its qualified uses arises from most prospectives' status as uninformed consumers of law degrees, at least as it pertains to post-graduation outcomes tied to particular schools. It is easy to be less skeptical of the available information because the product is education-related; but as we have shown, the one additional graduate. According to the NYLS Registrar, contacted via phone, there were 400 graduates in 2008. {Side note: when the ABA releases new data, cite to that}. (400 * .25) -13 = 87. 167 NYLS Employment Stats, supra note 161. 87/400 = 21.8%. 87 * .15 = 13. 13/400 = 3.3%.\ninformation is not as complete or useful as it facially seems. 168 Between prospectives' thirst for answers and not knowing what to look for while answering, prospectives will likely put L further right than they should. That is, prospectives will think they are more informed than they are.\nSome prospectives may realize they use information for non-qualified uses and just not care. Others may not realize it, but would not care even if they did. However, we think that the vast majority of prospectives are rational decision-makers like P. When presented with more facts like more appropriate definitions or ambiguities, we expect that P would change her beliefs about the amount and proper use of the information she acquired. At the very least, she will reevaluate the risks associated with matriculation at School X to see if the risks are too much to handle.\nHowever, even with a guide to the qualified uses of available information, we suspect that schools will fill up their graduating classes anyway. Part of the problem is that not enough people will see the persistent issues with the information. If rational prospectives do not discover the problems, they will not benefit from our identifying the problems, nor their ability to identify the problems themselves. Secondly, even where P reevaluates the risk associated with matriculating at School X, this does not mean she will act perfectly rationally. Optimism bias may color her final determination about her employment prospects. Even if P determines that she will need to finish in the top 10% of the class to achieve the job she wants, it should surprise nobody that she might say, \"I did not work hard in college, but I will work hard in law school and finish in the top 10%.\" Alternatively, when P identifies gaps in information, she may feel about expected outcomes. We posit that increasing information to reduce uncertainty will curb some optimism bias because less uncertainty, via a more accurate picture of prior outcomes, will reduce the gaps where P has to guess. Eliminating optimism bias is not realistic, but reducing it is an enviable and plausible goal.\nThe ultimate issue is the ability to hide undesirable outcomes in aggregate statistical forms. Just about every tool enables this behavior, which, while misleading, often complies with the ABA and U.S. News reporting standards. When that is the case, the standard is the problem and not the law schools that comply in good faith. The standard for employment reporting is precisely what we undertake to improve.\n---\nII. Creating a New Tool\n\nExisting tools fail to inform prospectives' questions about post-graduation outcomes at ABA-approved law schools in a number of ways, often providing incomplete or unrepresentative information, or offering proxies in place of substantive information. The result is a perpetual flow of information that fails to show the full picture, leaving tens of thousands of prospectives guessing or otherwise believing that they know what their job prospects look like when they do not. Now that we have said there is a problem that needs fixing, what comes next?\nThis Part introduces the concept of an ideal tool that eliminates information deficits by providing perfect information. The ideal tool is relative to each prospective. In terms of P, her ideal tool comprehensively answers every question she has about post-graduation outcomes.\nWhile previous attempts to collect information have been admirable, 169  the tool could document everything from family connections and physical appearance to grades, journal membership, and satisfaction levels with respect to job offers.\nEach prospectives' ideal tool could look very different, as its structure depends on the questions the individual prospective asks. Our goal is to standardize some combination of components in order to help prospectives be informed as to post-graduation outcomes. Due to the personal nature of what information to include and how, we think categorically in an effort to identify potential components of the tool as a first step towards creating a new employment reporting standard.\nWe understand and emphasize that this requires huge data and information collection costs. We explore these costs by examining the candidate components through the lens of various stakeholders, including students, law schools, employers, and the legal profession. Later, in Part III, we propose a way forward to enable a standard that balances the needs of prospectives with non-prospective stakeholders, while minimizing compliance costs.\n---\nA. P's Ideal Tool\n\nP's ideal tool would provide P all of the information that she wants about a school's historical performance. This amounts to perfect information -information that she believes she needs to answer post-graduation outcome questions -and enables her to make a fully informed Not all law students were created equal, even at the same school. In order to provide perfect information the ideal tool must include a category describing how each individual fared in life prior to law school matriculation. These components include undergraduate institution, previous honors, prior work experience, and competitive scholarships or fellowships.\nPersonal attributes matter too, as an endless number of personal attributes could influence a person's employability and the decisions they make. These attributes fit into both the pre-and post-matriculation categories. This ever-present category includes everything from physical law firms rely on heuristics, using signals like school status, grades and law review membership as shortcuts when selecting potential hires). appearance 174 and height, 175 to family members, networking ability, 176 and race. It may even include undergraduate and graduate debt. This is new territory when describing job prospects at American law schools. With the exception of some personal information collected by NALP and the occasional study showing a correlation between personal attributes and employability, this category of components would expose an otherwise invisible aspect of how people advance in the legal industry. The ideal tool would identify and list them all.\nThe breadth of relevant input components develops continuously because the content of P's perfect information depends on her thirst for answers. However, the information P uses to answer her questions must use pre-output information. This is because these inputs produce the output, so once school X's class graduates and the post-graduation outcomes are measured, the inputs freeze forever. Development only comes with P learning about what more to ask.\n---\nOutput Components\n\nIn contrast with the enormous variety of input components, output components are easier to describe. An output component is any data or information that P desires to know about a law school graduate's post-graduation outcome. These components describe the job in detail, looking at salary, duties, fringe benefits, hours, market-adjustments, etc. Components independent from the individual graduate include location, whether a law degree and bar passage are required, and any rankings or other measures of the employer. Components related to the specific graduate would include practice group preference, employer preference, expected quality of life, proximity to family or friends, and anything else that may add or detract from the value each graduate expects to receive from the job. As with the input components, if P thinks of an output that might be important for her purposes of selecting a law school, then the ideal tool will tell her about it.\n---\nB. Facing Reality: P's Ideal Tool Is Unworkable\n\nTheoretically, a prospective law student could know everything about the past effect of her potential investment on post-graduation outcomes. 177 But how feasible are these candidate components, particularly the ones that involve systematic measurement of personal attributes? Documenting some personal characteristics like race or economic background is certainly not outside the limits of current reporting strategies, though making this information public would raise privacy considerations. 178 At least one study looking at the legal profession has measured and assigned values to a highly subjective characteristic, finding a correlation between physical appearance and career advancement. 179 Other factors such as interpersonal or networking skills are also examinable through a standardized interview process. 180 But measuring all of these attributes for the tens of thousands of students each year would prove both difficult and costly, 177 As we discussed in Part I, past success is not indicative of future performance. not to mention the dirty issue of creating guidelines on how to describe a student's physical build or ability to tell a joke.\nThese limitations and difficulties do not get to the root of the matter. While the costs associated with collecting so much information -including the outcome components -are great, it may still be a worthy investment for the legal profession. The real issue is with competing stakeholders. As shown in Part I, prospectives are in a bit of a hole. Many believe they make informed decisions when they are really uninformed consumers. If we assumed all stakeholders believed it was in their best interest to get as much information as possible to prospectives, the issue would be over how to do it, not what to include. 181 After all, we are talking about billions of dollars in loans that finance tuition and living expenses, and tuition continues to escalate. 182 Instead, the issue is that various stakeholders would object to total transparency. Prospectives are not the only stakeholders, so we must address the criticisms candidate components will cause; there are hurdles to enabling better-informed consumers of legal education.\n---\nC. The Balancing Act\n\nProspectives make up just one subset of all stakeholders affected by the release of employment information. While their bargaining power is substantial in the aggregate, their geographic dispersal at thousands of colleges and companies around the country and the world make it difficult to talk about collective action. As such, we leave that discussion to Part III.\nInstead, we begin this balancing act by asking whether standardizing P's ideal tool to benefit 181 Prospectives constitute, after all, virtually 100% of all future law students, graduates, attorneys, hiring partners, judges and faculty, not to mention a substantial number of political leaders at various levels of government. It is not unthinkable to suppose the people who currently hold these positions would want the very best for their eventual successors. 182 \n---\nThe Schools\n\nGiven what we know about law school administrators' opinions about existing tools, 183 it is likely that schools will meet any new external attempts to gather information with some resistance or skepticism. This could be due to a variety of reasons. Schools might be concerned about their students' privacy, particularly those who would not otherwise feel comfortable reporting things like salary. 184 Schools may not wish to reveal how deeply in a class certain employers actually hire students for fear of alienating employers who are willing to take a more holistic approach to recruiting. 185 For similar reasons, they might be concerned about who else other than prospectives will have access to the information; competitor law schools could use some candidate components in the same way corporations use the federal Freedom of Information Act to seek records about the competition. 186 They may also worry about the financial costs of complying with additional reporting requirements on top of what the ABA, NALP, and the U.S. News already require. 187 If the requirements are only recommended rather than mandatory, schools will need to weigh the benefits of full disclosure against the competitive harms if they end up being the only school to show where the bottom of the class ends up. Additionally, law schools should be rightfully concerned about diverting more of their limited resources away from curriculum offerings or faculty scholarship and into administrative costs. 189 Schools already collect a significant amount of data and information about each graduate, and may be hesitant to expand those efforts. Law schools collect much of their current data through user-friendly online surveys, 190 but many personal attributes require more than filling in a bubble because some attributes would not be self-reported or multiple choice. These components are likely too expensive to incorporate into a standard that publishes information about every individual in a graduating class. 191 Schools have spoken out against external measurements of their program, particularly rallying against tools that attempt to collapse different education programs into a single national rank. 192 P's ideal tool, of course, would provide perfect comparisons, adjusted to correct for the complaints schools may raise about comparing apples to oranges. But standardizing her ideal tool may not always do this. It would also permit the uninhibited creation of derivative tools by interested members of the public, many of whom could tailor the results to suit their own needs before making the results of the derivative tool public. This may concern law schools that value their unique educational models and suffer unfairly from comparisons to programs that are entirely different.\nSchools heavily invest in the recruitment of high academic talent, fundraising from alumni and members of the community, and producing scholarship. The level of competition for the most significant information-forcing mechanism on the market, the U.S. News Rankings, is very high; current reporting methods provide incentives to underreport that law school administrators cannot ignore. 193 It is therefore dangerous to be a first mover if doing so would cause negative public scrutiny, while other programs continue recruiting as if their outcomes or information sharing are sufficient or even better.\nLaw schools may ultimately split on the issue of increasing transparency about postgraduation outcomes. Law school deans have recently begun calling for schools to acknowledge a moral responsibility to stop misleading prospectives, even going so far as to caution prospectives about attending a law school that does not offer a good return. 194 And at least three law schools have begun publishing spreadsheets that list employer name and city for nearly every graduate in a class. 195 This is certainly a step in the right direction. But it remains to be seen whether more schools jump on board and voluntarily release more information because 193 Professor Henderson and Professor Morriss document the various ways schools underreport information in order to achieve a higher U.S. News rank. Morriss, supra note 26, at 815. Most glaring is the decision of nearly fifty lower-tiered law schools to refuse to disclose the number of students employed at graduation in 2006 because under the U.S. News methodology their scores fared better that had they opted to disclose. each law school is ultimately responsible to their own batch of current students and alumni.\nPublic choice theory suggests that schools will not participate in bringing a new standard to market if their own self-interest is better served by refusal. 196 Within each administration, rentseeking may cause varying levels of accuracy as different schools allocate different amounts of money into compliance costs. 197 Even assuming we achieve full compliance from all schools, we must still worry about ways in which administrations may game the reporting requirements. 198 Even if the collection costs are negligible, some schools may still oppose a tool that focuses solely on the entry-level job market. Many schools do not report the percent of the class employed at graduation because significant portions of the class do not find jobs until after passing the bar exam. 199 Schools with a less-developed early interview program that rely on this delayed hiring model may lose out. However, this does not infringe on the need for prospectives to have more information about entry-level employment. Payments on most loans start six months after graduation. 200 Schools who strongly believe their best employment outcomes are further afield should convey that to prospectives, particularly because the only employment statistics a law school must report to maintain its ABA accreditation are entry-level employment and bar passage rate. 201 Despite some intrusion, a standard based on P's ideal tool would also provide a service to law schools. By keeping track of alumni from other law schools and identifying the successful 196 See Sterk, supra note 189, at 1160. 197 Id. at 1160 (showing how lobbying efforts aimed at law school administrations -which compete for the best prospective students while facing pressures to allocate limited resources among other interest groups like faculty and current students -compare to traditional special interest lobbyists that target government agencies). 198 This already occurs with current reporting requirements, whether imposed by the regulatory power of the ABA's accreditation arm or by the market power wielded by the U.S. News. Supra note 193. 199 Class of 2008 National Summary Report, supra note 8. 200 Federal Stafford Loan Plain Language Disclosure, http://www.ifap.ed.gov/dpcletters/attachments/FP0904StaffordPLDHEOA032709.doc. 201 ABA Questionnaire, supra note 41, at 3-6.\nones, schools could continue evolving their admissions criteria in lieu of the information provided to P by her ideal tool. 202 This might even shift the strong tendency of schools away from emphasizing LSAT and undergraduate GPA in the admissions process towards other criteria. 203 We think schools would consider this a good thing, particularly if it would serve to detach a school's reputation from its U.S. News rank.\n---\nThe Students\n\nThe overarching concern among students204 would be the potentially serious ramifications of having personal, identifiable information open for public study (and available years later). P's ideal tool would include all graduates, regardless of whether they were successful in the hunt for their first job after law school because she seeks perfect information.\nGiven the importance of social status and stigma among the legal community, and the seemingly permanent nature of information stored online, it is important not to unfairly associate individuals with failure before their careers even begin. P may benefit now from learning intimate details about current students at the law schools she considers, but she may see things differently in a few years when it is her time to be a subject of case studies. With additional detail may come additional embarrassment: listing everyone's occupation and further labeling them when they do not require a JD may shame some individuals that could have been avoided\nhad they simply been listed as employed in Business or Academia. 205 Some students -likely those who are outside of the top performers -may eventually benefit from the ideal tool. By not hiding some students in aggregate employment statistics, the ideal tool would incentivize law schools to do more than just find students any job for the purposes of marking them as employed. Schools might come up with new ideas on how to best secure employment as a result of greater information-sharing between schools. Additionally, law school graduates must find ways to pay off their loans. Having to report components of the ideal tool might incentivize schools to increase efforts to help struggling graduates find work that credibly illustrates the worth of a law degree.\n---\nThe Employers\n\nSome employers may dislike certain candidate components, particularly employers who do not want prospectives to know what they are getting themselves into for fear that the truth may scare away too much talent. This may decrease demand for prestigious legal jobs, like highsalaried and time-intensive work at elite law firms. Of course, the reality might be that betterinformed prospectives outweigh the decreased demand. Students may end up more attractive to employers because, before and during law school, they can further sharpen the skills that employers desire. And if greater information about salary information leads to better debt preparedness, and fewer law school graduates are saddled with six figures in debt, they might find employees who want to be there, rather than only working there for the six-figure salary. 206 Employers may also resist increased information access if it significantly deterred individuals who would otherwise attend law school from attending at all. If employment information ceased to reveal only the top performers and instead described all graduates with equal clarity, fewer prospectives may choose to gamble on a legal career. There may be a fear that too much transparency regarding the undesirable employment outcomes -which were safely 206 At least one academic argues that demand for these jobs would decrease if fewer students were graduating with significantly less debt loads. Patrick J. Schiltz, On Being a Happy, Healthy, and Ethical Member of an Unhappy, Unhealthy, and Unethical Profession, 52 VAND. L. REV. 871, 898 (1999).\nhidden until the ideal tool came along -will discourage people with high potential from pursuing legal careers. After all, the model of law as a meritocratic profession dictates that some who attempt it will fail. Even when the legal community is saturated, the legal profession should still want large numbers of people competing within the law schools to produce the best-trained minds for entry into the legal community.\nSome employers may desire the same privacy as some individuals, demanding that information about their hiring practices or work environment remain confidential. For example, public salaries may upset clients or harm morale in the office. 207 The risk is retaliation by employers against the law schools, refusing to hire from schools that facilitate increased disclosure. In this regard even minimum information about certain inputs like the GPAs of new hires may be too intrusive to some employers. A standard that pits employers against law schools by over-disclosing information would fizzle or die fairly quickly because schools will not undermine opportunities for students. 208\n---\nThe Rest\n\nSchools and employers are the obvious stakeholders who might object to many candidate components. But what about the legal community and society at-large? Besides criticisms from individual employers, alumni of a particular school who have set up so-called \"feeder programs\" into their firms might object strongly to components that reveal the GPA or class rank of the 207 E.g., Elie Mystal, Salary Cut Watch: Gardere Wynne Cuts Salaries that 'Just Do Not Make Sense', ABOVE THE LAW, May 6, 2009, http://abovethelaw.com/2009/05/salary-cut-watch-gardere-wynne-cuts-salaries-that-just-do-notmake-sense/ (\"Many clients, both ours and those of other law firms, have been upset with these salary levels and as a result have asked law firms to not use first or even second-year lawyers on their matters.\"). However, it is worth pointing out that many law firms already disclose to NALP first-year salaries. NALP Directory, supra note 152. 208 This seems true on a larger level, while it is not clear on a school-by-school and firm-by-firm basis. students they consistently hire, particularly when those metrics are lower than the advertised hiring standards. Some may object to increased transparency because it may reveal a systemic pattern of employers hiring less-qualified candidates solely because of diversity considerations.\nWhat if nepotism plays a larger role in employment outcomes than academic achievementwould the revelation that law is an aristocracy rather than a meritocracy sit well with a country founded on ideals? To put it another way, is the truth behind how people get where they are too ugly for us to in good faith make visible to anyone who wants to see it? Some things may be best left behind closed doors.\nIf one accepts the proposition that there are too many lawyers in our society, then increased transparency may further harm society if it somehow increased demand for JDs or encouraged more law schools to open their doors. Conversely, if the effect of additional transparency decreases the demand for legal education and some law schools eventually close, the decrease could harm the subsets of the population that are reputedly underserved by legal services (e.g. indigents, immigrants, or the poor). No matter what one's personal opinions may be about the inequality or saturation of the legal market, any major change to how lawyers are educated needs to be viewed in light of how it may impact the people who depend on lawyers' services.\n---\nD. Identifying the Problem Is Not Enough\n\nIn this Part, we identified candidate components and potential criticisms of standardizing the reporting of these components across all law schools. This effort is by no means the end of discussion. For many of the objections stakeholders might raise to certain components, the only solution is to reach a compromise that offers less information than what standardizing P's ideal tool would offer. Much of our discussion has so far ignored the practical limitations of introducing a new standard to the market, namely cost, feasibility, and fairness. Some schools, individuals, and employers may simply refuse to participate, leaving gaps in the data and information that could harm its overall usefulness. 209 How can such a change come about? Part\nIII offers one answer to this question.\n---\nIII. A Way Forward\n\nIn Part II, we identified components that a new standard of employment reporting may include by considering what P would include in her ideal tool. This Part discusses the effect of stakeholder criticisms on building a new standard and bringing a tool to market that improves the information available to prospectives. We believe that a successful information-forcing device will change things for the better. Competition among law schools, as with any industry, will ultimately benefit from public scrutiny into the services they offer. Sunlight, as one former Supreme has said, is the best disinfectant.210 Justice Brandeis was referring to the need for bank disclosure almost 100 years ago; it is finally time for American law schools to follow suit.211\nDespite the novelty of our proposal, calls for transparency in the legal education industry are not new. In 1992, the MacCrate Report recommended a joint effort between the Section of Legal Education and Admissions to the Bar (the ABA's accreditation team), the Law School Admission Council, and the Association of American Law Schools. 212 According to the report, the aforementioned entities should \"seek to assure that prospective law students who are selecting a law school can obtain information about all law schools relating to . . . graduation 209 Beyond the short-term information provided by post graduation outcomes, long-term studies may eventually be needed to shed light beyond the entry-level market. The decision to pursue a career in the law should be a considered choice reached with a full awareness of its implications . . . .There are three critical stages of decision-making en route to becoming a lawyer: 1) Perhaps the most significant, whether to enter the legal profession at all: 2) which law school to choose; and 3) what career path to enter after law school. Each occasion should be a time for careful reflection and self-assessment based upon sufficient information to make an informed choice . . . Timely and accurate information about the legal profession and the function of law schools as the gateway to the profession helps prepare prospective applicants for a future in law and may help prevent some from becoming locked into a career from which they draw no real satisfaction, for which they are poorly suited and in which they perform marginally. Such individuals need access to comprehensive and objective information . . . Prospective law students generally are not knowledgeable about the profession: what certain jobs entail; what different paths for entry into the profession may be; how students should prepare for their careers; and how law schools may differ in the preparation they offer. Law students tend to be passive consumers of legal education: they simply assume that the law school experience adequately prepares them for practice. 214 It is arguable whether this statement applies directly to post-graduation outcomes, but their conclusion is clear; prospectives were uninformed about the legal profession, and access to both current and accurate information may protect some consumers from making a bad decision. 215 The MacCrate Report's observations square well with Brandeis' goal of improving transparency. Consumer protection is advisable whenever the potential for suppliers to mislead or defraud their consumers is present. 216 Other reasons for pursuing a new standard serve a more equitable purpose. Professor Morriss and Professor Henderson argue that schools do not have a right to continue misleading prospective students -a sentiment that law school administrators frustrated with their inability to change the accepted (and sometimes deceptive) practices occasionally echo. 217  law schools to permit comparisons. Compromise on these qualities is unacceptable, while compromise on which components to include is necessary.\nOne major criticism raised in Part II is that some components infringe on stakeholder privacy, particularly the individual graduates. Any time one describes facets of a thing, the possibility of identifying that thing enters into the equation. The more identifying facets, identification becomes more probable. Eliminating a name from individual's data obfuscates identity a bit, but given that the goal is to reveal some greater level of information than what is currently available, the LST standard does not completely alleviate the privacy concerns.\nTo successfully implement a new standard, the components chosen ultimately require a policy determination as to where the balance lies between individual privacy concerns and open access to information. We believe the balance is struck best by requiring two separate lists of anonymous graduates from every law school. While some schools may be comfortable associating names and certain components, we do not think most schools would comply, nor do we think current and future students would support this affront to privacy. However, prospectives could easily ask the school for a name, and we expect a school would cede the request on a case-by-case basis. For the graduates this would be no different from what already occurs across law schools nation-wide.\nThe first list's components will include a single row for each graduate, tying together seven possible components per row. The second list's components will do the same, but will only include four possible components. This list will be independent from the first list. 224 The components are: Job List \"unemployed.\" Like the ABA, we do not distinguish between mental states as it relates to unemployment. Additionally, this component should say \"pursuing graduate degree\" or \"unknown\" if the graduate counts as such under the ABA standard. If the graduate is unemployed, pursuing a graduate degree, or unknown the school should not report the rest of the components. If a school wants to supplement the lists with an explanation for a particularly high unemployment or unknown rate, we encourage them to do so fairly and transparently. As shown in Part I, the ABA categories do not group like-jobs. 228 This limitation requires some calibration.\n---\nEmployer Name (Job List)\n\nThis component is the first crucial step to ensuring that current and prospective students secure employment commensurate with their expectations prior to enrolling or seeking employment. By including the names of each employer, students can research actual outcomes through firm websites and opt-in professional listings like Martindale and Linked-In, allowing them to understand various aspects of the legal profession, especially the entry-level legal market. 229 Taken together, a full or near-full list of entry-level employers can illustrate that the legal profession is much more than biglaw. Prospectives then do not have to rely on how schools frame their graduates; they can see for themselves where graduates selected to work. Over time, this picture will broaden the perceptions of the entry-level legal marketplace. Such a picture can help prospectives observe the enormous cross-section of legal employers who collectively employ tens of thousands of recent graduates each year. This is often lost on prospectives students, current students, and even recent graduates. 230 In our view, this is an appropriate service to the legal profession.\n---\nPosition Name (Job List)\n\nThis component is the second crucial step to ensuring that current and prospective students secure employment commensurate with their expectations. Including the name of the job has become increasingly important as legal employers shift their business models; not everyone who obtains a job with a law firm after graduation is an associate, let alone an attorney.\nEven more troubling, the employer name does not necessarily indicate how a graduate uses his J.D. For example, a Google employee may be general counsel or software engineer. Position name will give prospectives the opportunity to see, at least facially, how a graduate uses his J.D.\n---\nBar Passage Required, Preferred, or Neither (Job List)\n\nGiven that this Article is about helping prospectives ascertain the value of a law degree so as to make an informed decision, it should come as no surprise that the new standard requires data about whether a graduate had to be admitted to practice law. 231 Even position name, particularly those that prefer rather than require bar passage, can be ambiguous. Who knows whether \"Policy Analyst\" requires legal competency? Schools that place graduates into policy positions like this may even benefit from explaining data typically not highlighted by attempts to inform prospectives. Many law schools already extol the virtues of how a law degree provides graduates opportunities in all aspects of society; this component proves it by identifying the graduates who choose to pursue unique career trajectories that many prospectives never considered possible with a law degree. Along with the other components, it will also help 230 We do not pretend that we are not included among this group. We know there is more out there than we can describe, but we cannot do the content of this fuzzy image justice. 231 As reported to NALP, following the NALP guidelines.\nprospectives -once the standard has been around for a while -determine which schools have established alumni networks in other fields. them to show why their degrees are of special value to their graduates, despite the appearance that their graduates suffer by working only in certain regions, states, or cities.\n---\nFull-Time\n\nAgain, introducing new components must be done in light of the privacy concern that combining more components increases the ability to identify individuals and employers with salaries in a way that disincentivizes school compliance. Adding the full-time/part-time will surely reveal the salaries of a few employers, albeit indirectly, because so few graduates work part-time. Similarly, adding office location and drilling down the ABA employment category will do the same, though on a larger scale. But many schools place their graduates in concentrated locations, so we do not think this worry is too weighty as some compromise on the salary issue must be made. Salary data is simply too important and lacks requisite meaning without context.\n---\nD. The Standard's Place in P's Journey\n\nSo what happened to all the other components mentioned in the ideal tool? These two lists wipe out much of the detailed information that P wants to use to decide where to go. Gone are all the juicy details about each graduate's looks, family connections, grades, race, and undergraduate institution that may have played a part in the hiring process. Gone are all of the components describing the actual options available for each graduate.\nOf all of these missing components, GPA is probably of greatest interest to prospective law students. So why leave it out? The curt answer is that P must give some stuff up in the real world. As we touched on in Part III.C.7, we do not seek to dictate employer-hiring practices or school competitive advantages for matching students and employers. Other input components would be too difficult or too costly to gather. Some might be unpalatable even if the funds did magically appear to collect and measure the components on a wide scale basis. Having a team of trained eyes rate the physical appearance of law school graduates, while juicy, is probably not in everyone's interest. 238 Some of the candidate components may also require special approval from individuals, particularly where the requested information goes beyond the scope of information currently collected by NALP, the ABA, or U.S. News. Others, frankly, just are not important enough to our core goal to include because we fear that going too far jeopardizes our chance to compel schools to adopt the LST Standard.\nIn the end, we include the Part III.C components because they achieve an adequate level of information without placing too great of a burden on the stakeholders, and without requiring significant additional costs for law schools to gather and submit the information. Some components will still raise the ire of the stakeholders. Employers may not want to have their names associated with certain law schools; law schools who have large graduating classes may not want to track down the starting salaries for every graduate; and not every graduate will want their job title publicized, particularly if the list format only takes a February 15th snapshot. But taken together, each of these components help explain the post-graduation outcome of every graduate. Of course some people will be upset. Some will think we went too far. Others will think we did not go far enough. Recognizing a problem requires that we look into fixing the problem; so that is what we have done. When P tries to make an informed decision, it is in the legal profession's interest that she can do so if the costs are not too high.\nThis standard corrects for top-performer overrepresentation by requiring data on every single graduate. The lists aims to show a picture of post-graduation outcomes that prospectives can actually rely on for their decision-making. It shows what graduates do to a certain extent, but 238 We do not suggest that there is no value in conducting studies to reveal biases in how legal employers select graduates from the nation's law schools.\nenables prospectives to research particular job features further because the list includes the employer name, office location, position name, and bar passage requirements. In the same way, it resolves the ambiguity of the ABA employment type categories, while simultaneously grounding the new data with the ABA information. On the other hand, it does not do a good job of carving up graduating classes by credentials. It will certainly allow it, as it is common knowledge in law schools what jobs people at the top of the class seek. Although journal status certainly helps, the standard does not make it easy. Yet this decision will ultimately make implementation easier, so we believe it is the right decision to make.\n---\nE. Implementing the New Standard\n\nImplementing the new standard requires law school cooperation. Under the LST standard, schools will provide the data and bear risk with increased transparency. Schools accordingly need some sort of incentive to participate. This incentive can come internally from faculty, administrators, and students, or externally from prospectives, alumni, university trustees, the legal community, and the media. We hope that this Article engages some of these people, motivating them to ask law school decision-makers difficult questions.\nIncentives for schools to comply with a new, non-compulsory standard may come in many forms. Maintaining the current standard may invoke guilt, empathy, sympathy, and moral imperative to facilitate change. Where these fuzzy feelings do not sway decision-makers to comply, they need to feel like they stand to either gain or lose. Faculty dissonance, waning alumni donations or university-wide budget allocations, and negative press may cause decisionmakers to think twice about declining to cooperate because happy faculty and money are two elements without which a school does not want to operate. 239 Some schools will likely see increased transparency as a way to benefit prospective recruitment. 240 When a prospective chooses between an LST-compliant school and non-compliant school, that person may select away from the school that does not meet the LST standard, especially when the current tools are particularly unhelpful. 241 So why have schools not tried a self-started, heightened standard already? The reality is that they have.\nIn fact, it is because of prior behavior that we think a first-mover problem is not as concerning as it could be. Adoption faces a first-mover problem despite reasonable incentives to adopt the standard. Events over the last few years leave us less concerned than one may expect for such an enormous shift. In March 2008, Vanderbilt shared an employment list with admitted students because we informed the admissions office that there was wrong information on TLS. 242 We then published this list online; about one week later Duke shared their employment list. 243 While neither list qualifies under the LST standard, neither is far from compliance. 244 Chicago has since published a similar list for the Class of 2009, although access was initially limited only to alumni. 245 While the Chicago, Duke, and Vanderbilt lists are great steps in the right direction, they are not the only schools to have voluntarily exceeded the ABA and U.S. News standard in some fashion. Each time schools publish data and information beyond the minimum required standards, they incrementally increase prospectives' level of informedness. 246 As we emphasized earlier, the economic state of the legal profession lays a ready foundation for reform. 247 We expect some schools to say, \"we actually did pretty well despite this economy, let's show the world so that we can recruit more effectively.\" We hope that there will then be a snowball effect as schools see -or fear -other schools benefiting from more transparency. In other words, schools will comply as a reaction to peer participation.\nOf course, a new, widely accepted standard will not just happen on its own. There must be a concerted effort to make schools aware and make schools care about the standard. LST will coordinate effort on three fronts to facilitate acceptance. Prospective collective action, press, and a private certification system will together generate buzz and pressure, hopefully with enough force to compel compliance. All coordination efforts will center on our website so that one location serves as the go-to place for the heightened standard's results.\nDespite the potential bargaining power of a massive consumer group like prospective law students, there has been minimal effort to harness this power. 248 Schools really care about this group, although many will surely object to describing prospectives as consumers. Schools do not want to fill their classes with just anybody; schools would not otherwise offer scholarships, read applications, or spend money recruiting. But whether or not schools will admit it -and some do -schools care about their U.S. News ranking, even if it is only because certain stakeholders 246 E.g., knowing that X graduates make $160,000, even if just a small percentage of the class, increases the total information known to P. 247 Supra note 5 and accompanying footnote text. 248 TLS publishes school profiles and interviews from time to time. TLS, supra note 16. The site's popularity incentivizes schools to get involved.\nintegrates law school administrators and prospective law students, prospectives (and the general public) can serve as auditors of the school-provided data and information. This offers a low-cost policing function that will both engage site visitors and further incentivize schools to comply with the standard requirements in good faith. As a practical matter, searching the websites is easy, so prospectives would have access not only to each school's publicly disclosed information, but to the collective thoughts (but also paranoia) of other prospectives who came before them. By reviewing an open discussion among prospectives, law schools can observe in real-time how people respond to new data and information. This, in turn, may permit rapid adjustments in how schools approach their recruiting methods.\nThere is another practical reason to include prospectives in the auditing process.\nInformation asymmetry is one of the greatest failures identified in this Article; sophisticated producers of a good (the law schools) sell to uninformed consumers (the prospectives). This new tool would not only reduce the information gap by providing important knowledge to the people thinking about purchasing the good, but it would do so at no cost and with minimal effort by the consumers.\nHowever, prospectives' collective voices may not be loud enough. Even assuming we can direct a sufficient volume of prospectives to the website, we must also find ways to inform nonprospective stakeholders about the project. There is a thriving community of legal blogs, including Above the Law, The Wall Street Journal's Law Blog, and PrawfsBlawg to supplement traditional media. Combined, a promotional campaign on these fronts is our best shot at success through market pressure.\nLaw schools have a history of adapting to market pressure. One only has to look at the U.S. News to see a third party market force that effectively compelled a higher reporting standard. This does not mean we want to be like U.S. News. In fact, we emphatically do not want to be like U.S. news. We want to help students find work commensurate with their expectations pre-1L and facilitate data and information sorting by prospectives. We do not want to rank schools, although we expect any widely accepted standard will find use as a composite ranking input. For law schools, this may be an unattractive consequence. But a graduate who is not blindsided by a lack of opportunities of a certain kind is a graduate who will be more inclined to donate time, money, and other resources like mentorships, internships, and externships. Fearing misuse is a reason to transparently explain data in good faith, not a reason to avoid a heightened standard altogether.\nAdditionally, for both the certification mark and prospective collective action, prospectives need to believe that the standard we push is worthwhile and that Law School Transparency is the right body for the job. We began this project with no scholarly article in mind. We realized, however, that implementing this standard cannot be understood separate from justified behavior. While all it took was passion to start this project, more is required to justify this kind of fundamental shakeup. This Article serves as our justification to the legal world, present and future members alike, that this sort of change is the kind of change we should and can engage in.\n---\nF. Potential Consequences\n\nThere are many consequences, both good and bad, that might follow from this initiative's success. 250 Although nobody can tell for certain where the future will lead with this project, it is 250 We ignore in this paper the consequences of our attempted efforts if our efforts are unsuccessful, even though some further action would likely occur in that event. It is also possible that other solutions come to light before we manage to fully embark in this attempt, which could render this entire scholastic exercise moot. Circumstances outside the focus of this article may even bring changes to our standard in ways that are not currently foreseeable. In any event, we have embraced having the opportunity to explain what we believe needs fixing and how we are currently preparing to go about it.\nworthwhile to consider the possibilities. Consequences will vary depending on the level of compliance. Assuming (optimistically) that all or many ABA-approved law schools comply, a number of things may happen. First, improved transparency about where all graduates work-as opposed to only a certain top percentage-may diminish consumer demand for law degrees to the point where the talent pool of aspiring lawyers shrinks. We are not just talking about the possibility that a market correction produces fewer applicants, but also that prospectives may overreact to a rush of new information. The rapid shift from the current level of transparency to a higher standard may create an additional level of alarmism that the actual job prospects do not warrant. Anchoring may affect how prospectives perceive the \"new\" employment outlook, at least for those who are aware of how post-graduation outcomes appeared before the new standard. 251 We want the country's future lawyers to be able to see what the job market actually looks like before they dive in without causing them to behave irrationally.\nIn the alternative, removing uncertainty about job prospects may actually increase the talent pool by encouraging more risk-averse applicants to actually attend. Only a portion of all people who consider attending law school actually take the LSAT, and of them only a portion are accepted. Better information about job prospects may encourage more people to take the next step, some of which will eventually decide to matriculate.\nWe may see a temporal shift whereby some prospectives wait to save money and lower their debt burden before choosing to attend law school. This may create a dearth of applicants in the short term as more prospectives leave the applicant pool. In the long term, however, temporally-shifted prospectives may be more inclined to pay full tuition. We may see a spatial realignment whereby prospectives choose to go based on the new information. For example, 251 For a discussion of anchoring effects, see Amos Tversky & Daniel Kahneman, Judgment Under Uncertainty: Heuristics and Biases, SCIENCE, 185, 1124-1130 (1974).\nstrong regional schools may see their student-body quality increase as prospectives who wish to practice in that region stop traveling across the country to attend a law school ranked a few spots higher by U.S. News. Or we might see more prospectives accept a scholarship at a less reputable school due to greater precision with their expected earnings after graduation.\nProspectives may collectively punish law schools for which the gap between actual and advertised job prospects was the greatest. If prospectives interpret the information as proof that some law schools were previously intentionally misleading prospectives, the market may punish those schools in the same way consumers punish perceived irresponsible corporate behavior. 252 Such punishment may be severe, perhaps resulting in a sharp enough decrease in student quality at some schools that they risk losing ABA accreditation. 253 More likely, however, decreases in student quality at some schools may see changes in their U.S. News rankings.\nAdaptation by the schools and competition for applicants may increase as schools try to find new ways to improve their post-graduate outcomes. This could mean reducing or increasing class sizes, lowering or increasing tuition, changing the geographic makeup to improve overall job placement, and allocating resources away from or towards employer recruitment and career services. Which direction each school will move is up to respective administrations and demand.\nMany of these potential changes are market corrections that, in the long run, will help ensure that prospective law students will be better financially prepared, make better decisions about which law school to attend based on individual career goals, and three years later be more willing to accept their job prospects as being commensurate with expectations. While we expect some  to different economic climates. In this sense, even the lists for relatively bad years are useful. We hope that collecting this data now will make it easier to tell prospectives the legal profession's story.\nThe above discussion hypothesizes what may result from a new standard in which all or many ABA-approved law schools complied fully. We recognize it is unlikely that every law school will see to it to comply fully with the new standard. Because of this, we expect -and in fact aim to encourage through discussion and public involvement -that the impact will be worse for any noncompliant programs. Schools that refuse to comply with a new standard may see competitor schools receiving \"transparency premiums\" in the market, while the noncompliant schools risk losing top applicants who distrust a lack of openness about employment reporting. 254 It is also possible (though we think unlikely) that the converse emerges: noncompliant schools may benefit by continuing to overemphasize top performers in the graduating class, while the schools who attach their names to less lucrative or desirable postgraduation outcomes suffer the brunt of the backlash. 255 Permitting open access to this information also raises some of the criticisms explored in Part II. One major concern is how we can ensure that people do not use the data and information in a manner that distorts or undermines the data. By offering free data we encourage others to develop derivative tools without direct oversight to ensure those tools are crafted fairly. What if these derivative tools overshadow the data we present, exacerbating the ongoing problems presented in this Article? Clear guidelines and explanations, along with an accessible and active staff will go a long way to stave such misuse, but the solution must be a community effort.\nThe alternatives -restricted access to data and information or too strong of oversight of how this data and information are used -would go against the purpose of this new standard. It would continue producing information asymmetry in a market that already sees tens of thousands of people each year potentially making uninformed financial decisions. Providing open, uninhibited access to information is the only way to fulfill Justice Brandeis' mandate that \"the disclosure must be real. And it must be a disclosure to the investor.\" 256 254 This \"transparency premium\" is already occurring at schools that have been first movers in improving access to employment information. E.g. posting of thickfreakness to TLS, http://www.top-lawschools.com/forums/viewtopic.php?f=1&t=113124&start=25 (April 5, 2010 7:51 a.m. (\"I think that's probably why I'm trying to pick between Duke and Vandy right now. To me, the commitment to transparency and reality with career services, faculty, etc. is one of the most important factors in a school (besides cost, yikes!). The situation's never going to be flat-out bad at either school, so why be deceptive, especially when most people who do a few minutes of casual research can figure out that most of those salary figures are bogus anyway?\") 255 See, e.g., posting of thickfreakness, supra note 240. 256 BRANDEIS, supra note 6, at p. 106.\n---\n\n\nprospectives is still desirable after considering the interests of all stakeholders. We showed in Part I that, insofar that prospectives want to be informed consumers, prospectives would benefit from having more access to employment outcomes, both for deciding which school is the best fit among their acceptances and to measure the level of financial risk they assume with each choice.\nBut what about other parties involved with the law school game, such as alumni or current students of a particular law school, whose present and future job options at least partially depend on how their school fares in the job market? What about the schools themselves, as competitive as they must be in recruiting the most academically gifted students each year, in maintaining close relationships with historical employers, and in raising funds from their alumni to continue shaping the nation's lawyers in a pedagogically responsible and U.S. News-friendly manner? And what arguments might legal employers have against a new regime in which employers do not have control over the level of disclosure regarding their hiring practices? Might the legal industry -or certain subsets of it -object to a departure from current public conception that paints a fairly uniform picture of what the majority of new lawyers do for work? And, even if all of these parties were to somehow agree to grant more access to employment information for prospectives, who would implement the standard and ensure accurate reporting?\nHere we explore potential arguments against greater transparency. Perhaps the following arguments will show that the overall costs to stakeholders outweigh the benefits for the same stakeholders, including the nation's would-be lawyers. We sort these criticisms according to who might present them. Critics include law schools, current and recent students, individual employers, and the legal community and society at-large.\n---\nA. Law School Transparency\n\nWe argue that a nongovernmental organization can successfully establish a new employment standard by creating a new post-graduation tool. To do this, Law School Transparency will follow five major operating principles.220 First, LST's new standard will balance all stakeholder interests, rather than focusing only on the interests of prospective law students. The components of this standard flow from our discussion in Part II. We undertake to eliminate components that unfairly infringe on stakeholder interests. Second, LST will freely provide all data and information on a website in an accessible form. The form will permit unrestricted use for everybody, including those wishing to develop derivative tools. Third, LST will incentivize law school cooperation with a certification mark that schools can use to certify that the data they provide meets the standards set by LST. Fourth, LST will facilitate and encourage participation by prospectives, current students, and recent graduates in order to provide for an ongoing public audit of the school-provided information to further encourage compliance and focus public scrutiny on reporting abnormalities. Fifth, LST will open a public dialogue between prospectives, law school administrators, and the general public to assist in a growing, collective effort to improve knowledge about the legal profession and the type of preparation prospectives should engage in prior to making the decision to start a legal career.\n220 The components such a tool would include will likely depend on the authority and market power of the enabling mechanism. The ABA is a regulatory organization that accredits schools based on certain standards. Should the ABA determine the current standard is unsatisfactory, they could simply demand more or less transparency, regardless of school objections. In the alternative, some schools could band together because they might see greater disclosure as a way to gain a competitive advantage in the market for prospectives. This could create a race to the top with fluctuating levels of transparency. We take the ABA's inaction on the issue and law school competition as indication that these two enabling mechanisms are an unlikely source of improvement, at least for now.\n---\nB. Why Law Schools?\n\nThe first question we must ask is who is going to be responsible for collecting the data and information about each graduate. Compromise is the goal of the new standard, taking into consideration the complex relationships of all stakeholders. P's ideal tool would seamlessly gather all information, but in reality someone must put in the time to track down each of the 44,000 or so individuals who graduate from ABA-approved law schools each year. 221 Our optimized tool aims to describe the outputs from each school, showing where the graduates go every year and how much value they received from their degree.\nThe responsibility should fall to the schools given that they are in the best position to collect and distribute data and information about where their students go. 222 To reduce compliance costs, our standard requests data about every graduate, as of nine months from spring graduation, from a particular period (such as the graduating Class of 2010). This aligns our deadline and period with the ABA, NALP, and U.S. News requirements (February 15th of each year). 223\n---\nC. The LST Standard\n\nWhat data should schools actually provide? This is where the need for compromise reveals itself. The most serious handicap of the current standard revealed by our analysis of existing tools in Part I is that it allows outcomes to be hidden in aggregate statistical forms. The most important qualities of the new standard will ensure that it doesn't. The new standard envelops all graduates, includes at least some data about every individual, and is the same across 221 Class of 2008 National Summary Report, supra note 8, at 1. 222 See supra Part I.C.1. 223 Id. schools, and somewhat important for hiring at certain jobs. The Job List does not require the journal name. It simply requires that schools designate whether the graduate was on a primary journal, secondary journal, or no journal. While multiple journals could qualify as secondary, each school will only be able to count one journal as the primary journal. \n---\nSalary (Salary List)\n\nTo be sure, the Salary List is the product of particularly reluctant compromise. We strongly considered a solution that ties specific salaries to the Job List components (i.e. including the salary component on the Job List). The benefits to prospectives would be immense, but schools may credibly claim that this amounts to a violation of graduates' privacy and risks school-employer relationships. While it does seem that discussing salary is less taboo than it once was, some will consider salary a private matter. For those salaries the NALP Directory lists, this is not much of a consideration. However, for the sizeable percentage of employers who do not openly share starting salaries, we expect schools to claim that sharing this data would jeopardize relationships with employers. 235 Rather than biting the bullet with these criticisms as collateral damage, we want to push the standard as far as we can without running into similarly credible and useful criticisms, while still enabling prospectives to make informed decisions. Of all the components, including salary information may be the most important. The problem prospectives face is largely a financial one; tuition is high, debt is high, and expected starting salaries are murky at best. Salary must come 235 Associating jobs with salaries may also cause some graduates to stop reporting. This would reduce the quality of salary information already available and hurt NALP's studies of the legal profession as a whole. into play in some fashion, but we must also consider the legitimate privacy concerns of individual students and smaller employers who do not already publicize starting salaries through NALP registration or other methods. Although salary is a necessary component in calculating risk and determining appropriate debt loads, it is less clear that each salary must be tied in with other identifiable information. In other words, P may be adequately informed with a list of all starting salaries without knowing that an individual graduate made $65,000 at Law Firm Z to make an informed decision.\nRepackaging the current standard for wide scale use is a helpful start in our task of fairly pushing the Salary List as far as we can: Including the office location component serves an even larger function by facilitating cross-regional comparisons. For example, P might want to know who is better financially better off among graduates of two schools that predominately place their graduates in different regions.\nIf P only knows the salary, P will not know how far each dollar will get her without knowing what kind of cost of living adjustment to apply. Because cost of living adjustments vary within regions and even states, honing in on the city will provide the best vehicle of comparison.\nKnowing where each graduate works with some degree of specificity is necessary to determine the relative financial burden P will face. A $40,000 job in Brownsville makes it much easier to pay off loans than a job with an equal salary in Houston. 237 Cost of living and quality of life indices often help shape the personal choice about where to live, particularly for an ever-mobile citizenry. This will benefit many schools handicapped by regional market pay bases by allowing 237 We compare Brownsville cost of living to Houston rather than an obviously more expensive city like New York because both cities are in the same state and \"region\" according to the ABA questionnaire, NALP survey, and U.S. News survey.\nthink they should. Some prospectives already use their position to pressure schools. They share information to help each other negotiate scholarships, gain acceptances after being waitlisted, and determine 'feelings of fit' for those who are unable to visit prior to matriculating. 249 LST can use similar information-sharing tactics to harness the collective bargaining power of prospective law students and signal to them that certain schools comply, while others do not. To do this, LST will provide law schools the opportunity to use a certification mark that represents to the world that they meet the heightened standard. While the mark in no way certifies the quality of the school or the school's post-graduation outcomes, it does certify that prospectives have more access to information than under the ABA and U.S. News standards. As owners of the mark, LST retains the right to set a standard that, if met by schools, permits them to use the mark. That is, LST will provide licenses to law schools to use the mark in return for compliance with the standard. When a law school uses the mark on their website or in their print materials it signals full compliance with the standard. The license will require a warranty to prospective law students guaranteeing that the information law schools provide is complete and accurate -the same guaranty that law school deans make to the ABA each year for the annual ABA questionnaire. The license will clearly outline how a school may pass certification, how a school may lose certification, and the qualified uses of the mark.\nIn order for a certification mark to have the intended effect, prospective law students will need to know to look for the mark, and be moved to inquire why law schools have not undertaken the heightened standard. In part, this means empowering prospectives to believe that they can make a difference. We believe they can. By housing the information on a website that 249 Some prospectives search www.LawSchoolNumbers.com for scholarship awards and use this information as a baseline for scholarship negotiations. Scholarship Negotiation?, http://www.lawschooldiscussion.org/prelaw/index.php?topic=4017857.0 (last visited Apr. 6, 2010).\n---\nThis Is Not a Conclusion\n\nAlthough no socially optimal standard would provide access to all the input data that prospectives value, we have identified one way to create a low-cost tool that would provide open access to valuable employment data and information. A way forward may also come about through other ways that need to be explored further. Notably, regulatory reform or collaboration among ABA-approved law schools could help prospectives in ways a market-based mechanism cannot.\nWe hope that by enabling this new tool and facilitating the creation of derivative tools aimed at transparency, LST brings about significant, long-term change in legal education. Given the enormous individual costs thousands of prospective law students face each year, students, regulators, and administrators need to dedicate more time to engage in this debate. This Article emphasizes the need for greater collaboration along what the MacCrate Report called \"the education continuum,\" where prelaw advisors, prospectives, law schools, and employers could all participate in determining a truly optimal level of employment transparency. It represents our first attempt to engage academics in this debate and include them in the continuum, while we move forward with our actual attempt at improving the standards of transparency in American legal education. More work needs to be done, to be sure, but we are on our way forward.",
        "\n\nproblems\" that are too large and multifaceted for any one expert, or single organization, to fully comprehend (Morgan & Fitzgerald, 2014).\nSolutions to these kinds of problems require coordinated action between people of widely different backgrounds and perspectives to effectively change systems. In other words, networks are needed (Plastrik, Taylor, & Cleveland, 2014). In this article, we draw on examples from our professional experience and the literature to make the case for networks as the future of Extension's work as it pertains to improving systems of the 21st century. We propose that networks are key because (a) networks represent a contemporary mode of organizing data and people, (b) networks change systems, and (c) networks counteract the centralization of power in society.\n---\nNetworks Are the Future\n\nA network is simply a \"group or system of interconnected people or things\" (Lexico, 2019, \"network\"). One need not look far to find examples of how networks are the dominant organizational structure of the 21st century. The Internet and social media are obvious examples that demonstrate how networks are disrupting the command and control, vertical flow of information emblematic of the expert model-the educational mechanism land-grant universities have traditionally relied on. What may be less obvious are the ways that nonprofit organizations and educational institutions are shifting away from hierarchy and buying into the network mind-set through peer-to-peer learning, interorganizational resource sharing, and innovations related to the open-source concept, such as the massive open online course (Kop, Fournier, & Mak, 2011).\nIn their 2014 book Connecting to Change the World, Plastrik et al. (2014) made the case for networks as an organizational survival strategy in this brave new world where resources such as time, money, raw materials, and mental bandwidth all seem to be diminishing and duplicative programs are impossible to sustain.\nPerhaps tired of hearing refrains such as \"Why call Extension when you can ask Google?,\" Extension professionals also are beginning to embrace network building and other facilitative practices as the future of our institution (Morgan & Fitzgerald, 2014). Furthermore, funding and staffing constraints affecting both Extension and our organizational partners often require us to serve better and deliver more by being more efficient and resourceful. To meet these demands we need a different approach, one that networks can provide.\nWe work with dozens of networks across the state of Minnesota that bring together diverse groups of people to positively influence the food system. This work has burgeoned from a growing recognition shared by funders, partners, participants, and staff that a focus on educational strategies to address nutrition is not enough to reverse growing rates of diet-related chronic disease (Morgan & Fitzgerald, 2014). Teaching evidence-based nutrition curricula, demonstrating proper knife skills, and inspiring children to garden are all important ways to create a culture of healthful eating but do little to alleviate many of the causal factors of hunger or obesity related to social determinants of health (Morgan & Fitzgerald, 2014). Furthermore, these kinds of direct-service programs reach only a small proportion of the population that could benefit from them, and large increases in funding for nutrition education programs to expand their reach likely is not on the horizon (Morgan & Fitzgerald, 2014).\nNetworks offer a way for organizations such as Extension to be nimble, adaptive, and strategic with limited resources (Plastrik et al., 2014). By applying skills possessed by many Extension professionals, such as meeting facilitation, agenda development, and strategic relationship building, we can be at the forefront of network development and leverage this role to be more innovative. This concept is especially important in the work related to policy, systems, and environmental change that is becoming more prominent in Extension's wheelhouse. By definition, this shift requires our work to focus on changing embedded policies, complex and interconnected systems, and structural environmental barriers. Our own experience with networks has shown that they allow us to both address long-term issues and enact immediate short-term solutions.\nBy serving as core members of Cass Clay Food Partners for more than 8 years, Extension staff from the University of Minnesota and North Dakota State University, including members of our author group, have engaged in food systems work in new ways. The regional network aims to improve the food system by advising local policymakers, engaging the community in cocreating a regional vision, fostering connections, and reducing duplicative work across a myriad mix of businesses, nonprofits, faith-based organizations, government entities, educational institutions, and individual community members.\nCass Clay Food Partners has recently applied a network approach to address on-farm food waste through creation of GleaND, a volunteer-based program that redirects local produce that otherwise would be wasted to emergency food banks and pantries (GleaND, n.d.). This program grew from an assumption that the problem of food waste in farmers' fields was rooted in a labor shortage during peak harvest season and thus warranted a different approach than other strategies, such as consumer education campaigns or local food policy initiatives. Although there are potential long-term policy solutions, such as surplus produce reimbursements and farmworker protection and wage laws, these take time to enact and are not always relevant at a city or county scale. Because the network was not bound to one particular program or policy initiative and because it could leverage the collective wisdom and creativity from a diverse group of people representing different parts of the food system, Cass Clay Food Partners conceived of and rapidly implemented a solution presumably faster than one organization could have done alone. In a world where technological and communication systems continue to evolve at an ever faster rate, networks that can adapt quickly and use these tools effectively will be essential for organizations seeking to stay relevant and innovative in the face of future challenges.\n---\nNetworks Change Systems\n\nOur current offering of Extension programs that focus on direct education are insufficient to solve \"wicked problems\" or \"grand challenges\" that are systemic in nature. The food system is a prime example of why it does not work to simply target one component of an issue in isolation. Efforts to teach a family healthful eating will not work if that family cannot purchase nutritious foods in a convenient way at an affordable price. On the other hand, handing out free kale to parents who are unfamiliar with how to prepare the vegetable so that it tastes delicious to their children is also not an adequate solution. Chronic disease and other systemic health disparities warrant a networked approach because of the unknown nature of the problem and the unpredictable nature of solutions (Provan & Lemaire, 2012). Diverse perspectives are essential to develop innovative solutions, to check assumptions, and to increase capacity through networks to experiment with creative approaches while spreading the risk across organizations. In other words, networks avoid the pitfall of putting all the eggs-the risks and rewards of innovation-in one organizational basket.\nWithin University of Minnesota Extension, Supplemental Nutrition Assistance Program Education is a prominent nutrition education and obesity prevention endeavor. In the last 5 years, the federally administered program has increasingly emphasized efforts to change policy, systems, and environments to support healthful eating and active living. During this time, our partnerships have played a key role in harnessing the power of our networks to bring about systems change initiatives. The Metro Food Access Network (MFAN) includes over 400 food justice advocates in the Twin Cities and was created (and is facilitated) by Extension staff and the network's membership. Started in 2012, MFAN has had a collective impact over time that exemplifies the ways Extension professionals can leverage the resources and collective skill set of an effective network to tackle complex issues such as racial equity and systems change.\nMFAN has achieved systems-level results in three significant ways. First, through a comprehensive planning action team, the network supported local units of government to include food access language in their comprehensive plans. By including this language, multiple local governments across the metro region began to formally operationalize efforts to increase food access in their areas. Second, through MFAN, hunger action team relationships among a diverse group of cross-sector partners led to the creation of an intervention for increasing healthful food options and improving client experience at food pantries. This novel approach, funded by a $3 million grant from the National Institutes of Health, has resulted in food pantries increasing their ability to distribute healthful food and transforming their environments into dignified, welcoming spaces that center on the choices of clients. Lastly, MFAN hosted a series-Critical Conversations on Race-to help network members learn about and address the deep connections between racial inequities and health disparities. This initiative led to increased understanding among partners that racism is a system that infiltrates every aspect of food access (Metro Food Access Network, 2018; Minnesota Department of Health, 2014) and prompted network partners to begin addressing issues of bias and promoting inclusion within their own organizations to better address racial disparities in food access.\nEach of these three examples effectively changed a policy, environment, or system in a way that also bolsters existing direct education efforts to reinforce positive behavior change.\n---\nNetworks Decentralize Power\n\nPerhaps the greatest hallmark of 20th-century systems in the United States was the process of centralization. Whether the monetary system, the political system, the food system, or any other, the tendency was for power and decision making to become concentrated in the hands of fewer and fewer individuals, organizations, and businesses (Grusky & Hill, 2018). This produced many benefits, such as the efficient and cheap distribution of food all over the globe (Hendrickson & Heffernan, 2002). Centralization also has created many challenges, particularly in efforts to promote a more just and equitable allocation of resources (Kania & Kramer, 2015). In the 21st century, the resistance to centralized power is readily apparent in movements such as Occupy Wall Street, in the advent of cryptocurrencies, and in the emergence of the sharing economy (Geissinger, Laurell, & Sandstrom, 2018).\nNetworks, by nature, defy the traditional organizational hierarchies that fundamentally reinforce centralized power structures (Plastrik et al., 2014). Networks empower individuals to work in nonhierarchical and nonlinear relationships across organizations (Plastrik et al., 2014). These relationships create power through social capital, leading to collective action when members feel closely connected to one another (Kania & Kramer, 2011). These relationships, in turn, support the transition to a less competitive environment that\n---\nFeature\n\nThe Art and Science of Networking Extension JOE 58(2) \u00a92020 Extension Journal Inc.\nspawns resource sharing and creativity (Provan & Milward, 1995). Networks also foster a sense of safety and a place for members to exchange new ideas and take professional risks beyond what they might experience within their own organizations, thus becoming an ideal place for the diffusion of ideas and innovation (Plastrik et al., 2014).\nAt University of Minnesota Extension, participatory grant making is a process used for making democratic, decentralized decisions about how funding is allocated (Gibson, 2017). Participatory grant making is fundamentally about the implicit creation of networks. Individuals give funds across organizations, naturally identifying synergies and breaking down competitive barriers as part of the process. We have executed participatory grant making more than five times: externally with food networks in Minnesota and internally through Extension's Issue Area grants and in allocating funding to support individual projects and staff development within the health and nutrition program area. Evaluation data from these efforts have shown, like other studies (Gibson, 2017), that participatory grant making is an effective strategy for increasing collaboration, promoting mutual investment and respect between individuals and organizations that might otherwise feel highly competitive with one another, and potentially creating better outcomes than a traditional model devoid of peer-to-peer interactions between grantees.\n---\nNetworks and Extension\n\nNetworks are powerful social structures that can exponentially increase the reach and impact of our work in Extension. Networks are also complicated and unpredictable. Network theory points toward many characteristics of networks that influence outcomes (Turrini, Cristofoli, Frosini, & Nasi, 2010), but Extension professionals working with networks must balance the science of networks with the art of navigating complex and dynamic social environments through skilled facilitation.\nEvidence suggests that networks can be optimized by paying attention to the context (network stability, availability of resources such as time, funding, and personnel), structure (network size, constitution of members, guiding rules and practices), and function (how network structures are being managed and whether they are improving over time) (Turrini et al., 2010). Support such as technical assistance and capacity-building resources can be more influential on network outcomes than simply the presence or absence of grant funding (Provan & Milward, 1995;Turrini et al., 2010). Therefore, Extension staff who can dedicate time and expertise to support network operations through skills such as planning, facilitation, coordination of shared work, and evaluation of impacts are incredibly valuable assets to networks. There are many network evaluation frameworks in the literature that can be applied to assess network effectiveness and determine whether structural or functional changes are needed (Provan & Lemaire, 2012;Termeer, Drimie, Ingram, Pereira, & Whittingham, 2018;Turrini et al., 2010). Whichever framework is used, network principles of trust and transparency guide how a network evaluates its contextual, structural, and functional components.\nWorking with communities in an authentic, relationship-based way is an art as much as a science and is the essential approach to working effectively with networks. The most successful examples start with Extension building trusting partnerships that engage all people who have a stake in the issue at hand. Relationship building is the foundation of a successful network and therefore the foundation for successful Extension programming. A critical component of these partnerships is ensuring that stakeholders who have less positional power or who have been historically marginalized or overlooked are reflected in the network.\nTaking the time to engage diverse partners and building trust with them is critical to long-term network success, as Adrienne Maree Brown suggests in Emergent Strategy: \"[We must] move at the speed of trust\" (Brown, 2017, p. 41). We also have found the triangular framework outlined by Curtis Ogden (2014) to be a powerful tool for understanding the natural progression of network activities. This framework places connectivity (i.e., the relationships between members of the network) at the base of the triangle because such relationships must be solidified before the next level-alignment of values and goals-can be achieved.\nOnly when connectivity and alignment are in place can members effectively join together to implement concrete actions-the pinnacle of the triangle.\nAnother artful concept that Extension educators must practice is \"network weaving.\" Developed by June Holley (2012), network weaving occurs when leaders intentionally help networks become more integrated through increased connections among members as well as between members and external partners.\nExtension educators often intuitively engage in network weaving simply by acting as a supportive connector between partners. Network weaving takes this intuitive relational practice and elevates it into a key role that Extension educators can play within networks to improve their function and impact.\nA key way to implement network weaving is through facilitation practices that encourage relationship development. At University of Minnesota Extension, we have implemented a highly successful series of 2day workshops called Cultivating Powerful Participation that have equipped staff and community partners to facilitate dialogues on collective action that can address food justice. Evaluations from the workshops have shown that Extension staff from differing units and roles leave with the skills they need to take on the complex work of network building and weaving. By engaging in both the science and art of building and strengthening networks, Extension professionals can reduce organizational silos and duplication and exponentially increase the reach of our work to effect systems change. In particular, we can harness our unique niche straddling university research and community engagement to hone a new area of expertise in network facilitation.\nOur final example of the University of Minnesota Extension's network leadership is the Farm to School Leadership Team's 2016 evaluation report. The report's findings included a compilation of internal challenges and critiques that network members identified during anonymous interviews. Extension staff leaned into 5 years of leadership and relationship building within the network to call on members to openly address the evaluation findings of dynamic tensions aired in the interviews, a strategy that allowed network members to repair relationships and reengage with the purpose of the network. This experience enabled members to build trust with one another and create a common understanding not only of what the network had achieved together but how to make the needed improvements for the network to continue to be effective. Like the Cass Clay Food Partners and MFAN examples, the Farm to School Leadership Team demonstrates how Extension professionals can leverage their role as network leaders to foster collective action, which in each of these cases led to a more healthful and just food system through multiorganizational efforts.\n---\nConclusion\n\nIn the years to come, Extension will be expected to continue to adapt methods to effectively reach our audiences. We will be expected to take on roles beyond that of the expert of the past in order to become the change agents of the future. Direct education has been, and will continue to be, a core element of Extension programming, and can be enhanced through the work of networks and their ability to effect systems change. Now is the time to engage in this complex, collaborative, and political work if we are to continue to be a trusted institution that people look to for guidance in a digital era where there is an endless array of places to turn. If we are going to be successful, we will need to embrace both the art and science of networks.",
        "Introduction\n\nOver the past decades, with the expansion of women's participation and completion of tertiary education, women's ability to access professional occupations and managerial positions in labour markets has also increased [1,2]. Yet, despite these improvements in education, women continue to be underrepresented in technical and managerial occupations requiring specialised, tertiary-level education. Based on the most recent statistics available from the International Labour Organization (ILO), women comprise 44.8% of those in managerial, professional and technical occupations, compared to a male share of 55.2% [3]. Looking at senior positions, the gender gaps are even larger. In the majority of the 67 countries with data from 2009 to 2015, fewer than a third of senior-and middlemanagement positions were held by women [4]. These gender inequalities emerge from a complex interplay of factors, such as differences between men and women in human capital and fields of study, gender discrimination and negative stereotypes associated with women in professional roles, challenges in combining work and family life that disproportionately disadvantage women, limited access to social capital, networks and resources for professional advancement, and gender differences in values and interests [5][6][7][8][9][10].\nReducing gender inequalities in economic, social and political domains is essential for the attainment of global sustainable development. The promotion of gender equality features prominently in the United Nations' (UN) Sustainable Development Goals (SDGs), a key instrument in setting the agenda around global development, both as a standalone goal (Goal 5) as well as in relation to other goals (e.g. access to education) [11]. Target 5.5 within SDG 5 on gender equality emphasizes women's economic empowerment and recognises the importance of ensuring \"women's full and effective participation and equal opportunities for leadership at all levels of decision-making in . . . economic . . . life\" [12]. Measuring and understanding gender gaps in skilled, technical and managerial occupations is integral to monitoring progress on SDG 5. This paper analyses global professional gender inequality by leveraging aggregated information about LinkedIn's user population, available through its advertising platform, as a type of 'digital census' of this online population. The rapid uptake of social media sites such as LinkedIn over the 21st century has generated large and diverse online user populations across different platforms, and these online populations have in turn generated new types of social data through the 'digital traces' they leave behind on these platforms. A growing body of work in the area of digital demography [13][14][15][16] has sought to understand the demographic characteristics of users on different social media platforms, analyse their demographic representativeness, and examine their potential to study gender inequality [17][18][19][20][21][22]. In a complementary vein, with the call for a 'Data Revolution' in the context of the post-2015 global development agenda [23,24], a growing body of research has sought to examine the potential of non-traditional big data sources for measuring different indicators and social processes linked to the SDGs. This body of research has used diverse types of big data sources, including mobile call log data (e.g. [25,26]), night-time satellite data (e.g. [27]), web and social media data (e.g. [18,22,[28][29][30]), to assess their utility for providing cost-efficient, timely, and more granular coverage to complement traditional data sources such as surveys or censuses. These data sources offer added value in the context of low-and middle-income countries where traditional data sources are often lacking, incomplete or outdated [31]. In relation to SDG 5 on gender inequality, recent work has used social media advertising data from Facebook and Google to model indicators linked to internet and mobile gender gaps [18,22]. Our study builds on this work by examining social media advertising data from LinkedIn for gender inequalities in the professional domain.\nLinkedIn is the world's largest social networking platform targeted at professionals with a user base of over 700 million spanning over 200 countries. The site is used for jobseeking, recruiting, networking and marketing, and its mission is \"to connect the world's professionals to make them more productive and successful. \" 1 As technical, professional, and managerial occupations in the world of work have become virtually mediated, social networking sites such as LinkedIn hold the potential to enable economic opportunity, provide new sources of professional information, and expand networks strategically for job opportunities and career advancement. Although research on the use of social media and more specifically on LinkedIn for economic opportunities and career advancement is still limited, emerging survey and qualitative evidence suggest that users do indeed experience some of these benefits [32][33][34][35][36][37].\nThe payoffs of easier access to professional information and networks through platforms such as LinkedIn have the potential to be larger for those populations who otherwise face greater barriers to access these resources, such as women and ethnic or racial minority groups [7]. Existing evidence from the US points to a greater role for the informational and network benefits of internet and social media technologies for groups who are disadvantaged in the labour market [38,39]. Studies focusing on low-income countries find that digital technologies have larger impacts on women for outcomes linked to health, well-being and economic opportunity, as women conventionally face greater barriers in accessing information and have smaller social networks compared to men [40,41]. For economic opportunities, networking behaviours can offer a useful strategy to break through the glass ceiling for women, although empirical evidence suggests that men are more likely to engage in some forms of networking, such as socialising after work [42], which especially disadvantage mothers with child-care responsibilities. In this context, online and more flexible forms of networking offered by platforms such as LinkedIn could help overcome some of the barriers experienced by women in the offline world. On the other hand, although online platforms promise to be open, flexible, and democratic spaces, existing literature also shows that women are often significantly underrepresented in online communities catering to more specialised or technical environments such as those of Wikipedia editors [43,44], StackOverflow [45] or GitHub users [46]. While these gender gaps in part reflect women's underrepresentation in technical fields such as software engineering, they also reflect factors such as gender biases within these online communities that disadvantage women, cultural or algorithmic features that discourage female participation and result in faster dropout among female users, and behavioural differences in the use of these platforms.\nBeyond country-specific studies from the US [21,47] and UK [48], or analyses focused on smaller samples for specific occupations [17,49,50], little is known about gender differences in the LinkedIn user population in a global, comparative perspective, as well how these gender differences on LinkedIn's user population vary across age, industries and seniority. More broadly in the research on social network sites, studies of LinkedIn are considerably fewer than those of Twitter or Facebook [48]. Similarly, while the growing body of research in digital demography has often drawn on aggregate audience estimates from social media advertising platforms to analyse these online populations, most of these 1 https://about.linkedin.com/?trk=homepage-basic_directory_aboutUrl studies have used Facebook (e.g [30,51,52]) or more recently, Google advertising (Ad-Words) audience estimates [22]. In contrast, LinkedIn's advertising audience estimates have so far only been used to study professional gender gaps for a selection of information and communication technology (ICT) industries [17], and for a selection of cities in the United States [21]. Both these studies point to the potential value of these LinkedIn data for studying socio-demographic phenomena.\nThe first objective of this paper is to analyse gender gaps on LinkedIn's user population by computing different country-level LinkedIn Gender Gap Indices (GGIs) by age, industry and seniority to examine how gender gaps manifest themselves across different characteristics on this online population. Our second objective is to compare and validate the LinkedIn gender gaps against ground truth indicators of country-level professional gender gaps derived from nationally-representative labour force surveys, available via the International Labour Organization's (ILO) Statistical Database (ILOSTAT), to examine the feasibility and biases of predicting ground truth measures using LinkedIn's online population. The novel data from LinkedIn cover a large number of users across the world and show wider geographical coverage than the data from ILOSTAT, including better coverage in low-and middle-income countries where data on these indicators are often limited or lacking. Nevertheless, gender gaps on LinkedIn are those measured on an online social media population and the biases of this social media population are not properly understood. By comparing against ground truth data, our analysis enables us to address whether online gender gaps on LinkedIn broadly reflect professional gender inequalities in the labour force across countries, or whether gender gaps observed on LinkedIn reveal unique gender biases and selection effects, and if so, how these are patterned.\n---\nData sources and methods\n\n\n---\nDataset\n\nFor our study, we build a dataset of country-level indicators derived from different sources, including those from audience estimates obtained from LinkedIn's advertising platform, labour force indicators obtained from ILOSTAT, as well as other indicators linked to global development (e.g. Human Development Index), ICT penetration (e.g. internet penetration), and gender equality in different domains (e.g. in educational attainment or internet access). We describe these different indicators below.\n---\nLinkedIn ad audience estimates\n\nWe rely on a type of digital trace data called 'advertising audience estimates' , which are available for all large social media platforms such as Facebook, Google, Twitter, LinkedIn, and others [31]. The basic idea behind these data sources is similar across all platforms, even though the types of estimates provided varies across them. For example, while Facebook's ad platform provides counts of monthly or daily active users, Google AdWords provides ad impression estimates (i.e. the number of times an ad would be seen) rather than an estimate of users [22].\nPotential advertisers on online platforms can specify a desired audience for their ads based on targeting criteria, such as gender, age, geography, and other characteristics. This serves as a kind of real-time digital census over the user base of the considered platforms, providing aggregate answers to questions such as \"How many female Facebook users aged 18 years of age live in Nigeria?\" Ad audience estimates from Facebook and Google have been leveraged previously to model digital gender inequality indicators linked to internet access and mobile access gender gaps, which are targets within SDG 5 on promoting gender equality [18,22,53]. These studies motivate the assessment of ad audience estimates from other platforms to model gender inequalities in different domains. This paper considers indicators within SDG 5 linked to women's economic empowerment.\nIn contrast to Facebook or Google, which are platforms of broader appeal, LinkedIn is the world's biggest social networking site targeted at professionals. The platform is used for job-seeking, recruiting, networking and marketing. LinkedIn allows users including workers, employers or recruiters, to create a profile based on their professional affiliation and connect to professional contacts within and outside their professional networks. Users turn to LinkedIn for different reasons, including professional self-promotion, accessing information about career opportunities, organizations and professional development, strategic networking, and communication with professional contacts [32,33,35,37,[54][55][56]. Similar to other social media platforms, one of the key aspects in LinkedIn's business model is to make revenue through targeted advertising offered on the platform through its ad campaign manager. 2 Audiences can be targeted on the basis of geographic location, demographic criteria such as gender or age group, and job criteria such as company industry or job seniority. Before advertisements are launched and shown to specific target audiences, the advertiser is shown an approximate number of how many individuals match the targeted audience. For example, at the time of data collection there were approximately 1.3 million male directors in the United Kingdom on LinkedIn. This feature allows us to collect data from a sizable population, which are available in real-time and capture about 17% of all \u223c4 billion Internet users [57].\nTo build our dataset, we collected aggregate counts on numbers of LinkedIn users by querying the ad campaign manager via its application programming interface (API). The data were collected over February-April 2019, which corresponds approximately to the year with the most recently available labour force surveys (2019, followed by 2018) in the ILOSTAT, from where we obtain our labour force ground truth indicators (see next section). Using these aggregate counts, we generate different gender gap indicators of the LinkedIn population at the country level for as many countries for which these estimates were available. Previously, LinkedIn advertising data have been used to examine variation in global gender gaps in the ICT sector [17] and professional gender gaps in 20 US cities [21]. Both these studies find gender gaps disfavouring women on LinkedIn, but also document significant variations in these gender gaps by different characteristics. For example, [21] find that gender gaps in US cities vary by age with more men relative to women at older ages, and considerably by industry with education being female-dominant and construction being male-dominant. [17] find that when looking only at users within the information and communication technology industry on LinkedIn, women are significantly underrepresented compared to men, but that even within ICT, variations in sub-fields exist with male-bias being stronger in hardware industries compared to software.\nWe compute gender gaps on LinkedIn in terms of a gender gap index (GGI), which is defined as LinkedIn GGI = Number of females on LinkedIn with characteristic Number of males on LinkedIn with characteristic\nCharacteristics for which we collect data in this study are countries, age groups, company industries, job seniorities, job function and field of study, which are always disaggregated by gender. For example, the most general LinkedIn GGI for a given country would be the LinkedIn overall GGI, which is defined as the ratio of the number of females to males where the data obtained are by gender and country. The LinkedIn age 25-34 GGI, for example, would be defined as the ratio of the number of females aged 25-34 to males aged 25-34 in a given country. This definition of a gender gap as a female-to-male ratio is akin to the gender gaps as defined in other global indices such as the Global Gender Gap Report [58]. Our choice of this formulation of a gender gap indicator is also aligned with other work modelling digital gender gaps using social media advertising data [17,18,22]. We remove all observations for which the total audience count (men plus women) is less than 1000 to capture larger audience sizes. This is also similar to the approach applied in other studies using social media advertising data in which country observations with small audience counts are filtered out to ensure more stability in estimates and to avoid highly specialised features or categories [18,22]. As a result of this filter, observations are generally removed for countries with small populations (e.g. Bouvet Island, Cocos (Keeling) Islands, Cook Island, Falkland Islands (Malvinas), Montserrat, Svalbard and Jan Mayen, Wallis and Futuna, Tokelau, Tuvalu, Saint Pierre and Miquelon, S. Georgia and S. Sandwich Islands, etc.) and categories (e.g. specific language and literature studies (such as Khmer/Cambodian, Uralic or Ukrainian)) that are less represented on LinkedIn. Furthermore, the application of this filter does not change the observed correlations between the LinkedIn overall GGI and the three ILO GGIs that we describe later, because in the overall data the filtering removes only six countries for which the ILO GGIs were not available in the first place (i.e. Bouvet Island, Cocos (Keeling) Island, Heard and McDonald Island, S. Georgia and S. Sandwich Islands, Svalbard and Jan Mayen and Tokelau).\nFor any given country, we generally find that the larger the number of categories within a characteristic (e.g. field of study or company industry), the higher the number of zero audience counts in specific categories within the characteristic. In other words, data sparsity is greater in more detailed or specific categories. This is particularly the case for field of study (301 categories) and company industry (147 categories), where there are a considerable number of categories with zero counts, and to a smaller extent for job function (26 categories), job seniority (10 categories) and age group (4 categories). Gender was available in three categories -male, female and unknown -and we dropped unknowns when computing GGIs. Like with gender, unknown values exist for all characteristics, and these are dropped when computing any GGI. At the time of our collection, aggregate counts with non-missing gender and country information were available covering a population of 460.18 million users; with non-missing gender, country and age group information aggregate counts were available covering 165.02 million users; and non-missing gender, country and information on any industry (before merging these into more aggregate International Standard Industrial Classification of All Economic Activities (ISIC) 1 or Science, Technology, Engineering and Mathematics (STEM)/non-STEM categories) were available for 368.83 million users. Descriptive statistics summarising audience counts by gender for different characteristics as well as number of countries available for each are provided in Table 1.\nCompared with traditional data sources such as labour market statistics, the LinkedIn data offer a number of advantages. LinkedIn data cover a large number of users across a large number of countries, provide harmonised data across them (which makes crosscountry comparison easier) and offer the benefit of low latency. However, similarly to other social media ad audience estimates, the LinkedIn data also suffer from a number of weaknesses, such as issues of non-representativeness, limited metadata to understand the data generating process, and the potential for algorithmic confounding [31,59]. To better understand their strengths and limitations, we therefore compare and validate them against external indicators as described in the next section to better understand who we are capturing online on this platform.\n---\nInternational Labour Organization's Statistical Database (ILOSTAT)\n\nTo validate the LinkedIn gender gap measures, we compare them to three different ground truth measures from the ILOSTAT, namely the ILO professional GGI [3], the ILO total management GGI [60] and the ILO senior and middle management GGI [4]. The data available through ILOSTAT are derived from the most recently available country-specific labour force surveys for each country, which can vary between countries. The modal year for the latest available labour force surveys in the ILOSTAT database was 2019, followed by 2018. On average, high-income countries have more recent data coverage, whereas in low-income countries, the last available labour force survey is older. We choose these three measures because they are closely aligned with the highly skilled population on LinkedIn, and capture different dimensions of professional gender inequality. The three gender gaps are defined as follows:\nILO professional GGI = Number of females in level 3 or 4 skilled occupations (ILO) Number of males in level 3 or 4 skilled occupations (ILO) ,\nILO total management GGI = Female share in total management (ILO) 100 -(Female share in total management (ILO) ,\nILO senior and middle management GGI = Female share in senior and middle management (ILO) 100 -(Female share in senior and middle management (ILO) .\n(\n)4\nFor the ILO professional GGI, we only consider the highest International Standard Classification of Occupation (ISCO) skill levels 3 and 4 occupations because LinkedIn is mainly targeted at those in skilled and managerial occupations. Occupations classified as ISCO skill level 3 require complex and practical tasks as well as technical, factual and procedural knowledge, usually acquired during a 1-3 year degree following secondary education [61]. Skill level 4 occupations require problem-solving, decision-making and research skills as well as theoretical and analytical knowledge usually acquired during a 3-6 year degree in higher education. Both skill levels 3 and 4 require adequate numeracy and literacy as well as intercommunication skills. Among the three ILO GGIs in terms of data availability, country coverage is best for the ILO professional GGI, followed by the ILO total management GGI, and the worst for the ILO senior/middle management GGI.\nIn a similar way as for LinkedIn data, ILO data can be queried based on characteristics or strata like age group and economic activity, of which some are available by gender. In parts of our analyses, we further match age-specific and industry-specific LinkedIn indicators to age-specific and industry-specific ILO indicators to examine correlations between them. Note that these industries are referred to as \"economic activity\" in ILOSTAT. To do so, the ILO age group of 15-25 has been matched to LinkedIn's category of 18-24; ILO categories 35-44 and 45-54 have been merged to match LinkedIn's 35-54 age group; and ILO 55-64 and 65+ age groups have been combined to match LinkedIn's 55+ category. The industry-level matching between LinkedIn and ILOSTAT data has been done manually on the basis of International Standard Industrial Classification [62] sections (ISIC level 1) using matching data provided by [63].\n---\nOther development, ICT, and gender inequality indicators\n\nOur dataset also includes other indicators linked to economic or human development, ICT penetration, and gender gaps in educational and ICT domains derived from different sources such as the UN, World Bank and World Economic Forum. We use these indicators to examine the biases of our predictions of professional gender gaps derived from the LinkedIn measures, as we describe in more detail in the upcoming Methods section. For general measures of economic and human development we include GDP per capita [64] as well as the human development index (HDI) for females, males and both [65]. The HDI is a composite measure capturing education, economic and health dimensions of well-being for a country's population. For ICT penetration, we include levels of internet penetration [66], as well as online model estimates of gender gaps in internet access from the Digital Gender Gaps (DGG) project [53]. 3 The choice of these indicators is motivated by prior work that draws on web or social media data, and these help to inform our expectations about the nature of the bias we may expect in the predictions generated from LinkedIn data.\nAt low values of internet penetration and the human development index, we may expect the online population to be more selective and less representative of the entire population [67]. Internet penetration, however, has been shown to be gender-differentiated, and particularly in regions of South Asia and Sub-Saharan and Northern Africa, women have lower levels of internet access than men [18,22]. Internet access gender gaps may have implications for our gender gap predictions generated using LinkedIn data, although the direction of this bias is theoretically ambiguous. On the one hand, the fact that fewer women are online relative to men may lead us to overestimate professional gender inequality using LinkedIn data, as women are less likely to be online and may consequently use internet-related technologies such as LinkedIn less. On the other hand, existing literature also suggests that online selectivity can work in counter-intuitive ways. For example, Magno and Weber showed that in countries with greater offline gender inequality (e.g. Pakistan, Egypt), the online status of women as measured on Google+ was higher [68]. The authors speculated that this observed selectivity was due to a 'Jackie Robinson Effect' , akin to the scenario where female politicians perform better than male politicians as just performing equally would not suffice to get them to such a position in the first place [69]. Similarly, [70] found that the women with profiles on Wikipedia were likely to be more notable than men, suggesting an analogous type of glass ceiling effect where women had to pass a higher threshold to be captured on this online population. Whether similar types of selectivity also manifest themselves on LinkedIn remains to be assessed.\nTo further examine these links between online and offline gender selectivity, we include indicators linked to gender gaps in educational attainment, enrolment in secondary and tertiary education and the labour force from the Global Gender Gap Report [58], and gender gap indices in ICT and STEM education [71]. The bias observed in predictions from LinkedIn may reflect gender differences in the use and incentives to use the platform, which may depend on the levels of gender equality in the educational and economic domains. However, the direction of the bias is theoretically ambiguous. On the one hand, social media platforms promise to be open and inclusive spaces with the potential to enable content-sharing, expanded access to information and the opportunity to connect with wider networks for those who may lack access to these resources through conventional channels [33][34][35]37]. For example, women with children experience greater time pressures due to the dual burden of managing work with childcare responsibilities [42,72]. In this context, social media may provide more flexible models of networking than those that involve post-work hours socialising, which significantly disadvantage women with childcare responsibilities. The incentives and benefits linked to the use of digital technologies may consequently be larger for women, particularly those who may have limited opportunities to expand their networks in other ways, such as those in countries where their educational or economic attainment is poorer, and/or those in male-dominated fields [40,41]. This line of reasoning suggests that in countries where women's relative economic or educational attainment is lower, predictions from LinkedIn data may lead us to overestimate gender equality in the actual labour force.\nOn the other hand, studies of technical and specialised online communities such as GitHub, StackOverflow or Wikipedia editors show that women continue to be underrepresented on these online communities in a way that reinforces offline gender gaps. More specifically, these gender inequalities may arise due to the experience of gender discrimination based on observable information on profiles [46], cultural norms or algorithmic features of the platform that discourage female participation [43,45,73] and reward more male-oriented behaviours [74,75]. In contrast to platforms such as StackOverflow or GitHub that cater to software communities, little is known about gender inequalities in the use and incentives to use LinkedIn, which caters to a broader range of professional occupations. Nevertheless, it is plausible that an overrepresentation of male users or male senior managers in countries with greater offline gender inequalities may further disincentivise female users from joining or participating, particularly if networking behaviours exhibit homophily by gender, as recent experimental evidence suggests [76].\nIn addition to these broader socio-demographic country-level factors, the biases observed may reflect the extent to which LinkedIn is used as a platform. To assess if our biases differ based on the degree of LinkedIn penetration among those in the highly skilled (ISCO skill levels 3/4) labour force in a given country, we compute overall levels of and gender gaps in LinkedIn penetration, where LinkedIn penetration is defined as the ratio of the number of people on LinkedIn and the number of people in highly skilled jobs in the ILOSTAT [3]. For example, if LinkedIn is a widely used platform in a given country, we might expect less bias than in countries where LinkedIn penetration is low. Further, differences in LinkedIn penetration may vary by gender, which may reflect compositional differences between the male and female populations on LinkedIn, or differences in behaviors or incentives to use the platform. An example of a compositional characteristic is age structure. To illustrate, if the LinkedIn population in a country comprises disproportionately of younger users and has a younger age structure, the aggregate gender gap indicator may reflect these younger cohorts more. As younger cohorts are likely to be more gender egalitarian [1,2,77], a larger presence of younger cohorts in a particular country's LinkedIn population would lead us to overestimate professional gender equality in predictions using LinkedIn data.\n---\nMethods\n\nOur analysis proceeds in three steps. First, we perform descriptive analyses to examine how gender gaps on LinkedIn manifest themselves across different parts of the world and across different characteristics, such as age groups, job seniorities, job functions, company industries and fields of study. Second, we examine correlations between LinkedIn GGIs and the three ground truth measures obtained from ILOSTAT, also by age group and company industry as well as other external development and gender inequality indicators. We then build regression models using LinkedIn GGI measures to predict the three ground truth ILO gender gap measures. Our modelling exercise starts with the most parsimonious, one-variable linear regression model using the LinkedIn overall GGI, which has the best country coverage. In each regression table, we report the total number of observations for which the dependent and independent variables are available in the full sample (N obs ) as well as the total number of observations for which predictions can be made (N pred ) in the full sample (i.e. for which the independent variables are available) to highlight the gains in coverage, if any, of using the LinkedIn data source. Linear regression allows us to avoid overfitting and to have interpretable results. Sensitivity analyses have shown that binomial regressions on the proportion of women of total (women / (women + men)) do not result in improved performance compared to linear regressions on the GGI. While the regression coefficients for the model are based on analyses on the full sample, we also provide measures of out-of-sample prediction performance using five-fold crossvalidation. We use k = 5 for a reasonable bias-variance trade-off, and perform five repeats to reduce the bias in the estimator, given that the folds in non-repeated cross-validation are dependent (because samples used for training in one fold are used for testing in another). The measures of performance we report are the means across the folds and repeats of the mean absolute error (MAE), the root mean squared error (RMSE), and the R 2 . Note that this cross-validation (CV) R 2 is the square of the Pearson's correlation coefficient of the observed and predicted values (as implemented in the R package caret) [78].\nAs our dataset contains a variety of LinkedIn gender gap indicators computed across different characteristics for a given country, such as age, industry and seniority, we further assess if using a wider range of LinkedIn variables can help improve the predictive performance of our single-variable model. We expand the number of candidate predictors to include the four age group values available (i.e. 18-24, 25-34, 35-54 and 55+), as well as gender gaps across seniorities (ten unique values) and company industries (147 unique values reduced to 19 ISIC level 1 classes, as explained in Sect. 2.1.2), which along with the LinkedIn overall GGI result in a total of 34 candidate predictors. Due to considerable data sparsity (i.e. zero audience counts) within unique job functions and fields of study, and due to their poor availability across different countries, we omit these two characteristics from this analysis. We consider an indicator a candidate when it is available for at least 125 countries or at least the number of countries for which the dependent ILO GGI is available, whichever is larger. Note that this does not necessarily mean that the regression will be performed on a sample of at least 125 countries, because missingness patterns across a selection of variables may result in a smaller number of complete observations across these variables. After the application of these filters, we are left with 17 candidate variables for the ILO professional GGI, 19 for the ILO total management GGI, and 34 for the ILO senior/middle management GGI. We use lasso regression to fit these models [79], where variable selection is performed across the available candidate predictors. To find the optimal value of shrinkage parameter \u03bb in the lasso, which in turn influences which variables are selected, we use five-fold cross-validation on the data to choose the value of \u03bb that minimises the MSE. The model corresponding to this optimal value of \u03bb is then reported. We report the adjusted R 2 and RMSE for the lasso regression for the full sample, as well as error metrics (cross-validated (CV) R 2 , MAE and RMSE) computed using five-fold cross-validation.\nThe third step of our analyses focuses on analysing patterns of bias in the professional GGI predictions generated using the LinkedIn GGI. For this, we focus on analysing the residuals (difference between the observed ILO GGIs and the ILO GGIs predicted from the LinkedIn overall GGI, i.e. yy) of the aforementioned parsimonious single-variable model as our outcome of interest, with the different development and gender gap indi-cators described in Sect. 2.1.3 as possible predictors. As our goal here is to better understand which variables are useful for explaining the bias of our predictions, we use hybrid (mixed forward and backward selection) variable selection to find the variables that maximize the adjusted R 2 . We choose the adjusted R 2 as selection criterion because we want to maximize the variation explained in order to characterise those contexts where we systematically under-or overpredict professional gender inequality using the LinkedIn data.\n---\nResults\n\n\n---\nDescribing global gender gaps on LinkedIn\n\nFigure 1 shows the LinkedIn overall GGI for all countries for which we are able to compute it. A value below one indicates females are underrepresented relative to males on LinkedIn, whereas values exceeding one indicate female overrepresentation relative to males, and one indicates gender parity. While LinkedIn GGI values indicating greater gender inequality disfavouring women (GGI about 0.2) exist in most parts of Africa, the Middle East and some parts of Asia (including India and China), the proportion of women is closer to 50%, corresponding to a GGI of at least one, in most parts of the Americas, Europe and Oceania. Some of the most gender egalitarian countries -where women even outnumber men on LinkedIn -include Latvia, Lithuania, Moldavia, Georgia, Myanmar, Vietnam and Bhutan.\nTo further examine gender gaps across different characteristics for which we are able to collect audience counts for different countries, Fig. 2 and Table 2 show the LinkedIn GGI for different characteristics. Note that for field of study, job function and company industry, the categories are manually divided into either STEM or non-STEM here using the STEM-Designated Degree Program List [80] to generate broader aggregations. For example, the age group 35-54 represents the ratio of the number of women and men in this age group as obtained from LinkedIn data by country, gender and age group. While Fig. 2 shows the medians of all boxes lie underneath the dashed line (parity cut-off at GGI = 1 where male and female representation is balanced), there is substantial varia-Figure 1 LinkedIn Gender Gap Index (GGI), computed as the female-to-male ratio of LinkedIn users in a country, across the world (New Equal Earth Projection) tion in gender equality across these different characteristics. While there are almost as many women as men in younger ages, non-STEM industries, fields of studies and job functions, and low-level seniority jobs, gender inequality disfavouring women is highest among older people, in STEM fields, and among those with higher-level job seniority on LinkedIn.\nSome recurring outliers that show LinkedIn GGIs with high values greater than one, i.e. countries where women outnumber men, include Latvia, Lithuania, Myanmar and Vietnam. Previous findings using these data to assess gender inequality in the information and communication technology (ICT) sector have also shown that these countries tend to be more gender egalitarian when it comes to LinkedIn users [17]. We further inspect these outliers against ILO ground truth data. Out of all 34 countries for which the overall LinkedIn GGI is larger than or equal to one, the ILO professional/technical GGI is also larger than or equal to one in 29 countries, missing in three countries and smaller than one in two countries. These observations support the notion that the \"outliers\" we observe in the LinkedIn data are not caused by measurement issues in those data, but rather actually represent countries where professional gender equality is relatively high.\nFigure 3 shows the distribution across categories in age, field of study, company industry, job function and job seniority for women and men, aggregated across countries. Table 1 shows the corresponding audience counts across different categories by gender, as well as country coverage across different categories. Here, we observe that the female population on LinkedIn is relatively younger than the male population, that men on LinkedIn are more likely have or work in STEM fields of study, industries and job functions, and that women are more likely to hold entry-or senior-level jobs while men are more often Vice Presidents, Chief X Officers, directors, partners, owners and managers. We show two country-specific examples of one high-income country, the UK, and another lower- middle income country, India, both with large LinkedIn populations, of some of these patterns by age and ten disaggregated industries (classified by ISIC level 1 industries) in Table 7 in the Appendix. For these two countries, we also observe that women are younger than men on LinkedIn. The industry data show that professional, scientific and technical industries, along with information and communication technology industries are the two most represented industries on LinkedIn for these two countries. Compared to ILO data on these industries in these respective countries, we observe that LinkedIn considerably overrepresents these industries for both men and women. Other industries such as manufacturing, education and health are also commonly represented on LinkedIn, but their share is comparatively underrepresented on LinkedIn in these countries. In the next section we examine correlations of gender gaps on LinkedIn against ILO ground truth data across different characteristics for all available countries to understand how broadly representative gender gaps across this online population and the labour force are.\n---\nValidating global gender gaps from LinkedIn\n\nOne of the main advantages of data on LinkedIn's user population is that they are generated as by-products of the platform's use, and can be queried in real-time. Labour force surveys, in contrast, are expensive to field and consequently not routinely available, especially in resource-constrained settings. Furthermore, the LinkedIn audience estimates are globally comparable and cover a large number of countries. While gender-and occupationspecific data from ILO only cover 190 countries, LinkedIn data cover 240 countries. More importantly, LinkedIn data have better coverage and recency for low and middle income countries. Nevertheless, LinkedIn data are from an online population of professionals, and it is unclear how the data observed on this platform compare with patterns in the labour force. Therefore, to explore the validity of LinkedIn data, we compare global gender gaps on LinkedIn to the three aforementioned ILO GGI measures from ILOSTAT. Figure 4 shows a scatter plot of the LinkedIn overall GGI and the ILO professional, total management and senior/middle management GGIs. The stars indicate the statistical significance of the correlations according to * p < 0.1; * * p < 0.05; and * * * p < 0.01. The correlation of the LinkedIn GGI with the ILO professional GGI is strongly positive at 0.71, and statistically significant at the 1% level. The correlation is even higher (0.81 * * * ) when truncating GGI values of greater than one to one, as is the approach followed by commonly reported gender gap indices such as the Global Gender Gap report [58]. This suggests that the LinkedIn data may be better at capturing gender inequality in terms of women's disadvantage rather than capturing women's economic empowerment (where values exceed one). Despite these improvements in correlations, we choose not to truncate GGI values at one in our analyses as our objectives are not purely predictive but also to explore the indicator and its biases at different levels of gender inequality, including when it is high as well as low.\nTo further explore the correlations at different levels of professional gender inequality, we re-estimate these correlations removing the 34 aforementioned \"outliers\" from the data (e.g. Vietnam, Myannmar, Latvia) that display significantly high levels of professional gender equality both on LinkedIn and in the ILO data. Here too we find the observed correlations with the ILO GGIs are incrementally higher when these countries with very high levels of professional GGI values are removed than when containing these observations. That is, the correlation of the LinkedIn overall GGI with the ILO professional/technical GGI increases from 0.709 to 0.714; that with the ILO total management GGI increases from 0.416 to 0.526; and the correlation with the ILO senior/middle management GGI increases from 0.356 to 0.363. These observations again point to the idea that at higher levels of professional gender equality, LinkedIn data are broadly less representative of gender gaps in the labour force.\nThe red line in Fig. 4 is the x = y equality line and the figures also report the Pearson's correlation coefficient (Corr) between the LinkedIn GGI and the three ILO GGIs. Looking at this, we see that generally, the ILO professional GGI values lie above the diagonal relative to the LinkedIn overall GGI, especially at lower levels of gender equality. In other words, there are relatively fewer women compared to men on LinkedIn than in the professional labour force. The correlation of the LinkedIn overall GGI with the total and senior/middle managerial ILO measures is 0.36 and 0.42 respectively (both are statistically significant at the 1% level), which is lower than with the ILO professional GGI but nevertheless reasonably high. In contrast to the ILO GGI indicators, the LinkedIn overall GGI is generally larger in value for a given country for both managerial ILO GGI measures and ILO senior/middle management GGI.\nFigures 5 and6 show the correlation between age-specific and industry-specific LinkedIn and ILO GGIs, respectively. The correlation between the age-specific LinkedIn and corresponding age-specific ILO measures is higher for older age groups, with almost no relationship among 18-24 year olds and a correlation of 0.54 among 55+ year olds. In other words, gender gaps in older ages appear to be generally more representative of the working population in professional occupations for older than younger age groups. We observe a greater variation in the younger age groups (18-24 and 25-34) in both the LinkedIn GGI and ILO professional GGI indicators. In contrast, there is much less variation across countries in professional gender equality among older age groups (men outnumber women in almost all countries), which might help explain the improved correlation in these age groups. Alternatively, LinkedIn users among younger age groups may also use LinkedIn in more varied ways, including for finding new opportunities and networking while still in education, and might therefore not be captured in the ILO professional GGI indicator which refers more specifically to individuals employed in these jobs.\nThe industries for which LinkedIn and ILO GGIs correlate strongly include education, finance, professional/technical/science, human health and social work, other services, real estate and public administration. Low correlations occur in agriculture, transportation and storage, mining and quarrying, wholesale and retail trade and manufacturing. These patterns generally suggest better correlations among more highly skilled sectors than lower skilled sectors, and more active use of LinkedIn in these higher skilled sec- tors could underpin these patterns. One exception, however, is the information and communication industry, which is a highly skilled industry but for which the correlation between ILO and LinkedIn GGIs is relatively weak, albeit still positive and significant. This could be linked to greater variations in gender gaps across countries in this sector in both data sources [17], and/or gender differences in incentives to use LinkedIn across countries.\nHow do external measures of development and gender inequality compare to the correlation of the LinkedIn overall GGI with the ILO GGI measures? Table 3 shows correlations between the LinkedIn overall GGI, the three ILO GGIs and the other development and gender inequality indicators outlined in Sect. 2.1.3. The ILO professional GGI correlates most strongly with the LinkedIn overall GGI (0.71) -more strongly than other socioeconomic indicators in the dataset, such as the ILO senior and middle management GGI (0.66), and the GGIs in economic opportunity (0.57) and educational attainment (0.55) from the Global Gender Gap Report. These results suggest that this simple indicator from LinkedIn's online population has significant value as a predictor for the ILO GGI. The ILO total management GGI correlates most strongly with the economic opportunity GGI (0.57), followed by similar correlations with the ILO professional GGI (0.49), ILO labour force participation GGI (0.37), LinkedIn overall GGI (0.36) and ILO senior and middle management GGI (0.35). The ILO senior and middle management GGI additionally correlates with the educational attainment GGI (0.47), LinkedIn overall GGI (0.42) and economic opportunity GGI (0.33). Interestingly, the GGIs in ICT and STEM education are generally negatively correlated with other measures of gender inequality and development. Although seemingly unexpected, these results are in line with the \"STEM gender equality paradox\" [81]. This paradox asserts that in low and middle income countries, women tend to study and work in STEM fields more often than in high income countries, in order to provide them with higher economic security. Overall, the ILO measures correlate strongly with the LinkedIn overall GGI. These findings indicate that our LinkedIn overall GGI indicator is a relatively good measure that is broadly representative 1.00 * * * -0.42 * * * -0.06 -0.27 * * * -0.05 (7) 1.00 * * * 0.12 0.81 * * * 0.13 (8) 1.00 * * * 0.17 * * 0.42 * * * (9)\n1.00 * * * 0.14 (10) 1.00 * * * Note: * p < 0.1; * * p < 0.05; * * * p < 0.01.\nNote: 1 = LinkedIn overall GGI; 2 = ILO professional GGI; 3 = ILO total management GGI; 4 = ILO senior and middle management GGI; 5 = UNESCO education ICT GGI; 6 = UNESCO education STEM GGI; 7 = ILO labour force participation GGI; 8 = GGG educational attainment GGI; 9 = GGG economic opportunity GGI; 10 = GDP per capita. of professional gender inequality in the labour force, albeit clearly not one with a perfect correspondence with the ground truth gender gaps.\n---\nPredicting ILO GGIs with LinkedIn GGIs\n\nTable 4 shows the regression coefficients for the models predicting the three ILO GGIs from the overall LinkedIn GGI for three different settings: all countries (total), countries where gender equality in the ILO GGIs is lower (ILO GGI < median ILO GGI) and countries where gender equality in ILO is higher (ILO GGI \u2265 median ILO GGI). The coefficient on the LinkedIn overall GGI is positive and statistically significant for all ILO measures and settings, except for the high-equality settings for the two ILO managerial GGIs. The coefficient is close to one particularly in the total (0.79) setting for the ILO professional GGI, which our LinkedIn measure seems to predict best. The best performance on the full sample is achieved on the ILO professional GGI (with an R 2 = 0.5) relative to the other ILO GGI measures linked to management and senior management. We note that data availability of the ground truth ILO indicators for model fitting is also best for the ILO professional GGI indicator compared with the other two indicators. Looking further at the cross-validation fit metrics that illustrate the performance of these parsimonious models on unseen data in Table 4, we observe that the best performance in terms of RMSE, MAE and R 2 is similarly achieved on the ILO professional GGI, followed by the ILO senior/middle management GGI and the total management GGI. Further, we find that better predictive fit, both on the full-sample and cross-validation metrics, is achieved on the sample of countries in the low equality scenario relative to the high equality scenario. The models perform worse across the high equality scenarios across all three ILO GGI indicators. Consistent with the correlations reported earlier, these results further suggest that LinkedIn gender gaps are better at predicting professional gender gaps in settings where gender inequality disfavouring women is larger. In other words, when women are missing from the LinkedIn population in these low equality settings, this serves as a reasonable predictor that they are not in skilled professions in the labour force altogether. In contrast, when women are missing from the LinkedIn population in higher equality settings, this is less effective at predicting that they are not actually in the labour force in skilled professions, which may suggest more differentiated patterns of LinkedIn use in these settings. These patterns are consistent also with findings in [18] and [22] who found that Facebook and Google AdWords gender gap indicators were better able to predict internet access gender gaps in settings where internet access gender inequalities disfavouring women were larger.\nThe single-variable, parsimonious overall LinkedIn GGI performs well when it comes to predicting the ILO GGIs, albeit with better performance for the ILO professional GGI than for the other two managerial ILO GGIs. For all indicators, the ILO predictions generated using the LinkedIn GGI also come with improved country coverage, due to better data availability of the LinkedIn indicator, and this improvement in country coverage is greater for the two management-linked ILO GGIs. For the ILO professional GGI indicator, LinkedIn data enable predictions for 234 countries relative to 185 countries in the ground truth data, from 167 to 234 countries for the ILO total management GGI and for 89 to 234 countries for the ILO senior/middle management GGI. In terms of region, the biggest expansion in coverage occurs for countries in Africa.\nThe single-variable model relies on the overall LinkedIn population of users disaggregated by gender. We now consider the potential to improve predictive fit by considering LinkedIn gender gaps computed across other characteristics in our dataset. Table 5 shows the results from the lasso regression models where we consider gender gap indicators across age groups, job seniority and company industry as possible predictors.\nThe results in Table 5 show that for the ILO professional GGI, adding the GGI in senior jobs in addition to the LinkedIn overall GGI in the total scenario from Table 4 marginally increases the R 2 computed on the full sample from 0.50 to 0.55, as well as the CV R 2 , and decreases the MAE and RMSE from the five-fold cross-validation. This however comes at the cost of decreasing the number of countries for which we can make predictions (234 in the simple model versus 209 with the more complex model). For the ILO total management GGI, gender gaps in director and manager job functions increases the R 2 from 0.13 to 0.14, but does not offer improved performance across the MAE, RMSE and R 2 computed using five-fold cross-validation.\nThe best increase in model performance occurs when predicting the ILO senior/middle management GGI from LinkedIn GGIs among VPs as well as those working in accom- modation and food service activities and ICT, rather than from the LinkedIn overall GGI, resulting in an R 2 increase from 0.17 to 0.42, as well as improved performance in crossvalidation MAE, RMSE and R 2 . Overall, while these more complex models that rely on different characteristics appear to incrementally improve performance compared to the LinkedIn overall GGI model, it remains that our ability to use LinkedIn data to predict the total management and senior/middle management ILO GGIs is weaker than for the ILO professional GGI.\n---\nExplaining sources of bias in predicted global professional gender gaps from LinkedIn GGIs\n\nTable 6 shows the variables selected when we model the residuals of our single-variable regression models (total scenario) reported in Table 4 using development and gender gap predictor variables. These predictors have been selected from a set of 14 aforementioned candidate variables using stepwise hybrid (combined forward and backward) selection, whereby candidates are entered and removed in a stepwise manner -based on the adjusted R 2 of the resulting models -until there aren't any predictors left for entering or removal. 4 Figs. 7, 8 and 9 show for all three ILO GGIs scatter plots of their actual (x-axis) and predicted (y-axis) values, whereby the points are coloured by the statistically significant (at p < 0.05) variables selected in each model as given by Table 6. Points that lie on the x = y diagonal line indicate those countries for which the observed and predicted data correspond perfectly, whereas points above the diagonal are those where LinkedIn gen- der gaps overpredict professional gender equality and points below the diagonal are those where LinkedIn gender gaps underpredict professional gender equality.\nThe results in Table 6 show that significant predictors of the residuals in The proportion of variation explained for the residuals for the two managerial ILO indicators is less than the ILO professional GGI. For the ILO total management GGI, the selected variables are able to explain around 35% of the variation in the residuals. The selected, statistically significant variables include LinkedIn penetration GGI, internet access GGI, internet penetration, gender gaps in secondary educational enrollment and human development index (males). Gender gaps in internet access, LinkedIn penetration, in labour force participation, secondary education, as well as and levels of overall inter- net penetration explain about half of the variation (0.47) in the residuals in the ILO senior/middle management GGI. The scatter plots in Figs. 7, 8 5 and 9 further help to visualize the direction and extent of the bias for the residual predictions. Across all three ILO indicators, the figures indicate that we tend to overpredict professional gender equality in countries with lower levels of gender equality in internet access (smaller internet GGI values). This pattern of bias indicates that in countries with greater gender inequality in internet access, higher status women are overrepresented online on LinkedIn, leading us to be more optimistic about professional gender equality in these settings than is actually the case in the labour force. This pattern of gender selectivity, where women are overrepresented on LinkedIn in countries where internet access gender gaps are larger, likely indicates the selective nature of the female online population in these settings. Across all three indicators, at higher values of the LinkedIn penetration GGI, we tend to overpredict gender equality than that observed in the ILO data. This suggests that the relatively higher penetration of LinkedIn among women relative to men in the labour force (i.e. when LinkedIn penetration GGI values Figure 8 Scatter plot of ILO total management GGI (actual) (x-axis) and ILO total management GGI (predicted) y-axis). Points in each panel are colour coded by values of the variable listed are greater than one) also results in our model overpredicting the gender gap in favour of greater gender equality. The predictions tend to be more accurate when LinkedIn penetration is gender balanced (close to 1).\nFor the two managerial gender gap indicators (Figs. 8 and9) the model tends to underpredict gender equality at higher levels of the ground truth. For the ILO total management GGI, in addition to the already noted patterns of bias linked to internet access gender gaps and LinkedIn penetration, Fig. 8 also shows that in countries where secondary gender gaps are very low (<0.8), the model predicts greater gender equality in total management gender gaps than observed in the ground truth. We see a similar tendency of the model to overpredict gender equality in senior/middle management in Fig. 9 at very low values of the labour force GGI. In addition to the patterns of gender selectivity linked to internet access discussed above, the overrepresentation of women on LinkedIn in countries where gender gaps in economic and educational attainment show greater inequalities could reflect a stronger need for women in these contexts to make themselves more visible, and leverage opportunities offered by digital technologies such as LinkedIn to improve their economic prospects. Furthermore, Figs. 7, 8 and 9 show that our predictions are more accurate (points closer to x = y line) at higher levels of internet penetration and/or LinkedIn penetration, and where internet access is gender balanced (close to 1). Essentially, when in a country internet use is more widespread and gender balanced in its population, and LinkedIn is more widely adopted among those in the work force, our predictions of professional gender equality are more accurate than in countries where fewer people are on the internet or on LinkedIn.\n---\nDiscussion and conclusion 4.1 Summary\n\nThis paper leverages a novel source of digital trace data, LinkedIn's ad audience estimates available from its advertising platform, to analyse global professional gender inequality. To the best of our knowledge, this is the first global analysis of professional gender gaps on LinkedIn, alongside an assessment of the feasibility of using these data to model global gender gap indicators linked to women's economic empowerment in the context of the global sustainable development goals (SDGs). This study builds on other proof-of-concept studies that have used the same data source to study gender gaps in the context of US cities [21], and gender gaps in the IT industry [17].\nOur work contributes to a growing body of literature in digital demography, which examines the demographic characteristics and biases of online populations and the potential of digital trace data to measure and model socio-demographic processes. We document that, as with the offline world where women's participation in professional, technical and managerial occupations remains disadvantaged relative to men, the online world of LinkedIn also displays gender gaps, albeit with significant variations across countries, age groups, industries and seniorities. We find that gender inequality is particularly high in Africa, the Middle East and parts of Asia, while the Americas, Europe and Oceania are more gender egalitarian when it comes to having a LinkedIn profile. Additionally, gender inequality is larger among older individuals, those having studied or working in STEM sectors and industries, and among higher levels of job seniority on LinkedIn's population.\nOur simple aggregate LinkedIn measure of gender equality, the LinkedIn Gender Gap Index (GGI), correlates positively and strongly with gender gap indicators from traditional labor force surveys available from the ILO, although these correlations are stronger for the general professional gender gap indicator than those linked to managerial or senior managerial gender gaps. These correlations indicate that LinkedIn gender gaps are more broadly representative of gender inequalities in technical and skilled populations in the labour force, while being less representative of managerial gender inequalities. We also find stronger correlations with ground truth measures for some industries (e.g. health, finance and insurance, education) over others (e.g. mining, agriculture, construction). The LinkedIn GGI also strongly correlates with other measures of gender equality in educational and professional domains, which suggests the potential for future work to explore the use of these data for modelling other outcomes linked to global sustainable development.\nA parsimonious single-variable model using the LinkedIn GGI provides very good performance for predicting the ILO professional gender gap, and the model's ability to predict professional gender gaps is better in low professional gender equality contexts. This indicates that the absence of women from the online LinkedIn population in these settings serves as a good predictor that they are not in the professional labour force altogether. The model using LinkedIn data also expands country coverage of professional gender gaps beyond the ILO data. Although models that draw on LinkedIn gender gaps across other characteristics help improve predictive performance of the ILO senior/middle management gender gaps, these more complex models come with lower country coverage as more detailed features are not available for many countries.\nDespite being a promising indicator, our work highlights interesting biases in professional gender gap predictions generated using the LinkedIn GGI. We find that our predictions are generally more accurate in countries with better and more gender balanced internet penetration, as well as LinkedIn penetration. A striking bias we find is that in countries with less gender egalitarian internet access, LinkedIn tends to predict greater gender equality than the ground truth. Similar patterns also emerge with the managerial indicators and gender gaps in educational and labour force indicators. These findings, which align with those in [68] for Google+, suggest that higher status women are disproportionately overrepresented online where connectivity gaps might be larger, and economic barriers faced by women are greater. They also suggest the possibility that women in disadvantaged contexts may seek to signal their visibility more in online settings to overcome the barriers they face, or draw on digital technologies to improve access to resources or networks they cannot access through other channels. This is consistent with other findings from low-income countries that show that digital technologies have the potential to lower the costs of information, connectivity and networks, with the potential for bigger payoffs for those who face greater barriers such as women [40,41]. Further work, both qualitative and quantitative, is needed to understand how gender differences across diverse contexts emerge in the use of the internet and social networking sites, and the implications of these gender gaps for economic and labour market outcomes.\n---\nLimitations\n\nWe acknowledge a number of limitations with our study. First, like all studies using social media advertising data, how user characteristics are determined to provide audience counts is not documented and historic estimates of past user counts are not available to track retrospective changes. Although LinkedIn states that this information is inferred from characteristics that individuals put on their profiles, how exactly this is done remains unknown. A further caveat here is that these data only allow us to assess whether women or men have a user account and do not say anything further about the intensity or types of use on LinkedIn, or duration of being a LinkedIn user. Research on gender-differential types of use and self-presentation on LinkedIn, while limited and based on small, specific samples from the US (e.g. a cohort of MBA graduates), suggests that women may provide less custom information on their profiles [49] and present emotions more while men present status [50]. It could be, for example, that the variation we capture across different industries, or seniorities, also reflect gender-differential patterns of disclosure of these characteristics rather than the presence of these groups on LinkedIn altogether. Moreover, little is known about the gender differences in the experiences of using this platform. Our work points to the presence of gender gaps on the platform, particularly among some groups such as senior managers, but the mechanisms for why these inequalities arise cannot be captured by our data. Further work is needed to understand these processes of disclosure of professional information and engagement on these platforms, as well as their implications for networking and economic opportunities.\nLinkedIn data come from an online, social media population and our study has sought to understand how representative it is by comparing against ground truth indicators computed from labour force surveys, as well as analysing patterns of bias in predictions generated using LinkedIn data. Our work has shown that LinkedIn gender gaps are more predictive of general professional gender gaps indicators than those linked with managerial gender gaps. Particularly with the managerial gender gap indicators, more work is needed to understand how predictions using LinkedIn data sources could be improved. This may involve considering a wider range of features from within LinkedIn, although this may come at the risk of reduced country coverage as our work has shown. Other more promising approaches involving post-stratification [59] or the adoption of correction factors (e.g. [16,18]) could serve as useful extensions. The variables we have shown to be useful in explaining the bias of our predictions could be incorporated into these models directly to help improve the performance of predictions of ground truth data.\n---\nConclusions\n\nDespite these limitations, our work provides evidence of how social phenomena in the offline world -in this case, professional gender inequalities in the labour force -also manifest themselves on the online world, yet with distinctive biases. It further illustrates the value of data generated from a novel online data source for monitoring policyrelevant social indicators within the global sustainable development goals, with the potential to fill important gender data gaps by complementing traditional data sources. The analysis we have presented provides one particular snapshot or cross-section, and our correlations show that online gender gaps capture cross-country variation in professional gender gap indicators in the ILO ground truth well. Further work is needed to examine if this extends to modelling changes over time through routinely collecting these data prospectively and comparing them against more ground truth indicators from labour force surveys as they become available. This kind of approach, in which more data from labour force surveys and ongoing data from LinkedIn are integrated, could be usefully applied for nowcasting professional gender gaps to provide higher frequency coverage than that provided by conventional labour force statistics, particularly in low income countries where ground truth data are often limited. Other applications might be to use these data to evaluate the impacts of specific policies or interventions on professional gender inequality and gender biases in online populations of professionals. Country-specific analyses drawing on more detailed characteristics on LinkedIn (e.g. skills) that are not often or easily captured in existing traditional data, or information for more local labour markets at more spatially granular levels would also be valuable extensions from the perspective of using these data for measuring policy-relevant indicators.\nThe gender gaps we document in this online, social media population have implications both for researchers as well as job recruiters and prospective employers. For researchers they indicate how social media populations such as LinkedIn display distinctive biases, particularly in contexts with greater internet access inequalities. This necessitates further work to better document and understand the nature of these biases, and emphasises the need to correct for these biases when studying or recruiting participants for research from such online populations. The presence of these biases also indicates the need for integrating different types of complementary data for a more inclusive understanding of social processes. For recruiters and prospective employers, our results suggest the use of LinkedIn could help to diversify hiring by gender in some contexts and cases -e.g. by recruiting younger or early career women who are more likely to be on the platform, in specific industry contexts, and in countries where women's educational or economic attainment is low, where we find that women are overrepresented relative to the labour force.\n---\nAvailability of data and materials\n\nThe dataset with compiled gender gap indicators and analysis scripts supporting the conclusions are available at https://github.com/fverkroost/epjds-professional-gender-gaps.\n---\nAppendix\n\n\n---\nCompeting interests\n\nThe authors declare that they have no competing interests.\n---\nAuthors' contributions\n\nRK conceptualised and designed the study. Both authors built the dataset. FV conducted the analyses in discussion with RK. Both authors interpreted the results and wrote the manuscript. Both authors reviewed and approved the final manuscript.",
        "Introduction\n\nClinical trials (CTs) are used to evaluate new therapies and are the mechanism through which research is translated into standards of care. The effectiveness of this translational process is greatly dependent on the number and representativeness of participants enrolled in trials, yet less than 5% of all adult cancer patients enter CTs. Despite a nearly 20-year effort by the National Institutes of Health (NIH) to enhance CT accrual, these rates are not improving, and even lower participation rates are reported in minority and underserved populations [1][2][3][4][5][6][7]. Low representation of these populations in CTs results in inequity in access to the latest technologies and cancer treatments, compromises the generalizability and external validity of the CT results [4][5][6][7][8], and may fail to identify important positive or negative treatment effects among under-represented populations [7][8][9][10].\nBarriers to CT accrual that are related to patient and physician characteristics and the health care system have been identified [11][12][13][14][15][16][17]. Eligible patients may refuse to participate for many reasons, including concerns regarding experimentation and loss of control over treatment decisions [11][12][13]18] and failure to understand important trial procedures, such as randomization [19,20]. Physician barriers may represent a significant component of nonaccrual, due to difficulty initiating CT discussions and reconciling the dual roles of treating physician and clinical researcher [15,[21][22][23][24][25][26].\nPatient race has been shown to be associated with trial eligibility and refusal [27,28]. Despite the number of studies exploring barriers resulting in underrepresentation of minorities in CTs [28][29][30][31][32][33][34], there has been limited research that explores racial differences in reasons for ineligibility and for refusal to enroll in cancer CTs at the time of recruitment [28,33,34]. Those that did collect prospective information often lacked adequate representation of minorities in the study samples to permit comparisons of different racial groups [3,35]. Of the 36 studies identified in a systematic review of barriers to minority participation in trials, none reported the barriers according to racial groups [28].\nTo examine reasons for ineligibility and refusal among cancer patients in real time, we prospectively collected data on patients evaluated for CTs at an urban cancer center that treats significant numbers of both African-American (AA) and White patients. The aims of this study were to examine characteristics associated with being designated as ineligible for a trial and characteristics associated with refusing trial participation. The focus of this analysis is on comparison of reasons for ineligibility and refusal among AA and White cancer patients.\n---\nMethods\n\n\n---\nStudy patient identification\n\nPatient data for this study were captured and reported through the Clinical Trials Eligibility Database (CTED). CTED is an automated software system developed at Virginia Commonwealth University's Massey Cancer Center that facilitates the clinical research evaluation process by linking longitudinal patient data from multiple electronic sources that include clinical reports (e.g., pathology and radiology reports, inpatient and outpatient clinical notes), clinical laboratory results, as well as billing records and scheduled visits. Clinical research staff (clinical research associates and research nurses) evaluate a patient's eligibility for a specific CT through review of the linked data in CTED and/or by review of additional records in the Electronic Health Record (EHR) or from reports obtained from outside providers. For each evaluation, clinical research staff manually record a variety of parameters relevant to the patient's status with respect to a specific CT or set of trials. These details include eligibility, specific reasons for ineligibility and refusal status and reasons for refusal, and referral source for the specific combination of patient and study. A detailed description of CTED functionality has been published [36].\nDuring the development period for CTED, potential reasons for trial ineligibility and refusal were obtained by interviewing clinical research staff, by performing a comprehensive literature review, and then revising the initial ineligibility and refusal list to include additional reasons identified during the first 6 months of usage that had not been captured through the initial processes. Reasons were subjected further to qualitative thematic analysis techniques [37,38], and the resulting discrete categories were programmed as pull-down menus to be used by research staff to select pertinent reasons for ineligibility of and refusal by the individual patients evaluated. Reasons not included in the pull-down list could be specified by research staff. Thus, the CTED application provides a repository for clinical research staff to easily enter information for every patient evaluated for a CT. This system feeds data to the Clinical Trials Data Management System (OnCore \u00a9 ) that is used to track the patient once enrolled.\n---\nStudy sample\n\nThe initial sample for this study was derived from all the patients with a scheduled visit to a National Cancer Institute (NCI)-designated cancer center in Richmond, Virginia, from June 2006 to March 2010 and who had an eligibility evaluation record completed in the CTED system. The analytic sample included patients with scheduled visits who also had a billing diagnosis for a specific cancer. The billing diagnosis for cancer in populations with a high prevalence of cancer patients has been demonstrated to have a high sensitivity, specificity, and positive predictive value [39,40].\nExclusion criteria for the analytic sample included age less than 21 years, incarceration, and race other than AA or White. Also excluded were patients who were evaluated only for nontherapeutic, companion, or ancillary studies.\n---\nMeasures\n\nSeveral data domains were captured and used in these analyses. Each is described below:\n1. Demographic characteristics: Demographic information included gender, race (race determined either from the insurance record or entered by a registration clerk), age, and insurance status (commercial/health maintenance organization (HMO), Medicaid, Medicare, uninsured) available from patient scheduling or billing records and were used to represent characteristics that have been reported to be associated with CT enrollment.\n---\n2.\n\nCancer diagnosis: Patients were classified as having a primary diagnosis of either a solid tumor cancer (e.g., breast, colorectal, lung) or a hematological malignancy (e.g., lymphoma, leukemia).\n---\nTrial characteristics:\n\nThe trial phase (I, II, III, or IV) was obtained from the linked Clinical Trials Data Management system (OnCore) used to track patients post CTs enrollment.\n---\nOutcomes variables:\n\na. Eligibility status: Trial eligibility status (eligible or ineligible) was recorded in CTED by the clinical research staff who completed the assessment according to the eligibility criteria for an individual trial protocol. Those records with an eligibility ascertainment noted as pending (including records where the full evaluation had not been completed or eligibility status had not been determined at the time of data capture for this analysis) were coded separately as 'final eligibility status not determined'.\nb. Refusal status: Patients who were trial eligible were followed to ascertain whether they chose to participate in the offered CT and were classified as refusing or not refusing a trial.\n---\nCT ineligibility reasons.\n\nReasons for patient ineligibility were coded for each patient. (A list of these reasons is provided in Table 1.) Reasons for ineligibility that were determined by the research nurse based on an interview with the patient or discussion with the attending physician, which included noncompliance and inappropriate mental status. Noncompliance included (1) consistent 'no-shows' for clinic appointments, (2) active substance abuse, and (3) 'perceived instability'. Ineligibility due to inappropriate mental status was recorded whenever a patient was deemed incapable of understanding and providing informed consent.\n---\n6.\n\nCT refusal reasons. Reasons for refusal of participation were coded for each patient (a list of these reasons is provided in Table 2). The refusal category labeled 'perceived extra burden' included the following reasons: additional out-of-pocket expenses, lack of insurance coverage, extra clinic visits, and travel.\n---\nStatistical approach\n\nDescriptive statistics were computed for gender, age, insurance status, and cancer type for the total sample, by race, by phase of trial, and by eligibility status. Frequencies for the reasons for trial ineligibility and for refusing the index CT (index CT = CT for which the patient was judged eligible) were also computed for the total sample and by race. Either a ttest or chi-square test was used to compare distributions among AAs and Whites. Probabilities of 0.05 or less were defined as statistically significant, without accounting for multiple comparisons of AAs and Whites.\nA logistic regression model with eligibility status as the dependent variable was used to identify characteristics associated with ineligibility for a therapeutic cancer CT. A second logistic regression model with refusal status as the dependent variable was used for the subset of patients eligible for a CT to identify factors associated with refusal to participate in a therapeutic CT.\nCovariates were entered simultaneously into each logistic regression model; these included race, age (grouped into quartiles), gender, insurance status (using commercial insurance as the reference category), type of cancer (using hematologic cancers as the reference group for diagnosis), and phase of trial (using either phase I or III, alternately, as the reference for other phases). The data from the patients for whom evaluation records had no study phase indicated (N = 207) were excluded from the regression analyses (Typically, no study phase was specified whenever a patient had been considered for a group of CTs but was determined to be ineligible for any of them before eligibility for a specific trial was considered.) Statistical significance of regression parameters was based on the probability that the odds ratio (OR) was not unity (p < 0.05) and the 95% confidence interval (CI) on the OR did not include unity.\n---\nResults\n\n\n---\nPatient sample\n\nTable 3 presents by race and for the total sample the characteristics for 1955 patients who had at least one evaluation for a therapeutic CT during the study interval. The majority of these patients were White (64%) and female (68%); the mean age was 56 years.\nApproximately half (49.8%) had private insurance, while 28.6% had Medicare and 11.2% had Medicaid; the remaining patients were uninsured. More than half of the patients in the sample (56.5%) were assessed for a phase III trial. Rates of ineligibility by trial phase were as follows: phase I = 55.2%, phase II = 49.0%, phase III = 43.8%, and phase IV = 35.7%. A total of 38% were eligible for a therapeutic study; of these 742 patients, 46.6% refused to participate in a CT for which they were eligible.\nCompared to AA patients, White patients were slightly older (57.6 vs. 55.9 years, p < 0.004) and more often had private, commercial insurance (59.9% vs. 32.3%, p < 0.0001). AA patients more often had been deemed ineligible compared to White patients (47.8% vs. 40.8%, p < 0.004). A higher proportion of eligible AA patients refused trial enrollment than did eligible White patients (54.6% vs. 42.8%, p <0.003).\n---\nReasons for ineligibility and refusal\n\nWe identified 18 reasons for ineligibility for a therapeutic CT among the analytic sample (Table 1). The most common reason was comorbidity (18.8% of the total number of ineligible patients); the second most common reason was that the patient had received prior treatment that excluded them from the index trial (17.3%). Other important reasons for ineligibility included extent of the disease and histo-pathologic findings that did not satisfy requirements for the study.\nAmong the differences in reasons for ineligibility among AAs and Whites, AA patients more often than White patients had been designated as ineligible due to 'mental status', that is, perceived or documented inability to provide informed consent, and expected noncompliance based on a history of missed appointments, drug abuse, and so on. White patients were more frequently ineligible than AA cancer patients due to a history of prior treatment. The proportions of AA and White patients were similar with respect to comorbidity and cancer-related characteristics (Table 1). Few patients in this sample were excluded due to age.\nAmong the 11 categories of reasons for refusal to participate in the index trial, the most common category overall was the extra financial or logistical burden associated with CT participation (patient costs of participation, extra visits, travel time, etc.), accounting for 17.6% of the 346 refusals (Table 2). Other reasons for refusal were lack of interest in the trial, not wanting a specific treatment (primarily chemotherapy) provided as part of the trial, and preference for a specific treatment. No reason for refusal was recorded for 12.1% of the evaluations for which the patient refused participation.\nAs with reasons for ineligibility, some reasons given for refusing trial participation differed by patient race (Table 2). Whites refused about twice as often as AAs due to additional financial or logistical burden associated with CT participation (22.2% vs. 10.0%), because they had a preference for a specific treatment (13.0% vs. 7.7%), and due to discomfort with randomization (9.3% vs. 4.6%). Eligible AA patients about twice as often as Whites refused due to a lack of interest in the index CT (25.4% vs. 12.0%). AA patients also declined to provide a reason for refusal more often than White patients (16.1% vs. 9.7%). Eligible AA patients cited family pressures or cultural factors as reasons for refusal more frequently than White patients.\n---\nLogistic regression model for eligibility status\n\nThe first logistic regression model used ineligibility for a therapeutic cancer CT as the dependent (outcome) variable and sociodemographics, disease, and trial characteristics as the independent variables. Adjusted ORs for ineligibility among AAs compared to Whites and their 95% CIs are shown in Table 4. AAs were more likely to be ineligible for a trial but the lower bound of the CI was 1 (OR = 1.26, 95% CI = 1.0-1.58). Women were more likely to be ineligible than men (OR = 1.77, 95% CI = 1.41-2.25). Medicaid, Medicare, and uninsured patients were each more likely to be ineligible than commercially insured patients with ORs of 1.64 (95% CI = 1.14-2.35) for Medicaid, 2.35 (95% CI = 1.64-3.36) for Medicare, and 2.02 (95% CI = 1.38-2.94) for the uninsured patients. Patients were more likely to be ineligible when evaluated for a phase II or III trials (OR = 1.66, 95% CI = 1.14-2.43) compared with patients evaluated for a phase I study. Conversely, in a second model, in which phase I was replaced by phase III as the reference category, patients were less likely to be ineligible for phase III trials (OR = 0.76, 95% CI = 0.60-0.95), and there were no substantive changes to the other model parameters (data not presented). Neither age nor cancer type was associated with ineligibility after accounting for other covariates. In a third regression model, we removed age and cancer type from the independent variables; race, gender, and type of insurance remained associated with ineligibility (data not shown).\n---\nLogistic regression model for trial refusal\n\nIn a separate analysis of factors associated with trial refusal among patients eligible for a therapeutic CT, the logistic regression model used refusal status as the dependent variable and sociodemographics, disease, and trial characteristics as independent variables (Table 4). AA patients were 1.8 times as likely as White patients to refuse participation in a therapeutic CT (OR = 1.79, 95% CI = 1.27-2.52). Cancer type was the only other covariate in this model that was significantly associated with refusal; patients with solid tumors were nearly twice as likely to refuse trial participation as patients with hematologic malignancies (OR = 1.92, 95% CI = 1.16-3.18). When the phase III replaced phase I as the reference phase in this regression model for comparison to all other phases, patients were nearly twice as likely to refuse phase III trials (OR = 1.91, 95% CI = 1.33-2.75) compared to trials of all other phases. In this model, cancer type was no longer significantly associated with refusal, but there were no other substantive changes to parameter estimates (data not shown). We repeated the analysis using a model without age and cancer type; the association of refusal with race remained.\n---\nDiscussion\n\nThis study is the first to compare directly ineligibility and refusal reasons captured prospectively in AA and White cancer patients. The data are consistent with earlier studies that indicated that AA patients more often are deemed ineligible [15,33,41] and when eligible, more often refuse participation [27,28]. This prospective analysis adds to our current understanding of why there are racial differences in CT enrollment and provides data to suggest interventions to address barriers pertaining to the patient, the physician, and the health-care system [42].\nEarlier studies that captured data prospectively either did not focus on cancer [14] or had insufficient numbers of AA patients to provide meaningful results for racial comparisons [3,35]. Studies that retrospectively capture reasons for refusal may suffer from recall bias [43]. Capture of reasons at the time of a patient's evaluation and decision regarding participation in a CT reduces such biases [9,16,[44][45][46].\nIn comparing differences in reasons for ineligibility among AA and White cancer patients, some reasons for ineligibility were based on subjective judgment on the part of the nurse or physician. These reasons were recorded more often for AA than White patients. While a relatively small proportion of all reasons, impaired mental status and expected noncompliance have been identified in other studies as reasons for ineligibility [15,33,41,47]. These reasons assigned by the health-care provider may reflect social or economic barriers for the patient, such as time or cost of travel, alternate child-care arrangements, or other factors that impact the ability or willingness to attend scheduled visits [47]. At our urban cancer center, AA patients are more often uninsured and have less social support than our White patients. These logistical issues may be viewed by research staff as impediments to successful trial participation [47]. Nevertheless, identification of these reasons for ineligibility offers an opportunity to expand access by designing trials that reimburse expenses related to participation, that require fewer clinic visits, or that provide for assessments at locations more convenient to participants.\nReasons for refusal to participate in a trial for which a patient had been judged to be eligible differed between AA and White patients. AA cancer patients more often expressed family or cultural barriers as reasons for nonparticipation. These barriers emphasize the need to elicit family participation in the decision-making process. A higher proportion of eligible AAs than Whites refused participation because they were overwhelmed with the decision-making process, suggesting the need for additional support infrastructure designed to serve minority and under-served populations (or any patients in need). Identifying and addressing these cultural differences have been shown to have a positive impact on enrollment of AA patients in at least one study [48].\nMethods to address exclusionary criteria, such as limited insurance coverage of CT-related costs, may reduce disparities in enrollment and simultaneously increase generalizability of CT results as studies have shown that providing financial coverage of the CT-related costs eliminated insurance as a predictor of trial participation [49,50]. Nevertheless, disparate enrollment and limited generalizability among AA and patients with lower socioeconomic status likely will continue until other barriers, such as the additional direct costs that are unlikely to be covered by insurance (out-of-pocket expenses, extra visit co-payments), indirect costs (travel, child care), and lack of social support for CT participation, are eliminated [25,48,50].\nThe proportion of AA patients who cited fear of randomization or extra burden as reasons for refusal of trial participation was only half that for Whites similar to findings by Gadegbeku et al. [12]. He identified health-related factors and other burden-related factors rather than psychosocial factors as influencing CT participation by AAs [12]. In an analysis of refusers who cited 'extra burden' as the reason for refusal, we found that AA patients more often reported the higher costs and additional visits associated with CT participation as the primary reason for refusal, while White patients in this refusal category more often stated that they preferred another more convenient location for treatment.\nTreatment preferences also were reported as a reason for refusal more often among White patients than AA patients. However, overall, nearly 23% of all the patients cited either a positive or negative preference for the treatment under consideration as a reason for refusal to participate in a CT. Patient refusal due to treatment preference may hinder recruitment to phase III trials in particular where standard-of-care alternatives are available outside the trial.\n---\nLimitations\n\nWhile the data reported herein were collected prospectively from patients, they were captured by clinical research staff who entered the data into the system. Thus, reasons for refusal and ineligibility were filtered through the nurses' perspectives and understanding of the patients. However, the findings are consistent with other prospective studies [3,14,15,23,25,35,47]. The data were drawn from the patient population of a single urban cancer center, which may limit the generalizability of results. Conversely, an important benefit of our concurrent data capture within a single system is that the information directly reflects the cancer patient population seen at that institution so that the findings are locally applicable.\nWe were not able to assess whether differences in comorbidity that may confound the differences in eligibility were associated with insurance status (i.e., Medicare population) because comorbidity was a reason for ineligibility and therefore could not be assessed as an independent variable.\nFinally, our study sample does not contain races other than AAs and Whites, limiting our ability to make statements about other under-represented groups. Because race may not be self-reported, there may be misclassification of patients according to racial group. Furthermore, given the modest sample sizes for AA patients, care should be taken to assess in future research the possible role of variables not found to be significant in the present sample.\n---\nConclusion\n\nIn conclusion, there are marked differences in barriers to eligibility and participation in CTs among AA and White patients. Systematic use of the type of information reported here may allow cancer centers to address barriers specific to their patient population, select CTs that most appropriately target their population, and avoid commitments to participate in trials for which accrual may be difficult or impossible [47]. Such data could impact trial development on a broader scale to direct changes in eligibility criteria that restrict the participation of minority patients and help improve the CTs infrastructure [4,5,51]. Ongoing use and additional testing of the CTED system or similar systems in other cancer centers have the potential to provide data that could directly impact CT recruitment. a Includes evaluations for patients who had insufficient time or too much time elapsed to meet a particular eligibility criterion.\nb Physician determined that a patient was not a good candidate for a trial or physician preferred standard therapy for the patient.\nc Judged by research staff or attending physician as patient having an inability to comprehend informed consent.   \n---\nAcknowledgments\n\n",
        "Introduction\n\nDecisions made by employers are critical to improving employment rates among working-age adults with disabilities. During the more than two decades since the Americans with Disabilities Act (ADA) was first proposed in the late 1980s, many researchers have surveyed employers about their attitudes toward hiring and retaining workers with disabilities and their experiences with accommodating such workers. The picture that has emerged is generally rather rosy, reflecting ''a veneer of employer acceptance of workers with disabilities'' [1]. Answers to general questions about workers with disabilities reflect particularly favorable attitudes. For example, two early studies of Fortune 500 corporations indicated favorable attitudes toward hiring people with intellectual and other significant disabilities, benefitting both the worker and the employer [2,3], and positive views of the job performance of workers with disabilities generally [4].\nMore recently, human resource and other high-level managers responding to one survey indicated generally favorable attitudes toward workers with disabilities [5]; respondents to that and a second survey expressed a moderate level of commitment to hiring workers with disabilities [5,6].\nA similar picture emerges when employers are asked about their experiences with accommodating workers with disabilities. In a 1998-1999 survey of private businesses and Federal agencies, a majority of human resource professionals from both types of organizations reported that they had accommodated workers with disabilities in each of the following ways: made their facilities more accessible, created flexible human resources policies, restructured jobs, modified the work environment, provided written job instructions, provided transportation accommodations, and modified equipment [7][8][9]. Additional accommodations available from a majority of employers, according to a 2010 survey, include flexible work schedules, telecommuting, and ergonomic redesign of workstations [10].\nEmployers report that accommodations provided to workers with disabilities typically cost little or nothing [11][12][13][14][15], but are generally effective [13] and ''worth the investment'' [6] in terms of retaining experienced workers and increasing productivity [12,16], as well as improving organizational culture and climate [16]. In several general employer surveys, only a small minority cited concerns over the cost of accommodations as a reason for not hiring workers with disabilities [6,7,10,11]. Another potential financial concern is fear of litigation under the ADA or other non-discrimination laws, but employers rarely cite this as a barrier to hiring workers with disabilities. In one study, 4% of employers cited fear of litigation as a principal barrier [11], and, in another, this concern appeared fairly low on the list of most-often cited barrier to hiring workers with disabilities [17].\nNotwithstanding a few other studies revealing somewhat negative attitudes, especially those asking employers about more stigmatized types of disability [18][19][20], most employer surveys appear to paint a picture of successfully accommodated workers in a more or less welcoming environment. If we were to accept such findings at face value, we would be left wondering why the employment situation for working-age adults with disabilities remains dismal a full two decades after the enactment of the ADA [21,22]. Workers and job seekers with disabilities, for their part, often cite employer attitudes and workplace discrimination as barriers to acquiring or keeping a job (see, e.g., [23][24][25][26][27]).\nOne explanation is that true employer attitudes and experiences are not being obtained from employer surveys, either because employers are not being completely honest or because only employers with positive attitudes and experiences are responding to the surveys. The former could be the result of social desirability bias [28], in which respondents essentially report what they think the interviewer wants to hear rather than expressing their true attitudes, which are socially unacceptable and may run counter to legal requirements [1,[29][30][31]. The latter explanation, that employers with negative attitudes are not part of the survey samples, might come about because such employers either decline to participate or, in surveys whose sample is selected from businesses expressing interest in hiring or accommodating people with disabilities (e.g., [12,15]), are not part of the sampling frame. Studies focusing on employers with a history of successful accommodation are unlikely to detect negative attitudes toward or unfavorable experiences with workers with disabilities.\nThe present study attempts to address both limitations. To reduce social desirability bias, we asked human resource professionals and managers why they thought other employers might not hire or retain people with disabilities. And to compensate for selection or non-response biases in other studies, we purposely sought employers known or reputed to be reluctant to complying with disability non-discrimination laws. Our results directly contradict many prior findings, and offer participants' perspectives on strategies that could help improve hiring and retention of workers with disabilities.\n---\nMethods\n\nWe began with the hypothesis that our study would yield distinctly different results from prior studies if we were able to collect data from a set of ''ADA-recalcitrant'' employers-businesses and government entities known or reputed to be reluctant to hire and accommodate workers with disabilities. We identified such employers from among those who were referred to or otherwise known by the DBTAC-Pacific ADA Center, one of ten regional Disability and Business Technical Assistance Centers (DBTAC) offering information and guidance on complying with the ADA and other disability laws to businesses, government entities, workers, and other consumers; the DBTAC maintains partnerships with local organizations throughout Federal Region IX, and these affiliates also identified candidate employers. Employers were considered ADA-recalcitrant if they had directly expressed resistance to complying with the ADA to DBTAC or affiliate staff; had established such a reputation among DBTAC staff, its local affiliates, or the disability community; or had been referred to the DBTAC because of an actual or threatened legal action or complaint against them or as part of a settlement of a lawsuit or complaint.\nEarly attempts to question a few such employers directly about their attitudes and experiences were not successful, with participants becoming defensive and answering disingenuously, according to the interviewers' perceptions. Rethinking our strategy, we decided instead to use indirect or structured projective questioning, a technique suggested in the literature and found to be effective in reducing social desirability bias [32][33][34]. Instead of asking about the participants' own attitudes and experiences, we ask them to speculate as to the attitudes and behaviors of employers in general, not necessarily their own business or government entity. In a pilot test, this indirect method proved much more effective in engaging the participants to consider the reasons that employers might be reluctant to hire or retain workers with disabilities.\nWe developed a pair of paper-and-pencil questionnaires, the first on barriers to hiring and retaining workers with disabilities and the second on practical and policy strategies to improve hiring and retention. The first questionnaire contained two sets of statements asserting reasons that employers might be reluctant to hire (for the first set) or retain (for the second set) workers with disabilities, with each set beginning with the instruction, ''Thinking about employers in general, and not necessarily the organization you work for, please give us your opinion about the following statements.'' The statements were prefaced by the question, ''Why don't some employers hire people with disabilities?'' or ''\u2026retain workers with disabilities?''\nThe statements that followed were of the form, ''Some employers don't hire people with disabilities because\u2026'' followed by a reason and response choices of ''Strongly agree,'' ''Agree,'' ''Disagree,'' and ''Strongly disagree,'' along with ''Don't know.'' The section on reasons for not hiring people with disabilities contained 14 statements, beginning with the most innocuous (''\u2026they rarely see people with disabilities applying for jobs'') and ending with the least innocuous (''\u2026they discriminate against job applicants with disabilities''). Similarly, the section on reasons for not retaining workers with disabilities presented 12 statements, e.g., ''\u2026they believe that workers with disabilities can no longer do the basic functions of their jobs.'' These statements were developed by the project team based on our review of the literature, our own prior research, and the experienced garnered through frequent interactions with employers on ADA and other disability non-discrimination issues; they were then refined and augmented after a pilot test. Following each list of statements, space was provided for respondents to add additional reasons and offer comments.\nThe second questionnaire, which asked respondents to rate the helpfulness of suggested practical or policy strategies in improving hiring and retention of people with disabilities, followed a similar format. Following another instruction to think ''of employers in general, and not necessarily the organization you work for,'' statements were of the form, ''Employers would be more likely to hire and retain workers with disabilities if they had\u2026'' or ''if there were\u2026.'' Eight statements focused on practical approaches, such as ''a written company policy of nondiscrimination that includes disability,'' and another eight on policy strategies, such as ''tax breaks for hiring and retaining workers with disabilities.'' Response categories were ''Very helpful,'' ''Helpful,'' ''Not very helpful,'' and ''Not helpful at all,'' plus ''Don't know.'' Again, space was provided for additional strategies and comments.\nQuestionnaires were distributed to human resources professionals and managers working at ADA-recalcitrant organizations who attended ADA or other disability-related trainings provided by DBTAC-Pacific ADA Center and its affiliates. An introduction to the first questionnaire, required for human subjects approval, emphasized the voluntary nature of the survey and its confidentiality. Some 463 respondents, each attending one of 38 trainings, completed and returned questionnaires. Few attendees refused to participate entirely, although many declined to provide a response to one or more statements. To maximize anonymity, no information about the individual or the employer was collected on the questionnaires or provided to the researchers.\nItem non-response (includes missing, ambiguous, or otherwise invalid) averaged about 3% for the first questionnaire and 2% for the second; ''don't know'' averaged about 8% for the first questionnaire and 5% for the second. Both missing and ''don't know'' responses have been excluded from the analysis; i.e., the percentages reported are of known responses.\n---\nResults\n\n\n---\nReasons for Not Hiring or Retaining Workers with Disabilities\n\nTable 1 lists the potential reasons offered to respondents as to why employers might not hire people with disabilities, ranked by the proportion in agreement with that reason (either ''strongly agree'' or ''agree''). The top three reasons, each endorsed by more than four-fifths of respondents, refer to the cost of accommodations, lack of awareness as to how to deal with workers with disabilities and their accommodation needs, and fear of being stuck with a worker who cannot be disciplined or fired because of the possibility of a lawsuit. The next tier of reasons, agreed to by roughly 70% of respondents, are difficulty assessing an applicant's ability to perform job tasks, concerns over extra supervisory time, other cost worries, concern that the person with the disability won't perform as well as non-disabled workers, and lack of job applicants with disabilities.\nMore than half of respondents agreed that employers didn't hire workers with disabilities because they feel that workers with disabilities cannot perform essential job functions, and that employers discriminate against applicants with disabilities. The latter reason, however, was one of only four statements generating more than 10% strong disagreement.\nProposed reasons for not retaining workers with disabilities are shown in Table 2, again ranked by the proportion of respondents expressing agreement. Once again the three top-ranked reasons have about 80% or greater agreement, and the reasons are similar to those for hiring: lack of awareness as to how to handle the worker's needs; concern that workers acquiring disabilities will become liabilities, whether legal or financial; and concern over the cost of accommodations. Next follow concerns over job performance, other costs, difficulty assessing whether the worker can do the job, and belief that the person cannot do the job, all at 65% agreement or more. Only one additional reason, a belief that workers developing disabilities become less dependable (as opposed to less dedicated), was endorsed by more than half of the respondents.\nGiven space to write in additional reasons for not hiring or retaining workers with disabilities, or to comment on their responses, most participants either left the spaces blank or reinforced their agreement with the reasons presented to them, often supplying details or going beyond the statements we provided. After classifying the verbatim responses by topic (Table 3), we find that the most frequent remarks refer to employer concerns about job performance or qualifications. Many respondents felt that employers believed (or stated that they themselves believed) that a worker with a disability ''doesn't pull his own weight,'' ''can't do the job 100%,'' or ''might not have the same capacity'' as other workers.\nSome respondents referred specifically to essential job functions, but others said the problem was more subtle, related to what one respondent called ''the 'other things' that come with the job.'' One wrote, ''Employers want employees who are flexible and can do more than one task. They feel people with disabilities are limited.'' Another pointed out that ''in this day and age workers need to multitask and assume different roles during emergencies,'' something he or she thought might be a problem for workers with disabilities. A separate, frequently mentioned issue was ''greater absenteeism,'' ''always calling in sick,'' Response categories were ''strongly agree,'' ''agree,'' ''disagree,'' and ''strongly disagree.'' Responses of ''don't know'' are treated as missing and not included in the percentages * Response is ''strongly agree'' or ''agree'' ''absence from work too often,'' or ''time off from work for doctor's appointments''; several respondents appeared to hold these views themselves, indicating that they associate disability with poor health. The next topic most often mentioned by respondents was ''lack of knowledge or experience with people with disabilities,'' ''misconceptions as to what a person with a disability is capable of,'' and lack of knowledge ''about discrimination laws or reasonable accommodations.'' One respondent wrote, ''A lot of employers misunderstand or do not know the laws; they need to be educated.'' In addition to learning more about the ADA, ''employers should have training on disabilities and how to accommodate or handle them.'' One respondent offered specifics: ''Sometimes employers need help in 'rethinking' how the work can be completed. They need help in seeing how jobs can be done in a new way.'' Next among the comments were concerns over costs, including those of making the worksite accessible, Response categories were ''strongly agree,'' ''agree,'' ''disagree,'' and ''strongly disagree.'' Responses of ''don't know'' are treated as missing and not included in the percentages * Response is ''strongly agree'' or ''agree'' increased insurance premiums, and individual accommodations. Respondents also reinforced their agreement that fear of lawsuits and discrimination complaints was a central issue, along with the broader concern over liability in case of injury or accident. Less frequently, respondents brought up issues that we had not included in our list of reasons for not hiring or retaining workers with disabilities. Several referred to the ''hassle,'' ''paperwork,'' or ''trouble and effort'' related to hiring or employing workers with disabilities, such as having to ''spend time on issues they have never had to address before,'' ''deal with government bureaucracy,'' or ''be bothered researching accommodations.'' One respondent summed it up: ''It is a big hassle to hire a person with a disability because there are a lot of government regulations to follow. Employers have so many laws they have to follow already.'' Another topic introduced by the respondents was ''fear of the unknown,'' repeatedly expressed in those or very similar words. One respondent felt that employers ''may be afraid of people with disabilities, afraid of the unknown, and also afraid of certain disabilities more than others.''\nA related topic, also not included among our list of reasons, was discomfort in the presence of people with disabilities. Employers ''see so few people with disabilities that they don't know how to act when they meet one.'' They ''must always be careful of how different things need to be handled and must edit what is said or spoken to a person with disabilities,'' according to one respondent. Even if employers are themselves comfortable around workers with disabilities, they may fear that their customers or clients are not, according to several respondents. They might have vague concerns about ''image,'' or may worry that ''customers or members of the public that deal with the employee may have a reluctance or uneasiness in dealing with employees with disabilities,'' in the words of one respondent.\nA final topic not included in our list was attitudes of workers and job applicants with disabilities, mentioned by only a few respondents. One referred to an ''attitude of entitlement'' that another summed up as, ''I'm special, so treat me special.'' A third respondent explained, ''Some people with disabilities expect employers and coworkers to give them special treatment and assistance beyond reasonable accommodations.'' Many respondents wrote their comments in the first person and described their own experiences with and attitudes toward workers or applicants with disabilities, indicating that they were, at times, putting themselves in the position of the ''other employers'' they were asked to characterize. A few revealed disturbing attitudes reflecting personal prejudice and ignorance. One remarked-in the ''any comments'' area rather than the space for offering reasons other employers might not hire people with disabilities-that ''people with disabilities don't think the same way as normal people.'' Another wrote, ''I think that people with disabilities can't do the same things as people without disabilities. '' In contrast, many other respondents, despite working for ''ADA-recalcitrant'' employers, expressed positive views of the potential and performance of workers with disabilities. These opinions were often grounded in personal experience with disability or success in hiring, retaining, or working with people with disabilities.\n---\nStrategies to Improve Hiring and Retention of Workers with Disabilities\n\nFor each of the practical strategies we proposed that organizations might use to improve hiring and retention of workers with disabilities, at least four-fifths of respondents regarded the strategy as either ''very helpful'' or ''somewhat helpful.'' These strategies are shown in Table 4, ranked according to the proportion of respondents rating them as ''very helpful.'' More or better training is ranked highest, followed by an organization-wide source of expertise on accommodation issues; both were rated ''very helpful'' by more than two-thirds of respondents. These were followed closely by written guidelines for dealing with disability and accommodation issues and an organization-wide system for handling accommodation requests.\nAlso regarded as ''very helpful'' by about 60% of respondents were external guidance on disability and accommodation issues and a diversity specialist within the organization to deal with disability issues. More than half of the respondents believed that a centralized fund to pay for job accommodations would be very helpful, as would a written non-discrimination policy that included disability status.\nIt is interesting to note that expertise in disability issues is the focus of most of the top-ranked strategies. These include increasing knowledge among managers and supervisors themselves (#1), making available experts either within (#2) or outside the organization (#5) for managers and supervisors to consult with, or transferring the burden of solving accommodation problems from the managers and supervisors to an internal expert (#4 and #6).\nEndorsement of public policy strategies to improving hiring and retention of workers with disabilities was not quite as enthusiastic, with ''very helpful'' ratings ranging from about one-third to nearly two-thirds of respondents. As shown in Table 5, accommodations subsidized or entirely paid for by a government agency ranks at the top of the list, followed by no-cost, outside help with solving disability-and accommodation-related issues. Just over half said that tax breaks for hiring and retaining workers with disabilities would be very helpful. Some 45% indicated that an initial trial employment period for workers with disabilities would be ''very helpful,'' but this idea is controversial, with about one-quarter regarding it as not helpful, the largest negative rating of any proposed solution. Also rated very helpful at about the 45% level were salary subsidies for workers with disabilities and external mediation to help resolve issues before they result in legal action.\nIn the spaces provided for additional practical and policy strategies and other comments, respondents who wrote anything at all mostly reinforced and offered details on the strategies they had already been presented with. As shown in Table 6, the most frequent responses pertained to educating employers ''to change [their] mentality and perception against people with disabilities.'' Respondents suggested, for example, ''training\u2026that changes organizational perceptions of disability'' including ''an orientation for supervisors and managers about not discriminating against a person with a disability,'' ''testimonies of successful employees with disabilities'' and ''presentations by employers who've hired successfully.'' Education is seen as ''the silver bullet'' to ''de-mystify the myth that [workers with disabilities] can't do the job as well as someone without disabilities.'' Managers ''should be exposed to persons with disabilities working so they can see first hand what they can do and how well the job gets done.''\nThe next two topics mentioned in the comments relate to the bottom line, either with regard to incentives for hiring and retaining workers with disabilities or to subsidies for accommodations. Incentives could include tax breaks or subsidies for new workers, or programs similar to those offering incentives to minority-or women-owned suppliers or contractors: ''Get them to hire them first and see their abilities. Then at least the worker has a foot in the door.'' Improving corporate culture is a strategy mentioned by respondents but not explicitly included on our list. One respondent wrote, ''Instilling diversity values in a corporation allows an arena for inclusion no matter what a person's orientation, race, or abilities.'' Another pointed out that ''the commitment has to come from the top and filter down. Managers and supervisors who are change agents should be rewarded for their efforts.'' A third recommended a ''partnership with ADA organizations to create a cohesive and accommodating work environment.'' A second topic not included in the list of strategies involves training, not of employers but of workers and potential workers with disabilities. Respondents suggest that ''agencies provide coaching or mentoring to the job applicant'' or ''offer a class to help develop re \u00b4sume \u00b4s,'' and that there be ''retraining of blue-collar workers to do whitecollar jobs.''\n---\nDiscussion\n\nHuman resources professionals and managers working for ''ADA-recalcitrant'' organizations were asked to use their experience to speculate as to the reasons that other employers fail to hire or retain workers with disabilities. The approach was aimed at reducing two perceived sources of bias in prior studies: social desirability bias, in which respondents don't report their true, negative beliefs to avoid social stigma, and selection or non-response bias, in which employers opposed to hiring or accommodating workers with disabilities failed to respond or were excluded from the sample. In the present study, respondents often revealed negative attitudes and mentioned discriminatory practices toward people with disabilities that are contrary to the ADA, and they indicated many reasons they felt employers might oppose having workers with disabilities on their payrolls.\nIn 468 questionnaires that were filled out and returned, four-fifths of respondents consistently endorsed three primary barriers to hiring and retention of workers with disabilities: Ignorance According to our respondents, employers often lack an awareness of how to deal with and accommodate workers with disabilities. As a result, they may feel that employing such a worker will entail an added burden to managers, supervisors, and human resource staff, in having to learn about the employer's responsibilities under the law, research appropriate accommodations, evaluate their costs and benefits, and deal with unforeseen issues that arise. Employers may not have been exposed to successfully employed and accommodated workers with disabilities, performing their jobs as well as anyone else, or to success stories from other employers. This lack of familiarity can manifest itself as reliance on stereotypes of people with disabilities as poor job performers, an erroneous belief that people with disabilities are often absent from work, and general social discomfort around workers and job applicants with disabilities. Despite two decades of programs to train employers on the ADA and familiarize them with disability issues, it is clear that there is a great deal more work to be done.\n---\nCosts\n\nConcerns over the potential expense of accommodating a worker with a disability are also a major issue, according to the participants in our study, who contradicted findings from several other studies [6,7,10,11]. Despite the many studies [6,[11][12][13][14][15] indicating that the typical individual accommodation is inexpensive and more than pays for itself in increased productivity and the ability to retain an experienced worker, employers often see their obligation to provide ''reasonable accommodations'' as a substantial financial burden, one that harms the bottom line. They may also fear that they will need to make the entire workplace accessible, entailing a substantially larger cost. Beyond accommodations, cost concerns also extend to increased premiums for health insurance or workers compensation, as well as indirect costs such as extra supervisorial time or time needed to complete paperwork and deal with bureaucratic details.\n---\nLegal Liability\n\nIn the opinion of the survey respondents (but not of participants in previous studies [11,17]), employers often worry that employing a worker with a disability puts them at risk of a lawsuit or a formal discrimination complaint (for example, to the Equal Employment Opportunity Commission or its state-level equivalent), or perhaps at legal and financial risk should a workplace injury or accident occur. They may partly fear making a mistake that gets them into trouble, or may see workers with disabilities as particularly litigious, people who might threaten legal action if they are terminated or disciplined for poor performance.\nAnother area of major concern was job performance. Most respondents believed that employers are concerned that workers with disabilities might not work up to the same standards as other workers, might present problems with illness and absenteeism, or might not be able to perform either essential job duties or other tasks needed to be effective employees in an increasingly demanding workplace. A major source of uneasiness appears to be a belief on the part of employers that the law prohibits them from asking applicants with disabilities whether and how they can perform job tasks. Again, their lack of familiarity with the ADA, and their fear of doing something that could get them sued, may contribute to this problem.\nMore than half the respondents agreed that discrimination is a reason that some employers don't hire workers with disabilities. If true, then reliance on stereotypes and old notions of disability is no doubt part of the motivation. Some respondents suggested additional motivations: fear of the unknown, which is occasionally mentioned in the literature [35,36] as a reason employers are reluctant to employ workers with disabilities; and discomfort around people with disabilities, arising from lack of exposure (or ''social distance''; see, e.g., [37]). The person doing the hiring might feel the discomfort himself or herself, or he or she might project that discomfort onto potential co-workers, clients, or customers: Will the person fit in? What will the customers think? Fears regarding reactions of customers are generally unfounded, according to one recent study revealing positive attitudes toward businesses that hire workers with disabilities [38].\nParticipants also rated proposed practical and policy strategies for improving hiring and retention of workers with disabilities. Solutions rated as most helpful addressed the principal barriers mentioned above:\n---\nAwareness and Expertise\n\nThe single solution most often endorsed by respondents is increased and improved training for supervisors and managers on disability issues. About three-quarters rated this strategy ''very helpful,'' a finding may come as a surprise to the many organizations that have been training and providing ADA resources to employers for nearly two decades, as it did to the authors. Respondents' comments suggest that the need for information extends beyond employer responsibilities and recommended accommodations, to include exposure to successful employees with disabilities and to success stories from employers, strategies for rethinking job duties and engaging with workers with disabilities to understand accommodation needs and ways of achieving job tasks, and any type of presentation that would help dispel misconceptions and stereotypes and overcome prejudice.\nAside from instilling a general awareness in managers and supervisors, there was strong support among respondents for sources of expertise that managers and supervisors could turn to when needed. These include people within the organization tasked with the responsibility to assist with or handle accommodation and other issues, external resources to be consulted on such issues, and government-funded or other freely available experts from outside the organization who could come in and assist with solving specific accommodation problems. In general, transferring the decision-making burden from individual managers and supervisors to others within the organization-whether through formalized guidelines or specialists-was seen as highly beneficial.\n---\nSubsidies and Financial Incentives\n\nA program of government subsidies for worker accommodations was rated ''very helpful'' by nearly two-thirds of respondents. Other highly rated solutions addressing cost concerns involved public policy strategies, namely tax breaks or salary subsidies for employing workers with disabilities, and practical strategies such as a central budget within the business or government entity for accommodations, so that the organizational units are not, in effect, financially penalized for hiring a worker with a disability.\n---\nProtection from Legal Risks\n\nSupport was less than universal, but still substantial, for two policy strategies that could reduce employer concerns about being faced with lawsuits or discrimination complaints after hiring workers with disabilities. The less controversial approach was mediation in lieu of legal proceedings, in which an external service would be offered to resolve disability and accommodation issues, endorsed as ''very helpful'' by 45% of respondents. Endorsed by the same percentage, but also opposed by one-quarter of respondents, was a trial initial employment period, one that would allow the employer to dismiss a worker with a disability whose performance had not met expectations, without risking a lawsuit or complaint. Such a policy would, on the one hand, allow workers with disabilities to demonstrate their abilities, but might also open the door to treating such workers as casual employees subject to dismissal at the end of the trial period.\nRespondents suggested solutions of their own, such as improving corporate culture to increase respect for disability as an aspect of a diverse workforce, better pre-application preparation for job applicants with disabilities, and job training so that workers acquiring disabilities can transition, for example, from blue-collar to white-collar occupations.\nThe findings from this study differ substantially from those of many previous studies, in that they paint what we believe is a more realistic picture of the concerns, fears, and general attitudes of employers toward workers with disabilities. They also offer recommendations for actions that, we feel, could substantially ease those concerns and fears and improve attitudes.\nOrganizations providing ADA and disability training to managers, supervisors, and human resources personnel need to expand their focus to emphasize not only legal requirements but also problem solving strategies, information resources, and concrete solutions to accommodation and disability issues. A greater emphasis needs to be placed on communicating to employers that people with disabilities can be effective, productive, and reliable employees; one approach would be to feature employed people with diverse disabilities as trainers or as participants in the training. Advice to employers should also include guidance on procedures they could implement to improve the accommodation process and ensure a more hospitable workplace for employees with disabilities.\nEmployers, for their part, could take a greater role in acquiring and centralizing the necessary information and expertise to better understand disability, appreciate workers' abilities, and solve accommodation problems. They could also create company-wide procedures, policies, and mechanisms to place less responsibility and burden on individual managers and supervisors and could work to improve corporate culture and better support managers and supervisors who are open to hiring and retaining workers with disabilities. The result might be a more diverse and accepting workplace for all employees, a more flexible approach to retaining skilled workers and hiring new employees, opportunities to increase productivity and take advantage of untapped talent, and a greater focus on job skills and performance rather than fear of potential future problems. Bringing in external experts to help with disability and accommodation issues, furthermore, could not only offer a broader range of solutions, but also demonstrate good faith and ensure fair treatment, and therefore potentially reduce legal liability.\nPublic policy regarding employment of workers with disabilities could be enhanced in several ways, the most obvious of which would be to use various means, including training, public awareness campaigns, and enhancing or better publicizing available resources, to encourage employers to take the above steps. Policies that ease the financial burden, whether imagined or real, of employing workers with disabilities should also be considered. Finally, to the extent that employer concern over lawsuits is a real barrier to employing workers with disabilities, it might be useful to evaluate policy changes that could reduce or address such fears. We believe that these practical and policy strategies, taken together, could help alleviate the intractable problem of low employment rates among working-age adults with disabilities.",
        "\n\nhighly contested from a variety of quarters since its inception (see (Francis, 2006\u037e Paechter, 2007). We assume that sketching out gender theories provides a helpful starting point to the study. In ordinary discourses, men are thought as human males and women as human females. Many feminists endorse the sex/gender distinction to counter biological determinism (Mikkola, 2011). Biological determinists believe and argue that behavioral differences between girls/women and boys/men is the inevitable product of inherent, biologically programmed differences between men and women (Francis, 2006). Geddes and Thompson (1889) argued that social, psychological and behavioral traits were caused by metabolic state: women conserve energy (being anabolic) which makes them conservative, passive, lazy and least interested in the [public domain] and politics. Women therefore should not be involved in the public domain, especially in politics. Men expend their surplus energy (being katabolic) and this makes them variable, energetic, eager, dynamic, passionate, and thereby, interested in [the public sphere] and politics (quoted from Mikkola, 2011). Similarly, corpus callosum is thought to be responsible for various psychological and behavioural differences. On the basis corpus callosum it was claimed that women's thicker corpus callosums could explain what 'women's intuition' is based on and impair women's ability to perform some specialized visualspatial skills, like reading maps (Gorman 1992). The essentialist and biological differences is argument is found across disciplines, 'including within feminism, some radical and difference feminists have supported this idea and, often maintaining that women's biological differences from men and ensuing behaviour should be celebrated' (Francis, 2006, p 8). It is argued that women/girls and men/boys are 'predestined to gendered expression of behavior, which are fixed and inevitable (Francis, 2006, p 9). Feminists take up a serious argument with biological and evolutionary psychologists' explanation of women nature on multiples grounds: the corpus callosum is a highly variable piece of anatomy\u037e differences in adult human corpus callosums are not found in infants\u037e this may suggest that physical brain differences ender' is causally constructed social category, hence a social construction (Haslanger, 1995\u037e Skelton et. al, 2006). However, the sex/gender debate is not so easily solved enterprise and G actually develop as responses to differential treatment (FaustoSterling 2000 b)\u037e their infancy and, and no one can yet determine what impact brain differences have, or the ways in which these are manifested (Rose, 2001). de Beauvoir argues that one is not born, but rather becomes a woman, and that \"social discrimination produces in women moral and intellectual effects is so profound that they appear to be caused by nature\" (de Beauvoir 1972(de Beauvoir [original 1949]. Feminist also argued that the conclusions about human behaviours are made from primate behaviors because human beings control their natural and social behaviors whereas other primates lack these abilities (Levine and Hole 1973, p 173). Similarly, feminists strongly reject Sigmund Freud's thesis of 'pennies envoy' (Ullah, 2006\u037e Millett, 1970). Criticizing the biological essentialism, Levine and Hole (1973, p 172) argues that social unequal position of women throughout the history is not the result of their biology, but rather the result of the values society has placed, at any given time on the biological differences of the sexes. These values are not natural, they are social judgments, which consign women in the name of natural interpretation of biological on scientific, moral and technological grounds.\nChallenging biological and brain differences theories, feminists point out the role of social institutions in producing gendered expression of behaviour. Social learning theorists explains, rather assert, that gender identity is learned by children via social institution such as family, school, mass media, peer and so on. Many first waves feminist pointed the role of socioeconomic practices and expectation embedded in the legal system and social conventions and institutions as constraining women's lives and behaviour (Francis, 2006, p 10). This means, Beauvoir would argue, one is not born a woman or man but rather becomes a woman or man through social forces (also see Stanworth, 1981\u037e Millet 1971). It can be argued that gender socialization turns children into feminine and masculine individuals. In other words, femininity and masculinity are the products of socialization (nurture) how individuals are brought up. Gender differences, Haslanger (1995, p 8) would argue, are causally constructed: Kate Millett (1971, p 2829) argues that For Millett, gender is the complex whole of 'parents', the peers', and the culture's notions of what is appropriate to each gender by way of temperament, character, interests, status, worth, gesture, and expression. (Millett 1971, p 31). An alternative views were developed by cognitive development theorists. They (cognitive development theorists) argue that children learn gender identity (and gender stereotypes) through their mental efforts to organize their social world. This perspective 'suggested that children's understanding of their gender identity depended upon their stage of cognitive development (Francis, 2006, p 10). This means that children learn about gender and how to \"do gender\" because it is central to the way we organize society. They learn culturally appropriate ways of thinking and being as they follow routine rituals and respond to the everyday demands of the world in which they live. This means that socializing forces (family, peer and school etc) inculcate constant and forceful messages about how boys and girls should behave and act shaping us into masculine and feminine individuals.\nSex role/socialisation theories were very useful at first in second wave feminism as these offered the possibility of change. The common feature of this early work was a tendency to gender identities as fixed, and also to treat girls as a homogeneous group, as though their experiences were unified\u2026these readings present a single version of female experience\u2026it ultimately rely on dichotomous sex distinctions (Walkerdine and Ringrose, 2006 p 31). Believing of gender identities as fixed, girl as homogeneous category and feminine and masculine GENEROS Multidisciplinary journal of gender studies, 1 (3) 219 social forces either have a causal role in bringing gendered individuals into existence or (to some substantial sense) shape the way we are qua women and men.\ngender differences are essentially cultural, rather than biological bases that result from differential treatment.\ngendernorms were/are thought problematic as such approach to the understanding of gender fits with and reinforces females' subordination: they learn to be docile, emotional, passive, ignorant (see Millett 1971). It is important to highlight that gender theory was still in process of development and the understandings of how children 'learned' gender started to shift away from socialisation theories to those where child was a more active agent. It was this development in gender theories whereby some feminists (poststructuralist) criticized sex role socialization theories for their inadequate account for change and taking individual as passive recipients of socialization (see Skelton et al, 2006).ed (see Dillabough, 2006). Similarly, it was claim that people don't all share or experience the same construction of gender (Walkerdine and Ringrose, 2006)\u037e and the discourse was evoked to gender fluidity, femininities and masculinities in plural (see Skelton et al, 2006). However, the success of this stage was that the concept 'gender' was seen as social category, distinct from 'sex' biological characteristics that differentiate between men and women. The crux of this body of work was: gender expression of behaviour is socially produced rather than biologically inherited and determined. For detailed critique of this perspective see Connell (1987)\u037e Davis (1989)\u037e Walkerdine and Ringrose (2006). Thus, the social category of 'gender' (and also gender inequality) for social constructionists arises from interaction. However, there are many social constructionists whop see individual as biologically sexed, with consequences flowing from this bodily difference in term of the ways other interact with them. This mean that individual interact with each other with different expectation depending on the individual's apparent sex which in turn perpetuate gender differences in behaviours (Francis, 2006). There are other social constructionists who go further, seeing biological sex itself as socially constructed (Davies, 1989\u037e Butler 1990\u037e Paechter, 2001 are of great worth in this regard). This group of people is particularly influence by poststructuralism. For feminist poststructuralists, 'gender', 'gender inequality' and sexuality arise from discourse. The emphasis here was the relationship between discourse, subjectivity, and power. Much use of the word discourse was/is influenced by the work of the Michel Foucault, who explained the use of language and other sign systems as a means to control people's actions. Foucault's explanation of power as operating through discourses was able to clarify the phenomena of resistance and contradiction which had proved problematic for sex role theory [perceiving individuals as passive recipients of socialization via which social relations are reproduced (Francis, 2006, 10). Francis, citing Davies (1989), further writes: Foucault's theorization of people as positioned in and produced by discourses can also explain the gendered nature of society as produced by gender discourses that positioned all selves as men or women, and present these categories as relational (p 11).\nTaking Foucault into account, Butler (1990) argues that 'maleness' and 'femaleness' are simply produced by discourses\u037e sex itself is socially and discursively constructed. Butler views gender (and sexuality) as performative in acts, gestures and enactments. She further argues 'that the gendered body is performative suggests that it has no ontological status apart from the various acts which constitute its reality' (ibid, 336). Butler describes gender and sexuality as constituted effects of performance or of discourse. According to Butler, 'it is individual actions, gestures, enactments and institutional practice which produce the category of gender, gender identity and sexuality\u2026the political regulations and disciplinary practice produce that ostensibly coherent gender' (Butler, 1990, p, 337). Thus Butler very emphatically argued that gender is socially constructed rather than inherent, gendered traits are not tied to biological sex (Butler, 1990). Girls/ women can act and behave in 'masculine' ways. This mean that gender need to be understood how men and women are portrayed in discourse as well as in relation to existing social and cultural power structure.. In the light of the above discussion there seem a division between social constructionists and poststructuralists (i.e. in West and Zimmerman's analysis, gender lives in interaction\u037e in Butler, gender lives in discourse). Therefore, some feminists argue that the terms 'women' (MacInnes, 1998\u037e Francis, 2000, Whitehead, 2001cited in Walkerdine and Ringrose, 2006, p 32). Thus poststructuralist account argues that GENEROS Multidisciplinary journal of gender studies, 1 (3) 221 language is central to the development of subjectivity. Language is multiple and varied with no guarantees of the transference on intended meanings so, too, subjectivities are multiple, varied, contradictory and fluid. Defining discourse as relationship between language and its real power context, gender and discourse studies, including this study, focus on ways men and women are portrayed in discourse, analyzing how men and women are viewed in public communication (in this study textbooks discourse and educationists' views), how men and women themselves use language and so on.\n---\nMethodology and the study\n\nThe data for the paper comes from a larger study. 28 (11 female and 17 male) educationists were selected for the study through purposive sampling. The selected respondents were interviewed with the help of unstructured interview guide. It is important to make it clear that we use the concept of educationists in this study encompasses curriculum designers, working in federal ministry of education Islamabad\u037e subject experts and textbooks authors working in the textbooks board KPK\u037e executive education officers\u037e and head teachers in the selected public and private schools. The selection of respondents was made in line with the Glaser and Strauss (1967) model of research process which stresses the selection of respondents for the study in accordance with their relevance to the research topic. So the respondents of this study were not selected to construct a statistically representative sample of the population with the aim of reducing complexity by breaking it down into variables. But the aim was to increase complexity by including context and variety of respondents in the educational bureaucracy. This decision was made with the belief in the relevance and richness of data and less fussy about representativeness of the sampling.\nIt is reiterated that the study is informed by social constructionist understandings of gender. In particular, the writings of feminist poststructuralists have provided some useful concepts for analysis and discussion. Concepts such as discourses, positioning, and power/knowledge relation, as used by Foucault (1980), Davies and Harre (1990) and Walkerdine (1990), has been engaged for interpretation and analysis of primary data collected from educationists (who were working in various capacities in the educational bureaucracy. Drawing on insight from feminist poststructuralist, the paper, eemploying discourse analysis, explains how educational movements and reforms are political and frequently functions in favour of powerful groups (males). The study sheds light on how curriculum designers, subject experts, textbooks authors, and teachers working in the educational bureaucracy of act as agents of state/male dominated society by reproducing different and differentially located 'categories' of citizens: for example, girls as 'Other' of boys and essentializing women across space and time. With this in mind, each participant (educationist) in this research has been understood to have been constructed by, as well as constructs, her/his historical legacies (family socialization, educational and career journey, and the type of social capitals and opportunities available to her/him). They were viewed as producers of knowledge based on their experiences from which they claim the only 'real' and 'objective' knowledge. Throughout, the study attempts to highlight the belief systems and social forces which appear to operate as the basis for developing textbooks. It also considers, what are its far reaching implications? Particular attention is given to the stance adopted by female educationists with the aim of explaining how they are constructed and positioned by dominant discourses around gender\u037e and how they act to position children as female or male within the existing male dominated social structures. The aim of highlighting females' responses is to draw attention to the fact that gender power dynamics are not simply a matter of 'males dominate and females suffer' but that some women are also involved in maintaining and naturalizing gender hierarchies so that these continue to reflect male hegemony (Gilbert, 1989a). The paper, therefore, challenges essentialist perspectives using social constructionism as a lens. We contend that 'commonsense assumptions' frequently work in favour of society's powerful groups (males). It is argued that government's attempt of establishing and introducing gender equality in textbooks and to alter the prevailing gender power knowledge relation seem to have failed due to insufficient understanding of the complexity of such relations and to gender blindness of those dealing with curriculum and textbooks. The paper concludes by opening out a space within which Pakistani government official commitment to elimination of all kind of gender bias from Contrary to the above responses some respondents emphatically asserted that females have limited role in society and equal representation of females and males is not necessary. Some of these are quoted as under:\nFemale subject specialist (Urdu) textbook board KPK: I think underrepresentation of women in the textbooks does not discriminate them. You know women's roles are limited in society and where it is required (repeat and stressed) genuinely required, they are presented both in the text and illustrations.\nAnother female subject specialist, holding a senior position, reinforced the above views by asserting and believing in females' limited role in society.\n---\nFemale subject specialist (social studies) of textbook board KPK:\n\nSee textbooks represent what prevail in the wider society. If you look and count activities and works around you, women have limited roles in society. Therefore, they are lesser in number in school textbooks. I don't think their lesser number makes any difference.\nIt is important to highlight that for a considerable number (7) of the respondents, gender imbalance in textbooks, is unnecessary and pointless discussion. A male curriculum designer, MoE, Islamabad argued: GENEROS Multidisciplinary journal of gender studies, 1 (3) 225 I think these are trivial things that you [the researcher] are pointing and discussing. Society is suffering from many other serious issues. Don't you think we need to focus on other key problems in education instead of such secondary issues? [He continued] corruption, absentees and ghost 1 schools.\nA very identical view was expressed by the Principal of Higher Secondary School for Boys Peshawar city. He contemptuously said:\nThe West [people in the west] has reached space and trying to live there [expression of contempt] we are still wasting time in these useless issues [gender bias material in schools resources]. Can we not focus on important aspect of education?\nThese quotes suggest that how respondents' understanding and experience of the social world and their place in it, is constructed through discourses (Davies and Harre, 1990) and how their experiences (family socialization, schooling and interaction with the larger society) inform their approach to gender issue which, in turn, seems to support gender biases in textbooks.\nA very significant finding emerged when the questions regarding equal representation of females and males in the textbooks was asked to a female executive education officer. She argued that: I don't think it is not important to focus on how many of women exist in the texts and illustrations, what is significant are: in which roles and positions women are depicted. If there are more women than men but all of them are shown in the traditional stereotypical role of housewives, or depicted busy in domestic chores, it is more discriminatory than their less number against men.\nExecutive District Education Officer (female)\nThe argument then is that balancing up the gender character, names, nouns and pronouns in textbooks, but portraying them in the traditional gender roles will not solve the problem. Rather the stress should be what role they (females) are presented in. This is exactly the argument put forward by poststructuralists feminists (see Walkerdine, 1990\u037e Skelton, 1997).\n---\nGender based division of labour\n\nOn inquiry about whether men and women should be assigned different social roles (male in public domain and women in the private domain) due to their differential biology, a range of opinion and responses were expressed by the study respondents. Majority of both men and women viewed gendered social order as natural and inevitable. They believed that behavioral differences in females and males as the result of biological differences between the sexes. For them biology is destiny.\nA male curriculum designers MoE curriculum Wing, Islamabad argued:\nI think and believe that men are more suitable for the work in the public domain because of their stronger bodies, physical strength and rough and aggressive nature\u037e whereas women are more suitable for the private domain of home because women find it difficult to keep up with the long and odds hours that public domain demand.\nAn almost similar stance to the above question was taken by few women educationists. For example:\nFemale subject specialist textbook board KPK: \"God had created women inferior to men in term of their physiological and biological composition structures (pause)\u037e therefore, it is men duty to earn and spend on their women. Gender based division of labour in society seems natural and real division of labour. Nevertheless, there can be cross participation (women in the selected fields in the public domain and men in the private domain) on need basis.\n---\nGENEROS Multidisciplinary journal of gender studies, 1 (3) 227\n\nAnother senior female educationist viewed that women's participation in different fields of public sphere as a potential threat to the moral fabric of society. She argued:\nFemale subject specialist textbook board, KPK: Women's participation in all fields of public domain has given birth to too many moral evils in our society. I think it is better that they should be encouraged to develop liking for career in selected fields such as teaching and medicine.\nPatriarchal societies, Skeggs (2002) would argue, give women the responsibility for the maintenance of social order and safeguarding human races through their virtues. Some of the female educationists were quite critical about domestic ideology.\nPrincipal Girls' High Schools Peshawar City: Domestic chores are not women's natural roles. These are assigned to females by society. However, these are socially created and deeply ingrained in our culture which is difficult to escape. How can wives force their husbands to share domestic chores or say them you work at home and I am earning, ohhh (expression of helplessness).\nExecutive education office (female) Peshawar city argued that: Involving one's husband in domestic chores belittles the husband status in his social circle and no woman wishes that her husband be labeled negatively. The family goes smoothly when the wives/women sacrifice, remain submissive and subordinate to their husbands\nThe following extract from female educationists' responses shows that women in two paycheck families feel strongly overburden due to second shift.\nWe want our men to help us in household chores as we feel over burden after a daylong work in the public domain and also looking after the children and kitchen. We have to manage job and domestic chores for many reasons: a) it saves the family, otherwise it may lead to marital maladjustment\u037e b) we don't force husbands to share domestic chores as people around us will talk about our husbands in bad terms\u037e c) wives love to serve and care their husbands\u037e and d) it positions a woman as a respectable and good wife when she scarifies, remains submissive and subordinate to her husband.\n(Extract from the majority opinion)\nOne female respondent very forcefully and emphatically argued that men's involvements in domestic activities are not compatible with our culture. She argued: See we are not living in western society to ask men to carry out domestic activities. Rather, as you know, we live in a culture where it is considered bad to ask men to do household chores. I think it cements marital relation. To be good in domestic chores actually elevate female's position.\n(Female subject specialist, textbook board KPK)\nMost of these discourses seem unidirectional: justification of domestic chores as women's responsibility. It is eminent from these discourses that women use their feminine capital (domestic services and submissiveness) as 'bartering agent' for the acceptability and family security. The responses of the few (three female and two male) participants, who disagreed with sex based division of labour as natural but accepted it as cultural imperative, Fairclough would argue, are so profoundly naturalized within a particular culture that people are not only quite unaware of these most of the time, but find it extremely difficult, even when their attention is drawn to them, to escape from them in their course, thinking and action. (Fairclough, 1995, p 195).\n---\nSubject choice and gender\n\nSex of the children has decisive effect on their choice of subjects (see Page and Jha, 2009). Boys and girls, for example, do not pursue the same subject as the dominant ideology pushes them to study subjects which would best prepare them for their natural roles (argument of the essentialist and innate differences theorists). When choosing subjects boys and girls may be influenced by what they have learned about femininity and masculinity in early socialization (Sharpe 1976, 1994, Skelton et, al. 2006). On inquiry about which subject are more suitable for girls to pursue as an academic career, mixed but almost balance feelings and reactions were shared by the study's respondents. Almost half of the respondents expressed beliefs which strongly bind male and female role in society with biological differences between the sexes. Extract of some of these responses are:\nFemale subject specialist Textbook board KPK: girls, if they can, should study medicine or social science.\nThe above position was reinforced with a more detailed answer to the question by another senior female curriculum designer. She argued that:\n(\u2026) girls should study medicine as females are better doctors than males. However, all girls cannot and don't qualify for the medical college\u037e therefore, the best fields for girls [after medicine] are psychology and home economics [giving the reasons] ultimately females have to look after the family and socialize children. You know well, these subjects help them in homemaking and child rearing in the best manner.\n(Female subject specialist (Urdu) textbook board KPK)\nSimilarly, principal government higher secondary school for boys Peshawar city opined.\n---\nHazir Ullah & Johar Ali Male Hegemony Education\n\nBoys and girls hear differently, boys like cooler colour, girls like brighter colour, boys take risk, girls avoid risk [he argued] there is biological differences and we cannot equate males and females.\n[Similarly] boys are good in natural sciences as compared to girls.\n[Therefore] I believe that social sciences suits girl more than natural sciences.\nAn opposing point of view was held by an almost equal number (12) of respondents, consisting both genders, claimed that academic discipline should not be gendered as boys and girls can pursue any subject they wish in line with their aptitudes.\nA senior curriculum expert, Ministry of Education: (\u2026) gender makes no difference and I believe that no subject is masculine or feminine. Girls and boys can be equally good in a subject depending on their aptitude. But if females intend to pursue career, you know, there are many cultural issues for them which clearly affect females' choices of subject selection.\nThe above extract indicates how patriarchal structure of society constitute a framed whereby power is exercised through norms, hidden social threats which channel females to limited academic and job options without officially promulgated rules, prohibition and oppression. An almost the same position was held by another respondents.\nMale subject specialist (English) textbook board KPK: \"\u2026[A]ll fields are appropriate for girls if the patriarchal structure of society allows females to join any job they wish. Since society does not encourage to females to enter any jobs they wish due to restricted mobility, purda, therefore, parents and other social forces compel females to study subjects which either help them in running the family or guarantee a job in medicine, nursing and teaching etc.\n---\nGENEROS Multidisciplinary journal of gender studies, 1 (3) 231\n\nExecutive education officer female, criticizing cultural bias, argued:\nGirls can study all subjects if our society [patriarchal social structure] provides them opportunities for employment. I think cultural factors and some time lack of science laboratory and teachers in the neighboring School compel girls to study selected subjects [arts and humanities] and pursue education whatever is available.\nBoth these groups of respondents shared different opinion about the effect of children's sex on their choice of subject. The second category of responses is superficially not gender discriminatory and apparently very progressive. However, these, when critically analyzed, are not different but equally gendered by depriving children from their decision power on the ground of biological differences, cultural and structural factors such as purda, restriction on females' mobility to avail education away from their homes, and lack of educational facilities in female schools.\n---\n232\n\nHazir Ullah & Johar Ali Male Hegemony Education\n---\nWomen in traditional female careers\n\nThe study unpacked a very traditional mind set when the question regarding best profession for women was asked to the respondents. High majority, irrespective of their gender, agreed that the best professions for women are school teaching and medicine.\nPrincipal Girl high School: Teaching is the best profession for women as it has more and more vacations which give women the edge to look after household management.\nTeaching suit women as it is a job between breakfast and lunch time which does not affect women mothering role and she can easily manage domestic chores after school time. School teaching is best for women as it gives them an opportunity to educate and socialize their children in the best way.\nExtract from interview School teaching needs pyar (love) not mar (beating) and women are very kind hearted and, therefore, very fit for teaching profession.\n---\nExtract from interviews\n\nPrincipal Peshawar Cambridge (a private school): Teaching at school level involves less interaction with male members\u037e therefore, it keeps the parda intact. Therefore, I believe teaching is the best for women.\nThese responses justify the appropriateness of female as school teacher on grounds common in other society such as 'women being kindhearted, women are the best for teaching children (Solomon 1985\u037e Foster, 1993), it is in consonance with cultural norms (Joncich,1991), women needed income, they were anxious not to marry, they wanted to be more independent, and they were interested in fostering social, political and spiritual change (see Drudy, 2008\u037e Smulyan, 2006, p 471\u037e Hilton and Hirsch, 2000\u037e Hoffman, 2003cited in Francis, 2006, p 47) However, in addition to the exhaustive and multiple explanations coming from the western scholarship, this study's findings add additional reasons and justification for school teaching as women's job in the context of KPK, or may be generalized to the entire Pakistani society. These are: 'more vacations' and 'a job between breakfast and lunch time', both of which don't affect the 'domestic ideology' thesis. This is because of these gendered beliefs that textbooks are embedded with messages applauding school teaching for women with the attach messages that women in teaching also carry out all domestic chores (Ullah and Skelton 2012).\nFew respondents (5 males and 3 females) believed that biological differences between the sexes should not affect females' choices of career.\nWomen can enter any field and do any job except those that are physically strenuous and involve long hours.\n---\nGENEROS Multidisciplinary journal of gender studies, 1 (3) 233\n\nOne of the male respondents asserted that There are many qualities that women are bestowed with by nature and many they adopt from the environment which enable them to flourish and prove their worth in any sphere of social and professional life provided they receive conducive environment.\nSubject expert (English) textbook board KPK: Women can be the best in any field of the public domain if they are given opportunities by the male dominated culture.\nOne of the female respondent argued: women are better than men in doing any job as they always remain clear with and dedicated to their goals as compared to men. However, men don't let them to join all fields because they fear that women will threaten their supremacy by outperforming them.\nThese responses seem very encouraging coming from men as well as some women educationists. However, both categories have an implicit message which positions women in the subordinate positions in the society. As many believed that women are not capable of performing jobs that are 'physically strenuous' and involve 'long hours'\u037e it also believes that 'women are bestowed with some natural qualities' which give them superiority over men. Both these positions are very essentialists. These support the thesis of psychological and biological differences between the sexes which, in turn, give men the space to argue that women are best fit for selected fields in the public domain like teaching and medicines which are not physically demanding and need the caring nature of women. Moreover, this position on the issue negates the fluid nature of the 'gender' as a temporality which is embedded in the power of language (Davies, 1989\u037e Butler, 1990). It alsoreaffirms the crude gender (sexual) division of labour with little reference to the social complexity underlying the formation of ideas and beliefs about 'masculinity' and 'femininity' in family, schools, media, peer interaction and state (see Connell 1987, andWalkerdine 1990). Similarly, some of these responses stress a universal womanhood and its celebration. It links women's subordinate positions to patriarchy without giving attention to particularity of context and the manner in which gender identities are shaped by social institutions and children response to their socializations (Measor and Skies, 1992). Essentialists thinking are embedded in the claimmale dominated culturewhich asserts that all men are oppressors and all women oppressed. In both categories there seems an implicit politicsavoiding or pretending to understand and challenge the root causes of 'gender codes' and 'gender order'.\n---\nWomen and political leadership\n\nThere is a dramatic shift and evolution in women's entry to politics around the world. Nevertheless, women's inclusion and exclusion as political actors depends on a combination of economic, cultural, social, political and religious reasons. To find out the reasons of women's invisibility in the position of political leaders in the textbooks (see Ullah 2006), opinion of the educationists were sought on the question 'can/should women be political leaders? Mixed responses were received from respondents which are transcribed and presented into two broad categories: women shouldn't and cannot be political leaders.\nSecondly, they can and should be. Response of each participant was sorted into the relevant categories irrespective of his/her gender and a general extract has been derived from these responses. Majority (11 out of 17) male and (7 out of 11) females respondents opined that women cannot be effective political leaders and therefore they should not try to be political leaders. Their responses are presented in the following quotes:\nHow can we talk about women to b leader in society in which men of characters and good reputation are afraid to participate in the dirty Pakistani politics?\nExecutive Education Officer (female) Peshawar: She particularly argued thatwomen cannot be an efficient political leader as they cannot keep secret and top political positions require politicians to keep state secrets. She further added that it is in women nature and psychology to share their stock of information with other and they enjoy telling 'half baked' stories\".\n---\nGENEROS Multidisciplinary journal of gender studies, 1 (3) 235\n\nWhy we should talk about things which are useless, women are not allowed by the religion Islam to be political leader\". A belief held by 3 male and 4 female respondents.\nThere were few (6 males and 4 females) respondents who believed and supported women's role in politics.\nMale subject expert (English) textbook board KPK: Women can be better political leader than men if they were provided opportunity and were allowed by men to participate in politics. They referred to Benazir Bhutto as the most efficient political leader after her father Zulfiqar Ali Bhutto.\nThey can be efficient political leaders provided they get conducive cultural environment to demonstrate their leadership talent. Nevertheless, society's elites don't want their wives to be political leaders as they are afraid their women may threaten their authority. If women of the elite class cannot be part of the politics how can we talk about the rest of women in Pakistan? Some of them even pointed out Benazir Bhutto, Hina Rabi and even Hillary Clinton to have gain political prominence through their families, benefiting from their family connections. These belief are so deeply established and held that curriculum and textbooks are not only silent about women role in politics but the role of the few prominent political figures (i.e. Fatima Jinnah) have been masked and highlighted with their feminine characteristics such as loving, sacrificing and kind instead of their political activities (see Ullah and Skelton, 2012).\n---\nGender, sports and physical activities\n\nWhen asked about 'whether boys and girls should play the same games? Majority of the respondents, irrespective of their gender, expressed that they should not play the same games. They shared various reasons and explanation for their beliefs. After constant comparison of the explanations and positions that the respondents had on the issue of gender and sports, following extracts were obtained which was common among the majority respondents. Some sports which involve more physical strength like cricket, hockey, football etc don't suit girls due to their physiology. Therefore, females should not play these.\nExtract from interviews Subject expert textbook board KPK: Girls are not created with the capacity of running and jumping. One can remain healthy even without playing any sport.\nPlying sports may break girls' hymen which can create future social complication for girls at the time of marriage. Keeping the hymen intact and saving it from breaking is what ensures her virginity at the time of marriage.\n---\nExtract from interviews\n\nSports and games don't have gender and these should not be engendered. There is neither male sport nor female sport. Girls and boys can play any sport they wish. However, in the existing cultural environment women don't have the opportunity to play any game. We have to change the culture first. Extract from interviews GENEROS Multidisciplinary journal of gender studies, 1 (3) 237\nThe essentialist and biological imperative argument seem to have limited and continues to limit females' participation in sports and physical activities. Lumpkin (1984) argued that [historically] women were not provided equal opportunities because of the perceived physiological differences between the sexes (cited in Everhart, Pemberton and winter 2001). Analyzing the above illustration with Foucault's (1980, p 39) notion of 'power as circulating, existing in the individuals' action\u2026 touching their bodies, inserting into their attitudes \u2026and everyday life' make good sense. The 'hymen myth' is equally restricting women participation in sport. The prevalence of hymen belief among majority male and female participants alludes to what Foucault called surveillance and or the ways Foucault (1980) and Walkerdine (1990) came to understand power as something beyond the power of the state which is visible and invisible, manifest and hidden and that exist everywhere. Here the power is invisible but exist in every site to control females' sexuality and maintain their modesty. The shift from manifest textual discourses to verbal discourses is actual a shift from visible to invisible apparatus of regulation and power relation (Walkerdine 1988). Taking into account the 'hymen myth' is the sole marker of female virginity and modesty is irrational and ideologically embedded discourse which serves the interest of male domination. Modesty is demanded in the religion Islam from both males and females.\n---\nConclusion\n\nThis study explored the contradiction that educationists have regarding the issue of gender and education, especially with reference to the gender equality efforts in textbooks. The findings reveal that, on the surface level, there seem a tiny group of educationists who understand the notion of gender equality but their understanding of gender equality is in term of balanced number of male and female illustrations rather than in a way that would explicitly challenge gender stereotypes. Majority of female educationists, not all, were found more conformists in protecting and promoting the dominant notion of femininity and masculinity. They firmly believed in the essentialist dichotomies of each gender. On the whole, educationists see 'gender' issue as 'sex' issue where boys/men and girls/women are seen as separate entities abiological perspective that reduces 'gender' to the essentialists views of males and females. This malefemale binary confounds any meaningful discourse on 'gender'\u037e we will say block thinking and discourse of the varieties of 'femininities' and 'masculinities' that exist out there (see Connell, 2006). To be more robust in the claim, we argue that the study findings suggest educationists' beliefs (which inform school textbooks and school process) clearly underpin and support gender biases and stereotypes in school textbooks. Hegemonic masculinity and feminine subordination is naturalized and legitimized through the powerful discourses of 'social role conformity on biological differences between the sexes', 'institutional responses to females participation in education and the work world', women as the custodians and bunkers of morality'. To ensure gender equality in and through education, a comprehensive gender awareness training of educationists cannot be ignored and taken lightly. Female can think out of the traditional gender roles when they come across multiples role models. Thus presenting children with a 'wider range of experience' (Walkerdine, 1990, p 89) [options, roles, and positions] may change children's view of themselves and possible course of actions (Skelton, 1997, p 43).\n---\nGENEROS Multidisciplinary journal of gender studies, 1 (3) 239\n\n\n---\nNotes\n\n1 Schools that exists only on paper and are functional in the government's record but teachers and students do not come for teaching learning.",
        "600\n\n\n---\nJohn Turner and others\n\nThe history of psychiatry in post-war Britain has largely been told through two interlinked narratives: the rise of psychopharmacology and the process of 'decarceration'. As Hess and Majerus noted in 2011, 1 such narratives share with manifold histories of nineteenthcentury psychiatry a concern with the issues of rights, confinement, treatments and the level of the asylum population. While these tropes have illuminated many aspects of the history of psychiatry, there is a striking divergence between the 'single-issue mythologies' 2 developed in these works and the sheer diversity of approaches to understanding and managing mental distress and disorder that characterises the British mental health services at the beginning of the twenty-first century. The scope and rapidity of change has left many developments in social policy, legislation, medico-legal practice, service design, service delivery and clinical practice without systematic historical analysis. New emphases in service provision, such as person-centred care, well-being, recovery, the involvement of service users and increased access to psychological therapies, lack a historical context. Indeed, the language of mental health has changed. A historical narrative structured around rights (the right to health and the right to liberty) is now complicated by the rise of new organising categories such as 'costs', 'risks', 'needs', 'inclusion' and 'equality', which contemporary actors use to define competing visions of mental health services. As a first step in tracing out the new language and landscape of mental health care, this paper sets out a research prospectus in the form of a report on a series of witness seminars and interviews concerning the history of mental health services since the Mental Health Act of 1959, which replaced the legislation under which services had been provided since the 1890 Lunacy Act. Seminars were held at the Wellcome Unit for the History of Medicine in London in 2010 and 2011, and were supported by the Wellcome Trust. 3 \n---\nMethod\n\nThe explicit purpose of the seminars was to develop historical questions rather than to generate replicable answers. This had implications for the selection of speakers, the conduct of the meetings and the construction of the report which follows. The principal contributors were fifteen speakers: practitioners, policymakers, historians, service users and social scientists with diverse backgrounds and professional roles. These included two civil servants, three clinical psychologists, a psychotherapist, five psychiatrists, three social scientists of diverse specialisations and a third sector provider. 4 In view of the purpose of the seminars, the group was not constructed as a representative sample. Some of the contributors were invited because they had played key roles in policymaking or debate in the recent or more remote past; willingness and availability played a large part in the final roster. It would in any case be difficult to embody a 'typical' range of clinicians in a few members of each profession and, indeed, any such selection on our part would have involved begging the question. While the absence of a biologically inclined psychiatrist, from a roster in which psychiatry was, if anything, over-represented, was perhaps the most\nThe History of Mental Health Services in Modern England 601 obvious lacuna in the representation of professionals, there were many other varieties of psychiatry and psychology, and particularly of nursing and social work, which could only be represented by proxy in the accounts of the professional contributors. Many of the speakers had overlapping roles. Of the seven primarily involved in policymaking, six had also been practitioners; two of the psychiatrists and two of the clinical psychologists had extensive policymaking and management experience; three practitioners were also historians. One witness and a number of the other participants had experience as service users.\nEach seminar was conceived to explore where possible the perspective of a particular group or profession. The first was devoted to service users; subsequent seminars addressed the views of psychiatrists, psychologists and policymakers, with further sessions addressed by single individuals. Seminars were also attended by a group of interested academics and postgraduate students, who contributed to the discussion. A number of speakers and many of the other participants attended for more than one seminar, lending a degree of continuity and integrity to the discussions in which service users and professionals were engaged.\nSpeakers were initially asked to act as witnesses rather than historians, reflecting upon their own experiences with a view to elucidating important themes in the history of mental health services in the period. Each seminar began with brief and relatively informal presentations, followed by questions and discussion. All the proceedings were taped and transcribed to provide the basis for this report. However, to encourage freedom of expression, speakers contributed under modified Chatham House rules, in that explicit permission would be required for any comment to be published.\nLike many participants in \u00e9lite oral history, contributors found it difficult to separate the roles of witness and historian. For the purposes of this project, this common pitfall was less of a problem than it would have been if we had been trying to use the material to create or validate an historical narrative. 5 Many of the contributors had been engaged in practice and debate during the events which they were recollecting. Memories were well rehearsed, and framed by the language and assumptions of those debates. On certain sequences of events, they were sometimes demonstrably wrong. On certain subjects, nonetheless, it could be seen that participants were authentically reporting past perceptions. There was a striking congruence, for example, between the tone adopted by some contributors in discussing the early years of community-based services and the anxious, slightly sceptical foreboding which appeared in contemporary published accounts. 6 This might be taken as corroboration of their interpretations, or as an example of the tendency of elite witnesses to create a story and stick to it. In some instances, by contrast, it became clear that the experience of participants contributed only a very preliminary stage in the development 5 See A. Seldon and J. Papworth, By Word of Mouth -'Elite' Oral History (London: Methuen, 1983) and Lynn Abrams, Oral History Theory (London: Routledge, 2010). Most methodological discussion of elite oral history concentrates rightly on the limitations of memory, the need for triangulation between witnesses and with contemporary documents, and the pitfalls of a narrative co-constructed as a conversation between historian and witness (on which see in particular E.M. McMahan, Elite Oral History Discourse: a Study of Cooperation and Coherence (Tuscaloosa: University of Alabama Press, 1989)). These issues are of particular salience when the objective is to produce the most accurate possible account of a sequence of events. Our project was to generate themes for enquiry, and in this case the challenge was more to situate the interpretations of contributors in existing literature. A more forensic and less discursive approach will be necessary if, in a subsequent study, contributors are to be asked to provide evidence for answers to the questions which might be posed in such enquiries. 6 Cf. J.K. Wing  of an important question. A case in point is the discussion of resource allocation, in which participants knew that there was an issue, because they had felt the consequences, but did not formulate the question or identify methodological problems in the way that historians must. In other cases, such as the exploration of the influence of anti-psychiatry, it was evident that speakers were joining an ongoing conversation rather than conveying a coherent account of a situation in the past, and in such cases we have had to refer to existing literature to clarify their argument (and ours). Our investigation revealed further problems with the language of description, littered as it is with terms of art such as 'recovery' and 'community care' which not only divide professionals from the general public but are also given varying meanings within the professional community.\nIn the original project for the seminars, service users were identified as stakeholders on an equal footing with practitioners and policymakers. This was easier said than done. The service user view was put forward in at least four ways: by service users who were also academics with an interest in the service user voice; by other service users in the audience; by proxy in the contributions of mental health professionals who were not service users; and by proxy in the contributions of academics who were interested in service user issues but who were not themselves service users. None of these modalities can be regarded as perfect. Necessarily, because service users as a category are even more heterogeneous than professionals or policymakers, no claim to typicality should be made for the service user contributors; some spoke as individuals, some spoke in a role defined wholly or partly as a service user role which would be construed as representative. Service users, however counted, were also hugely outnumbered by professionals of some ilk. As a result, although the service user voice emerges pervasively in the discussion, it does so rather more diffusely than the professional voices.\nThe material presented in the rest of this paper represents a distillation by the authors of the key concerns which emerged from the seminar discussions, informed by existing literature on mental health services in the period, which is referenced in footnotes. The contributions of the speakers are to a large extent privileged over the comments of the audience, which generally appear as background commentary without footnote reference. Through a series of consensus meetings in which the authors collectively reviewed the transcripts, we have selected for discussion those themes which were most salient to all three 'stakeholder interests' -practitioners, policymakers and service users -but note also, in the conclusion, other prominent themes which demand fuller historical exploration. The main discussion themes, interestingly, tended to recur across groups and were not confined to one session. Notwithstanding these caveats, we would contend that the seminar discussions provide the basis for a new programme of enquiry which would reflect the lived experience of participants and, inter alia, respond more fully to the service user perspective than more traditional accounts. again through the 1990 National Health and Community Care Act, which laid the principal responsibility for community-based care on local authorities. Until 1997 the improvement of care for people with severe mental illness was the principal focus of policy. This was marked by a preoccupation with 'dangerous' people as much as with the provision of resources.9 Throughout the period, public spending on mental health was low compared with physical health, particularly with regard to services for children, adolescents and the elderly.\n---\nThe context\n\nThe 1997-2010 Labour governments increased spending on mental health, though not by as much as the general increase in NHS expenditure. The plan was laid out in Modernising Mental Health Services in 1998 and executed as part of target-driven reforms of NHS provision. The 1999 National Service Framework (NSF) for Mental Health set out specific objectives, though only for adults of working age. Following this, mental health was one of three declared clinical priorities alongside cancer and heart disease, in the 2000 NHS Plan. Targets were accompanied by promises of funds and an unprecedented level of detailed guidance from the Department of Health. In 2006 the Improving Access to Psychological Therapies (IAPT) programme was introduced with the explicit rationale of reducing the economic burden to the country of mild to moderate mental illness. Labour also sought to extend and clarify the powers of compulsion over mentally ill patients which had been codified in the 1959 Act and mitigated substantially in the 1983 Mental Health Act. The new Mental Health Act was finally passed in 2007. The Labour government's programme dominated the recent working lives of the seminar contributors. The coalition government's policy paper, No Health without Mental Health, was only published in February 2011, while the seminar programme was under way.\nChanges over the period can also be described, if only partially, in figures. Table 1 illustrates some key aspects of change. In the broadest of terms, the mental health service in 1950 compulsorily detained most of its users for long periods in large asylums, attended by nurses under the supervision of doctors. Sixty years later, most service users passed most of their lives without legal constraint outside hospital, supported by a number of professions. The range of problems addressed by the services was much larger, and the expectation of cure or relief of symptoms was much higher. That said, those with severe mental illness were still liable to spend time in and out of hospitals (as demonstrated by the changing ratio of admissions to occupied beds) and may indeed have been spending longer in total than similar patients as much as a century before. 10 Since the 1990s, the number of those compulsorily detained has begun to grow again. The table does not show the provision of services by primary care or by local authority social service departments, which took increasing responsibility for residential care and some community-based care across the period. Nor does it show the growth of provision outside the NHS, in the form of private practice by psychiatrists, psychologists and counsellors, the provision of services by user groups and charities, and the growth of private hospitals offering services directly as well as by contract to the NHS.\nThe 'patient journey' into specialist mental health services also changed substantially.  2. N/R indicates that official data for an element were not recorded or published on a national basis.\n3. The data are only as good as contemporary methods of collection and aggregation. Increases in the complexity of data collected across the NHS in recent decades, accompanied by a growth in the number and type of reporting entities, have improved the validity of some measures but reduced the reliability of national aggregates. Increasing use of data for performance management has increased the probability of manipulation both at the point of collection (at the patient interface) and at the point of reporting (Strategic Health Authorities or Trusts), and thus reduced the reliability of aggregates; this uncertainty applies particularly to admissions, length of stay and outpatient activity in 2000 and 2010. Patient data and nursing workforce data are rounded, partly to reflect the possible inaccuracies and inconsistencies in official statistics.\n4. Except where indicated, staff numbers are headcounts. Changing reporting conventions make it impossible to present whole-time equivalents on a consistent basis across the period.\nTable 1: Secondary mental health services: NHS in England and Wales selected summary data.\nhave been determined by the Physician Superintendent of the hospital. From then until 1983, the route from the GP would have been to a consultant psychiatrist at an outpatient clinic, or sometimes directly to a clinical psychologist; the clinical process would have been controlled by the consultant or the psychologist. Between 1983 and 2000, GPs would increasingly refer to multi-disciplinary Community Mental Health Teams, where other professionals, or GPs themselves, might assume responsibility for the clinical process. 11 The turn to Community Mental Health Teams was underwritten by the Department of Health's institution of the Care Programme Approach in 1990. Under this scheme, key workers from either the NHS or local authority social services would be charged\nThe History of Mental Health Services in Modern England 607 with coordinating the delivery of individual patient care.12 Under the National Service Framework after 2000, GPs might refer to one of a number of different specialised teams. Under the Increasing Access to Psychological Therapies (IAPT) programme after 2006, people experiencing mental distress could also refer themselves to some services without GP intervention. 13 For many, though, the patient journey never started, and for most it was very short. The prevalence of mental disorder not dealt with by specialist services became a topic of discussion in the 1960s. 14 A WHO study in the early 1990s estimated that for every thousand adults, between 250 and 315 were suffering from some sort of mental disorder, of whom only 101 were detected by GPs, only 20.8 were referred to specialist mental health services (including community-based services) and only 3.4 became in-patients. 15 In recent years the rate of referral per thousand adults has probably increased, 16 but it remains the case that the majority of mild to moderate illness is treated by GPs, if at all.\n---\nHistory as retrospective: contributors' reconstruction of their own past\n\nAt least for the earlier years a broad consensus emerged about the main characteristics of the mental health services. The 1960s were acknowledged as a period of slow but necessary change. Services were dominated institutionally and intellectually by psychiatrists, who began to establish a more distinct professional identity and formal training under the Royal College of Psychiatry (chartered in 1971). 'When I was being trained in psychiatry we thought that we were the experts and we decided.' 17 Senior psychiatrists tended to see the 1959 Act as benign in its impact on services and patient experience, partly because it allowed them, through the procedure of voluntary admission to mental hospitals, to implement improvements in treatment and care which had been foreshadowed in the 1950s. '. . . within limits, particularly financial limits, doctors could do what they thought was best. If somebody had a bright idea for a new service, they could do it, provided it didn't cost very much.' 18 The immediate impact on mental health services of Enoch Powell's Hospital Plan was to achieve the union of psychiatry with medicine and create conditions for effective treatment in the community, while promising the eventual closure of the old asylums. For this reason, long-serving psychiatrists saw Enoch Powell as a progressive figure in mental health reform. 19 At the same time, contributors acknowledged that psychiatric treatment ('some sort of a mystic process' 20 ) was barely supported by evidence and often bizarre ('the eccentricities of some of the treatments used when I was a trainee simply would make your hair stand on end' 21 ), and care was perceived by service users as 'awful'. 22 Dissatisfaction with the status quo, as recollected by contributors, was manifested in a number of ways. The intellectual challenges of 'anti-psychiatry' in the writings of David Cooper et al. coincided with a general counter-cultural challenge to medical authority. 23 The therapeutic pessimism of the asylum system was challenged by hopes for new pharmacological interventions, while the lack of sufficient resources for community care of the mentally ill was already causing frustration to clinicians. The earliest service user movements appeared in the early 1970s, demanding civil and economic rights for patients in the community, and, in parallel, pressure groups such as MIND began to agitate for changes to the 1959 Act. Yet the 1970s also saw significant innovations in treatment and service delivery, led by clinicians responding to these challenges. There was increasing use of psychological treatments with an evidence base and widespread acceptance that the services needed to acknowledge and counteract the social devaluation of their users. 24 From the mid-1970s, structural reorganisation and disruption figured more strongly in recollections, though it was still said that 'in spite of Enoch Powell's apocalyptic threats in 1962, the 1980s was the best time that the mental hospital ever had. Money wasn't flowing freely but there was more than there had been; numbers (of patients) were falling, while the staff numbers were being preserved. So the standard of nursing care was going up. And there was, in most cases, enough to make the old accommodation certainly tolerable, if not quite satisfactory.' 25 The major reorganisation of the NHS initiated by Keith Joseph in 1974 demanded a new strategy for delivering communitybased care, and the subsequent Labour government, operating under great financial stringency, accepted the thrust of the reorganisation but increased the mental health budget by a mere 1.8% to deal with the consequent needs. Clinicians remembered this process as doubling the number of managers in the NHS, 26 though some of these were undoubtedly existing senior clinicians rebadged as managers. More significant changes followed the first Griffiths Report in 1983 into the management of the NHS. 27 This recommended the introduction of regional and district managers into the health service and the devolution of decision-making to hospital level. The second Griffiths Report of 1988 28 and the 1990 National Health and Community Care Act were seen to further increase levels of managerialism, creating the purchaser/provider split in the NHS and the clear allocation of the principal responsibility for community-based care to local authorities. Griffiths also 'promoted the use of the independent sector, as it was primly called at\nThe History of Mental Health Services in Modern England 609 the time. This is private medicine of course.' 29 Witnesses were manifestly aware that in this somewhat chaotic climate they themselves, as front-line workers, were significant in shaping, for better or worse, the public policies they were supposedly implementing, when community care was 'a kind of shared myth' without clear definition. 30 This did not reduce the remembered frustration at the inadequate resources made available: 'for a long time community care was largely a myth: everybody agreed it was a good thing and we should have more of it, but resources were simply not allocated in that direction.' 31 Thus for the last two decades of the twentieth century a consensus memory was superseded by multiple, interlocked narratives. Professionals divided into negative and positive camps, with the division cutting across professional boundaries. The disenchanted professional view held that, as above, the movement of patients out of psychiatric hospitals into the community -'decarceration' -was a good idea spoiled by inadequate resources. From this perspective, public policy was at fault, because ministers would not fund or direct community care but pandered to public prejudice by imposing greater restrictions on patients and professionals, especially in the 1990s and after, when politicians renewed their interest in mental health policy. Witnesses argued that a renewed emphasis on control and confinement was a bad policy and that it grew from a political reaction to scandal: specifically, homicides by psychiatric outpatients, such as the killing of Jonathan Zito by Christopher Clunis in 1992. This was held to have led to an emphasis on forensic services, with a diversion of resources from other aspects of mental health into High Dependency and Medium Secure Units, 'the new lunatic asylums that Frank Dobson dreamt up'. 32 Response to crisis was held to explain new organisational and procedural demands on mental health services such as the Care Programme Approach ('a central imposition [in the] 1994 guidance arising out of the Clunis affair' 33 ) and the development of specialist structures targeted largely at individuals deemed to be a risk to themselves or others. These included assertive outreach teams and crisis resolution and home treatment teams created under the 2000 NHS plan, which were associated by some clinicians with fragmentation of care. 34 A more optimistic professional perspective saw a set of positive developments in the same period. On the one hand, closure of the long-stay hospitals was associated with significant advances in psychiatric rehabilitation, involving multi-disciplinary teams working in the community. 35 This was associated with new professional aspirations, particularly for social workers, in a climate where 'actually the mission was about helping 29 31 Freeman, op. cit. (note 18), 9. 32 Goldberg, op. cit. (note 17), 6. 33 Turner, op. cit. (note 23), 11-12. In fact the Care Programme Approach, a set of management procedures for assessment, co-ordination and review, was developed by the Department of Health well before the Zito episode, as part of the implementation of the National Health and Community Care Act. 34  people achieve an ordinary life, whatever that meant to them'. 36 Practice was influenced by the advice of service users, often brought in to train professionals, and the role of the professional was increasingly that of an advocate for the service user. While a focus on the most needy was associated with the management of risk, the profile of mental health services was raised by the 1992 Health of the Nation white paper which 'put mental health on the map in slightly sort of morbid terms because suicide was the target, and people at the time talked about that being a kind of proxy, a way of getting some attention to mental health.' 37 Over the same period, evidence-based developments in psychological treatment expanded the range of interventions available, and, more generally, the development of a scientific literature led to a greater homogenisation of methods, 38 though the disenchanted view was that drug treatment still predominated and that 'despite the powerful research literature and the heavy artillery of guidance from the National Institute of Clinical Excellence' [founded in 1999], psychosocial interventions had not penetrated into routine clinical practice. 39 The same years furnished wholly different insights from and about the users of services. Notwithstanding innovation, goodwill and improvement on the part of providers, mental disorder for the service user continued to be associated with social exclusion and the denial of civil rights. Although the 1983 Mental Health Act brought in new procedures, in practice patients legally confined in mental hospitals were rarely successful in their challenges to the system, and although voluntary patients could make decisions about their treatment, 'if you don't say yes you're likely to find yourself detained'. 40 For the growing number of patients outside hospital, 'the problem with community care is not just about management; it is about misery, poverty and the style of mental health services that offers no real choice about the type of support available. The kind of service provided by mental health professionals is not the only, or necessarily even the main, issue that determines the quality of people's lives in the community.' 41 There was growing awareness that service users' experience was skewed by ethnicity and gender: young black men were disproportionately subject to compulsory treatment for severe mental illness, women were over-represented among users presenting with mild to moderate illness. The growth of the service user movement provided a forum in which these concerns and complaints could be articulated.\nThese three narratives, from disenchanted professionals, from optimistic professionals and from service users, formed the background to discussion of the development of policy and services under the Labour government, which (as previously noted) dominated contributors' recollections and concerns. Because, as one contributor remarked, changes in the recent period were, arguably for the first time, 'a led process', 42 a fourth, policymakers', narrative emerged. From this viewpoint, the Labour government offered a particular moment of political clarity, a model for implementation built on a target 36 Jim Symington, Transcript, 31 January 2011, 13. The phrase 'ordinary life' resonates with the title of a seminal King's Fund paper, and its use, like the reference to Wolfensberger's work above, illustrates the extent to which approaches originating in the care of people with learning disabilities generalised to other mental health services. King's Fund, An Ordinary Life: Comprehensive Locally-based Residential Services for Mentally-handicapped People (London: King's Fund, 1980). 37 Symington, ibid., 13. 38 Hall, op. cit. (note 20), 4-5. 39  The History of Mental Health Services in Modern England 611 culture, a belief (not shared by previous administrations) in the value of national leadership on social policy issues, and above all the availability of money, all of which led to large positive changes. There was also a desire to modernise mental health legislation by superseding the 1983 Act. Clinicians in the group were sceptical of the actual effect of the target culture on outcomes, reckoning that the unprecedented detail of the Policy Implementation Guidelines (the Department of Health's detailed instructions to NHS organisations 43 ) militated against the provision of locally appropriate services. The opposite case was put by those working in policy development who argued that new strategies needed to be monitored and measured because 'the NHS. . . is not quite a black hole, but it is a complex policy-eating environment'. 44 Where clinicians saw a reduction in their influence, department officials pointed to the large constituencies of professionals consulted in the external reference groups for the National Service Framework even though civil servants rather than the reference groups actually wrote the policy. 45 Similarly, service users and officials had different views of the reality of service user involvement in policymaking and of the 'user-friendliness' of policies, with the suggestion that the service user movement was sometimes 'hijacked' by clinicians and civil servants. 46 The proposition that New Labour's policy for mental health was unprecedentedly evidencebased was challenged in a number of ways. Policy innovation was traced to leaps of faith and changing values as well as to the evidence base. Yet it was argued that some policies held up as politically driven were in fact built upon earlier clinical arguments and experimental evidence: for example, Lord Layard's IAPT programme built upon evidence that untreated anxiety and depression placed a large burden on primary care services and that patients preferred non-pharmacological therapies, but laid a new emphasis on the economic case that high levels of mild to moderate mental illness in the population were a drag on prosperity. 47 Witnesses acknowledged that New Labour delivered large expenditure in mental health, but contested the effectiveness of that expenditure. For many user groups, notably black and minority ethnic groups, it was argued that there had been little change for the better despite broadly good intentions. 48 Moreover, the increase in expenditure on targeted activities had been accompanied by disinvestment in other areas, and a very large proportion of new spending had been directed at secure environments and forensic services rather than at the community services which might reduce the need for such services. 49 Services continued to vary significantly across the country. For clinicians, the development of clinical governance, which by implication made clinicians accountable to managers, and thus ultimately to the state and to the community at large, as well as to their patients, was a significant change. Alongside the recognition of extra expenditure on services was the view that New Labour had been socially authoritarian 50 and used 'Orwellian' institutions 612 John Turner and others to deal with personality disorder. 51 Service users and sceptical practitioners alike saw the 2007 Act as a means of social control with little benefit to the welfare of service users.\nStrikingly, the different narratives of recent developments in mental health services describe the fragmentation of a system which was regarded (arguably wrongly) as relatively homogeneous in an earlier period. 52 The very definitions of mental distress and disorder had been widened, and thus the scope of services had been increased. Responsibility for providing these services, which had previously rested with the NHS and local authorities, had been extended to include the third sector and, increasingly, profitmaking contractors. Psychiatric hegemony was challenged by new professions, by new conceptions of mental distress, and by a new assertiveness on the part of service users, as well as by the incursions of public policy and policymakers into clinical autonomy.\n---\nSignposts for historians\n\nThe four voices in these discussions -professionals (both approving and disenchanted), service users and policymakers -shared a number of common preoccupations which we argue should be central to future study of recent mental health services in England and probably elsewhere in the United Kingdom. The most important of these were the rise of the service user, the risk agenda, the allocation of resources, changing and contested definitions of mental health and psychiatric need, and the impact of changing professional values on the delivery of services. Any articulated narrative of change since 1959 must be informed by these concerns.\n---\nService users\n\nThere was consensus among witnesses that one of the most important and striking changes in the history of post-war British mental health care has been the rise of the service user perspective. Forms of organised advocacy for users of mental health services have existed in Britain since the formation of the Alleged Lunatics' Friend Society in 1845. 53 However, the standard account of the emergence of the modern service user movement starts in the 1970s with the founding of groups that included the Mental Patients Union (founded in 1972) and then, when this broke up, organisations including the Community Organization for Psychiatric Emergencies (COPE), Protection for the Rights of Mental Patients in Treatment (PROMPT) and the Campaign Against Psychiatric Oppression (CAPO) which connected the movement to developments in the 1980s. 54 The small scale and transient 51 Bartlett, op. cit. (note 40), 5. 52 It is significant, at least culturally, that the first reports of the Ministry of Health after the establishment of the NHS were proud to report that over 97% of mental patients were being treated in hospitals accountable to the Ministry. 53  The History of Mental Health Services in Modern England 613 nature of many of the service user groups has made accurate assessment of the scale of the movement difficult, but the leading authority on the subject suggests that it expanded from about a dozen groups in 1985 to over 500 by 2005. 55 The challenge for historians is to explicate the links between these developments and, on the one hand, the scientific and cultural critiques of psychiatric practice covered by the term 'anti-psychiatry' (and discussed further below) and on the other hand the internalisation of service user demands into the actual practice of mental health services towards the end of the period. Witnesses tended to echo Crossley's view that the early movements owed as much to social radicalism as to the intellectual anti-psychiatry of the 1970s, though service users in the seminars challenged the philosophical basis of diagnostic categories and treatment methods in terms which would have been familiar in those earlier debates. Discussion of the later period was much less definite. The optimistic professional view of developments since the 1980s was that the civilising influence of campaigning organisations such as MIND and other groups had successfully engaged professionals in collaboration with service users in the design and delivery of the services, and indeed that this was a relatively unusual and advanced aspect of British practice.\nThis generous and present-centred 56 perspective was contested from a number of directions. Contributors, including departmental officials, noted the lack of progress in addressing health inequalities in general, and in particular the disproportionately bad experience of service users from black and minority ethnic groups. The service user objections to the restrictions of liberty contained in mental health legislation are clearly alive and well after the 2007 Act, alongside a more consumerist critique of the inadequacy of resources to meet treatment needs. Nor have professionals and service users reached an easy consensus on treatment methods or service design. Research led by service users on electro-convulsive therapy, for example, met with considerable hostility from the Royal College of Psychiatrists before being acknowledged in the NICE guidelines on risk and consent. 57 Service users in the seminars also challenged the ethical basis of the 'screen and intervene' approach which might prompt services to subject too many people, too early, to active treatment for mental illness and argued that the emphasis on work and social inclusion in the recovery movement (a concept developed within the service user community) could constitute a threat of social control as much as a response to individual needs, echoing Joel Braslow's critique of 'recovery' as nesting 'neatly within the broader context of neo-liberalism'. 58 Greater exposure for the service user viewpoint has manifestly not been followed by complete satisfaction of service user demands, and this clearly requires nuanced historical treatment.\n---\nRisk\n\nA consistent theme in discussion was the distorting influence of a focus on risk, and the perception that this was a characteristic of recent policy and legislation. While contributions tended to be grounded in the specific 'risk agenda' of clinical and management practices within the mental health services and their significance for the social control function of psychiatry, some participants were clearly aware of the larger theoretical perspectives on risk raised by Ulrich Beck, Robert Castel and Niklas Luhmann and extended by Nikolas Rose for mental health. 59 As one witness argued, 'in terms of being admitted to psychiatric hospitals, and particularly under compulsion, there is an issue of risk, so it's not just diagnosis that gets you admitted, it's that you're \"risky\". And the big risk that all the psychiatrists worry about is violence.' 60 The 'risk of violence'perpetrated by the mentally disordered on members of the public -emerged as paramount in the circumscription of psychiatric patients' freedom. But the political salience of risk was recognised as being neither new nor confined to mental health. Psychiatrists acknowledged that . . . these waves of control and security, of course, don't just touch mental health services, they touch the whole educational system, they touch the way people are allowed to take risks at other things. You know the old \"'ealth and safety\" industry is around. . . I think that wave went with the wave of \"we must protect the members of the public from these raving lunatics at all costs\". 61 The effect has been that 'risk avoidance has been seen as a key public function of psychiatry . . . the current policy on mentally disordered offenders is almost wholly to do with public protection and not much to do with humanitarian concerns for the welfare of the individual.' 62 However, mental health/violence scandals have been around for much longer than the Clunis affair which ostensibly prompted the repressive aspects of recent policy and legislation. The now largely forgotten case of outpatient Ronald Derek Sowle, who killed an eighteen-year-old Bristol schoolgirl in 1961 while living in a psychiatric hostel, three days after having been released from detention under the Mental Health Act and reclassified as an informal patient, caused a series of questions to be asked in Parliament and a full enquiry by the local authority concerned but did not come anywhere near derailing the 'community care' or mass decertification processes arising from the Mental Health Act 1959. 63 For many years thereafter the 'problem' of dangerous offenders was subsumed in a nuanced discussion of the best way to promote co-operation between the justice system and the mental health services, an approach epitomised in the 1975 Butler report, which, among many other things, launched the development of forensic psychiatric services in the NHS. 64 It would seem that it is not the content of the 'scandals' that determined policy changes, but the broader traction granted by the rise of 'risk' as a discourse in the public sphere.\nIn the seminars, the risk agenda excited passions of two sorts. Clinicians tended to interpret it as an excuse for inappropriate bureaucratic interference in their clinical judgement:\n. . . the one thing that needs to be understood out of all that stuff about danger and risk, which is monstering us, is that because of the impositions of risk assessment and risk management, the forensic establishment and so forth, we're now in danger of neglecting the non-dangerous patient. 65 This echoes the views of Rose and Castel, who see the risk agenda as diminishing the standing of psychiatrists. 66 Clinicians also pointed to the constricting impact on practice of an emphasis on suicide risk and the associated formal structure of inquiries. By contrast, service users argued that consideration of risk, and especially new categories of risk, strengthened the control of clinicians over patients:\n. . . they [the government] wanted to impose an extraordinarily centralist, dangerously severe personality disorder, DSPD -a completely new construction -onto psychiatric activity; that somehow you could detain someone merely because they happen to have potential personality characteristics that are going to possibly cause trouble in the future. 67 More generally, other researchers have noted of the 2007 legislation that 'anything goes'. . . [d]ecision-makers can draw relevant ingredients from (i) clinical and/or (ii) non-clinical factors existing in a patient's diagnosis, characteristics, and/or circumstances. . . extensive scope for professional judgement within the [2007] legislation. . . [which] represents a significant threat to patient rights. 68 The question remains as to whether recent attitudes to risk in mental health policy are different in kind, or just different in degree and in consequences, from earlier perceptions. 69 In the regime of segregation and compulsory confinement that subsisted until the mid-twentieth century and beyond, it was taken as read that the risk of selfharm or violence was a reason for confinement, which could often induce families or communities to prefer the asylum in individual cases to other forms of care. 70 Sarah York has observed that the avoidance of suicide risk was a major driver for regimes of restraint and surveillance in the Victorian asylum. 71 However, as \u00c5sa Jansson has recently shown, the significance of suicide, Lunacy Commission suicide statistics and asylum admissions in the Victorian period were far from straightforward, and owe as much to administrative intervention as clinical judgment 72 -a tension also central in modern discussions of risk. Additionally, Harvey Gordon has noted that the specific risk of re-offending was a consideration in the discharge of criminal lunatics from Broadmoor. 73 Yet the political and public discourse about mental illness in the nineteenth and early twentieth century did not emphasise risk as a reason for change (or continuity) in policy. Occasional newspaper excitements were deflected by leading psychiatrists who on one occasion in 1896 observed that 'no alterations in the Lunacy Law will prevent . . . the occasional discharge of a patient who may subsequently become homicidal' and called sarcastically for 'another Act of Parliament providing that all persons who are likely to become homicidal shall present themselves periodically at the nearest lunatic asylum'. 74 Plus c \u00b8a change; but on this occasion, and for nearly a century thereafter, the moral panic which they were challenging was not translated into policy.\n---\nResources\n\nMuch of the substance of any public policy lies in decisions about the allocation of resources. Seminar members were greatly exercised by the cost of services, but discussion threw up as many questions as answers. A starting point was that for at least a century before the inauguration of the post-war welfare state, mental health was one of the betterfunded public welfare services, with large capital investments in asylums and a national regulatory system in the Board of Control. 75 In recent memory, though, services have been regarded as underfunded because NHS expenditure on physical medicine has grown very much faster than expenditure on mental health, and because definitions of need (and thus of unfulfilled need) have continuously evolved. Witnesses were able to point out that NHS expenditure had normally not kept pace with changes in need (except, briefly, in the increased spending of the last Labour government), that it continued to fall behind growth in expenditure on physical medicine, and that within the mental health budget there have been contestable allocations with rapid recent increases (as noted previously) in spending on forensic and secure services, an emphasis on services for adults of working age at the expense of children and older people, and persistent large variations between regions. The failure to recognise the resource demands of community-based care, from the very beginning of the de-institutionalisation programme, was taken as read. It was noted that ministers had demanded, and got, evidence of efficacy in minute detail before agreeing to fund, and then continue funding, the IAPT programme. 76 Given this widespread preoccupation with cost and resource constraints, historians will find it necessary, but difficult, to go behind these words and selected indicative statistics to build a balanced and informative picture of changes over time in actual resource allocation and the policies which drove it. The first attempt to synthesise data on the totality of NHS expenditure on mental health was only made in 2001/2, and at first covered only adults of working age. 77 Before that time, information on NHS expenditure existed in the reports of the Department of Health and its predecessors, but this was not collected on a consistent basis before 1974 and did not reliably separate mental health from other expenditure; and within mental health it did not necessarily report expenditure within categories which are meaningful for policy analysis. Even after 1974 'changes in accounting procedures mean that no meaningful comparison of spending over the period can be made'. 78 A more rigorous account of the total costs of mental health provision would require an assessment of all the agencies and institutions which have provided care and treatment; and in the differentiated environment which has emerged since the 1980s this would include local authorities, third sector and private providers, and increasingly the private funding of new forms of therapy such as counselling and of long-term residential care 74 Charles Mercier, reporting on recent medico-legal cases in Journal of Mental Science, 42 (1896), 230. Mercier was commenting breezily on reports which included a double murder, two other murders, a suicide and a number of assaults. 75 Freeman, op. cit. (note 30), 18. 76  for dementia sufferers. Local authority expenditure is published in annual departmental reports but does not separate mental health from other social services expenditure after the establishment of generic social service teams. Where data from third sector and private providers is available it does not reliably distinguish mental health from other forms of social support. An even more elusive and fluid problem is the resourcing of care for people with dementia. Successive acts of policy have moved patients away from NHS institutions and towards local authority provision or private or third sector institutions. As the number of patients in long-term residential care, or requiring care in the community, has increased, the share of the cost borne by public expenditure has dropped. 79 While this has prompted passionate controversy over many years about the principles of public funding for care, no consistent distinction has been made in that debate between mental and physical reasons for dependency and 'meaningful comparisons' are consequently difficult. On resource issues, as in many other perspectives on the mental health services, fragmentation of provision has led to fragmentation of evidence.\n---\nCritiques of psychiatry\n\nHistorians will also ask how the debate on the validity of psychiatric knowledge and practice -fascinating as it is to cultural historians -has been relevant to the development of services. In the 1970s, contestation of psychiatric theory and mental health practices came to be subsumed under the term 'anti-psychiatry', coined by David Cooper in 1971. 80 Witnesses looking back from 2011 differed emphatically about the relationship between classical 'anti-psychiatry' and the range of critical positions which have since been taken about psychiatric practice. Anti-psychiatry of the 1970s included three very distinct strands, represented totemically by R.D. Laing, Thomas Szasz and Erving Goffman. Laing, influenced by existentialism and psycho-analysis, saw schizophrenia as a sane response to an insane environment created, largely, by dysfunctional families and capitalism. Szasz regarded mental illness as a construct misappropriating medical concepts in order to control people whose behaviour was regarded as alarming or offensive. 81 Goffman regarded the asylum as a special case of the 'total institution', a closed society which manipulated its members into pathological behaviours, 82 a perspective anticipated more pragmatically in the United Kingdom by Russell Barton. 83 Questions raised, and answered very differently by seminar participants, include whether 'anti-psychiatry' had any influence beyond a brief flare-up, and what relation this alleged movement has to contemporary critiques of psychiatry and mental health. Colin Jones has argued that the term has included serious content-based critique of psychiatry, a 'gestural politics of carnivalesque inversion and symbolic performance', and the exploration of new paradigms of knowledge about what it is to be human. 84 At the time, British psychiatric commentators were ambivalent about Laing and generally hostile to Szasz. Writers in British Journal of Psychiatry were inclined to treat Laing's work 'not as contributions to science or philosophy. . . but as part of the contemporary literature of social protest', 85 which allowed them to respect the verve and immediacy of his observations while deprecating the value of his analyses. But Kathleen Jones, a non-psychiatrist delivering the Maudsley Lecture in 1978, remarked that 'behind the rhetorical excesses and the studied irrationality there are some serious points for psychiatry to consider', in particular that 'the patient's view of what is happening to him is as valid as that of the therapist, and therapists ought to listen as well as to prescribe'. 86 By the time of Laing's death in 1989 this position was uncontroversial and commentators were observing that many psychiatrists had since entered the profession aspiring to an empathetic bond with their patients and that Laing's contributions were 'in the main welcome'. 87 By contrast, Szasz was excoriated for 'flashing rays of darkness on the whole field of psychiatry' with an 'utter inability to support his belief in [schizophrenia's] nonexistence with a shred of relevant. . . evidence', 88 and he was a principal, though not the only, target of a sustained rebuttal of challenges to the notion of mental illness as disease. 89 Goffman got away lightly as a largely atheoretical observer whose 'observations are so rich and his imagination so fertile that his books could supply social scientists with hypotheses for another generation to come'. 90 In the seminars, one view, from senior psychiatrists, was that 'the most striking thing' about classical anti-psychiatry was that 'eventually it disappeared, with practically hardly a trace left behind' and that it all came to an end when 'Ronny drank himself to death'. 91 Another psychiatrist located its influence on front-line practice rather than psychiatric theory:\n. . . the notion of mental illness or mental health was decried and denied by your average social worker who saw this all as social construction, capitalism, call it what you will. . . . And even as you're standing on the corner of a Hackney street with a social worker going in to try and extract someone from a stinking house where they've been screaming and yelling for days and days, and neglect, pain, fear, and hallucinosis, you were still being told by your social worker friend, colleague, whatever, \"Well, they're not sure they're going to let this person come into hospital; they're not sure they're really ill\" because of their own political beliefs as opposed to any kind of medical or clinical understanding. 92 Others identified the continuing relevance of anti-psychiatry to contemporary debate, whether in the narrow sense of a Laingian influence on 'the rich tapestry of the psycho-analytic world', 93 or in a broader Foucauldian sense that 'the whole of modern psychiatry is permeated by anti-psychiatry' 94 in the form of contestation over the use of psychiatric diagnosis as an instrument of power. Witnesses noted for example that '. . . we're still having to make an argument, relatively recently, to our own profession [clinical psychology] that actually we should abandon diagnosis, and replace it by formulation.' 95 Several witnesses posited a relationship between critical psychiatry and the service user movement, in the sense that 'individual and collective action by mental health survivors' has done 'a great deal to challenge the designations and treatments that have been foisted on people with mental health problems.' 96 The link between the movement and classical anti-psychiatry has been challenged by others, and it has been argued that in the antipsychiatric rhetoric of cutting loose from the system and challenging its legitimacy there had in fact been little engagement with the real problems of modern mental health care. 97 It was noted, though, that the National Schizophrenia Fellowship of the 1970s was . . . very much in the wake. . . of the anti-psychiatry culture as they perceived it. They thought that they weren't being listened to, indeed that they were being perceived as pathological, and pathogenic, by psychiatrists, that they weren't taken seriously. And their demand was to be put back on the map as regular, decent citizens. 98 Historians of mental health services engaging with the history of psychiatric science will also need to address more recent and more eclectic critiques of psychiatry, connecting traditional anti-psychiatry challenges to the intellectual (and hence political) authority of psychiatric theory with new widely varying scientific and philosophical contributions to that theory, such as Thomas and Bracken's attack on 'bioreductionism' 99 , Fulford's promotion of a values-based psychiatry, 100 Moncrieff's critique of the scientific basis of drug treatment, 101 or Bentall's deconstruction of the conceptual basis of diagnosis. 102 Of the issues most relevant to practice and the delivery of services, witnesses emphasised the growing importance of neuroscience (incorporating behavioural genetics as well as more traditional psychopharmacology) in research and clinical practice. This has been the subject of intense debate within psychiatry, beginning with Craddock's 2008 'Wake-up call' in the British Journal of Psychiatry, which argued that although 'evidence-based psychological and social interventions are extremely important in managing psychiatric illness' they had resulted in a turning away from neuroscience which threatened the 95 Pilgrim, op. cit. (note 50), 9; Division of Clinical Psychology, Clinical Psychology: The Core Purpose and Philosophy of the Profession (Leicester: British Psychological Society, 2010), defines formulation as '. . . the summation and integration of the knowledge that is acquired by th[e] assessment process that may involve psychological, biological and systemic factors and procedures. The formulation will draw on psychological theory and research to provide a framework for describing a client's problem or needs, how it developed and is being maintained. . . . This provides the foundation from which actions may derive. . . Psychological intervention, if considered appropriate, is based upon the formulation.'. 96 Barham, op. cit. (note 41), 23. 97 Andrew Roberts, a service user who has taken a leading role in developing a service user perspective on the history of the movement, is one who has criticised this line of argument. For insight on the service user perspective on their own history and for a timeline of developments and a wealth of material on this history: http ://studymore.org.uk/mpu.htm. 98  above could be read as a vignette of inter-professional rivalry as much as a philosophical disagreement, and the same could be possibly be said of Craddock's 'Wake-up call'. Witnesses acknowledged that such tribalism existed, and associated it particularly with the separateness of training regimes, but raised further questions which clearly demand historical investigation. The orderly hierarchy of mental health professions which characterised the 1950s has been gradually replaced by an uneasy structure in which psychiatry maintains its supremacy with some discomfort (while still maintaining a separate pay scale not subject to inter-professional comparisons); mental health nursing has developed an autonomy and range of functions, especially those of community treatment, quite unknown to the nurses and attendants of the 1950s; clinical psychology has become established as a key agent in treatment of many disorders; a variety of other therapy professions has been created, some by policy and some organically, with distinct value systems and skill sets. Reports of the Department of Health's New Ways of Working project from 2004 describe, perhaps optimistically, how a different style of leadership and clinical responsibility shared between different professions has come to exist in NHS mental health services. 108 At the same time, front-line workers in the IAPT programme and support workers in recovery programmes generally work within their respective sets of treatment protocols but do not have specific professional allegiances.\nThe sharing of knowledge, skills and values between professional groups is of particular interest. In 2004, Thornicroft described a 'third period' of service delivery, following the rise and subsequent decline of the asylum, in which:\n. . . community-based and hospital-based services commonly aim to provide treatment and care that are close to home, including acute hospital-care and long-term residential facilities in the community; respond to disabilities as well as to symptoms; are able to offer treatment and care specific to the diagnosis and needs of each individual; are consistent with international conventions on human rights; are related to the priorities of service users themselves; are coordinated between mental health professions and agencies; [our italics] and are mobile rather than static. 109 The post-1959 services described by our participants aspired to all of those things, though there is room for profound scepticism about how far any of them were achieved. In particular, the delivery of community-based services depended on the relations between specialist services and GPs on the one hand, and between doctors (and psychologists) and nurses on the other. There has over the years been some research into the acculturation of GPs into a psychiatric mode of thought; 110 given the level of GP activity in mild to moderate illness, it will be important to track how this has changed over time, in intention and effect. In the absence of GP representation at the seminars, participants had little to say about it. More was said about the transfer of skills to nurses and to other types of therapist under IAPT, but it is important to recognise that this sort of skills transfer long predated the twenty-first century: the Maudsley hospital started to train nurse-therapists in psychological therapies in 1972, 111  Nurses has depended on the acquisition of skills formerly confined to psychiatrists and psychologists. 112 Historians need to account for the development of value systems within each of these professional communities and programme groupings and for the changing patterns of interactions between professional groups, exploring not only the training regimes but also the patterns of recruitment into different professions (and hence the social distance between different professions and between professionals and their clients) and the structures supporting professional identity, such as professional associations and interest groups. Witnesses were clear that some professions were originally only open to those who could afford to pay for training, 113 while others have had a skewed ethnic or gender balance. 114 Intellectual developments outside the clinical sphere, from behavioural psychology to genetics, have been transmitted through the lens of university training and acculturation into the clinical practice of different professions. The adoption of new professions into NHS training systems and into regulatory structures such as (most recently) the Health and Care Professions Council on the one hand validates the professions, yet on the other serves to constrain their autonomy and arguably distort their identity -hence the movement among some psychotherapists to resist regulation by the HCPC. 115 Histories of professional groups, thus informed, will complement histories of mental health practice which focus on the distinctive contributions of individual psychiatrists or other professionals, thus providing an alternative 'history of psychiatry'.\n---\nTowards a history of modern mental health services\n\nWhere should all this lead us? We began by remarking that a traditional historical narrative structured around rights is now enriched by new organising categories such as costs, risks, needs and values such as the aspiration to equality and inclusion. The seminars allowed an effective exploration of these issues, if only within the very real epistemological constraints of elite oral history. That said, they confirmed that the subject of the historical narrative has expanded and changed. Historians of the period up to the mid-twentieth century have rightly and fruitfully concentrated on madness and its management, exploring principally the experience of those identified (and mostly legally defined) as lunatics and the social, cultural, political, medical and institutional context of their treatment. 116 Such a scope, valuable and innovative though it has often been, cannot serve the history of a period as complex as the one covered in these seminars. The history of modern services, for example, will necessarily include the experience of all service users, including a majority who never enter an institution and whose mental distress may well be entwined with various social and economic disadvantages whose causes and consequences are also part of 112 See J.W. Rawlinson and A.C. Brown, 'Community psychiatric nursing in Britain', in Bennett and Freeman, Community Psychiatry, 463-87. 113 Notably child psychotherapy at the Tavistock Clinic: Rustin, op. cit. (note 105), 10-11. This is still true for counselling psychology. 114 Applications for clinical psychology training have been heavily skewed towards women, with an underrepresentation of ethnic minorities. 115  The History of Mental Health Services in Modern England 623 the story. 117 The voice of the service user will be represented not only as a set of individual narratives but also as a social and political movement, part of civil society, and an important strand in the history of clinical practice. Further, the history of mental health services will not make sense unless it is set within a broader analysis of the health economy and the organisational development of the NHS, of social services within local authorities, of the charity sector and of prisons, all of which deliver services to these users. We would expect a history of the professions to have more a sociological than an institutional character, exploring the evolution of value systems as well as patterns of recruitment and training; on the other hand, the political origins of state regulation and its consequences for the development of professions and their practices should be explored without preconception, Foucauldian or otherwise. In studying the politics of mental health policy, we would look to the significance of moral panic about risk in driving political discourse, but also to the lower-level interaction between officials and expert and lay networks in shaping policy and legislation, whether this was about resource allocation or civil liberties. Our history of psychiatric science would engage with classic anti-psychiatry and the critiques of psychiatry by psychiatrists, but also examine the impact of developments in psychology and neuroscience and the social and institutional structures in which research is done and disseminated into clinical practice.\nReflection on the fragmentation of mental health services since 1959 leads us to further, sometimes uncomfortable, speculation about the opportunities and risks in the historiography of earlier periods. Acknowledging, as our witnesses would lead us to believe, that the NHS in England and Wales had not by 1959 created a coherent system out of the people and institutions which it inherited on its establishment, should we not also regard the great asylums of the 'water tower' period, and the psychiatry practised within them, as rather less a monolithic and inclusive system for the care or control of deviants than a part of a much larger range of mixed institutions and contexts in which the 'service users' of the time had to live out their lives? The notion that the users of mental health services have more in common with other disadvantaged categories (the physically ill, the poor, ethnic minorities, immigrants) than they have special features is a powerful heuristic, widely discussed for the nineteenth and early twentieth centuries,118 and it could take us usefully beyond the conceptualisation of such categories as 'deviant'. Just as we ask, of the modern period, how and why (and to what extent) certain modes of treatment are incorporated in practice and others dropped, should we move beyond the classic studies of developing psychiatric thought in the works of great men to a search for explanations of the practice and scientific beliefs of the front-line asylum doctors and general practitioners who delivered 'care' in the nineteenth and early twentieth centuries? Could the copious records of the Victorian and Edwardian civil service yield a more nuanced account of the interaction of laymen, professionals, politicians and officials in the 'triumph of legalism' in the 1890 Lunacy Act119 or the more contestable triumph of eugenic thought in the 1913 Mental Deficiency Act, and the institutional changes which followed those pieces of legislation?\n---\nJohn Turner and others\n\n'downgrading of medical aspects of care' and a 'creeping devaluation of medicine' in psychiatry. 103 More recently, Bracken has argued in opposition to that view that: Psychiatry now faces two challenges it cannot ignore. First, a growing body of empirical evidence points to the primary importance of the non-technical aspects of mental healthcare . . . second, real collaboration with the service user movement can only happen when psychiatry is ready to move beyond the primacy of the technical paradigm. . . Substantive progress in our field will not come from neuroscience and pharmaceuticals (important as these might be) but from a fundamental re-examination of what mental healthcare is all about and a rethinking of how genuine knowledge and expertise can be developed in the field of mental health. 104 Within and beyond psychiatry, witnesses also emphasised the continuing controversy over the application of the paradigms of evidence-based medicine to mental health problems -a paradigm which was perhaps most prominent in the introduction of psychological therapies through IAPT, but one which discomfits many clinicians whose favoured therapies do not fit easily into randomised controlled trials. 105 NICE guidelines, overtly based on research evidence, have been the principal engine of change in psychiatric practice in Britain in the last decade, but historians are bound to ask whether the conclusions drawn from evidence have been influenced by competing professional interests as well as by evidence, and to seek to measure how far the guidelines have in fact been absorbed into practice.\nUnderlying this discussion, and especially important for the historiography of mental health services, is the changing understanding of the definition of mental health or the scope of mental illness, and thus the 'need' for services. The willingness of many service users (and many non-users) to challenge professional definitions of their distress as illness is an evident legacy of the anti-psychiatry movement, but debate about scope and definition has been just as intense within and between communities of service providers. Psychiatric diagnosis is denounced as an instrument of power by critics of the disease model of mental distress, and the major diagnostic schemas, DSM-IV and ICD-10, are attacked by critics because, inter alia, they have been seen to include more and more people in stigmatised categories, especially by defining behaviour disorders as illness. On the other hand, the idea that mental illness is definable and treatable as a disease process like any other has been fundamental to many developments in service delivery in the period covered by these seminars. Diagnosis is hardwired into the logic of NICE guidelines. 106 Understanding service development therefore depends on an historical understanding of the debate between formulation and diagnosis, and of the evolution of dimensional models of mental disorder; and these discussions extend far beyond the ideological struggle around classical anti-psychiatry.\n---\nProfessions and values\n\nTo the outsider the mental health professions appear notably tribal, even to the extent of identifying specific positions within clinical and scientific debates with loyalty to specific professional communities. 107 The Hackney street corner anecdote recounted 103 \n---\nJohn Turner and others\n\nWe acknowledge that this is not a comfortable prospectus, whether for the modern or the more remote period. The very concept of mental health and mental illness has been enlarged and transformed in the last half-century; mental health policy has become confounded with many other aspects of public policy and mental health services have consequently grown and fragmented. Interaction between the fragments makes for a complicated story, but the alternative is a 'single-issue mythology' which will mislead rather than enlighten.\n---\nAppendix\n\n\n---\nList of Contributors\n\nVictor Adebowale, Chief Executive of Turning Point mental health charity and social enterprise.",
        "INTRODUCTION\n\nPopular social media users are having a growing impact on consumer behavior, public information campaigns, political debate, voting behavior, and crisis management (Stephen & Galak, 2012;Shaozhi, 2020;Sundermann & Raabe, 2019). For this reason, many scholars have investigated the origins of popularity on platforms like Instagram, YouTube and Twitter. 1 The academic literature on this topic, however, remains fragmented and concentrated in disciplinary silos.\nResearchers in the fields of business, marketing and advertising are interested in how companies can use their own accounts or collaborate with other users to increase brand value and revenue (e.g., Kwok & Yu, 2013). Political scientists, sociologists and economists are naturally more intrigued by the political, social and economic effects of popular users (e.g., Choi, 2014). Representing a third group, scholars of computer science are focused on the role of influential users in the process of information diffusion in social networks (e.g., Meng et al., 2018). The aim of this article is to review and organize this sprawling literature, identify areas of consensus and disagreement, and encourage interdisciplinary cooperation and synthesis. The article begins by outlining the procedures used to locate studies for a structured literature review. In the next section, Barnlund's (2008) transactional model of communication will be discussed in order to organize and visualize the predictors of social media popularity and based on this model online interactions will be reviewed. In the third and fourth sections, the findings will be summarized and the parts of the transactional model that have received the most and least attention and consensus from scholars will be identified. In the final section, suggestions for future research have been emphasized.\n---\nMETHOD\n\nFollowing Sundermann and Raabe's (2019) approach to conducting structured literature reviews, the search for articles was carried out in two stages. First, a list of relevant literature was constructed based on electronic database searches of a university library's \"Communication Source\" (a merger of high-quality EBSCO databases, Communication and Mass Media Complete and Communication Abstracts), Jstor and Google Scholar. The keywords \"influencer marketing,\" \"social media influencer\" and \"influencer communication\" were used to search each database. After exhausting these one-term searches, dual-word searches were utilized with the term \"social media\" and each of the following: \"followers,\" \"engagement,\" \"popularity,\" and \"content analysis.\" Backward searches of all related sources cited in original articles were carried out, as well as forward searches using Google Scholar to identify later studies that referred to original articles.\nThese procedures would produce a massive number of articles, most of which predate the rise of social media and pertain to the decades-old literature on social influence and persuasion. Strategic criteria were required to narrow the scope of this review to a feasible number of articles. For this reason, only studies based on quantitative, naturalistic observations of users on Facebook, YouTube, Twitter, Sina Weibo, or Instagram were included in the sample. This excluded three important areas of the literature. First, studies of popular people who engage their audiences through blogs and other websites were not considered. Also excluded was a vast portion of influencer marketing research that utilizes experiments and surveys. While providing valuable findings, experimental and survey-based studies typically focus on a range of dependent variables, such as source credibility, that are conceptually different from this study's operational definition of popularity, which is rooted in naturalistic behaviors, such as likes, shares and follows. Finally, studies based on qualitative research designs were not selected. Qualitative methods like digital ethnography are useful for understanding communities and real-life social interactions online, but their findings are difficult to compare to the bulk of research based on quantitative analysis. All studies in this review utilized some form of quantitative content analysis based on manual methods (coding by humans) or automated methods (using computer software to assist in the coding process).\nFour additional inclusion criteria were as follows: 1) a measure of popularity based on users' reach (e.g., followers, fans and subscribers) or engagement (e.g., likes, comments and shares) on Facebook, YouTube, Twitter, Sina Weibo, or Instagram, 2) analysis of at least one predictor of popularity, 3) written in English, and 4) published in a peer-reviewed journal or conference proceeding. The selection procedure produced 68 articles, from which 88 distinct predictors of popularity were identified. Any significant statistical measure indicating a relationship between two or more variables was considered a predictor. As shown in Tables 1 and2 in the appendix, for each predictor, the type of user, the social media platform, the measure of popularity, and a citation to the respective study were noted. By accounting for user and channel types, this review evaluates the literature's degree of consensus on the various predictors of popularity, as well as assesses each predictor's consistency across different types of users and platforms.\nThe following types of social media users were found in this review: Business organizations, celebrities, governments, ordinary people, original social media influencers (SMIs), and universities. Original SMIs are defined as people who became well-known via social media, whereas celebrities are famous for their work outside social media (Piehler et al., 2021). This review identified studies of original SMIs working in multiple industries, including alcoholic beverages, automotive, banking, beauty and cosmetics, environmental sustainability, fashion, fitness, news, politics, public health, health care, science, sports, travel and video games. Two prior literature reviews were organized around Lasswell's transmission model (Sundermann & Raabe, 2019), or a revised version of it (Hudders, De Jans, & De Veirman 2021). Laswell's model is typically used to explain one-way, asymmetric flows of communication. It assumes that the effects of messages are determined by characteristics of sources, messages, channels and receivers. Positioning the source as the primary agent, the transmission model has been applied in several studies of persuasion, advertising, and organizational communication (Sundermann & Raabe, 2019). While the transmission model accounts for key components of the communication process, other frameworks may be more appropriate for organizing the literature around social media interactions. Barnlund's (2008) transactional model was used for this review because it includes most of the components of the transmission model, but also theorizes communication as a back-and-forth, continuous process as opposed to a linear one. Barnlund defined communication has a dynamic exchange, a progression of information flows where communicators cocreate meaning by encoding and decoding messages. Communication occurs when communicators turn thoughts into messages (encoding) and messages into thoughts (decoding). Through this process, people make sense of information by attending to the content of messages, characteristics of the source, and cues in the environment continuously and simultaneously.\nBarnlund's assumptions about how this happens were informed by the work of Erving Goffman (1973). Goffman theorized communicators as goal-directed impression managers. They are self-aware and pursue their goals with a sensitivity to their surroundings and the perceptions of their audience (Barnlund, 2008). Given that communicators create and interpret messages as if they were the other communicator, the act of encoding and decoding messages is always socially situated, interactive, non-linear, and interdependent.\nGoffman's theory of social interaction has been criticized for neglecting the differential power and status of communicators, and the broader social context that enables and constrains the outcomes of interactions (Gouldner, 1970). Yet, Barnlund's transactional model does include the psychological, relational, cultural, and social contexts that shape the communication process (Barnlund, 1968, 7). The co-created meanings of two or more communicators influence, and are influenced by, the communicators' cognitive and emotional experiences (psychological), the history of their interactions (relational), their shared or unshared values and beliefs (cultural), and the rules, norms and social structures that govern communication (social). In summary, Barnlund's model was chosen for this review because it is more appropriate for theorizing the back-and-forth communication of social media than Laswell's model, while also accounting for social-structural constraints on the communication process.\nBarnlund's transactional model was originally intended to theorize face-toface conversations, but some scholars have applied it to interpersonal computermediated communication (Eysenbach, 2018). To account for interactions between social media users, the diverse characteristics of social media platforms (channels) must be added to the model. Technical differences between platforms like Twitter and Instagram directly affect outcomes of communication; each platform also fosters a unique psychological, relational, cultural, and social context. As illustrated in Figure 1, a transactional model adopted for social media interactions posits that the mutual effects of user communications, including changes in users' popularity, depend on the characteristics of communicators and their messages, the channels through which they create and interpret messages, and the contexts of communication. Although the act of clicking a like button may seem simple, perhaps trivial, its causes and contingencies, as illustrated in Figure 1, may be varied and complex. Among the 68 studies in the sample, 54 examined the relationship between a social media behavior and popularity. The behaviors were grouped in seven categories and labeled as follows: 1) frequency and timing, 2) originality, 3) vividness, 4) interactivity, 5) emotion, 6) information, and 7) self-orientation. These labels were established inductively with the goal of matching the labels to the conceptualizations used in the studies under review. For the sake of organization, however, the categories included studies with related but distinct concepts and labels. For instance, the category \"originality\" encompassed studies on the effects of posting organic and unique content. Likewise, not all studies in the \"vividness\" category employed the term vividness or conceptualized this characteristic of content in the same way.\n---\nFigure 1. Framework for Organizing Research on Social Media Popularity\n\n\n---\nFrequency and Timing\n\nAccording to ten studies, the popularity of users was related to how often (frequency) and when (timing) they posted content. For instance, Jensen et al. (2014) analyzed 87 high-profile NCAA football coaches on Twitter and found a significant positive relationship between the coaches' total number of tweets and their followers, \"with each additional tweet being worth an additional six followers\" (273). As shown in Table 1 in the appendix, four additional articles reported similar results. A study by Hutto et al. (2013) looked at the effect of tweeting many times over a short period of time, known as \"bursting.\" They showed that bursting was associated with higher follower counts. However, the positive effect of bursting may be unique to Twitter and other microblogging sites where multiple interlinked messages or \"threads\" are common. On Facebook, in contrast, longer time periods between posts were positively related to likes, comments and shares (Banerjee & Chua, 2019;Brech et al., 2017). Finally, pertaining to the best days to interact, two studies found that posting content during weekdays compared to weekends was positively associated with popularity (see Table 1, appendix). These findings were consistent across Facebook and the Chinese microblogging platform Sina Weibo (sometimes referred to as Chinese Twitter).\n---\nOriginality\n\nOriginal content refers to social media posts that occur naturally and without paid promotion (organic), or those which are newly created by the users themselves (unique), as opposed to shared content, such as retweets. Five articles showed that posting organic content was associated with greater engagement, and that posts containing advertisements reduced their popularity (see Table 1, appendix). For instance, in a study of top fitness influencers on Instagram, Neal (2018) found that organic posts received more likes and comments than sponsored ones. As shown in Table 1 (appendix), similar results were produced by two studies of original SMIs on YouTube (\"YouTubers\"), one study of Sina Weibo and one Facebook study.\nUnique content was also associated with popularity. Zou et al. (2021) showed that unique content created by top health influencers on Sina Weibo produced more likes than their shared content. In a longitudinal study of Major League Baseball teams' Twitter accounts, posting unique content predicted increases in followers over time (Watanabe et al., 2015). The positive effects of organic and unique posts were consistent across five social media platforms and two types of users.\n---\nVividness\n\nNineteen studies looked at the relationship between the use of various media types and popularity. Social media platforms allow users to post text, images, photos, animations, videos, links and audio recordings. These media are thought to have varying levels of \"vividness,\" which facilitate varying levels of engagement from audiences. Although vividness was measured in different ways, which made it difficult to aggregate findings, there was strong, cross-platform evidence that visual content produced higher levels of engagement than other media types. For example, Cvijikj and Michahelles (2013) coded the vividness of 5,035 company Facebook posts from low to high as follows: 1) text only, 2) photos, 3) links and 4) videos.\nAccording to Cvijikj and Michahelles, posts with higher levels of vividness received more likes and shares and longer interaction durations from followers. Luarn et al. (2015) used a similar measure of vividness and produced matching results. As shown in Table 1 in the appendix, three studies combined videos with other types of theoretically vivid media and confirmed the positive relationship between vividness and engagement rates; four studies identified videos alone and an additional four studies measured photos/images alone as positive predictors of engagement. Some evidence questioned the assumption that videos, which have the highest level of vividness per some scholars, represented the most popular type of media. In a study of company Facebook pages, Dae-Hee et al. (2015) found that posts with photos received significantly more likes, comments and shares than posts with videos. Still, both videos and photos have greater vividness than text-only posts, which further supports the general assumption that posting images is positively associated with popularity. This finding held in studies of two types of users (businesses and ordinary people) and all three platforms that allow for high and low vividness, including Facebook, Twitter and Sina Weibo (note that Instagram and YouTube are specifically designed for sharing photos and videos). Only one study stood in contradiction: Kwok and Yu (2013) found that text-based posts on company Facebook pages generated more engagement than other media types, including videos and photos.\nThe evidence was intriguingly mixed on the effects of posting URLs on microblogging platforms. As shown in Table 1 in the appendix, four studies based on Twitter samples showed that including URLs in tweets was a positive predictor of retweets; however, three studies drawing on data from Sina Weibo revealed the opposite relationship. These contradictory findings suggest the need for crosscultural research that compares the effects of posting links on the US-based Twitter versus the China-based Sina Weibo.\n---\nInteractivity\n\nCertain types of social media content are designed to encourage users to react. Twenty-five articles explored the effects of interactive strategies on engagement and reach. These studies involved eight variables, including the use of 1) contests and incentives, 2) questions and polls, 3) platform optimization, 4) profile completeness, 5) responding to followers' replies, 6) tagging, 7) hashtags, and 8) following back. Like the case of vividness, interactivity was operationalized in different ways. Some studies used a scale of interactivity, classifying posts from low to high, while others employed a dichotomous measure and classified certain behaviors as interactive or not. Despite differences in operational definitions, the first five interactivity variables listed above were consistently and positively related to popularity (see Table 1, appendix). Much of this research analyzed company Facebook pages, but the positive effects facilitating interactions by asking questions, taking polls, and replying to the comments were similar for ordinary people on Twitter and original SMIs on YouTube.\nMore intriguing were the three variables that generated empirical controversy. The first variable involved tagging-that is, including the handle of another account within the body of a message, presumably for the sake of generating interaction with that user. Six studies based on data from multiple platforms (Twitter, Sina Weibo, Facebook) found a positive relationship between tagging and popularity, but two studies, both based on Twitter, found the opposite relationship (see Table 1, appendix). Second, the use of hashtags was examined in eight studies of microblogging websites. As seen in Table 1 in the appendix, four of them revealed a positive relationship between hashtag use and retweets; one suggested that limiting hashtags to two or fewer per post predicted increases in Twitter followers over time; and three studies found a negative association between hashtag use and popularity on Sina Weibo and Twitter. Third, while Hutto et al. (2013) found that following many other accounts was a positive predictor of having followers, two competing studies showed that following fewer other accounts was positively associated with retweets and likes (Zhang & Peng 2015;Valsesia et al. 2020). Studies of tagging, hashtag use, and following back represent a contested area of the literature.\n---\nEmotion\n\nTwenty-five articles examined the relationship between expressing emotion and popularity. The bulk of evidence, drawn from studies of multiple platforms and user types, showed that expressing various types of emotion in posts was positively associated with engagement and follower counts. Consistent results were found in research rooted in diverse methodological frameworks and based on various operational definitions of emotion. Four studies identified emotional content, without specifying its valence, as a positive predictor of multiple popularity metrics (see Table 1, appendix). Nine studies found that the use of positive sentiment, feeling or emotion was a predictor of popularity (see Table 1, appendix). Content coded as entertaining or interesting was also linked to engagement. For instance, investigating brand marketing on Facebook, three studies revealed an association between entertaining content and more likes, comments and shares; another study identified a link between posting interesting tweets and being retweeted (see Table 1, appendix). Expressing negative sentiment, feeling or emotion was also a positive predictor of popularity, according to nine studies (see Table 1, appendix). Negative or critical content appeared to be especially engaging to audiences in the context of news topics and political debate. In addition, Naveed et al. (2011) found that using negative emoticons encouraged retweets, and Kivran-Swaine and Naaman (2011) demonstrated a positive association between expressing sadness on Twitter and follower counts.\nA small minority of studies offered caveats or findings that conflicted with the majority view. For example, although certain types of controversial messaging generated engagement, using negative emotional language that stigmatized groups was shown to diminish retweets and likes on Twitter (Schwartz & Grimm, 2017;Jain et al., 2020). As shown in Table 1 in the appendix, research on the effects of fear appeals produced mixed results.\n---\nInformation\n\nThirteen articles considered the informational appeal of social media messages. Content coded as informative was shown to increase engagement in four studies (see Table 1, appendix). Yesiloglu and Waskiw (2021) found that providing information in a conversational tone increased the number of comments on Instagram. Beauty influencers on YouTube received more comments when posting information-rich product reviews compared to four other video types (Delbaere et al., 2021). YouTubers in the automotive sector who used more \"concreate language\" tended to have more views and subscribers than those who used less concrete language (Lee & Theokary, 2021), while the use of tentative words like \"maybe\" and \"perhaps\" on Twitter was negatively associated with retweets (Kim et al., 2016). The presence of longer, more complex words was correlated with an uptake in follower counts on Twitter (Hutto et al., 2013). The total number of words in posts was positively related to retweeting on Sina Weibo (Zhang & Peng, 2015) but negatively related to engagement indicators on Facebook (Banerjee & Chua, 2019). Focusing Twitter content on a narrow coherent set of topics attracted more followers over time (Wang & Kraut, 2012) and more retweets (Cha et al., 2010), suggesting that practical information, tailored to a specific audience, tends to boost the popularity of users.\n---\nSelf-orientation\n\nSeven studies examined the link between various forms of self-orientation and popularity. Lee and Theokary (2021) found that the use of self-referential pronouns was positively associated with increases in views and subscribers on YouTube. Thoughtful discussions centered on the YouTuber's personal experiences with a product (\"reflective theme\") were more engaging than five other video themes (Lim et al., 2021). Including a human face in Instagram posts increased the number of comments (Yesiloglu & Waskiw, 2021). However, the effectiveness of centering the self in social media posts may only hold for original SMIs. Four studies of ordinary people and business organizations found that placing an emphasis on the account holder diminished reach and engagement. For the average person on Twitter, using self-referential pronouns was negatively associated with follower counts (Hutto et al., 2013). Tweets about one's self tended to generate fewer retweets than posting content that addressed broader public interests (Naveed et al., 2011). In the case of company Facebook pages, self-oriented content involved references to a corporation, brand or product rather than a person, and was shown to diminish likes, comments and shares (Dae-Hee et al., 2015;Swani et al., 2017). While most studies focused on the behaviors of users, 31 of the 68 studies in the sample looked at how the users' social characteristics predicted their reach and engagement. The predictors were categorized as 1) popularity, 2) organizational resources and status, 3) individual status, and 4) geography. These categories were established inductively and labeled based on the language used in the corresponding studies, though some conceptual differences exist among the studies in each category.\n---\nPopularity\n\nOne of the strongest and most consistent predictors of social media popularity was popularity itself, a conclusion drawn in thirteen studies. Much of this research conceptualized popularity as reach, and showed that users with more followers, fans or subscribers generated more engagement than those with fewer followers (see Table 2, appendix). Rodr\u00edguez-Vidal et al. (2020) found that having more influential followers (those with many followers themselves) was positively associated with having more followers in general. As shown in Table 2 in the appendix, six studies of Twitter demonstrated that being retweeted in the past was a strong predictor of being retweeted in the future. Research showing the cumulative advantage of being popular covered three user types and four social media platforms.\n---\nOrganizational Resources and Status\n\nSix articles examined the economic resources and status characteristics of organizations as predictors of popularity. Sports teams with higher operating incomes (Scelles et al., 2017) and teams that hired advertising agencies to manage their social media accounts had more fans and followers on Facebook and Twitter than teams with fewer resources (Hopkins, 2013). Six related variables-appearing on national television, employing players with large social media followings, having high attendance turnouts at games, playing in older stadiums, being a historically newer team within a league, and winning games-were also strong positive predictors of the reach of professional sports teams on Facebook and Twitter (see Table 2, appendix). A study of university Facebook pages showed that schools that enrolled more students and achieved higher prestige rankings generated more engagement and reach than schools with fewer students and lower prestige rankings (Brech et al., 2017).\n---\nIndividual Status\n\nEleven studies focused on the status characteristics of individual account holders. The variables considered were verification status, occupational status, level of experience, age and race. Having a \"verified badge\" on Twitter increased the likelihood of retweets in three studies, but one study of Sina Weibo found that verified status was negatively related to retweets (see Table 2, appendix). The authors of the latter study argued that most verified accounts were controlled by the Chinese government and perceived by many people as propaganda, which made them less likely to be retweeted.\nFive articles looked at occupational status. Celebrities tended to have more followers than original SMIs on Instagram (Zeren & Gokdagl\u0131, 2020). In the context of Covid-related crisis communication, celebrity and original SMIs produced greater engagement rates on Instagram than politicians, public health officials, science communicators and accounts representing news organizations (MacKay et al., 2022). The public health establishment and other institutional users were also retweeted less frequently than other types of users in the discussion of the opioid crisis (Jain et al., 2020). In the context of natural disasters, however, institutional users, such as emergency-related agencies, were retweeted more often than other types of users (Liu et al. 2012). Jensen et al. (2014) found that the most influential factor explaining the popularity of big-time college football coaches on Twitter was their university's prestige and the long-term success of its football program.\nFour articles showed that users with more years of experience on Twitter tended to have more followers and were more likely to be retweeted than those with fewer years on the platform (see Table 2, appendix). Only one study looked at the effects of race on user popularity. Watanabe et al. (2017) compiled a large sample of Twitter accounts held by active Major League Baseball (MLB) players from the 2014 and 2015 seasons. Hispanic players had significantly fewer followers on Twitter, even when controlling for several other variables, than white players. The study also considered the age of players; older players tended to have more followers than younger ones, but popularity gains declined over time as players aged.\n---\nGeography\n\nFour studies looked at differences in popularity across geographical regions. Most of them compared the reach of users located in areas of varying population sizes. Mainka et al. (2015) examined the social media accounts of several international cities and found a positive relationship between the city's population size and its number of followers, fans and subscribers. Two studies showed that major league sports teams located in highly populated areas had greater reach on Facebook and Twitter than sports teams in less populated areas (see Table 2, appendix). Although the many studies reviewed for this essay originated from several countries, only one study demonstrated that the effects of certain types of social media content on popularity varied across nations and cultures (Khan et al., 2016).\n---\nDISCUSSION\n\nThis study systematically gathered, categorized and evaluated a reasonably large sample of naturalistic studies of social media popularity. The aims were to identify the variables that have generated the most and least interest from scholars and locate areas of the literature marked by consensus and disagreement. An adapted version of Barnlund's (2008) transactional model of communication was used to map this intellectual terrain. In brief, the model assumes that interactions between two or more users are shaped by who they are, how they communicate, and how they interpret each other's messages. This process is further influenced by the technical attributes of the given social media platform and by the psychological, relational, cultural, and social contexts. Each of the assumptions in Barnlund's model has attracted some scholarly attention, but researchers appear to be more interested in the communication behaviors that maximize popularity than the social structural forces that enable and constrain it. Among the 68 studies in the sample, 80 percent of them contained at least one predictor involving the behaviors of users, such as posting frequently or sharing emotional content; only 46 percent of studies investigated the effects of users' social positions, such as their age or race, on popularity. Among the 87 predictors of popularity identified in this study, 71 percent involved user behaviors; 29 percent involved their social, cultural and economic circumstances. Scholars were most interested in how emotion, interactivity, and vividness affect popularity, and least interested in the influences of geography, originality, and organizational status of users. Research on the effects of users' race, gender, sexuality, and socioeconomic status on their popularity was notably scarce.\nThe relative disregard for the social origins of popularity echoes Hampton's (2023) claims about the negligible role of sociology in the field of digital media and the need for more sociological theory and research. Social theory may be particularly useful for investigating social antecedents of popularity, such as race, class and gender, but it also may enrich the agency-focused literature on the behaviors of users. For example, many scholars have examined how emotional language can be used to attract and engage followers. Most studies, however, are agency focused and assume that individuals use emotions as a form of strategic communication. While rich in empirical insight, this literature has largely missed the opportunity to demonstrate how emotion work on social media links individual agency to social structure. Decades of sociological research has shown how the ability to manage emotions and use them strategically varies across gender and social class, and that reactions to emotional displays by men and women are likewise socially dependent (Hochschild, 1979(Hochschild, , 1983)). To the extent that emotional expression regulates the distribution of a socially valued resource -popularity -the use of it by users reproduces the gendered and class structures in which individuals are embedded.\nThis review also identified areas of the literature characterized by general agreement among scholars and areas where conditional or contradictory findings were common. To briefly summarize the most widely supported claims, users who posted frequently, produced original content and utilized visual images tended to be more popular than users who used alternative strategies. Messages that were overtly interactive, such as posting questions, organizing contests and actively responding to followers, consistently engaged audiences. That both emotional and informative content boosted multiple popularity metrics was also well-established in the literature. In most cases, these predictors of popularity were consistent across different social media platforms and user types.\nThough fewer in number, scholars who examined the link between popularity and social position rarely disagreed. As shown in this study, the past popularity of users was a positive predictor of their future popularity. That popularity itself was among the strongest and most consistent predictors of increases in reach and engagement may not be surprising, but it yields important evidence that social media, rather than nurturing equal opportunity, widen social inequality (see Table 2, appendix). Popularity, as argued by sociologists for decades (Merton, 1968), readily accumulates for those who already have it and leads to an ever-increasing gap between the popular and the unpopular. Users who enjoyed other structural advantages-access to economic resources and high social prestige-also tended to generate more reach and engagement than those who lacked these resources (see Table 2, appendix). The geographical context played a role, as users located near highly populated cities tended to be more popular than those in less populated areas. Only one study looked at how the effectiveness of social media strategies depends in part on the users' cultural and national context.\nIn some cases, the effects of predictors depended on the type of user or channel being studied. Posting several messages over a short period of time was more effective on micro-blogging sites than on Facebook. Including links in posts was associated with more popularity on the US-based Twitter but with less popularity on the China-based Sina Weibo. The posts of verified Twitter users were more likely to be retweeted; yet, this relationship reversed on Sina Weibo, where the posts of verified government agencies may be perceived as less worthy of being shared. Engagement increased with the number of words in posts on Sina Weibo but decreased with word counts on Facebook. Centering the self in posts and expressing personal interpretations of products and events appeared to be an optimal strategy for increasing reach and engagement for original SMIs on Instagram and YouTube, but not for company brands on Facebook.\nThese conditional effects, rooted in user and channel types, suggest the need for multidisciplinary research and more exploration of the contexts included in Barnlund's model. To account for the full complexity of social media interactions, a research team needs technical knowledge of platform capabilities, cultural knowledge of the values and beliefs associated with communities on each platform, and sociological knowledge of the structures that enable and constrain the various types of users. Given that the three predictors that produced the most disagreement (tagging, hashtags, and following back) involved overtly interactive behaviors, scholars should also attend to the relational context of communication-the personal relationships between users and the development and outcomes of their conversations.\n---\nLIMITATIONS\n\nThe primary weakness of this review is its narrow focus on studies based on naturalistic quantitative content analysis. This sampling criteria excluded surveybased and experimental studies, which have provided a foundation for decades of related research on social influence and persuasion (Gass & Seiter, 2022). Studies based on qualitative research designs were also excluded. Qualitative methods such as digital ethnography capture the naturalistic dimension of social media interactions. Research in this tradition, particularly qualitative studies involving the relational (Abidin, 2015;M\u00e4kinen, 2021), cultural (Raun, 2018), and social contexts (Duffy, 2017) of Barnlund's transactional model, could have provided important empirical and theoretical insight on social media popularity. Although the goals and concepts of related qualitative studies were deemed too difficult to incorporate and compare with the studies reviewed in this article, future reviews of qualitative research on social media popularity are in demand.\nGiven the practical need to identify a manageable portion of the literature, similar review articles have selected studies based on whether they included a certain type of social media user, such as original SMIs, or focused on articles that examined research questions typically covered by particular academic disciplines, such as business and marketing (Hudders, De Jans, & De Veirman, 2021;Sundermann & Raabe, 2019;Vrontis et al., 2021). This review's selection criteria were intended to provide a unique, interdisciplinary pool of studies that include a similar measure of popularity and share an interest in predicting the reach and engagement of a wide range of social media users. As the global population of active social media accounts continues to rise, popular users, from celebrities and original SMIs to businesses and governments, will likely shape important social, economic, and cultural outcomes. For this reason, research on the origins of social media popularity should interest scholars from a wide range of disciplines.\nIdentified by Ye et al. (2021) as a \"future direction in influencer marketing research\" ( 172), naturalistic research also has some advantages over other methods. In contrast to experimental research, naturalistic inquires tap into the interactions and relationships between influencers and followers. These relationships develop over time through multiple interactions and are difficult to replicate with mock influencers, experimental stimuli or cross-sectional survey designs (Delbaere et al. 2021). Experiments and surveys openly elicit responses from subjects, which threatens the validity of findings, whereas content-based indicators are unobtrusive and measure popularity based on observations of real-life behaviors. Mixed-method research combining qualitative and quantitative content analysis may be a particularly useful approach to studying the back-and-forth communication and relational context of social media.\n---\nCONCLUSION\n\nBased on the 68 studies reviewed here, research on social media popularity has coalesced around four specialized areas. Business scholars are primarily focused on predicting customer engagement on company Facebook pages. Another group examines the impact of original SMIs on specific industries such as fashion and fitness and gravitate toward the study of interactions on Instagram and YouTube. Drawing primarily on automated coding procedures and natural language processing, a third group of scholars concentrates on message diffusion (retweets) on microblogging websites. And a fourth group looks at the effects of popular users on a range of social issues, political controversies and public health concerns. Though conceptualized in different ways -as a form of social currency, social capital or popularity -the reach and engagement of users have origins and consequences that are captivating researchers from multiple academic disciplines.\nScholars have made broad strides in identifying the communication strategies and types of social media content that maximize popularity, but social structural influences have received far less attention. While the status characteristics of users, such as their race, gender, socio-economic status, age, culture and national origin, likely affect how audiences and sponsors react to them, relatively few studies in the naturalistic tradition have investigated the social origins of internet fame (Hampton, 2023). Given that much of the research reviewed here has been carried out in disciplinary silos among scholars with similar academic backgrounds, future studies may benefit from assembling multidisciplinary teams to study social media popularity. ",
        "Introduction\n\nAfter being informed about a negative prognosis for a fetus, the parents initially experience mental shock. The next stage is sadness, followed by obtaining information about the nature of the disease affecting the fetus. The psychological burden is very high, as the patient needs to make an independent decision and even submit a written request to have the procedure performed. Providing information about the child's illness is a very important aspect of contact between the parents and professional medical personnel [1,2]. Obtaining full information reinforces the woman's sense of control and ability to make a fully informed decision independently and to control the medical stressor. Of course, patients also look for information on the Internet and in the available literature [1,3]. The decision concerning the termination of pregnancy (TOP) is complex. It depends on internal factors, such as the type of fetal defects (a lethal defect, severe fetal defect, genetic defect without structural pathologies that might make the life impossible, e.g., Trisomy 21 and the mother's personality (family background, religious beliefs, previous mental disorders). The decision is also influenced by numerous environmental factors that may prove to be supportive or destabilizing [2,4].\nPregnancy termination is a stressful experience, which patients-depending on their adaptation skills-cope with in different ways. The process includes cognitive and behavioral actions aimed at changing the individual circumstances for the better. Coping strategies most frequently mentioned in literature with regard to TOP are: conversation (with the partner, family, friends, psychologist, physician), internalization of one's feelings, participation in support groups, psychotherapy, seeking information (in the literature, on the Internet, on television), denial and repression, concentrating on one's children, trying to conceive again quickly, seeking spiritual support, waiting, memorializing rituals, going on holiday with the partner, spending time with one's children [1,5]. Patients who have undergone TOP often rebuild their sense of control by limiting social contacts and distancing themselves from others [6,7].\nIn their paper from 1985, Cohen & Wills distinguished between four types of social support: appraisal support, informational support, emotional support and instrumental support [8]. In the present study, appraisal support is understood as acceptance by the partner, family, community and personnel performing the procedure. In the test group, informational support was mainly provided by professional personnel, i.e., the physician, psychologist, geneticist as well as written medical information. Emotional support included: the partner, family and friends, and, to a lesser degree-a physician and a psychologist, whereas instrumental support mainly included a properly performed procedure, hospital environment and its staff. Community support means that specific people from the woman's closest surroundings helped her deal with difficult situations [8]. Its quality and quantity depend on the personality of the person seeking support. This is most apparent in case of people with an extrovert or neurotic personality. Social support is a network of social bonds and relations having a direct or indirect impact on the person or social network whose quality and quantity depend on the quality of the interactions between the individual and their social surroundings [9]. A very important role is played by environmental resources, understood as the characteristics of the environment in which the respondent finds herself-which serve as a buffer in the situation. Generally, they can be divided into: available social support and a sense of control over the situation. A sense of control is the belief of a given person that their actions have an impact on the course of events. The woman individually determines the quality of the difficult life experience. In fact, the sense of control determines whether a stressful situation will have a mobilizing or demobilizing effect [5].\nCompared to other European countries, Polish law is one of the most restrictive in terms of indications for abortion. According to the Act of 7 January 1993, pregnancy may only be terminated if it endangers the life or health of a pregnant woman, if prenatal tests or other medical indications show a high likelihood of severe and irreversible fetal defects or an incurable disease, which is a threat to its life, or if there are reasonable grounds to suspect that the pregnancy is a result of an illegal act. In case of TOP for medical reasons, the allowable TOP period is precisely defined, until \"the fetus has become capable of living independently outside the mother's body\", i.e., until the 22nd week of pregnancy [10]. In Poland, TOP is proposed to women who received the diagnosis of severe fetal defects and abnormalities after prenatal diagnostics (USG (ultrasonography), amniocentesis, chorionic villous sampling). After the patients are thoroughly informed about the prognosis, they may choose to terminate the pregnancy. Due to ideological reasons, TOP is performed in only a few centers in the country.\nIn this paper the authors describe the approach of Polish professional medical personnel and the manner of executing the right to refuse to perform a procedure on the basis of the conscience clause. The affected women are referred for the procedure to large cities, especially the capital. In our center, a team consisting of the head of the clinic and three consultants confirms that TOP is indeed indicated and can be legally performed. Genetic defects, such as for example Trisomy 21, are considered to be severe or irreversible fetal defects, and finally, due to a varied spectrum of manifestations the general degree of impairment will be unknown for many years [11]. Therefore, patients have the right to terminate the pregnancy in numerous situations despite the lack of structural defects in the fetus if a chromosomal aberration was identified which might affect fetal development to a large degree. Diagnosing certain structural defects, such as heart defects (e.g., hypoplastic left heart syndrome (HLHS) [12]) may also be problematic. As far as heart defects are concerned, TOP can be performed when a consulting cardiac surgeon deems the defect surgically irreparable or if surgical treatment may result in permanent and severe handicap [12,13]. In this case the final decision regarding the presence of indications for pregnancy termination is made by a case conference at the hospital where the patient presented.\nThe paper specifies what forms of support and medical information women expect from professional medical personnel in Poland. It describes the approach of professional medical personnel, the scope of support they can provide and the manner of executing the right to refuse to perform a procedure on the basis of the conscience clause.\n---\nMaterials and Methods\n\nThe aim of the paper is to determine the patients' needs with regard to support provided by medical personnel and the healthcare system as well as to establish what forms of support the patients expect from their partner, family and people in their surroundings to experience their period of grief in the least traumatic way.\nTwo detailed hypotheses were established:\n1.\nSocial support, noticed and received, plays a protective role in the process of deciding on terminating the pregnancy.\n---\n2.\n\nA sense of control is an important factor in the process of deciding on terminating the pregnancy.\nAt the initial stage, consent was obtained from the Bioethics Committee to carry out the study (ethical approval code: 78/PB/2014). A board composed of four members (head of the department and three specialists) verified the eligibility of patients requesting termination for medical reasons at the Department of Obstetrics and Gynecology, Centre of Postgraduate Medical Education. After the verification of the medical grounds for termination in accordance with Polish Act on Family Planning, Human Embryo Protection and Conditions of Permissibility of Abortion of 7 January 1993 [10], all the patients admitted to the hospital between June 2014 and May 2016 were asked to complete an anonymous survey consisting of sixty questions. The patients were recruited prospectively. A doctor or a midwife asked the patient to complete the survey. To eliminate the medical staff's impact on the responses, the patients completed the survey in private (during their hospital stay). The termination procedure was performed afterwards. Surveys were returned at the time of discharge from the hospital.\nThe survey consisted of six sections: general information, general medical interview, pregnancy-based medical interview, religion, outlook on life, support and moral dilemmas. Some of the responses were provided on a five-point Likert scale (1-strongly agree, 5-strongly disagree). The questionnaire contained demographic data and information about the scope of medical information provided, expected forms of support and dilemmas encountered while making the decision. It took the patients around thirty minutes to complete one questionnaire. The physician provided assistance in case of any doubts. In total, one hundred and fifty surveys were collected. Statistical analysis was performed using Statistica software.\nDue to the subject of the study, the majority of variables were measured on a nominal scale. Therefore, descriptive statistics and descriptions were used. In the majority of cases, while examining the strength of the patients' beliefs, especially on the 5-point Likert scale, Spearman's rank correlation coefficient was used to measure the strength of the correlation. All the dependencies emerged in the cross-tabulation and division tables. Therefore, there were no indications for more advanced calculations.\nPregnancy terminations at the Department of Obstetrics and Gynecology are performed using Misoprostol administered vaginally-maximally five doses depending on the term of the pregnancy and the medical interview (in case of status post C-section or surgery on the uterine muscle, the dosage is cut in half) [14]. If ineffective, the procedure is repeated the next day. If the pharmacological method proves ineffective, a Foley catheter is used to induce miscarriage by mechanically widening the cervix, or possibly oxytocin is administered through an intravenous injection.\n---\nResults\n\nThe average time of diagnosing a fetus defect was week 15.6 of pregnancy, whereas the average time of termination was week 18.0 of pregnancy. The time of the procedure is mainly due to the need to wait for traditional genetic tests where the average waiting time is at least two weeks.\nIn our study group, TOP was performed in only four isolated cases of heart defects. Genetic defects constituted half (50.7%) of the diagnosed problems, followed by malformation syndromes (13.3%), and defects of the central nervous system (15.3%). Out of the genetic defects, trisomies were the most common (including Trisomy 21 (42%), Trisomy 18 (23%) and Trisomy 13 (8%)), followed by triploidies (15%). Minor isolated structural pathologies without the confirmation of a genetic defect, e.g., club hands, radial agenesis, agenesis of corpus callosum, were not indications for the TOP.\n---\nCommunity Support in Pregnancy Termination\n\nA vast majority of the respondents said that the opinion of people in their surroundings did not matter to them. Only 5% of women said that the opinion of other people in their surroundings was important to them, which is reflected in the high median and dominant (Table 1). The majority of patients believe that the Polish society is intolerant when it comes to pregnancy termination, whereas less than 9% believe that the Polish nation is tolerant. However, only 23% of the respondents were afraid of being stigmatized by the society. It should be noted that the mode was close to the \"no opinion\" response variant. This is probably due to the fact that not many women planned to inform people in their surroundings about the termination. They only shared such information with their closest family members (Figure 1).\nEven though the material standard of life of the majority of the respondents is medium, they said they would not be financially able to look after a sick child (45%). They believe that the state does not provide sufficient social support in that regard (81%). Even though the material standard of life of the majority of the respondents is medium, they said they would not be financially able to look after a sick child (45%). They believe that the state does not provide sufficient social support in that regard (81%).\nThe majority of the respondents did not indicate that they wanted to get in touch with support groups for people who had gone through similar experiences (64%). Of course, they provided the answers while undergoing the procedure, so their opinion might change afterwards, during recovery. The parents from our study population chose not to consult with parents of children with similar defects who did not terminate the pregnancy.\n---\nSupport from the Partner and Family in Pregnancy Termination\n\nThe majority of the respondents, who met with understanding and acceptance of their decision, sought support from their closest family members (partner, parent or sibling) (Figure 1). 84% of the patients said that their partner and family supported their decision. Only less than 3% of the patients said that their family were strongly against. Only 9% of patients said that one person from their closest surroundings with whom they had spoken about the termination was against their decision. The main support group in case of termination is the family rather than professional medical personnel. However, it should be pointed out that the patients did not inform extended family or other people in their surroundings about the termination. They only shared the information with the group of people who they felt supported by. Technical support was provided by professional medical personnel, while moral support was provided by the closest family members.\nPractically, all the patients had spoken to someone before making the decision to terminate the pregnancy. In most cases it was their partner (96%) and parents (55%) (Figure 1). Few patients had decided to speak to a psychologist (5%). The majority of the respondents (96%) also said that the partner's opinion on termination was very important to them. 8% of the respondents did not take their partner's opinion into consideration. The significance of the partner as a co-decision-maker is also suggested by median 1.0 and mode 1.0 with a slight standard deviation. The partner was also the greatest source of support for the patients (93%) (Figure 1) and supported their decision (91%). Other persons providing support to the patients included the closest family members (49%) and to a small degree a psychologist (4%) and a physician (12%) (Figure 1). Only a few patients said they had no support (2%).\n---\nSupport Provided by Professional Medical Personnel and Informational Support.\n\nNearly one-third (31%) of the respondents had not spoken to the attending gynecologist about their decision to terminate the pregnancy, and 48% of the respondents did not ask if their attending The majority of the respondents did not indicate that they wanted to get in touch with support groups for people who had gone through similar experiences (64%). Of course, they provided the answers while undergoing the procedure, so their opinion might change afterwards, during recovery. The parents from our study population chose not to consult with parents of children with similar defects who did not terminate the pregnancy.\n---\nSupport from the Partner and Family in Pregnancy Termination\n\nThe majority of the respondents, who met with understanding and acceptance of their decision, sought support from their closest family members (partner, parent or sibling) (Figure 1). 84% of the patients said that their partner and family supported their decision. Only less than 3% of the patients said that their family were strongly against. Only 9% of patients said that one person from their closest surroundings with whom they had spoken about the termination was against their decision. The main support group in case of termination is the family rather than professional medical personnel. However, it should be pointed out that the patients did not inform extended family or other people in their surroundings about the termination. They only shared the information with the group of people who they felt supported by. Technical support was provided by professional medical personnel, while moral support was provided by the closest family members.\nPractically, all the patients had spoken to someone before making the decision to terminate the pregnancy. In most cases it was their partner (96%) and parents (55%) (Figure 1). Few patients had decided to speak to a psychologist (5%). The majority of the respondents (96%) also said that the partner's opinion on termination was very important to them. 8% of the respondents did not take their partner's opinion into consideration. The significance of the partner as a co-decision-maker is also suggested by median 1.0 and mode 1.0 with a slight standard deviation. The partner was also the greatest source of support for the patients (93%) (Figure 1) and supported their decision (91%). Other persons providing support to the patients included the closest family members (49%) and to a small degree a psychologist (4%) and a physician (12%) (Figure 1). Only a few patients said they had no support (2%).\n---\nSupport Provided by Professional Medical Personnel and Informational Support\n\nNearly one-third (31%) of the respondents had not spoken to the attending gynecologist about their decision to terminate the pregnancy, and 48% of the respondents did not ask if their attending physician performed termination procedures. The majority of the respondents indicated that the gynecologist's opinion was very important to them (Figure 2) (median 2.0, mode 1.0, standard deviation 1.45), but they did not necessarily mean the attending gynecologist.\nphysician performed termination procedures. The majority of the respondents indicated that the gynecologist's opinion was very important to them (Figure 2) (median 2.0, mode 1.0, standard deviation 1.45), but they did not necessarily mean the attending gynecologist. Perhaps a lack of trust towards the attending gynecologist arises from the fear of social ostracism and the attitude of the physician aimed at avoiding the problem. The patients also stated that the geneticist's opinion was very important in the decision-making process (Figure 2), (median 1.0, mode 1.0, standard deviation 1.37). However, it should be noted that psychologists or psychiatrists played a minor role in the decision-making process (Figure 2).\n---\nMedical Concerns\n\nPatients indicated that medical information was very important in the decision-making process (Table 2). Only 10% of the patients reported having no medical information. The lack of medical concerns suggests that the patients had sufficient information, which made them feel comfortable and gave them a sense of control in the decision-making process. The importance of the information provided was also noticed when it comes to returned surveys. The first 60 surveys were given out by the physician who is the main author of the project, who explained the nature of the study in detail, achieving the return rate of 100%. Subsequent surveys were given to the patients by a random physician or a midwife at the time of admission to the hospital. In that case, the return rate reached Perhaps a lack of trust towards the attending gynecologist arises from the fear of social ostracism and the attitude of the physician aimed at avoiding the problem. The patients also stated that the geneticist's opinion was very important in the decision-making process (Figure 2), (median 1.0, mode 1.0, standard deviation 1.37). However, it should be noted that psychologists or psychiatrists played a minor role in the decision-making process (Figure 2).\n---\nMedical Concerns\n\nPatients indicated that medical information was very important in the decision-making process (Table 2). Only 10% of the patients reported having no medical information. The lack of medical concerns suggests that the patients had sufficient information, which made them feel comfortable and gave them a sense of control in the decision-making process. The importance of the information provided was also noticed when it comes to returned surveys. The first 60 surveys were given out by the physician who is the main author of the project, who explained the nature of the study in detail, achieving the return rate of 100%. Subsequent surveys were given to the patients by a random physician or a midwife at the time of admission to the hospital. In that case, the return rate reached 42.1%. In total, the return rate was 62.5%. This also shows the importance of a personal approach to patients.\nFemale patients participating in the study appreciated the support and empathy received from the medical personnel performing the procedure. However, only 5% of patients said that they had confided in a psychologist, and 44% in a physician. The patients who had talked to the attending physician about the termination were also more willing to talk to their partner and parents. The patients who did not talk to the attending physician spoke to their friends, extended family, a psychologist or a physician more rarely. The result may suggest a more introvert personality. Moreover, fewer of those patients sought support from extended family, friends or a physician.\n---\nDiscussion\n\nNumerous external factors influence the decision-making process when it comes to pregnancy termination: legislation, healthcare system, scope of medical insurance, access to healthcare services, activity of support groups, social status and access to medical knowledge [15][16][17]. The decision to terminate or keep a pregnancy depends on various factors, the chief ones being the psychological constitution of the mother, ideology, personal beliefs, concerns and doubts about the diagnosis, as well as hope that the child will survive [17,18].\nWhile making a decision, on the one hand, the woman feels in control as she can make the decision whether to give birth to a sick child and, on the other hand-she has to make a decision she never wanted to make [19]. Irrespective of how difficult that decision may be, the majority of women-as shown by this study as well as by studies carried out by other researchers-say they made the right decision [20].\nCouples analyze the social consequences of having a sick child. Many women also consider financial limitations associated with rehabilitation and hospital treatments [21,22], which was also reported by the patients of the Department of Obstetrics and Gynecology, Center for Postgraduate Medical Education. The scale of the problem was marked by very strong objections raised by parents of disabled children who feel neglected and abandoned by state authorities. Such parents have to give up important areas of their lives to provide the child with proper care [23,24].\nOne important aspect of the decision-making process is the view on the support provided by the partner [25,26]. More than half of the patients (55%) who had decided to terminate pregnancy due to Trisomy 21 diagnosis said that the reason was that they were afraid of the impact of giving birth to a sick child on their relationship with the partner [26]. In addition, 38% of the patients reported that their fears were related to a disagreement with their partner about the termination. In this study, a high level of acceptance from the partner with regard to pregnancy termination was observed (nearly 100%). The patients did not report their partner being against their decision; in fact, the partner was the main source of support. Another important aspect was a strong position of the partner as the decision-maker and authority for the female patients in our study. Such significant correlations may only be found in scientific reports from India, Nepal, and Bangladesh [27][28][29].\nA study by Antenatal Results and Choices-an organization providing assistance before, during and after prenatal tests in the United Kingdom-indicated that the most important aspect of advisory services is to provide full exhaustive information about the prognosis for the fetus and the technique of performing the procedure, which helps the parents believe that they did everything they could in such a difficult situation [30]. The benefits of deciding, and the resulting sense of control, is emphasized in an American study on the satisfaction with the possibility to choose the method of termination (surgical or pharmacological termination). Patients' sense of control contributed not only to higher satisfaction with the decision but also to long-term coping and grief resolution [31].\nAnother important issue is the process of informing the patient about the possibility of terminating the pregnancy and the place where such a procedure may be performed [32]. In Poland, the access to genetic advisory services and facilities where termination procedures are performed depends to a large extent on the place of residence [33]. Some patients do not undergo ultrasound scans and are not aware of being able to terminate a pregnancy in case of a sick fetus [34]. With regard to providing information about the prognosis and available options, some doctors invoke the conscience clause and do not inform patients about the possibility of terminating the pregnancy. Many women undergo terminations outside Poland, especially if there are doubts whether there are sufficient grounds for termination. The high rate of survey returns (62.5%) is related to the patients' gratitude for being able to have the procedure performed. Twenty-one percent of the patients used the final notes section to express their gratitude and share positive opinions on the personnel working at the hospital.\nIn the study described in this paper, the patients did not report any medical concerns or any issues with medical personnel interfering with independent decision-making process. However, it should be noted that they only asked their closest family members or physicians directly involved in diagnosing the defect and performing the termination procedure for their opinion.\nThe level of stress experienced during a termination procedure depends on the patient's basic resources (personality, values, support from partner and family) [21]. Negative factors include: previous psychiatric issues, planned and wanted pregnancy, pressure from people in the surroundings, no social support, a personality with a higher tendency to react negatively to stress (low self-esteem, pessimism, low sense of control). The same factors may cause mental disorders in women who decide to continue with the pregnancy [35,36]. Literature mentions protective factors, such as: support from the partner and close family, no past mental illnesses, higher education, no medical concerns and young age [37]. Regardless of the evidence described above, a higher risk of emotional complications among the patients participating in the study may probably be found in case of introvert patients, who had not sought support from the attending physician or people in their surroundings. According to the latest study by Kerns et al. (2018) conducted among online support groups higher decision satisfaction and shared decision making is associated with lower feeling of sadness and less frequent occurrence of post-traumatic stress disorder [31].\nIn a study involving 997 married couples from Nepal who decided to undergo an abortion, the main factor in the decision-making process was the husband and professional medical personnel [28]. A similar role in the decision-making process was played by husbands in a study conducted in India. In case of young women living with their husband and mother-in-law, the household members made the decision on abortion [29]. In some countries, the husband is the ultimate decision-maker, but the support group for women deciding on abortion includes neighbors, sisters-in-law, friends, and professional medical personnel [38]. The study discussed in this paper also indicated a significant role of the partner in the decision-making process. In a study by Major, the patients expressed a will to get in touch with professional medical personnel, support groups and organizations supporting women who had undergone a termination. They believed it may help them recover faster. The study indicates that social support is a key element of the recovery process. However, the patients stressed that talking about termination was a very difficult experience for them [39]. The authors suggested that the rationalization of the decision may make it easier for women to overcome negative feelings and sadness, especially social isolation. They stressed the importance of support groups for women after abortion. The majority of couples (72%) reported the need for contact with other people who had experienced a similar problem [39]. In a study from China, a group of women and their families, who were included in the intervention group and received psychological assistance during and after TOP, proved to have the lowest rate of subsequent psychological complications, such as post-traumatic stress and depression [40]. In the Polish reality, due to high social stigmatization, women tend to avoid support groups and contact with a psychologist, as they would then have to admit to having undergone termination. Such an attitude may strengthen the feeling of sadness, loneliness and blaming themselves for the termination. A study by Speckhard and Major et al. showed that women who display an avoidance attitude as a strategy of dealing with stress encounter growing emotional problems over time even if they initially seemed perfectly adjusted to the situation [39,41]. An important role in the emotional recovery process is played by support groups and therapeutic groups. They allow them to verbalize emotions and reduce the sense of isolation as well as to exchange views. The patients did not have a significant need to get in touch with someone with similar experiences or to talk to a psychologist. The main support group for them was the partner and closest family members. This model is usually found in societies with a multigenerational family model. A study on the psychological consequences experienced by midwives participating in termination procedures shows that a higher index of perceived emotional and instrumental support correlates with a lesser likelihood of developing post-traumatic stress disorder [42]. A survey was conducted at a private clinic in Houston to determine the need for support among 51 patients who had decided to terminate a pregnancy due to fetal defects. It was carried out during the procedure, six weeks and three months after the procedure and it showed that women experienced the need for support in different ways. Many women stated that they were not prepared mentally for the consequences of the procedure and they needed long-term professional assistance [43]. The main support group and co-decision-makers were the partner (96%) and family (88%), as well as the geneticist and friends. Only 5.9% of the patients said that they were going to seek help from a psychiatrist, psychologist or a trusted person. 15.7% reported no need for support. The survey was conducted at a private facility, where the patients were mainly highly-educated women, which excludes it from being representative of the whole country. However, the test group was similar in terms of their educational background to the group described in this paper. The authors recommended women undergoing termination to participate in a psychological consultation in order to prepare themselves for subsequent emotional consequences of the procedure [43]. Our patients represented a similar community group and also did not want to continue the cooperation, which probably resulted from social stigmatization. Research showed that women do not reveal the information of pregnancy termination for fear of social stigmatization [2,20]. Studies conducted in Germany demonstrated that patients living in the eastern region, which is more liberal than the western one, reported a lower sense of being stigmatized [44,45].\nWhile terminating pregnancy women are frequently unaware that they will later require psychological support due to a delayed sense of sadness that they experience. Usually, the first symptoms appear within four months up to a year from the procedure. A survey conducted among 2945 women whose child had Trisomy 21 showed that their main source of emotional support came from groups for mothers of children with the same disease [46]. Parents are aware that they will remember the decision they made for the rest of their life. Usually, they seek support and contact with others two or three months after the procedure. This correlates in time with the anniversary of the child's death and with the family's and friends' wish to go back to normal [30].\nIt should be noted that the majority of such studies are conducted among members of support groups for women who have undergone termination, which may have a significant influence on the results. Support groups for people who experienced an abortion in Poland are mainly run by religious organizations. Asplin et al. and Salvesen et al. indicated that women need systemic support later on at different stages of their life [47,48]. Mailing groups and online groups seem to be a good forward-looking solution, as they offer anonymity, ease of access and a sense of community. According to the authors, the dangers related to virtual forms of support include creating an unhealthy obsession and sharing incorrect medical information. The perfect solution would be to have a moderator with a degree in psychology, who could identify incorrect adaptation mechanisms in the process of coping with the problem. In the literature, we can find comments about a lack of professional support after a procedure [47,48].\nWomen commonly feel lonely and abandoned with this difficult problem [49]. However, it should be pointed out that the feeling of loneliness may occur in patients who isolated themselves from their surroundings. In the study described herewith, the patients did not feel the need to become members of support groups or to get in touch with people with similar experiences. The patients did not want to hide from the world, which is evidenced by the fact that the majority of them did not ask to take half of their maternity leave, which they have a statutory right to, i.e., several months of paid leave, and instead only asked for two to four weeks of medical leave. Afterwards, they were planning to return to work. This may result from the fact that they probably did not plan to admit to a miscarriage at work. Abortion creates the atmosphere of shame and secrecy. Many women are afraid of being judged. A study from Israel showed termination as a taboo hidden behind a wall of silence [50]. The majority of the patients only shared a section of the story with people in their surroundings-for example, saying that they miscarried. The Polish society stigmatizes both women who terminated a pregnancy and families with disabled children. The stigmatization creates negative cognitive, emotional and behavioral patterns, which may affect the social, psychological and biological functioning of the mother and her family. Women who internalize the feeling of stigmatization (blame themselves or think that they must have a moral deficit) bear a higher risk of psychological complications developing at a later time.\nPatients appreciate a nonjudgmental attitude of professional medical personnel [47,51]. The participants of the study described herein gave very positive feedback to the holistic approach of professional medical personnel and to the amount of medical information received. The right approach to the patient significantly helps eliminate stress and trauma associated with the procedure. In the literature, we can find accounts presented by many women talking about a lack of professional support after the procedure, which may have been a result of personal beliefs of medical personnel [47].\nThe present study demonstrates the problem of the TOP in Polish cultural, religious, ethical, and political reality. Several years before, the TOP problem concerning cultural aspects in Germany was presented in a study by Erikson [45]. A vast majority of women (90-100%) undergoing prenatal diagnostics decided to terminate the pregnancy after receiving the prenatal diagnosis of fetal defects, even if the defects were not severe. Erikson's observations showed that, similarly to Polish patients, women in Germany separated religious issues from decisions concerning procreation. The author presented a concept invoked by pro-life groups that viewed the TOP as neo-eugenic approach comparable with Holocaust. Therefore, a question arises whether the TOP due to fetal defects may be considered as eugenic abortion, especially with such broad indications for the TOP. The stigmatization of disabled children is another issue that is commonly neglected or considered to be uncomfortable to discuss [52]. According to Erikson, women mentioned situations in which they were asked by strangers whether the defect in their child could not have been diagnosed prenatally [45].\n---\nStudy Limitations\n\nA great number of scientific papers on abortion and termination bear significant methodological limitations due to the sensitive character of the subject at hand.\nThe first issue is the lack of a control group. It is difficult to select women with similar psychological traits and a similar social situation, from a similar cultural environment. There are few patients who decide to continue with the pregnancy that could be the control group. Women from countries with restrictive laws and religion are less willing to talk about abortion, which leads to a low response rate some time after the procedure. Women from places where abortion is stigmatized very often do not want to go back to talking about that difficult subject. Therefore, no further attempt was made to interview them again, once more time has passed since the procedure. It is difficult to choose the appropriate time for subsequent interviews, as there are no clear indications of when such an interview should be carried out.\nThe study was conducted at one facility situated in the capital of the country. As termination is not commonly available, the test group may not be representative for the rest of Poland. There are few centers which perform termination procedures in Poland, which is why the study was limited to only one facility performing nearly half of all the procedures in the Mazowieckie Province and one-sixth of all the procedures in the country. A strong point of the study is a high number of participants (150) with regard to the number of procedures performed in the country and a high survey return rate with regard to the sociocultural situation in Poland (62.5%).\n---\nConclusions\n\nDecision concerning the TOP is complex and depends on numerous internal and external factors. Women who decided to terminate the pregnancy positively assessed informational support which was provided by professional personnel in Poland (the physician who diagnosed the defect, psychologist, geneticist). Notably, a lot of women do not consult their decision with the attending gynecologist, which probably arises from the fear of stigmatization. The above results from the fact that numerous physicians in Poland invoke the conscience clause and refuse to terminate pregnancies or do not inform the patient about the possibility of the TOP. Women do not take the society's opinion into consideration. Such an approach and the resultant internalization of stigma may generate mental disorders in the future which was reported in the professional literature. Additionally, unwillingness to contact support groups or a psychologist is not caused by the fact that we differ from other nations, but rather from the fear of being stigmatized by the society. Strong emotional support from the partner and the closest family, who agreed with the women's decision in almost 100% of cases, is an optimistic phenomenon.\nThe present authors believe that the study will indicate the weakest links, i.e., no possibility to express one's trauma and strong fear of social stigmatization, in the process of mental recovery following TOP procedures in Poland.\nAuthors are planning a further study about how to improve TOP information support from medical professionals that deal with women who are deciding to terminate their pregnancy and a study in a group of women who have decided to continue their pregnancy despite the possibility of termination due to the fetal abnormalities -their psychological aspects and opinions about medical professionals support.\nThis study is the first attempt to tackle this difficult topic, which is not discussed openly and without reservations by the Polish population. It reveals a major social problem which we need to be aware of and attempts should be undertaken to solve it. \n---\nConflicts of Interest:\n\nAll authors declare no conflict of interest.",
        "D\n\nespite influenza transmission has been extensively studied, little is known about the differential transmissibility of influenza viruses in different social settings, e.g., households, schools, and workplaces. A wide literature exists aimed at understanding and quantifying social contacts between individuals in different social settings based on different techniques, e.g. surveys on contact patterns [1][2][3] , analysis of socio-demographic data 4 , time-use data 5,6 , and radio-frequency identification sensor systems [7][8][9] . However, due to the difficulty of gathering reliable epidemiological data describing how infection is transmitted from one setting to another, these techniques have not been used to estimate the relative importance of different social contexts in the spread of influenza.\nAdequate epidemiological data on influenza transmission are available for contacts between household members [10][11][12][13][14][15][16][17] and, since the 2009 H1N1 influenza pandemic, between schoolmates [18][19][20] . Although these elements have been investigated individually in previous work through various modeling studies and statistical techniques, an overall picture has yet to emerge. Influenza transmission in different social contexts (including, for instance, workplaces and the general community) remains poorly understood; in fact, previous modeling studies 14,[21][22][23][24][25][26][27][28][29] have been mainly based on educated assumptions, rather than on empirical estimates, although evaluating the relative proportions of transmission in the different social contexts is of paramount importance for identifying the most optimal intervention strategy. Indeed, the uncertainty regarding the contribution of the various settings at different stages of the epidemic process clearly limits the ability to properly evaluate the efficacy of interventions such as closure of schools/workplaces, household quarantine, case isolation, and antiviral treatment. In this work we aim to fill this gap.\nHuman-to-human influenza transmission depends on i) frequency of contacts between individuals, ii) duration of the contact, and iii) intensity and type of contact -in terms of virus transmission interactions between children at school are substantially different from that of, for instance, adults in workplaces. A highly detailed individual-based model parametrized using realistic socio-demographic and time-use data is developed in this work and used to account directly for the first two components of influenza transmission (frequency and duration of contacts in different social settings, e.g. household, schools, workplaces, and the general community) and indirectly estimating the transmission rate given an adequate contact. To do this Bayesian statistical techniques are employed here to analyze serological data collected before and after the 2009 H1N1 influenza pandemic in Italy 30,31 . This analysis allows us to parametrize the model and to estimate the fractions of infections generated in different social settings. More in detail, Italian time-use data inform on individuals routine during the day and the time spent in different social contexts of 55,773 individuals. The analysis of time-use data enable, at any given time step of the simulation, and according to the day of the week, to dynamically associate individuals either to one specific location (e.g., their own household, their own school, etc.) or to the general community. Here contacts in the general community are defined as all contacts not occurring between household members, schoolmates, and work colleagues; so, for instance, the general community accounts for contacts occurring on public transportation, restaurants, shops, etc. Accounting for the time spent by individuals of different ages in different social contexts and employment types allows us to mimic the complex heterogeneous mixing of individuals within the population. Similar approaches have already been proposed, for instance in 5,6,21,32 for studying airborne-transmitted diseases (like smallpox and influenza) and for deriving synthetic contact matrices by age. Such a high level of detail allows us to disentangle the contribution of the distinct social settings in the spread of the 2009 influenza pandemic. This information will be critical in deciding future control policies that will maximize the effectiveness of intervention strategies.\n---\nResults\n\nCharacterizing human behavior. As can be derived from Fig. 1A, the individual routine during a work day (Monday to Friday) is very much dependent on the occupational status (e.g., student, worker, retired/unemployed). Indeed, during the morning, school-aged individuals spend their time at school mixing with peers of the same age, i.e. mixing is assortative by age. Similarly, a large fraction of adults spend their time at work whereby they tend to have contacts with colleagues. This also implies that during the morning, contacts in households and in the general community mainly involve retired/unemployed individuals (i.e., mainly the elderly). During the daytime, adults and children spend most of their time in the general community during lunchtime hours and before and after work/school time, including the time required for commuting from home to school/work and vice versa. The elderly spend a considerable amount of time in the general community (3.5 hours per day per individual aged 651 years), with a peak of activity in the central part of the morning when students are generally attending classes and the presence of adults is marginal. During the late evening, and even more overnight, the mixing pattern is mainly characterized by contact between household members (about 95% of all contacts occurring between midnight and 6am is between household members).\nSuch a pattern of human activity is one of the main determinants of the infection process in different social settings. As clearly shown in Fig. 1B, according to model predictions, infections occur in different social contexts at different hours of the day -the overall transmission is also variable during the course of the day, see Supplementary Information -with peaks of transmission in schools during the morning, in the general community during the evening, and in households overnight. Simulations also show a peak of transmission in the general community early in the morning, ascribable to contacts in the time required to commute from home to school/ workplace. One aspect of influenza transmission not clearly analyzed yet is the role of weekends. Weekends could contribute to breaking the chain of transmission in schools and workplaces, because influenza generally has a short generation time -about the length of a weekendthus influencing the overall pattern of spread. Therefore, the weekly calendar was expressly considered to account for the effect of weekends. The activity of individuals is regulated in such a way as to cyclically follow time-use data collected on workdays for five simulated days and then to follow time-use data collected on weekends for the next two simulated days. Specifically, we define the activity of individuals during a weekend day to be the activity reported in the time-use survey during Saturdays and Sundays, without distinguishing between them. Weekends are characterized by much more time spent in the general community (3.4 hours per day per individual during work days compared to 4.8 hours per day per individual on weekends) and much less at school (5 hours per day per student during school days compared to 1.7 hours per day per student on weekends) or work during the daytime (see Fig. 1C) resulting in a larger fraction of cases generated in the general community in the late morning and afternoon (see Fig. 1D).\nOur analysis also highlights that, even if all individuals spend some time in the general community, mixing patterns in this setting are far from being homogeneous. In fact, different age groups spend their time in the general community at different times of the day, thus lowering the transmission probability of airborne diseases between different age groups.\nAge-specific seroprevalence. The epidemic transmission process is modeled according to a classic susceptible-latent-infectiousremoved (SLIR) epidemiological model, describing virus transmission in the general community (R), with explicit transmission in households (H), schools (S), and workplaces (W). At any given time step, infectious individuals can infect only susceptible individuals who are sharing their same location at the same time (see Methods section and Supplementary Information). Specifically, three different disease transmission models characterized by an increasing level of complexity and realism of the social structures have been considered, namely:\n. Model HR: only households and the general community are modeled and the latter accounts also for school and workplace contacts; . Model HSR: households, schools, and the general community are modeled and the general community accounts also for workplace contacts; . Model HSWR: households, schools, workplaces, and the general community are modeled.\nWe found that the two models explicitly considering transmission in schools (models HSR and HSWR) both perform significantly better than the simple structure model (model HR) in reproducing the observed post-pandemic age-specific seroprevalence (Fig. 2A). Model performances were evaluated by the Deviance Information Criterion (DIC; we recall that models with smaller DIC should be preferred). The DIC of the three considered models is 39.3 for model HR, 33.6 for model HSR, and 31.7 for model HSWR. Model HR results in overestimating the seroprevalence in pre-school children and underestimating seroprevalence in children and adolescents (see Fig. 2A). As for models HSR and HSWR, results show a significantly higher seroprevalence in school-aged children and adolescents (around 55-65%) -a high fraction of seropositive individuals (around 35-45%) is also estimated in pre-school children -compared to older age classes. These estimates compare well with observed data (see Fig. 2A). The fraction of H1N1 seropositive among elderly individuals does not significantly increase with respect to the pre-pandemic baseline (see Supplementary Information). All in all, the analysis suggests that 1) schools may have played a pivotal role in the transmission of influenza -this is why models HSR and HSWR, explicitly accounting for transmission in schools, outperform model HR -and 2) younger individuals could have been more susceptible, for either biological or behavioral reasons, to the disease than adults -this is why the inclusion of workplaces in the model does not result in significantly better estimates with respect to model HSR.\nEpidemic doubling time. The epidemic doubling time is the time required for the number of new daily cases to double their value, that is log(2)/r where r is the exponential growth rate of the incidence of new infections. The exponential growth rate of the simulated epidemics has been computed by fitting a linear model to the logarithm of the predicted daily incidence of new infections over a time windows of two weeks chosen in the initial phase of the epidemic, when the depletion of susceptibles is negligible and the incidence grows exponentially.\nEstimates of the doubling time are 7.6 days (95% Credible Interval, CI: 4.3-13.6 days) for model HR, 5.4 days (95%CI: 3.6-8.7 days) for model HSR, and 5.6 days (95%CI: 3.6-8.7 days) for model HSWR. As for models accounting for explicit transmission in schools, although they are calibrated only on the basis of seroprevalence data, which does not include any direct information about the growth rate of the epidemic, they lead to estimates of the doubling time in satisfactory agreement with those reported in three independent studies 31,33,34 (see Fig. 2B). Estimates provided by model HR are slightly larger than those reported in these studies (see Fig. 2B).\n---\nReproduction number.\n\nA key measure of the transmission potential of a disease is the reproduction number R 0 , which is defined as the average number of individuals infected by a typical infectious individual in a fully susceptible population. The effective reproduction number R e is used when a fraction of the population is already immune to the disease 35 . The procedure used for computing R 0 is detailed in the Supplementary Information. As a fraction of the population, mainly concentrated in the elderly, was immune to the virus at the beginning of the 2009 pandemic, we provide estimates of R e .\nWe found R e 5 1.29 (95%CI: 1.14-1.48) for model HR, 1.4 (95%CI: 1.23-1.58) for model HSR, and 1.39 (95%CI: 1.23-1.59) for model HSWR, in satisfactory agreement with independent estimates regarding the 2009 influenza pandemic in Italy, resulting from the analysis of different datasets and obtained by using different approaches 31,33,34,37,38 (see Fig. 2C).\nAge-specific susceptibility to infection. One peculiarity of the 2009 influenza pandemic was an age-specific pattern of susceptibility to infection, as observed by studies focusing on the United States 15 , Mexico 41 , and European countries 31,38 . All these studies have highlighted remarkably larger relative susceptibility to infection in school-aged children and adolescents compared to adults and the elderly. Differential susceptibility to infection by age is accounted by assuming that individuals aged 191 years are exposed to a lower force of infection with respect to younger individuals (see Methods for details).\nIn all tested models, we estimate a pattern characterized by lower susceptibility to infection in adults and the elderly compared to individuals 18 years of age or younger (see Fig. 2D), specifically 0.11 (95%CI 0.07-0.15) for model HR, 0.21 (95%CI 0.14-0.33) for model HSR, and 0.2 (95%CI 0.12-0.28) for model HSWR. Estimates provided by models HSR and HSWR (about 0.2 on average for all individuals 19 years of age or older) compare well with values reported in 31 .\nThese findings reveal the pivotal role played by schools in the transmission of the 2009 H1N1 influenza, acting as amplifiers of influenza transmission. In order to reproduce the observed profile of seroprevalence, model HR is forced to estimate a remarkably high (and hardly plausible) susceptibility to infection of school children and adolescents with respect to adults, in order to counterbalance the lack of contacts in schools between children and adolescents. In fact, as shown in 3,4 , these age groups are characterized by a higher number of contacts and a larger assortativity mainly due to school contact. Results reported so far allow us to exclude model HR from the rest of the analysis.\nInfluenza transmission by setting. According to model HSR, the resulting fraction of transmission per setting is 42.8% (95%CI: 39.9-45.6%) in households, 27.2% (95%CI: 21.1-33.2%) in schools, and 30% (95%CI: 25.9-34.3%) in the general community (which also accounts for workplace infections); by assuming model HSWR the figure becomes 41.6% (95%CI: 39-43.7%) in households, 26.7% (95%CI: 21-33.2) in schools, 3.3% (95%CI: 1.7-5%) in workplaces, and 28.4% (95%CI: 24.6-31.9%) in the general community. Results are summarized in Fig. 3A.\nEstimates obtained by assuming either model HSR or HSWR do not differ much; this is a consequence of the low level of transmission associated with contacts between work colleagues, as estimated by model HSWR. Such a low proportion of transmission within workplaces has already been found in 38 and is possibly ascribable to the low susceptibility to infection of adults compared to that of younger individuals.\nLower values of within-household transmission (about 33%) have been estimated for the 1999-2000 influenza season in France (see Ferguson et al. 14 ). Similar results have also been reported in 17 for seasonal influenza (estimated range 22%-35%) and for pandemic influenza (estimated range 23%-37%) in Hong Kong. However, as hypothesized in 22 and later confirmed in 42 , the fraction of transmission occurring in households strongly depends on the sociodemographic structure of the population where the virus spreads. The average age of the population and the household size are both factors critically affecting these estimates. Moreover, simulations show a high variability of estimates over time, especially in the initial phase of the epidemic when the number of cases is still low and the epidemic follows highly stochastic chains of infection. This should be cautiously taken into account when analyzing field data.\nEstimates of the proportion of transmission in households are not very sensitive to R 0 and adults susceptibility to infection compared to that of younger individuals, but the proportion of transmission in schools, in the general community, and to a lesser extent, in workplaces are (see Fig. 3B). We found that for each value of R 0 the proportion of transmission in households is higher for intermediate influenza seroprevalence by age group observed in the serological samples (grey; a sample is considered seropositive when HI titer is $ 40) and posterior distribution (mean, 50%CI and 95%CI) estimated with transmission models HR (blue), HSR (green) and HSWR (red). (B) Posterior distribution (mean, 50%CI and 95%CI) of the epidemic doubling time estimated with transmission models HR (blue), HSR (green) and HSWR (red). The grey bar refers to range estimated an independent study (Merler et al. 31 ); grey points refer to the average values estimated in Poletti et al. 33 and in Ajelli et al. 34 . (C) Posterior distribution (mean, 50%CI and 95%CI) of the effective reproduction number estimated with transmission models HR (blue), HSR (green) and HSWR (red). Grey bars refer to range estimated in independent studies (Merler et al. 31 and Dorigatti et al. 37 ); grey points refer to the average values estimated in Poletti et al. 33 and in Ajelli et al. 34 . (D) Posterior distribution (mean, 50%CI and 95%CI) of the relative susceptibility to infection of adults (191 year-old) with respect to younger individuals estimated with transmission models HR (blue), HSR (green) and HSWR (red).\nvalues of relative susceptibility (about 0.3-0.5). Higher values of relative susceptibility result in much more transmission in the general community (more than 40% of cases generated in the general community) and much less transmission in schools (less than 10% of cases generated through school contacts). The opposite effect is observed when considering lower values of relative susceptibility, with less than 30% of cases generated in the general community and more than 35% of cases generated through school contacts.\nTogether with R 0 , another measure of the transmission potential of a disease is represented by R index , which is defined as the average number of individuals infected by the first infectious individual (the index case) in a fully susceptible population 22,36 . The effective R index e is used when a fraction of the population is already immune to the disease. The use of R index e allows us to investigate the transmission potential by age groups. The procedure used for computing R index e is detailed in the Supplementary Information.\nAs shown in Figure 3C, the estimated R index e of the overall population is 1.13 by assuming model HSWR (R index e ~1:08 by assuming model HSR). Such a value is lower than the estimated effective reproduction number as the initial infective individual is randomly chosen and it cannot thus be considered a ''typical'' infector, as required in the definition of R e -the difference between the estimated R e and R index e is in line with literature values 22,36 . Our results show that younger individuals (especially students) have a higher transmission potential than adults. Specifically, we found R index  ~0:89 by assuming model HSR). This result is in agreement with the current knowledge on the 2009 influenza pandemic, suggesting that school age individuals have shown a higher transmission potential than adults (see for instance 39,40 ). Moreover, we can also observe the relative contribution of the different settings to the overall R index e\n. By looking at the entire population, we found that R index e is 0.57 in households, 0.18 in schools, 0.07 in workplaces, and 0.3 in the general community according to model HSWR (0.59 in households, 0.18 in schools, and 0.31 in the general community according to model HSR). A part from obvious differences in the transmission potential at school, we found that R index e in household is 0.77 for individuals aged 0-18 years according to model HSWR (0.77 by assuming model HSR), while for individuals aged 191 years it is 0.52 according to model HSWR (0.54 by assuming model HSR). This remarkable difference stems from the fact that younger individuals more likely live in larger households and with other young individuals 4 .\nThe role of weekends. To assess the impact of weekends on the spread of the 2009 H1N1 pandemic we compare the above results to those obtained from a theoretical scenario where changes in individuals' habits during the weekend are not considered, i.e., weeks consist of seven work days. Results show that weekends (with more time spent in the general community and much less at school and work) are responsible for a reduction of R e of 6.7% on average according to model HSR (without weekends R e increases to 1.5, 95% CI 1.31-1.72), and a 8% reduction on average according to model HSWR (without weekends R e increases to 1.51, 95%CI 1.33-1.73). We also found that the fraction of cases in different settings is affected by weekends. According to model HSWR, weekends contribute to increase transmission in households of 3.5% (without modeling weekends the fraction of cases in households decreases to 40.2%, 95% CI 37.9-41.9), to decrease transmission in schools of 19.3% on average (without weekends the fraction of cases in schools increases to 33.1%, 95% CI 26.4-40), to decrease transmission in workplaces of 23.3% (without weekends the fraction of cases in workplaces increases to 4.3%, 95% CI 2.4-6.4), and to increase transmission in the general community of 26.8% on average (without weekends the fraction of cases in the general community decreases to 22.4%, 95% CI 19.5-25.3). Similar results were found with model HSR (see Supplementary Information). While the estimated difference of transmission in households is not epidemiologically very relevant, results show that weekends may be responsible for a drop of about 19-25% of influenza transmission in schools. However, on the other side, a higher average proportion of transmission in the general community and, to a lesser extent, in households is also associated to weekends. This may be particularly relevant when considering school closure policies.\n---\nDiscussion\n\nIn this work we analyzed age-specific seroprevalence data collected in Italy pre and post the 2009 H1N1 influenza pandemic to identify the main routes of influenza transmission and to quantify their relative importance. Our results suggest that the two main routes of infection were household contacts (accounting for about 42% of all infections) and school contacts (accounting for about 27% of all infections). Only a negligible fraction of infection has been associated with within-workplace transmission (about 3%) while about 28% of all infections have been due to contacts occurring in other social contexts (e.g., public transportation system, leisure places, shops, restaurants, etc.). These figures, which are quite stable under different modeling assumptions, must be considered specific to the 2009 H1N1 pandemic. In fact, remarkably low levels of susceptibility to infection are associated with adults and the elderly compared to that of younger individuals, and it is not clear whether such a pattern is specific to the 2009 pandemic or a common signature of influenza pandemics. We estimated adults and the elderly to be about 5 times less susceptible to infection than children and adolescents. This value of relative susceptibility to infection, however, may depend on factors both biological (e.g., related to the strain of the virus and the host immune response) and behavioral (e.g., interactions between children at school are substantially different for chance of virus transmission than that of, for instance, adults at work). However, at this time, it is not possible to quantify the relative contributions of these factors in determining the overall pattern of susceptibility to infection. Different values of R 0 and, even more importantly, of relative susceptibility, give rise to completely different figures -transmission in schools may vary from 5% to 35% and transmission in the general community may vary from 25% to 45%. We also found that the transmission potential of younger individuals (aged 18 years at most) was about two times higher than that of adults. Moreover, our analysis highlights that weekends are responsible for a decrease of the effective reproduction number of about 8%. Moreover, we found different figures of transmission by setting during weekends, with a drop of transmission in schools, a higher proportion in the general community and in households. These findings may inform policies to optimize containment/mitigation measures when facing a new influenza pandemic.\n---\nMethods\n\nModeling population's demographic, social, and behavioral characteristics. We simulate a population of 100,000 individuals (roughly the average size of an Italian municipality). The model is informed with detailed socio-demographic data on the Italian population as described in 4 . Here we also consider that individuals can move among different locations during the course of the day on the basis of time-use data for the Italian population, stratified by age and employment, work days and weekends, at 10-minute time resolution. Details are provided in Supplementary Information.\nDisease transmission model. Influenza transmission is modeled according to the classic SLIR scheme: (S) susceptible, individuals who can acquire the infection; (L) latent, individuals who are infected but not able to infect yet; (I) infectious, individuals who are infected and able to infect; and (R) removed, individuals who are immune to the disease, for instance because they recovered from infection.\nOne of the most striking features emerged from the analysis of the 2009 pandemic has been a higher susceptibility to infection of children/adolescents with respect to adults 15,31,38,41 . On the other hand, differences in infectiousness by age have never been reported in the literature and this hypothesis has even been ruled out 15 . Therefore, according to the literature, we assume age-specific susceptibility to infection, but not age-specific infectiousness. We stress, however, that this does not necessarily imply that all individuals have the same transmission potential -what remains constant is the transmission probability given an adequate contact. In fact, for instance, students would have a higher transmission potential than the elderly, as they tend to have a large pull of contacts (e.g. at school), most of which are individuals with high susceptibility to infection. With regard to susceptibility to infection, the population is divided into two susceptibility age classes: children and adolescents (individuals aged 0-18) and adults (191 year-old individuals).\nAs each individual at any time step of the simulation is located in a specific location, we assume that susceptible individuals can get infected only through contact with infectious individuals who are sharing the same place at the same time. Specifically, at any time step t of the simulation, any susceptible individual i has a probability q i ~1{e {li t \u00f0 \u00deDt of being infected, where l i (t) is the instantaneous risk of infection and Dt is the length of the time step of the simulation. We assume homogeneous mixing between all individuals who are co-located in the same setting at the same time and thus the risk of infection can be computed at any time step of the simulation as:\nl i t \u00f0 \u00de~b r a i \u00f0 \u00deI hi t \u00f0 \u00de N hi t \u00f0 \u00de\u00f01\u00de\nwhere . h i (t) identifies the place where individual i is located at time t. h i can be the household of individual i, its school (if any), its workplace (if any), or the general community;\n. N hi t \u00f0 \u00de is the number of individuals co-located in place h i at time t. Thus, for instance, if h i (t) is a household, N hi t \u00f0 \u00de can be at most the number of household members;\n. I hi t \u00f0 \u00de is the number of infectious individuals co-located in place h i at time t; . a i is the age of individual i;\n. r is the age-dependent susceptibility to infection of individuals. We assume that r(a i ) 5 1 if a i # 18 to avoid over-parametrization and r a i \u00f0 \u00de~ rw0 if a i . 18.\n. b is the (setting independent) influenza transmission rate.\nAt each time step of the simulation, latent individuals enter the infectious phase with probability vDt, where 1/v is the average length of the latent period, which is assumed to be 1.5 days. Similarly, infectious individuals recover with probability cDt, where 1/c is the average length of the infectious period, which is assumed to be 1.2 days. This leads to a generation time of 2.7 days, in agreement with estimates given in the literature (see for instance 15,20,38 ). Recovered individuals are assumed to have acquired full immunity to the circulating pandemic virus. Moreover, to account for the presence of immune individuals in the population before the pandemic, we randomly assign individuals to be in the removed class on the basis of the observed pre-pandemic age-specific seroprevalence rates 30,31 .\nModel calibration. The model has two free parameters: the influenza transmission rate b and the susceptibility to infection r of adults (191 years of age) compared to younger individuals (0-18 years of age). Posterior distributions of the free parameters were explored by Markov Chain Monte Carlo (MCMC) sampling applied to the likelihood of post-pandemic serological data reported in 31 . Specifically, by assuming that for each considered age group the number of positive samples is binomial B(n, p), the likelihood is defined as:\nL~P a n a \u00f0 \u00de k a \u00f0 \u00de p a; h \u00f0 \u00de k a \u00f0 \u00de 1{p a; h \u00f0 \u00de \u00f0 \u00de n a \u00f0 \u00de{k a \u00f0 \u00de\u00f02\u00de\nwhere index a runs over the age groups considered in the serological surveys (i.e., 0-5, 6-18, 19-64 and 651 year-old individuals), n(a) is the number of samples tested, k(a) is the number of positive samples and p(a; h) is the seroprevalence for age group a as resulting from model simulations with parameter set h~b, r \u00f0 \u00de.\n---\nAuthor contributions\n\nS.M. and A.M. conceived the research, M.A. performed the computational work, M.A., P.P., A.M. and S.M. contributed to the interpretation of results and the writing of the manuscript.\n---\nAdditional information\n\nSupplementary information accompanies this paper at http://www.nature.com/ scientificreports Competing financial interests: The authors declare no competing financial interests.",
        "Introduction\n\nThe inclusion of indigenous knowledge and ethno-scientific strategies into present frameworks for conservation and sustainable management of natural resources is becoming increasingly important for the development of sustainable food systems [1]. Article 8 (j) of the Convention on Biological Diversity (CBD 2016), clearly endorses Traditional Knowledge (TK) as the base for sustainable development of food systems in a particular region [2]. Bio-cultural refugia [3] act as reservoirs of TK in traditional food systems [4] and have often supported survival during periods of famine [5]. The biological and cultural heritage associated with wild food plants and neglected and underutilized species (NUS) has been at the center of attention of research over the past few decades, and is regarded as a focal issue in battling the problem of hunger and malnutrition [6]. The focus on traditional plant foraging is vital to understanding its role in the sustainability of food systems in remote tribal areas, and in the promotion of novel local gastronomies [7]. TK helps in the discovery of traditional ingredients such as orphan crops, wild crop relatives etc., which can have an important role in combating food insecurity by reviving and reviewing the rich bio-cultural food traditions around which local traditional communities have developed their food systems worldwide [8]. Ethnobiological studies play a vital role in what we call food scouting, i.e., identifying, documenting and disseminating diverse food resources within indigenous communities [9]. Such studies have shown that indigenous communities represent a significant reservoir of disappearing plant and ecological knowledge that needs immediate documentation for developing sustainable food and healthcare systems [10]. Ethno-botanical field surveys can be a viable option for preserving this vital knowledge before it vanishes.\nDuring the COVID-19 pandemic, amid unstable conditions and issues related to the transport of food and other culinary items, wild food developed new significance as a safety net for food supply and food security. Food security is also jeopardized by climate change. WFPs (Wild-Harvested Food Plants) are vital to the diet of millions of people and contribute to food security, especially in rural and low-income communities, but little is known about their vulnerability to climate change [11]. Changing climatic conditions have already resulted in altered growing conditions for cultivated crops. Climate change will also decrease the per capita land area suitable for food production, making the identification, documentation and dissemination of knowledge about diverse wild food resources even more important [11]. The knowledge of wild animals and plants that grow around us could be essential, specifically in remote rural areas [12]. The erstwhile state of Jammu and Kashmir (hereafter J&K) is divided into four parts, Jammu, Kashmir, Ladakh and Azad Kashmir. The first three parts are controlled by Indian authorities while Azad Kashmir is controlled by the Pakistan government. Additionally, Ladakh region is controlled partly by China and India. In this paper, we hypothesize that the geographical, political, social and economic scenarios of the three neighboring countries (India, China and Pakistan) have led to different food and foraging practices in the erstwhile princely state. This geo-political impact resulting in emigration, religious conversions, cross-ethnic marriages, etc., has affected the usage of different animal and plant species for food in the historically unified region of Jammu and Kashmir, and even an untrained eye can see that while people live in a specific social and economic environment, different geopolitical systems have different uses for wild animals and plants. Traditional wild food knowledge is not only linked to local biodiversity and plant availability, but is also deeply embedded in daily food practices that are highly variable and influenced by a complex combination of socio-cultural factors such as the pervasiveness of industrialized food, food security status/socio-economic conditions, and the importance of cultural identity [10].\nIn this study, we therefore investigated the effect that linguistic and religious communities have on the consumption of wild plant and animal species in the remote western Himalayas of Jammu and Kashmir.In this study, we tried to answer the questions: (1) to what extent do the geo-political and socio-economic systems of the region impact the local cuisine and uses of wild animals and plants, and (2) why do different groups use wild foods differently in similar regions? Furthermore, this article aims to display comprehensive data about the use of wild animals and plants as food in the Himalayas.\n---\nMaterials and Methods\n\n\n---\nStudy Area\n\nThe present study was carried out in four biogeographic regions (Jammu, Kashmir, Ladakh and Azad Kashmir) (Figure 1), to study the factors leading to food diversity among the local population, differentiated based on religion, language and geography. J&K is located to the north of Himachal Pradesh and Punjab (India) and west of Ladakh. The region is divided into two provinces (Jammu, Kashmir). Ladakh, as mentioned earlier, is ruled by two countries (India, China) and shares its borders with Gilgit-Baltistan (Pakistan) to the northwest, Himachal Pradesh to the west and Tibet to the north. Azad Kashmir (AJK) shares its boundaries to the north with Gilgit-Baltistan (Pakistan), south with Punjab, west with Khyber Pakhtunkhwa and to the east with Kashmir ruled by India. J&K harbors a rich ethnic and cultural diversity. The main ethnic communities are Kashmiri, Gujjar, Pahari, Dogra, Bakarwal, Balti, Beda and Brokpa. The various languages spoken by these ethnic groups are Urdu, Dogri, Kashmiri, Pahari, Gujjari, Hindi, Shina, Balti, Hindko and Ladakhi.\nKashmiri itself is spoken mainly in the valley (Kashmir), some parts of Jammu, and some parts of Azad Kashmir. Despite the fact that Kashmiri is the most widely spoken language in the region, it is rarely used in education. Dogri, Ladakhi and Pahari are the most widely spoken languages in Jammu, Ladakh and AzadKashmir respectively. Urdu is the only spoken language in all bio-geographic regions (Kashmir, Ladakh, Jammu and Azad Kashmir). Islam and Sikhism are historically the prevailing religions in Kashmir. The Ladakhi people are influenced by the Buddhist faith, and Hinduism is dominant in Jammu (Table 1). Kashmiri itself is spoken mainly in the valley (Kashmir), some parts of Jammu, and some parts of Azad Kashmir. Despite the fact that Kashmiri is the most widely spoken language in the region, it is rarely used in education. Dogri, Ladakhi and Pahari are the most widely spoken languages in Jammu, Ladakh and AzadKashmir respectively. Urdu is the only spoken language in all bio-geographic regions (Kashmir, Ladakh, Jammu and Azad Kashmir). Islam and Sikhism are historically the prevailing religions in Kashmir. The Ladakhi people are influenced by the Buddhist faith, and Hinduism is dominant in Jammu (Table 1). \n---\nSurvey and Data Collection\n\nThe present study was based on the field survey of an area covering 223 villages within the four biogeographic regions (Jammu (n = 58), Kashmir (n = 62), Ladakh (n = 54) and Azad Kashmir (n = 49)) (Table 1, Figures 1 and2). Representative villages were visited during 2019 and 2021. Members of different ethnic groups like Pahari, Gujjar, Kashmiri, Dogra, Bakarwal, Beda, Brokpa and Balti, whofollow different faiths like Islam, Hinduism, Sikhism and Buddhism, were interviewed across the study area. A total of 952 respondents were interviewed, among them 729 men and 223 women. The representative sites were visited several times (37 visits) during the survey period. The sampling was carried out in all four seasons of the year i.e., spring (March-May), summer (June-August), autumn (September-November) and winter (November-February) to document the seasonal use of wild plants and animals across the regions. The stratified random sampling method was used for documentation, which included interviews (n = 716), followed by group discussions (n = 67) [13]. Prior informed consent was taken from the participants, and all the interviews were conducted in local languages using translator services if required. The ethnicity of the participants and the language information given are not disclosed, based on mutual agreement, as stipulated under the Nagoya Protocol. The Code of Ethics of the International Society of Ethnobiology was followed [14]. Information about the plant/animal local names, parts used, valuable key species, collection season and availability status, preferred species and demographic profile of the study participants were documented. The interviews were conducted across different age groups i.e., young (26.05%), middle (31.89%) and old (42.12%), and gender groups, i.e., men (77%) and women (23%). Traditional knowledge was collected from different occupational groups of the region [15,16] (Table 1). The most important knowledge holders were elderly people, and while most respondents were unschooled (65.65%), 21.11% had benefited from primary education, 12.18% from secondary-level education and only 1.05% from higher education. Ten languages (Urdu, Gujjari, Pahari, Kashmiri, Balti, Hindko, Kitwari, Shina, Dogri and Ladakhi) were documented across the study area, and among them, Urdu and Kashmiri were the only commonly spoken languages (Table 1).\nquired. The ethnicity of the participants and the language information given are not disclosed, based on mutual agreement, as stipulated under the Nagoya Protocol. The Code of Ethics of the International Society of Ethnobiology was followed [14]. Information about the plant/animal local names, parts used, valuable key species, collection season and availability status, preferred species and demographic profile of the study participants were documented. The interviews were conducted across different age groups i.e., young (26.05%), middle (31.89%) and old (42.12%), and gender groups, i.e., men (77%) and women (23%). Traditional knowledge was collected from different occupational groups of the region [15,16] (Table 1). The most important knowledge holders were elderly people, and while most respondents were unschooled (65.65%), 21.11% had benefited from primary education, 12.18% from secondary-level education and only 1.05% from higher education. Ten languages (Urdu, Gujjari, Pahari, Kashmiri, Balti, Hindko, Kitwari, Shina, Dogri and Ladakhi) were documented across the study area, and among them, Urdu and Kashmiri were the only commonly spoken languages (Table 1). One knowledgeable respondent from each research site was asked to accompany the researchers to collect plant specimens for verification and herbarium preparation. Murthi [17] and Menon [18] were used for identification, and respondents were shown pictures or live plants for local names. In addition, whenever there was any discrepancy in names, a group discussion was used as a methodology to remove any bias. The collected specimens were further checked for proper identification with the help of taxonomists at the Centre of Biodiversity and Taxonomy, University of Kashmir, Srinagar (J&K), while the specimens collected from Azad Kashmir were identified by Dr. ZahidUllah, Assistant Professor, Department of Botany, University of Swat, and Dr. Sher Wali Assistant Professor, Department of Botany, Islamia College, Peshawar. The online \"Flora of Pakistan\" was used to confirm the correct nomenclature (http://www.tropicos.org/project/pakistan accessed on 12 April 2021). The collected specimens were authenticated at KASH herbarium and at the herbarium of the Department of Botany, Islamia College, Peshawar.\n---\nData Analysis\n\nAssociational analyses among different regions and plant/animal compositions were carried out using Principal Component Analysis (PCA) [13] to find hypothetical variables (components) that accounted for as much variance in our multidimensional data as possible. For that, we used a matrix of presence/absence of animal and plant species in each of the four regions studied and calculated the singular value decomposition of the (centered and possibly scaled) data matrix. PCA was performed using the Software R Studio 4.0.1. With PCA, we were able to investigate how each, or a set of, species are related to each region evaluated. Cluster analysis was carried out to find out how the diversity is related to different ethnic groups using PAST software ver.3.14. This method allows characterization from the individual samples and then combines these into groups, in terms of their similarity. The presence or absence of each species was determined based on the species' use by the specific group in the region. The Venn diagram was prepared using Bioinformatics & Evolutionary Genomics software (available at http://bioinformatics.psb.ugent.be/webtools/Venn/ accessed on 12 May 2021).\n---\nResults\n\n\n---\nWild Food Domain\n\nWe recorded 209 species, broadly classified into plants (n = 152; 139 vascular plants, 13 mushrooms) and animals (n = 57; with 14 mammals, 22 birds and 21 fish) used by the local people from four biogeographic regions (Table S1). (Figures 3 and4). The local population consumed more than half (53%) of the plants as vegetables, followed by fruits (27%), and tea (8%) (Figure 5a). Food preparation has been studied by various researchers e.g., Majeed et al. [19], Manduzai et al. [20] from Pakistan Himalayas; Boesi [21] from Tibetan communities; de Medeiros et al. [22] from Brazil; Stryamets et al. [1] from Ukrainian and Romanian Bukovina. Here we report seven unique types of food preparation from different animal species in the region (Figure 5b). Cooked meat (37%) was the most popular preparation, followed by cooked fish (22%),soup (11%) and other preparations with less than 10% contribution are shown in Figure 5b. The prevalent usage of plants is seen as a result of diverse vegetation types (subtropical to alpine) across a wide elevational gradient [23], similar to other ethnobiological studies [24][25][26].      In Kashmir (valley of Kashmir), 81 plant and 33 animal species were documented as wild food, including mammals (n = 5), birds (n = 10), fish (n = 18). Among plants, leaves were the most used part (41%), followed by fruits (23%) and fruiting body (15%) (Figure 6a). The dominant plant families used were Polygonaceae (10%), Rosaceae (10%), followed by Asteraceae (6%), Lamiaceae (4%). This dominance of Polygonaceae, Rosaceae, Asteraceae might be due to suitable habitat, and favorable environmental conditions for the growth of the species belonging to these families. Traditional uses of these species are well recognized by the local inhabitants [13,27]. In Kashmir (valley of Kashmir), 81 plant and 33 animal species were documented as wild food, including mammals (n = 5), birds (n = 10), fish (n = 18). Among plants, leaves were the most used part (41%), followed by fruits (23%) and fruiting body (15%) (Figure 6a). The dominant plant families used were Polygonaceae (10%), Rosaceae (10%), followed by Asteraceae (6%), Lamiaceae (4%). This dominance of Polygonaceae, Rosaceae, Asteraceae might be due to suitable habitat, and favorable environmental conditions for the growth of the species belonging to these families. Traditional uses of these species are well recognized by the local inhabitants [13,27].\nwere the most used part (41%), followed by fruits (23%) and fruiting body (15%) (Figure 6a). The dominant plant families used were Polygonaceae (10%), Rosaceae (10%), followed by Asteraceae (6%), Lamiaceae (4%). This dominance of Polygonaceae, Rosaceae, Asteraceae might be due to suitable habitat, and favorable environmental conditions for the growth of the species belonging to these families. Traditional uses of these species are well recognized by the local inhabitants [13,27]. In animals, meat (43%) was found to be the dominantly consumed part followed by coagulated protein (24%) from fish, and gizzard (7%) from birds. Ishtiyak et al. and Altaf In animals, meat (43%) was found to be the dominantly consumed part followed by coagulated protein (24%) from fish, and gizzard (7%) from birds. Ishtiyak et al. and Altaf et al. [28,29] reported the same from Punjab, Pakistan. Among animals, Cyprinidae (40%) was the dominant family, followed by Columbidae (9%), and Phasianidae (9%) (Table S1).\nIn Jammu, we found 65 plant species and 17 animal species employed as wild food. Out of 17 animal species, 8 species were mammals, 2 species were birds, and 7 were fish. Contrary to Kashmir, Jammu had less diversity in wild food usage. This can be explained by the fact that Jammu is nearer to the agricultural state of Punjab, and the population mostly relies on cultivated food. Jammu also has a majority Hindu population who are vegetarian. Leaves were used as primary food (42%), followed by fruits (36%), and seeds (7%), similar to Kashmir (Figure 6b). Leaf greens and aerial parts were regarded as safe and sustainable [30,31]. Rosaceae (10%) and Polygonaceae (10%) were dominant families. The most favored animal parts were meat (30%), followed by coagulated protein (12%), trotters (11%), heart (11%), lungs (11%), and kidney (11%). Animals belonged to a variety of families; however, the dominant families recorded were Cyprinidae (29%) and Bovidae (29%), followed by Cervidae (12%) and Phasianidae (12%).\nIn Azad Kashmir, 60 plant species and 21 animal species were used as wild foods. Amjad et al. [32] also reported the use of animals (n = 4), birds (n = 6) and fish (n = 11) from Pakistan. The use of plant species was lower than Kashmir and Jammu, but animal usage was higher than Jammu and lower than Kashmir. The inhabitants of Azad Kashmir are mostly Muslim, and hence consume more animals than the Jammu population. However, their population is smaller than that in Indian Kashmir, which is also a Muslim majority area. Fruits (39%) were the most used plant parts, followed by leaves (35%) (Figure 6c) with dominant families Asteraceae (12%), followed by Rosaceae (7%) and Lamiaceae (5%). In comparison to cultivated species, wild fruits were mostly consumed raw, being known to contain more fiber, higher vitamin concentrations, and a greater range of secondary metabolites [33]. The dominance of Asteraceae was similar in other studies [32,34,35]. Among animals, the most used parts were meat (40%), coagulated protein (21%) and kidney (8%). Cyprinidae (39%) was the leading family, followed by Columbidae (17%) and Salmonidae (11%). People in Ladakh, with its high altitude and harsh climatic conditions, used only 27 plant species and 19 animal species (4 mammals, 6 birds and 11 fish) as wild foods. The study showed that the use of wild flora and fauna was more limited due to the harsh dry arid climate. We also observed that food usage was influenced by China due to historic trade via the Silk Route, which resulted in the influx of a variety of food traditions from China into the region, and later got blended into the local food culture. Furthermore, it is important to mention that present-day Ladakh was once ruled by China, and it was the Mughal Empire that annexed it with the Kashmir province. However, the deep-rooted Chinese culture is still clearly visible in the region. Leaves (53%), followed by fruits (19%), were the main parts of the plant used frequently (Figure 6d). Boesi [21] and Pala et al. [36] also reported leaves as the most utilized part from the Eastern Himalayas. Polygonaceae were the dominant (22%) plant family, followed by Asteraceae (11%), Rosaceae (7%) and Apiaceae (7%). Meat (36%) was the most used animal part, followed by gizzard (15%), spleen (8%) and kidney (8%). Bovidae (26%) was the dominant family followed by Phasianidae (21%) and Columbidae (16%). Haq et al. [13] also reported the dominance of Bovidae. Meat was praised as nutritious, and used mostly in Kashmir (43%) and least in Ladakh (36%) (Table S1). The regional variation in species use patterns can be attributed to the different regions' geopolitical and socioeconomic systems (Table 1). It is also worth noting that religious affiliations have an impact on every region's usage pattern. Hindu and Buddhist religious groups are reluctant to hunt wild animals for food, but the Muslim community does so. Similarly, while Muslim scholars in Kashmir prohibit using Allium semenovii before prayers, other religious groups do not. All these factors influence local cuisine and the use of wild animals and plants in the study area.\n---\nSeasonal Usage of Wild Foods\n\nThe usage of different wild foods was dependent on seasonality. Wild animals and birds were preferred food because of their availability in the winter season, when wild plants were not in abundance. Wild animals and birds were also preferred as food in cold, harsh weather because of their high nutrient and fat content. However, in the warm months of the year, plants were preferred as they were easily available. In addition, fish were also preferred during the summer season due to their easy availability, and they replaced the requirement of animals during the warm months of the year.\n---\nNovel Species Having Gastronomic Application\n\nIn the present study, some documented food species had never been reported from these biogeographic regions. The root bark from Abies pindrow, the rhizome of Bergeniaciliata, B. stracheyi, Rheum webbianum and Acorus calamus, and leaves from Cichorium intybus, Origanum vulgare were used as herbal tea. Young leaves of Vitis jacquemontii were used as salad. Male inflorescences of Juglans regia were used as vegetables, and fruits of Ziziphus jujube, Ziziphus nummularia were eaten. The meat of Jynx torquilla, Streptopelia orientalis and Tadorna ferruginea were used as food in the Kashmir region. Similarly, leaves from Cichorium intybus and Origanum vulgare, and the rhizome of Bergenia ciliata and B. stracheyi, were used for making herbal tea in the Jammu region. In Azad Kashmir, young leaves of Conyza canadensis were cooked with rice. Acacia modesta gum was used as honey replacement. Arnebia euchroma roots were used as spice, and leaves taken with tea to enhance flavor. Tender leaves and young shoots of Leucas cephalotes were cooked as vegetable, as were leaves and young plants of Onopordum acanthium. The ripe fruits of Sambucus wightiana were eaten raw. The meat of Capra falconeri, Muntiacus muntjak, Hrundo rustica, Oenanthe oenanthe, Passer domesticus, Streptopelia decaocto and Streptopelia tranquebrica was used as food. Columba livia was used to prepare soup. The meat of Barbus sarana, Catla catla, Cirrhinus cirrhosis, Labeo calbasu, Labeo dero, Oncorhynchus mykiss, Puntius ticto, Salmo truttafario, Schizothorax plagiostomus, Triplophysa kashmirensis and Glyptothorax kashmiriens was cooked. Prunus armeniaca was sun dried, stored and used in harsh winters as a vegetable in Ladakh.\n---\nWild Food Usage across Regions\n\nThe Venn diagram (Figure 7a) shows that the maximum use of plants was reported from Azad Kashmir, while Jammu region reported the minimum. The J&K region showed greater similarity, whereas the least overlap was observed between Jammu and Ladakh. A cross-cultural comparison of plant resources showed that 63 plants were overlapping between the four regions of the study area. The reason for the widespread use of plants in Azad Kashmir lies in the fact that the region is mountainous, rural and less developed, and thus local people are more dependent on wild plants when compared to the Jammu region, which is less hilly and more developed than other parts of the state. Also, because the Jammu region is well connected to the rest of the country, the easy availability of cultivated food reduces the region's reliance on wild food. The highest use of wild fauna was reported in the Kashmir region, followed by Ladakh, while the Jammu region reported a minimal number of animal uses. Azad Kashmir and Kashmir regions showed greater similarity, whereas the least overlap was observed between Kashmir and Ladakh. A cross-cultural comparison of animal resources showed that 22 animals were overlapping between the four regions of the study area (Figure 7b). The maximum use of wild fauna in the Kashmir region occurs because, in the winter season, the locals prefer to eat wild meat for energy, and people feel that wild meat is free of cost. The greater similarity between the Azad Kashmir and Kashmir regions is due to closer geographic, cultural and religious resemblance between the two regions. Similarly, the PCA showed considerable variation in different regions (Figure 8 a,b). Based on species presence/absence discovered in the case of plants, PC1 and PC2 described 66% of species distribution in the biplot, in which J&K region species are grouped on one side of the PCA and Azad Kashmir forms a separate cluster on the other side (Figure 8a). However, in the case of animals, the PC1 and PC2 explained 67.2% of the species distribution, with the Ladakh region forming discrete clusters from the rest of the regions (Figure 8b). Other studies, e.g., [19,20,37] from Pakistan, Stryamets et al. [1] from Ukraine, also reported the cross-cultural use of wild foods. The highest use of wild fauna was reported in the Kashmir region, followed by Ladakh, while the Jammu region reported a minimal number of animal uses. Azad Kashmir and Kashmir regions showed greater similarity, whereas the least overlap was observed between Kashmir and Ladakh. A cross-cultural comparison of animal resources showed that 22 animals were overlapping between the four regions of the study area (Figure 7b). The maximum use of wild fauna in the Kashmir region occurs because, in the winter season, the locals prefer to eat wild meat for energy, and people feel that wild meat is free of cost. The greater similarity between the Azad Kashmir and Kashmir regions is due to closer geographic, cultural and religious resemblance between the two regions. Similarly, the PCA showed considerable variation in different regions (Figure 8a,b). Based on species presence/absence discovered in the case of plants, PC1 and PC2 described 66% of species distribution in the biplot, in which J&K region species are grouped on one side of the PCA and Azad Kashmir forms a separate cluster on the other side (Figure 8a). However, in the case of animals, the PC1 and PC2 explained 67.2% of the species distribution, with the Ladakh region forming discrete clusters from the rest of the regions (Figure 8b). Other studies, e.g., [19,20,37] from Pakistan, Stryamets et al. [1] from Ukraine, also reported the cross-cultural use of wild foods.  S1.\n---\nWild Plants as Vegetables\n\nThroughout the entire study area, the use of herbs as vegetables was common. Leaves were the most favored part. Women were the leading source of knowledge about nutritional aspects of medicinal and edible species, given that they mostly collected comestible and medicinal species, and were directly involved in household and food preparations [38][39][40][41][42][43][44].\nThe fruits of Phylanthus emblica, Solanum nigrum and Trichosanthes cucumerina were cooked as vegetables. Similarly, the leaves of Malva neglecta, Medicago polymorpha, Oxalis corniculate, Oxyria digyna, Phytolacca acinose, Plantago depressa, Plantago lanceolata, Plantago major, Viola odorata, Taraxacum officinale, Rheum emodi, Rheum spiciforme and Rheum webbianum were also consumed. Young twigs of Dryopteris stewartii were boiled, dried and used as vegetable during the winter season. A number of reports from the Himalayas confirm this use of wild plants as a vegetable [21,[45][46][47]. People in Kashmir, Jammu province and Azad Kashmir collected especially the young and fresh leaves of Allium humile, Amaranthus viridis, Berberis aristata, Cardamine hirsuta and Nasturtium officinale as vegetables [48].  S1.\n---\nWild Plants as Vegetables\n\nThroughout the entire study area, the use of herbs as vegetables was common. Leaves were the most favored part. Women were the leading source of knowledge about nutritional aspects of medicinal and edible species, given that they mostly collected comestible and medicinal species, and were directly involved in household and food preparations [38][39][40][41][42][43][44].\nThe fruits of Phylanthus emblica, Solanum nigrum and Trichosanthes cucumerina were cooked as vegetables. Similarly, the leaves of Malva neglecta, Medicago polymorpha, Oxalis corniculate, Oxyria digyna, Phytolacca acinose, Plantago depressa, Plantago lanceolata, Plantago major, Viola odorata, Taraxacum officinale, Rheum emodi, Rheum spiciforme and Rheum webbianum were also consumed. Young twigs of Dryopteris stewartii were boiled, dried and used as vegetable during the winter season. A number of reports from the Himalayas confirm this use of wild plants as a vegetable [21,[45][46][47]. People in Kashmir, Jammu province and Azad Kashmir collected especially the young and fresh leaves of Allium humile, Amaranthus viridis, Berberis aristata, Cardamine hirsuta and Nasturtium officinale as vegetables [48].\n---\nWild Plants as Fruits\n\nFruits were mostly used raw, and sometimes dried [49]. In this study, fruits like Hippophae rhamnoides, Prunus armeniaca, Prunus domestica and Vitis jacquemontii were mostly used in Ladakh, where the local inhabitants would sundry and consume them in harsh winter. Punica granatum was used in Azad Kashmir and Kashmir. Local inhabitants used the raw fruit, and sometimes the juice. Similarly, Pyrus pashia, Zanthoxylum armatum and Ziziphus jujube were used in Jammu and consumed when fully ripe. The use of wild fruits is reported throughout the globe. Ojelel et al. [50] reported the use of wild fruits from Uganda; Mahapatra and Panda [51] reported the use of wild fruits from eastern India; and Khan et al. [52] reported the same use from Swat Pakistan.\n---\nWild Plants as Spices and Oil\n\nKashmir is known for its cuisine, especially in Wazwaan and other food recipes. The most common wild plant materials used were the leaves of Mentha arvensis. Similarly, Sesamum orientale was used to garnish local bread and cakes. In Jammu, dried flowers of Micromeria biflora were used by various tribal people. Most of the usage was as flavoring agents in curries and soups. Nepeta floccosa was documented from Ladakh, and the aromatic dried leaves and shoots were used to add flavor to local dishes. Our results agreed with Aryal et al. [45], who reported the use of wild plants for cuisine in the western Himalayas; and Bhatia et al. [33], who reported the use of wild plants as cuisine in Udhampur-Jammu.\n---\nWild Animals as Bushmeat\n\nMeat and internal organs of Boselaphus tragocamelus, Capra falconeri, Capra sibirica, Cervus elaphus hangul, Hemitragus jemlahicus, Lepus oiostolus, Marmota himalayana, Moschus moschiferus, Muntiacus muntjac, Naemorhedus goral, Ovis ammon, Ovis aries vignei, Procapra picticaudata and Pseudois nayaur were often eaten locally. Similarly, Altaf et al. [29] reported the use of various animal species from Pakistan. We found that the presence and usage of wild fauna was similar in the whole study area. In Kashmir, Moschus moschiferus was the most hunted animal species, both for meat and musk. Similarly, Capra falconeri was killed in Azad Kashmir, Boselaphus tragocamelus was poached in Jammu, and Capra sibirica in Ladakh. Similar exploitation was found by Mahawar and Jaroli [53]. Local Ladakhi people also hunted wild animals like Alectoris chukar and Pseudois nayaur (blue sheep), and dried the meat for the harsh winter months. On religious grounds, only certain species of birds and animals were used by Muslims as they follow Islamic teachings, especially in multi-religious areas. Muslim groups typically hunted some bird and mammal species, but Buddhists also gathered meat from animals killed by predators such as snow leopards, bears and wolves, or through natural death [13].Hindu people are reluctant to use species like Boselaphus tragocamelus due to their faith orientation.\n---\nBirds as Food\n\nThe meat, gizzard and eggs of Alectoris chukar, Anas acuta, Columba livia, etc, were taken as food (Table S1). Similarly, Mahawar et al. [53] reported the usage of birds like Columba sp. from Rajasthan, India; Powell et al. [54] also reported the use of wild birds from India.\n---\nFishes as Food\n\nThe meat of Barbus sarana, Catla catla, etc, was used as food (Table S1). The documented Schiziothorax species are cold freshwater fish native to J&K [55]. These species are known for their taste, and are preferred over Catla catla and Labeo calbasu. Schizothorax plagiostomus is also found in Azad Kashmir, and Schizothorax labiatus in Ladakh. Other famous fish species from Azad Kashmir include Triplophysa kashmirensis and Glyptothorax kashmiriensis. Our present study is in accordance with [29,56].\n---\nCross-Cultural Analysis\n\nLocal inhabitants living at or around higher altitudes of the Himalayas harbor ancient cultural practices in utilizing wild edible plant species [47,57,58]. In this study, we analyzed the utilization of wild foods based on preference, and seasonal and cultural availability. This was further supported by cluster analysis that grouped the eight ethnic groups into two primary clusters based on similarity in wild food use (Figure 9). The ethnic groups grouped together in the dendrogram were more similar in species utilization and lived in close proximity to one another. The Dogra and Kashmiri ethnic groupings were separated on the dendrogram's extreme left, establishing independent branches with the least degree of similarity. This can be ascribed to the variance in their language and religion, besides other social differences. On the other hand, Gujjar and Pahari ethnic groups formed the most comparable cluster, which was due to them having the same socio-economic status (Table 1), and the fact that both ethnic groups were seen to live close to each other, and share common environment and natural resources across the region. At cut level 2, the greatest resemblance was found between Balti and Brokpa. Both ethnic groups are unique to Ladakh, and hence share the same geography and livelihood practices like cattle rearing and horticulture. The use of Malva neglecta, Taraxacu mofficinale, Stellaria media and Thymus linearis was found to be common in Gujjar and Pahari diets, especially as a salad or soup, whereas Berberis lycium, Ficus carica, Fragaria nubicola, Rubus fruticosus and Rubus ellipticus were the most consumed fruits (Table S1). Both ethnic groups (Gujjar and Pahari) have a strong cultural belief that wild plants provide nutrients [59]. Maundu et al. [60] reported that wild varieties are rich in essential micronutrients, vitamins and minerals. Balti and Brokapa consider Urtica hyperborea soup as potential food that is rich in nutrients, while Prunus armeniaca is cooked as a vegetable. People following Islam use this species during Ramadan frequently, believing it as a rich source of energy. The gathering of wild edible mushrooms is a common and traditional practice, especially in Azad Kashmir and Kashmir. People enjoy their meals by including several species of mushrooms such as Morchella esculenta, Morchella vulgaris, Geopora arenicola and Flammulina velutipes in their dishes. These mushrooms also hold good economic value and are often sold [61].\nBiology 2022, 11, x FOR PEER REVIEW 15 of 21\n---\nCross-Cultural Analysis\n\nLocal inhabitants living at or around higher altitudes of the Himalayas harbor ancient cultural practices in utilizing wild edible plant species [47,57,58]. In this study, we analyzed the utilization of wild foods based on preference, and seasonal and cultural availability. This was further supported by cluster analysis that grouped the eight ethnic groups into two primary clusters based on similarity in wild food use (Figure 9). The ethnic groups grouped together in the dendrogram were more similar in species utilization and lived in close proximity to one another. The Dogra and Kashmiri ethnic groupings were separated on the dendrogram's extreme left, establishing independent branches with the least degree of similarity. This can be ascribed to the variance in their language and religion, besides other social differences. On the other hand, Gujjar and Pahari ethnic groups formed the most comparable cluster, which was due to them having the same socio-economic status (Table 1), and the fact that both ethnic groups were seen to live close to each other, and share common environment and natural resources across the region. At cut level 2, the greatest resemblance was found between Balti and Brokpa. Both ethnic groups are unique to Ladakh, and hence share the same geography and livelihood practices like cattle rearing and horticulture. The use of Malva neglecta, Taraxacu mofficinale, Stellaria media and Thymus linearis was found to be common in Gujjar and Pahari diets, especially as a salad or soup, whereas Berberis lycium, Ficus carica, Fragaria nubicola, Rubus fruticosus and Rubus ellipticus were the most consumed fruits (Table S1). Both ethnic groups (Gujjar and Pahari) have a strong cultural belief that wild plants provide nutrients [59]. Maundu et al. [60] reported that wild varieties are rich in essential micronutrients, vitamins and minerals. Balti and Brokapa consider Urtica hyperborea soup as potential food that is rich in nutrients, while Prunus armeniaca is cooked as a vegetable. People following Islam use this species during Ramadan frequently, believing it as a rich source of energy. The gathering of wild edible mushrooms is a common and traditional practice, especially in Azad Kashmir and Kashmir. People enjoy their meals by including several species of mushrooms such as Morchella esculenta, Morchella vulgaris, Geopora arenicola and Flammulina velutipes in their dishes. These mushrooms also hold good economic value and are often sold [61].  \n---\nWild Food in COVID-19 Pandemic\n\nA Food and Agriculture Organization (FAO) study reports that approximately 1 billion people worldwide use wild plants as a source of their diet [61,62]. The COVID-19 epidemic is expected to significantly expand the use of wild plants, e.g., as herbal ingredients in traditional Chinese Medicine (TCM) formulations as well as other herbal products around the world. In the study area, people from all four regions boiled wild plants and prepared \"kadda\" to protect against and cure symptoms of COVID-19. Some of the most commonly used plant species included Arnebia bentami, Adiantum capillus-veneris, Origanum vulgare, Saussurea costus and Taraxacum officinale, often used to treat respiratory ailments [42]. In our study, it was found that some medicinal herbs such as Datura stramonium (Datur-bool), Rheum webbianum (Pambchalan), Artemisia absinthium (Tethwan), Origanum vulgare (Wan baber) and Prunella vulgaris (Kale voth) were used as immunity boosters during COVID-19. Prunella vulgaris (Kale voth), Cichorium intybus and Salix alba (Yeed) were mixed and boiled to make herbal tea. Arnebia benthamii (Kahzaban) and Viola odorata (Banafsha) were boiled together to make a drink (Sharbat). This drink was also given to patients for relief from chest and throat problems, especially during the COVID-19 pandemic to enhance immunity.\n---\nFood Security and Wild Foods\n\nIn the study area, wild food plants have remained an essential component of the local food basket. We visited pastures in the upper elevations of valleys known locally as \"Gurez\" in Bandipore, \"Keran\" in Kupwara, \"Chinab\" in Ramban-Banihal, and \"Neelam valley\" in Azad Kashmir during the field survey. According to another study carried out in Chitral, Northwest Pakistan [10], the summer meadows are considered reservoirs of several important food and medicinal plant species. We also discovered other plants being sold in markets throughout the study, including Taraxacum officinale, Morchella esculenta L, Gyromitra esculenta (Pers.) Fr. and Rheum webbianum, which are popular wild plants in the studied region. These wild species play an important role in regional diets, especially when there is a food shortage due to drought or disease. People with limited income eat wild food species because they are plentiful and easy to obtain. However, there are certain factors that are becoming potential threats to these natural gifted species, and they include heavy livestock grazing and associated disturbance, habitat fragmentation, unwise development and poaching. The World Food Program reports that 130 million more people are hungry due to drought, and about 135 million are now at risk of extreme starvation [63,64]. Several intense hotspots of starvation have also arisen. As stated by the UN, about 45 million citizens, primarily in Asia and Sub-Saharan Africa, were in acute food shortage between February and June 2020 [65]. While some of these countries manage their food supply portfolios well, others face real environmental limitations in producing more food at home [66].\n---\nClimate Change and Wild Foods\n\nThe rising difference between food supply and the population is an important challenge for human survival in developing countries. In these countries, food malnutrition occurs due to less intake of fruits and vegetables. Thus, with higher cost or market value, the people of developing countries-mostly children-are unable to buy them and thereby become prone to mineral and vitamin deficiencies [67]. A different approach to food safety is to cultivate and use wild plants to mitigate food shortage and malnutrition. Wild edible plants are cheap, high in antioxidants, vitamins, fiber and minerals. Low transmission of traditional knowledge among younger generations and changing food habits are the leading cause for loss of traditional and beneficial knowledge among local inhabitants [68][69][70][71]. The involvement of more researchers is the need of the hour, transforming traditional aspects into scientifically proven conclusions. In our study, we found that there are many species, such as Portulaca oleracea, Amaranthus viridis, Capsella bursa-pastoris, Duchesnea indica, Fragaria nubicola, Mentha arvensis, Taraxacum officinale and Rubus ellipticus, which have great nutritional value and these species grow under changing climatic conditions as weeds. However, most of these species are neglected and underutilized, and are mostly used only by a few ethnic communities. Hence, if these plants are brought into the mainstream, they will play an important role in ensuring food sustainability and security despite future climate change. Amid changing climatic conditions, cultivated crops have become more prone to disease and other drastic climatic conditions. So, in place of these, wild varieties can be grown because they are resistant to pestsand changing climatic conditions, literally becoming a source of food security for rural and tribal communities. Hence, to bridge the gap created by climate change, food insecurity and population explosion, it makes perfect sense to switch to wild edible foods found locally.\n---\nWild Foods as Livelihood Generation\n\nShepherds feed their livestock in the grasslands of Ladakh and forests of Kashmir, Jammu and Azad Kashmir, and know much about wild plants with market value. Species like Aconitum heterophyllum, Bergenia ciliata, Dioscorea deltoidea, Amaranthus viridis, Geopora arenicola, Rheum webbianum, Taraxacum officinale, Saussureacostus, Morchella esculenta, Mentha arvensis, Urtica hyperborea and Viola biflora have been considered economically important plant species that can generate income for the local population. Farmers are managing their lands by growing wild plant species that provide them economic sustainment under agroforestry regimes, and these include Juglans nigra, Pyrus pashia, Punica granatum, Emblica officinalis, Hippophae rhamnoides, Vitis jacquemontii, Morus alba, Morus nigra, Zanthoxylum aromatum and Ziziphus nummularia. This demonstrates that such species management and acquisition of economic benefits can inspire the local people's interest in wild plant conservation and maintenance (Table 1).\n---\nConclusions\n\nThis study has gathered a lot of traditional knowledge on the usage of wild species, and contains the first scientific description of wild food species and their vernacular names in the Western Himalayas, Jammu and Kashmir. We recorded 209 species, broadly classified into plants (n = 152; 139 vascular plants, 13 mushrooms) and animals (n = 57; with 14 mammals, 22 birds and 21 fish) from four biogeographic regions used in traditional feeding systems by the local people. The local population consumed more than half (53%) of the plants as vegetables, followed by fruits (27%), and tea (8%). A surprising quantity of wild food plants as well as wild plants were utilized as snacks, possibly indicating the pastoral lifestyle these people have been leading for years. The most commonly consumed plant components in the research area were its aerial parts and fruits. A wide range of edible wild food species with multiple uses as fruits, vegetables, spices and processed food products are being utilized by the inhabitants of Jammu & Kashmir Himalayas. These food resources have long remained underexploited due to lack of awareness. Cultivating wild food plant species may provide people with the opportunity to supplement their household income, particularly in poor rural areas. Popularizing these species among farmers with due market support, specifically fruits and vegetables, could ensure their profitable farming and effect significant on-farm conservation of valuable germplasm. Moreover, large areas of degraded forest lands and other wastelands areas in Jammu and Kashmir could possibly be reclaimed by planting wild species, which are easier to grow even under adverse soil and climatic conditions. At the same time, these plantations would also benefit wild fauna by providing them with suitable habitat and food. There is a need to develop a value chain from production/processing to marketing/consumption of these underutilized wild food plants to obtain satisfactory economic returns. At the same time, the role of agricultural scientists is to provide technical know-how and find a suitable place for these species in the changing farming patterns due to inevitable climate change, thereby saving valuable wild food resources from extinction.\n---\nSupplementary Materials:\n\nThe following supporting information can be downloaded at: https: //www.mdpi.com/article/10.3390/biology11030455/s1, Table S1: Gastronomic usage of local flora and fauna in different regions of study area. \n---\nFunding:\n\nThe authors extend their appreciation to the Researchers Supporting Project number (RSP-2022-R483), King Saud University, Riyadh, Saudi Arabia.\nInstitutional Review Board Statement: Not applicable.\nInformed Consent Statement: Prior informed consent was taken from the participants. The ethnicity of the participants and the language information given are not disclosed, based on mutual agreement, as stipulated under the Nagoya Protocol.\n---\n\n\nData Availability Statement: All the data obtained during the study are included in this article and the supplementary material.\n---\nConflicts of Interest:\n\nAll the authors declare no conflict of interest.\n---\nEthics Statement:\n\nThe Code of Ethics of the International Society of Ethnobiology was strictly followed. All participants depicted in the images consented to their images being published. For plant collections, we collected the minimum number of specimens required to obtain appropriate vouchers. This study did not include any human or animal subjects.",
        "I. INTRODUCTION\n\nSocial systems evolve at many different spatiotemporal scales, from individual decision-making or interactions to the history of civilisations. The study of social networks, where individuals are represented by the nodes of the networks and links (ties) are summaries of their social interactions, has proven to be a valuable framework to understand the structure and evolution of these interactions [1][2][3]. To this aim, empirical data on social interactions have largely been collected through surveys [4,5] or direct observation [6,7].\nRecent technological developments have made data available at high temporal and spatial resolution [8][9][10][11][12][13][14], providing new proxies of social relationships and making it possible to describe social networks of face-to-face interactions at the spatial scale of a single place such as a conference, a school or a workplace, at time scales ranging from one minute to several days, even if such proxies do not include information about possible discussions or even physical contact, nor about which partner initiated the interaction [7].\nThe resulting data are typically represented as temporal networks [15,16], where we associate a node to each social agent and we draw an edge between i and j at time t if i and j were interacting at time t: this has allowed to study the statistical properties of a number of relevant observables such as the duration of interactions, or the time elapsed between consecutive interactions. The resulting distributions are typically broad with robust functional shapes across contexts [8,11,14]. Aggregating the interactions along the temporal dimension can also make structures at larger time scales visible: aggregated interaction networks typically exhibit a small world topology, a high clustering coefficient and broad distributions of edge weights (the edge weight being defined as the aggregated duration of interactions along that edge), with similar shapes in different social contexts [11].\nThe robustness of these properties has motivated the search for models of temporal networks that could reproduce the observed statistical distributions at diverse time scales [17][18][19][20][21], with a dual aim: on the one hand, understanding which social mechanisms lead to the emergence of these properties, and, on the other hand, producing synthetic realistic data sets that can be of use to study dynamical processes on temporal networks.\nThe main social mechanisms implemented in such models include (i) reinforcement processes, where the probability for two nodes to interact with each other increases after each interaction, leading to broad distributions of contact durations and edge weights in the aggregated network [17,19,20]; (ii) triadic closure, which states that a node is more likely to interact with a neighbour of a neighbour, and has been shown to account for the high clustering coefficient of the aggregated network; (iii) \"memory loss process\", which can be random or target unused social ties [21,22], and contributes to the emergence of community structure in the aggregated network of social systems [21,23,24].\nIn this paper, we extend the modeling of temporal networks of face-to-face interactions in two main directions. On the one hand, we go beyond the commonly considered observables mentioned above, as they do not cover the entire complexity of the empirical networks' structures. We do not intend to answer the question of which list of observables would fully characterize a social system represented as a temporal network, as this question is not fully answered even for static network representations [25]. However we extend the set of commonly used observables: we consider the distributions of the node activity duration and interduration, and of the duration of newly established edges, as well as structural patterns such as the size of connected components in the instantaneous graph of interactions, and spatio-temporal patterns like Egocentric Temporal Networks (ETN) [26], which have recently been shown to be useful building blocks to decompose a temporal network [27].\nOn the other hand, we propose a modeling framework based on a core hypothesis: the existence of an underlying (not observable) directed temporal network called the social bond graph B, which co-evolves with the observed temporal network of interactions denoted G. The weight of an edge in B, B ij (t), represents how much i is inclined to interact with j at time t (B is thus directed as the inclination of i towards j can differ from the inclination of j towards i), while the undirected temporal edge G ij (t) is simply 1 if i and j interact at t and 0 otherwise. The evolutions of B and G follow two feedback mechanisms. First, B(t) guides the interactions that will take place at t, i.e., influences the edges of G(t). Second, interactions have an impact on social bonds through a reinforcement mechanism [22]: if an interaction occurs between i and j, then B ij increases. Moreover, we take into account that the time and energy spent to maintain the tie with an individual is taken from a finite interaction capacity and is thus time not spent with others [28,29]. Therefore, if i and j do not interact but i interacts with another agent k at t, B ij decreases [22].\nWe integrate this framework within a well-known framework for temporal network modelling, the Activity Driven (AD) model [30]: in this model, nodes representing social agents are endowed with an intrinsic \"activity\" quantifying their propensity to form edges at each time step. The initial model [30] has been refined to introduce memory of past interactions (ADM model), as well as triadic closure and renewal of agents [20,21,31]. Here, through the co-evolution of the instantaneous network of interaction G and the social bond network B, we modify the implementation of these mechanisms, and integrate additional ones, namely: (i) the possible disappearance of a directed social bond when it becomes too weak; (ii) the influence of social context (e.g., two social agents belonging to the same group of discussion, having common neighbours, are more likely to interact with each other); (iii) the distinction between intentional and casual interactions driven by the context.\nTo investigate which of the proposed mechanisms are relevant for the study of social systems, we test several variations of the resulting models. We put forward a systematic way to compare them with empirical data sets, by computing the distance between model generated and empirical distributions for a given collection of observables. We use this method to optimize the parameters for each model version, and then to rank versions according to their distance to empirical data.\n---\nII. FRAMEWORK A. Interaction and social bond graphs\n\nOur framework consists in laws of evolution for two temporal networks, an interaction graph G and a social bond graph B. We recall that a weighted temporal net-work g can be defined in discrete time as:\ng : N \u00d7 V 2 -\u2192 R + (t, i, j) -\u2192 g ij (t)\nwhere V is the set of nodes and g ij (t) is the weight of the edge (i, j) at time t. We denote by N = |V | the number of nodes.\nThe interaction graph G is an undirected and unweighted temporal network in discrete time, with finite duration T . The N nodes of G represent social agents, and G ij (t) = 1 is interpreted as the fact that i and j are interacting at time t (else, G ij (t) = 0). We denote by E(t) the set of such active edges of G at t. The social bond graph B is a directed and weighted temporal network, on the same N nodes and same timestamps as G: the weight B ij (t) stands for the social affinity of i towards j at t. The egonet of i at time t is defined as the set of neighbours of i in B at time t, i.e.\n\u03b3 i (B(t)) = {j|B ij (t) > 0}.\nWe note here that we consider only positive interactions, for both G and B. While negative (hostile) interactions do occur in social networks, and negative social bonds exist as well, they are indeed typically difficult to observe concretely [32]. In fact, negative social bonds are often deduced from an avoidance of interactions (i.e., two individuals interacting less than expected by chance) [32][33][34][35], an assumption that has been shown to be able to provide support to social theories such as the social balance theory [33,34]. Here therefore we do not distinguish between an absence of interaction or of social bond and a negative one.\nThe evolutions of G and B are dependent on each other along the following lines. First, interactions taking place at t depend on interactions at the previous time: indeed, two agents belonging to the same group of discussion are more likely to interact in a close future. This can be formalized for instance by the existence of common neighbours in G at the previous time step, giving rise to an influence of G(t -1) on G(t).\nAgents also choose their partners based on a long-term memory of their previous interactions. In particular, the more two nodes have interacted with each other in the past, the more likely they are to interact in the future. Hypothesizing that the edge weights of B can encode this memory effect, it follows that the social bond weights at t also influence G(t) (a node will more likely choose a partner with whom it has a high affinity).\nReciprocally, the social bond graph is updated according to the interaction graph, following the reinforcement process of [22]: the weight B ij increases if i and j interact with each other, and stays the same or decreases if they do not:\nG ij (t) > 0 =\u21d2 B ij (t + 1) > B ij (t) G ij (t) = 0 =\u21d2 B ij (t + 1) \u2264 B ij (t).\nWe initialize B ij as being 0 for all times before the first interaction between i and j on G: \u2200t \u2208 N, \u2200i, j \u2208 FIG. 1. Sketch of the dependencies between the interaction graph G and the social bond graph B. Edges having a higher weight in the social bond graph B(t) are more likely to activate, i.e. to be part of the interaction graph G(t). The computation of B(t + 1) is done in two steps: first B is updated by the feedback of which edges were active in G(t): unused ties decay while used ties strengthen. The output of this first step is denoted by B(t+ 1  2 ) because it refers to an intermediary step between B(t) and B(t + 1), which is obtained from B(t + 1\n2 ) by a pruning process, consisting in removing weak unused social ties. The arrow from G(t -1) to G(t) is of a different nature than the arrow from B(t) to B(t + 1  2 ). The latter accounts for the inertia of the social bond graph, as B can only encounter gradual change from one time to the next (implementing long-term memory). On the contrary, the arrow from G(t -1) to G(t) does not ensure that G(t) will be similar to G(t -1): it simply describes a short-term social context memory through the fact that the more two nodes share partners in G(t -1), the more likely they are to be partners in G(t).\nV, G ij (\u03c4 ) = 0, \u2200\u03c4 \u2264 t =\u21d2 B ij (\u03c4 ) = 0, \u2200\u03c4 \u2264 t, thus assuming that no pre-existing social bonds exist between the nodes.\nIn summary, G(t) is determined both by G(t -1) and B(t), and in return, B(t + 1) is determined by G(t) and B(t) (see Fig. 1).\n---\nB. Social mechanisms\n\nWe use the framework described above to model several social mechanisms.\nThe first mechanism is a short-term reinforcement process with a long-term memory, through the co-evolution of G and B: social agents remember with whom they have interacted and reinforce their social ties with their partners at each interaction, while unused ties weaken. In addition, we assume that weakened ties may vanish: at each timestep B ij has a certain probability to be reset to zero. To capture the realistic assumption that a node tends to shorten unfruitful partnerships to save time or energy, this probability increases as B ij decreases.\nThe second mechanism we consider is the cyclic closure in the social bond graph B. This mechanism captures the fact that, when a social agent initiates a new partnership, it may give the priority to the partners of its partners. Through this mechanism, the existing social bonds drive thus the interactions on G.\nThe third mechanism grasps the fact that two nodes belonging to the same group of discussion are more likely to start interacting together, whether or not they know each other [36]. This can be translated by an increased probability of interaction in G(t) between nodes that were in the same connected component of G(t -1) or, more simply, between nodes that had common neighbors in G(t -1).\nThe fourth mechanism is a dynamic triadic closure driven by the current context, accounting for the fact that if a node interacts simultaneously with two different nodes, these nodes are likely to also be interacting with each other. It is important to note that this mechanism leads to interactions that are contextual and may thus be of a fundamentally different social significance than intentional ones. In particular, we will take into account that contextual and intentional interactions on G(t) might not influence the evolution of the social bonds in B in the same way.\nThe four mechanisms are summarized in Figure 2.\n---\nC. Model implementation\n\nLet us now translate the mechanisms described into microscopic rules of evolution. To this aim, we focus on the AD model in discrete time [20,21,30]: each node is endowed with an intrinsic activity parameter a i , which gives its probability to be active at each time step. The difference between an active node and an inactive node is that only active nodes can emit intentional interactions.\n---\nCreation of the temporal edges of G(t)\n\nAt each time t, each active node i makes m i attempts of intentional interactions, in a way depending on the interactions at the previous time step (G(t-1)) and of the current social bond graph (B(t)). At each such attempt:\n\u2022 With probability p g , i will extend its egonet, i.e., create an interaction with a node j with whom it has no social bond (B ij (t) = 0). In this case, i chooses an interaction partner either uniformly at random (with probability p u ), or, with probability mechanism). Specifically, the first neighbour k is chosen with probability P (i -\u2192 k) \u221d B ik (t), i.e., using the social affinity (independently from a social context). The choice of j as a neighbour of k can be interpreted as a recommendation from k to i; therefore, we include here the influence of a social context recently shared by k and j, and j is chosen among all neighbours of k with probability:\nP (k - \u2192 j) \u221d c kj (t -1)B kj (t).(1)\nThe coefficient c kj (t) is defined as:\nc kj (t) = 1 + |\u03b3 k (G(t)) \u2229 \u03b3 j (G(t))| ,(2)\nrepresenting the boosting of the social affinity by the potential sharing of common neighbours in the previous time step (\u03b3 (g) denotes the set of neighbours of a node in a graph g).\n\u2022 With probability 1p g , i does not extend its egonet, i.e., interacts with one of its neighbours in B. This neighbour j is chosen with probability P (i -\u2192 j) \u221d c ij (t -1)B ij (t), i.e., proportionally to i's affinity towards j, boosted by the potential existence of common neighbours in G at the previous timestep (first and third mechanisms).\nIn addition to these intentional interactions, casual, contextual interactions can occur (fourth mechanism). To Dynamic triadic closure. After computation of the intentional interactions in G(t), we identify its open triangles. Here (i, k, j) is such an open triangle with edges (i, k) and (k, j) (black straight lines). Closing the triangle means either i decides to interact with j (probability pij), or vice-versa (probability pji). pij depends both on how close are i and j in the current social context (cij) and how close they are relatively to their common partner of discussion (b ik and b kj ): if i gives a lot of attention to k, and k a lot of attention to j, then it is likely that i and j will interact. take this into account, we implement here a variation of the dynamic triadic closure. Namely, we consider that for each open triangle in G(t) made up of two intentional interactions e.g. (i, k) and (k, j), i and j interact with each other with probability P c (i, k, j) in a contextual, non intentional manner. For the open triangle (i, k, j) to close, either i or j has to propose the contextual interaction. Denoting the probability that i decides to close the triangle by p ij , we have:\nP c (i, k, j) = 1 -(1 -p ij )(1 -p ji )(3)\nIn our implementation (Fig. 3), p ij takes also into account whether or not i is in the active state: as only active nodes can emit interactions, p ij = 0 if i is inactive. Moreover, we assume that it depends both on the instantaneous social affinity b ik of i towards k and the instantaneous social affinity b kj of k towards j. We define this instantaneous social affinity of a node towards a node m as follows: if m is part of the egonet of , then we simply define b m as P ( -\u2192 m), i.e. b m \u221d c m (t -1)B m (t); if instead B m is zero, we use b m = p g (probability that grows its egonet). We thus use:\np ij (t) = p c b ik (t)b kj (t)c ij (t -1) 1 + p c b ik (t)b kj (t)(c ij (t -1) -1)(4)\nwhere 0 \u2264 p c \u2264 1 is a free parameter (we use c ij measured at t -1 as previously, as it is the social context of the previous time step that influences the link creation at t). This form ensures that p ij grows with p c b ik b kj c ij (i.e., is influenced by the social affinities and by the context) and remains between 0 and 1.\n---\nEvolution of the social bonds of B(t)\n\nThe interaction graph at t, G(t), is thus composed of the intentional and contextual interactions of all active nodes at t. We denote the set of intentional interactions by I(t), and the set of contextual ones by C(t). These interactions determine the change in the social bond graph from time t to the next time step t + 1. The corresponding update (first mechanism) consists in two steps: a Hebbian-like process and a pruning process. During the Hebbian process, edges of B(t) are either reinforced, weakened or let invariant, according to the rule introduced in [22]: if a node i interacts with j but not k, then B ij and B ji may be reinforced, but B ik is weakened (see Figure 4). If i has no interaction at all, its social bonds are not changed.\nAs a refinement of the reinforcement rule [22], we introduce a distinction between contextual and intentional interactions. To this aim, we denote by R(t) the set of social ties that will be strengthened between t and t + 1, and by W (t) the set of ties that cannot be weakened (among the ties starting from nodes that have an interaction in G(t), as the nodes with no interaction at t are not affected).\nWe choose R and W depending on the roles we give to intentional and contextual interactions. A first possibility is to put all interactions on an equal footing: then all active edges are reinforced independently on whether they were intentional or contextual, i.e. R = W = I \u222a C. If we consider only intentional interactions as relevant, and contextual interactions as noise, then edges from C are not taken into account in the process: R = W = I. Finally, if we consider that contextual interactions are neutral, they should give rise neither to a reinforcement nor to a weakening, i.e. R = I and W = I \u222a C. These possible choices are summarized in Table I. To precisely define the process, we need to specify at which rate a given tie strengthens or weakens. We denote strengthening rates by \u03b1 and weakening rates by \u03b2. In order to keep the weights of social ties bounded between 0 and 1 [22], we also consider rates in [0, 1], and we assume them constant. While these rates are also uniform in [22], we consider here that they can be different for different individuals or different ties. We write the general evolution rules as:\n\u2200(i, j) \u2208 R B ij (t + 1) = B ij (t) + \u03b1 ij (1 -B ij (t)) B ji (t + 1) = B ji (t) + \u03b1 ji (1 -B ji (t)) (5) and: \u2200i \u2208 R, \u2200k, (i, k) / \u2208 R \u222a W, B ik (t + 1) = (1 -\u03b2 ik )B ik (t)(6\n) where R denotes the set of nodes involved in the links of R: R = {i|\u2203j, (i, j) \u2208 R}.\nFIG. 4. Sketch of the Hebbian-like process describing the evolution of the directed social bond weights from B(t-1) to B(t) due to the interactions in G(t -1). Active edges are reinforced, inactive ties starting from an interacting node are weakened, and ties starting from a non-interacting node are unchanged.\nNote that in the original ADM model [21], the social bond weights are not bounded, and simply increase by 1 at each interaction.\nTo obtain B(t + 1), we include an additional step, namely a pruning of the social bonds, to take into account the fact that weak social bonds might vanish (in the original ADM instead, node disappearance is implemented uniformly at random [21], i.e., with no relation to the actual social bonds).\nTo quantify how weak is a directed tie (i, j), we compare the probability P (i -\u2192 j) \u221d B ij that i selects j among all its neighbours to interact with, with this same probability if all ties starting from i had the same weight. Denoting by d out i the number of out-links of i in B(t), a homogeneous partition of i's interest towards its neighbours would correspond to P hom (i -\u2192 j) = 1/d out i . Therefore, we use as the probability to remove the directed tie (i, j):\n\u2200i \u2208 R, \u2200j, (i, j) / \u2208 R \u222a W, P d (ij) = exp -\u03bbd out i P (i - \u2192 j) (7)\nwhere \u03bb is a tunable parameter. P d (ij) is thus large if P (i -\u2192 j) is smaller than its homogeneous counterpart, and decreases exponentially when the importance of j for i increases.\n---\nModel versions\n\nEven within the model implementation described in the previous paragraphs, we can define various versions of the model, with for instance different values or distributions of the parameters. Therefore, we first define a baseline version (version V1) with the following features:\n\u2022 a i is drawn from a power-law of exponent -1 with bounds a min and a max ;\n\u2022 m i is drawn from a uniform law in 1, m max ;\n\u2022 \u03b1 ij = \u03b2 ij \u2261 \u03b1 i depends only on i, and \u03b1 i is drawn from a power-law of exponent -1 with bounds 0.001 and 1;\n\u2022 the social context at the previous time step is taken into account through\nc ij (t -1) = 1 + |\u03b3 i (G(t -1)) \u2229 \u03b3 j (G(t -1))|;\n\u2022 contextual interactions are neutral (R = I, W = I \u222a C, see Table I);\n\u2022 the remaining free parameters are: p g , p u , p c , \u03bb.\nWe then implement variations with respect to the baseline, by changing in each case only one of the mechanism implementations, as summarized in Table II. We call these versions adjacent versions, because they differ from the baseline in one aspect only. We tested 12 adjacent versions, numerated from 2 to 13. The version 14 corresponds to the original ADM of [21], with the following properties:\n\u2022 the egonet growth rate is not constant. Instead of having a fixed probability p g of growing its egonet, each node i grows it with a probability depending on its egonet size:\np g (i) = c c+d out i , where c \u2208 N is a model parameter; \u2022 the recent social context is not taken into account: the direct influence of G(t -1) on G(t) is cut off, i.e. c ij (t) = 1;\n\u2022 no contextual interactions are considered, i.e. p c = 0;\n\u2022 B has a linear reinforcement process B ij (t + 1) = B ij (t) + 1 for each (ij) in G(t), and weakening of unused social bonds is not considered;\n\u2022 a node pruning process: instead of removing social ties, we remove social agents with a constant probability p d . After removing the social agent i, we reinsert it into the system to keep the number of agents constant, but with\nB ij = 0 \u2200j.\nThis version is thus actually a composite version (i.e., obtained by combining adjacent ones), because it differs from the baseline in more than one aspect.\n---\nIII. COMPARISON WITH EMPIRICAL DATA SETS\n\nWe consider as references several publicly available empirical data sets describing face-to-face interactions in different contexts, namely two scientific conferences, two schools and a workplace (See Table III and Supplemental Material, SM). As our aim is to evaluate which hypotheses made on some social mechanisms yield realistic temporal networks, we will thus evaluate how close are the temporal networks G generated by each model version to each reference empirical data set. Note that we compare G and not B, as the empirical data sets correspond to instantaneous interactions.\nThe properties of the temporal networks generated by each model version naturally depend on the version parameters. Some can be extracted or estimated directly from the reference data set: the number of nodes N , the duration T , and the observed minimum and maximum node activities, a min obs and a max obs . The other parameters are however a priori unknown and tunable. To limit the number of free parameters, we fix the bounds for the power-law followed by the strengthening and weakening rates of the social bonds, \u03b1 ij and \u03b2 ij (with \u03b1 min = 0.001 and \u03b1 max = 1). The list of remaining free parameters for each model version is given in Table IV.\nOur procedure is thus the following: we first define a set of observables of interest, and a comparison method (a distance) between the outcome of each model version and each reference data set. For each reference and version, we then use a genetic algorithm to find the parameter values minimizing their difference. Note that these optimal parameter values can be different for different references.\nFor each observable O, we can then gather the comparison between each model M and each empirical reference E into a distance tensor D[O] M,E , and subsequently rank all model versions by giving them a score for each observable: the higher the score, the closer the model observable with respect to the empirical ones. Combining the ranks for all observables yields then a global ranking of models.\n---\nA. Observables\n\nAs face-to-face interactions are local in space and time it seems natural to study observables related to small spatio-temporal scales, like nodes, edges or small subgraphs. The simplest observables related to such an object ob are:\n\u2022 its activity duration: number of consecutive time steps ob exists in the temporal graph;\n\u2022 its interactivity duration: number of consecutive time steps ob is absent from the temporal graph;\n\u2022 its aggregated weight: number of times ob has been present in total in the temporal graph;\n\u2022 its newborn activity: number of consecutive time steps ob exists just after its first occurrence in the temporal graph.\nIf ob is not a trivial sub-graph like nodes or edges, its size can also be an observable of interest.\nLet us now recall some useful definitions: \n- - - No - - - - - - - - No additional interactions neutral - - - - equivalent noise None - - - - - None egonet growth constant - - variable - - - - - - - - - variable social bond graph update Hebbian process \u03b1i linear - - - - - -\u03b1 \u03b1i, \u03b2i \u03b1ij, \u03b2ij - - linear (R, W ) (I, I \u222a C) - - - -(I \u222a C, I \u222a C) (I, I) (I, I) - - - - - (I, I) Pruning process social tie - social agent - - - - - - - - - - social agent mi U( 1, m max ) - - - - - - - - - - -constant constant node activity ai - - - - - - - - - - a - - TABLE II. Model versions.\nThe version 1 is the baseline version, while the version 14 is the original ADM. The symbolmeans identical to the baseline version. In the row entitled \"additional interactions\", we precise the role of the interactions obtained through the dynamic triadic closure mechanism. The indication \"None\" means that this mechanism does not exist, i.e. pc = 0. In the row entitled \"Hebbian process\", a symbol \u03b1i alone means three things. First, the Hebbian process used is an exponential process. Second \u03b1ij = \u03b2ij = \u03b1i, and third \u03b1i is drawn independently for each i from the same power-law of exponent -1. Similarly in the row entitled \"node activity\", a symbol ai means that ai is drawn from a power-law of exponent -1 independently for each node. On the contrary, an unscripted symbol, like a or \u03b1, means that the same value is assigned to every node. We put an additional symbol \u03b2i or \u03b2ij when the decay rate is drawn independently from the strengthening rate. However, \u03b1i, \u03b2i, \u03b1ij, \u03b2ij are all drawn from power-law distributions with the same exponent -1. In the row titled \"mi\", the symbol U( 1, m max ) means that mi is drawn independently for each i from the uniform law on the set of integers a. event: (see also Figure 5) An event is the combination of an edge (i, j), a starting time t 0 and a stopping time t f such that (i, j) is inactive at t 0 -1 and t f + 1, and is active \u2200t such that t 0 \u2264 t \u2264 t f . b. bursty period [18]: Two events are defined as adjacent if they are defined on the same edge and if the delay between them is less than a given time lapse \u2206t. A bursty period is a maximal collection of adjacent events (see Fig. 5).\nc. aggregated network: The aggregated network on the whole temporal interval 1, T is the weighted undirected graph A such that A ij is the aggregated weight of the edge (i, j), i.e., the number of time steps such that G ij (t) = 1.\nd. aggregation level: We define the interaction graph aggregated at level n, G (n) , as follows: (i, j) \u2208 G (n) (t) \u21d0\u21d2 (i, j) \u2208 G( nt, n(t + 1) ). Note that G (n) is unweighted and undirected. We have G (1) = G, and G (T ) is an unweighted aggregated network on the all temporal interval. Observables of G (n) are called the observables at aggregation level n.\ne. Egocentric Temporal Network (ETN) [26,27]: An ETN (see Fig. 6) corresponds to a representation of the diversity of the interaction partners of a given node (the ego, in red in Fig. 6) at d consecutive times. In Fig. 6, each ETN reads from left to right (time flow direction). Green circles represent neighbors of the red node. An horizontal edge is drawn between two circles iff they correspond to the same node at different times. The duration of an ETN is called its depth. A (d, n)-ETN is an ETN of depth d and aggregation level n.\nf. ETN vector: An ETN vector is a vector V where the component V i is the aggregated weight of the ETN i.\nWe can now define the set of observables we will use to characterize and compare the temporal networks. The observables related to (temporal) subgraphs are (see table V):\n\u2022 aggregated weights for edges, (2,1)-ETN and (3,1)-ETN;\n\u2022 size of connected components of the interaction graph;\n\u2022 activity and interactivity duration for nodes and edges;\n\u2022 newborn activity for edges;\n\u2022 number of events per bursty period (Fig. 5).\nIn addition, we also consider:\n\u2022 the clustering coefficient of the aggregated network;\n\u2022 the degree assortativity in the aggregated network;\n\u2022 the (3, n)-ETN vector including the weights of ETNs computed in the aggregation levels n from 1 to 10: this allows to take into account various timescales in a single observable. At the crossing between the row entitled \"activity duration\" and the column entitled \"events\", we wrote an asterisk because the observable we used with events as objects and activity duration as type is called the number of events per bursty period.\nFIG. 5. Number of events per bursty period. The sketch represents the activation timeline of a pair of nodes (i, j). A grey rectangle represents an event for (i, j), i.e. a maximal period of uninterrupted interaction between i and j. A bursty period is a maximal collection of events that follow each other in time by a delay less than a given threshold, taken here to be 3 time steps. We draw a green junction between two consecutive events if they belong to the same bursty period, i.e. if they are separated by \u2206t \u2264 3 time steps, and a red junction if \u2206t > 3. In the example shown, we obtain two distinct bursty periods, with 3 events in the first and 2 in the second.\n---\nB. Comparison method\n\n\n---\nDistance tensor\n\nWe want to quantify how close are a synthetic temporal network and a reference empirical data set, with respect to a given observable. In order to be able to aggregate across observables and obtain a global distance and score, we consider for each observable a distance bounded between 0 and 1. Moreover, we need to consider different metrics for observables for which we have either (i) a distribution (e.g. activity durations or aggregated weights), or (ii) only one numerical value for each network (e.g. the clustering coefficient) or (iii) only one vectorial realization (e.g., ETN vectors).\na. Point observables. Let us first consider an observable O for which we have only one realization per data set O(D) \u2208 R, where D is the data set. Then we take as metric:\nD[O] D,D = |O(D) -O(D )| 2 max(|O(D)|, |O(D )|) (8\n)\nThis metric is bounded between 0 and 1, and reaches its maximum value only when O(D) = -O(D ).\nb. Observables with multiple realizations per data set If O(D) is a variable whose distribution P can be sampled, we need to consider a distance between the synthetic and empirical distributions. However O(D) and O(D ) may yield distributions not equally sampled, possibly on different supports. We choose here to obtain distributions of equal size, by completing the least sampled distribution with zeros, and compare them with the Jensen-Shannon divergence (JSD), which is bounded be- tween 0 and 1. For two discrete distributions p and q:\n\uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 JSD(p, q) = 1 2 (KL(p||m) + KL(q||m)) m = 1 2 (p + q) KL(p||m) = i p i log 2 ( pi mi )(9)\nWe thus consider the metric:\nD[O] D,D = JSD(P [O(D)], P [O(D )]).(10)\nc. Vector observables. Now let us consider that O(D) \u2208 R d , with some d \u2208 N. In our context, this corresponds to the ETN vectors. As we are interested in the relative frequencies of the ETNs, we use the cosine similarity:\nsim(O(D), O(D )) = O(D) \u2022 O(D ) ||O(D)|| ||O(D )|| . (11\n)\nIn fact, in the case of (3, n)-ETN, we have a family of n vectors {v p } p=1,\u2022\u2022\u2022 ,n : the i th component of v p is the ratio between the number of occurrences of the motif i at aggregation level p and the number of occurrences of the most frequent motif at aggregation level p.\nWe thus define the similarity between the two families of n vectors as the product of cosine similarities between each pair of vectors at the same level of aggregation:\nSim(v 1 , . . . , v n ; v 1 , . . . , v n ) = n p=1 sim(v p , v p ),(12)\nand the distance between the two families is 1 -Sim. 2. compute the median of the distances between V and all empirical data sets:\nm inter O (V) = median(\u03b4[O] V,E )(13)\nwhere the index E runs over the empirical data sets;\n3. compute the characteristic distance between empirical data sets themselves:\nm intra O = median(D[O] E,E )(14)\nwhere the indices E, E both run over the empirical data sets (E = E );\n4. compute the interquartile range of distances between empirical data sets,\nQ 3 -Q 1 ;\n5. deduce the score of the model version V for observable O:\nscore O (V) = m intra O -m inter O (V) Q 3 -Q 1 . (15\n)\nThis procedure is illustrated in Fig. 7. A higher score corresponds to the fact that the model version has instances with statistical properties closer to the empirical ones, for the chosen observable. Note that, while this procedure is intended to provide a score to models, we can also apply it also to each empirical data set. The interpretation is then not a \"score\", but quantifies how close a data set is to the other ones.  II and the empirical data sets presented in Table III. For each data set, a different score is computed for each observable. The higher the score of a model, the closer the distribution of the associated observable is from the distributions in the empirical data sets. Said otherwise, a higher score means more realistic statistical properties for the associated observable.\n---\nC. Results\n\nWe first illustrate that our approach providing a score using the proximity tensor is compatible with a qualitative direct appreciation of the distributions. We then detail the genetic tuning of the free parameters. This allows us to identify the best model belonging to the class investigated here. We then investigate in more details the interplay between observables and the role of each mechanism in our model, i.e., which observables change when a given mechanism or hypothesis is changed.\n---\nIllustration\n\nFigure 8 displays the distribution of several observables for two empirical data sets corresponding to different contexts and three model versions. This illustrates how, for each observable that can be sampled, a higher score is associated with a distribution closer to the empirical ones.\nFor point observables (the clustering coefficient and degree assortativity of the fully aggregated network), the score associated with a point observable does not necessarily reflect the degree of proximity with an empirical reference (not shown): this is due to the fact that these point observables are highly variable from one empirical data set to another.\nIt is more difficult to check the accordance between a high score for the ETN vector observable and realistic motifs because we can visualize only a few motifs. As an illustration however, we display in Fig. 9(a) the five most frequent motifs at aggregation level 5 of the \"utah\" data set and the instances associated with this reference of the models with highest and lowest ETN scores. The \"utah\" instance of the version with the highest ETN score has exactly the same 5 most frequent (3,5)-ETN as the \"utah\" reference, while this is not the case of the instance of the version with the lowest score.\nFigure 9(d) moreover shows the ETN autosimilarity for three model versions and two references. We define the ETN autosimilarity of a data set at a given depth d and aggregation level n as the ETN similarity (defined in III B 1 c) between the (d, n)-and (d, 1)-ETN vectors of this data set. The empirical references are highly autosimilar, i.e. their ETN autosimilarity is close to 1 for various levels of aggregation. We also display in the figure the ETN autosimilarity of three model versions (V9, V12 and V14), using in each case the instance tuned to be as close as possible to the reference \"utah\". The higher the score of a model version, the closer its ETN autosimilarity curve to the \"utah\" reference.\n---\nTuning the models' parameters by a genetic algorithm\n\nFor each model version and each reference data set, we want to obtain the parameter values that yield temporal networks instances as close as possible to the reference. Recall that given a reference data set and a model version, there are three types of parameters: (i) frozen parameters that depend only on the version, like the bounds for the power-laws of strengthening and decay rates \u03b1 ij , \u03b2 ij ; (ii) readable parameters that depend only on the reference, like N , T , a min obs and a max obs ; (iii) free parameters, that depend both on the version and the reference, that we tune to get as close as possible to the reference data set, e.g., p c or m max (see Table IV for the list of parameters).\nTo tune the free parameters, we use a genetic algorithm (described in the SM), with a fitness set to the distance between the reference data set and the instance of the temporal network generated by the model. However, computing the distances for all observables is computationally costly while, in a genetic algorithm, the fitness computation should be fast as it is computed at each iteration and for each genetic sequence. Therefore, we choose here to use as fitness only the distance relative to the ETN vector with the first ten levels of aggregation, i.e. the (3, n)-ETN for n = 1, . . . , 10. This observable is indeed computationally efficient and covers various time and spatial scales.\nWe find that this is enough for the model to improve on other observables too: we illustrate this point in the SM by comparing random instances with tuned instances along several observables. Some distributions remain different from their empirical counterparts, in particular the distributions of sizes of connected components (\"cc size\"), which however differ also between data sets. A better agreement and better scores might be obtained at the cost of an increased computational effort, by including additional features in the genetic algorithm fit-FIG. 8. Illustration of the scores for several observables and models. The figure displays in each panel the distribution of an observable (\"cc size\": size of connected components) for two data sets (the conference \"conf16\" and the school \"utah\") and three model versions: the original ADM (V14), the adjacent version with the highest average score (V9) and the version with the lowest average score (V12). The three models were optimized with respect to the \"utah\" reference. The score is computed in each case by Eq. 15: a higher score is associated with a distribution closer to the empirical one. In the top left panel, \"utah\" has a low score because the \"cc size\" observable is the only distribution which is not similar for all empirical references.\nness. Overall, how to keep the computational effort of the genetic algorithm low while obtaining a good similarity between model and data statistics on a large range of observables remains an open interesting question. We have also checked that the fitness is positively correlated with the score of every observable, which means that, despite these limitations, the genetic tuning does what it was intended to: obtain instances with closer statistical properties from empirical references than random instances in every observable. In the SM, we also investigate how the values of the tuned parameters are distributed across versions and references.\n---\nMost realistic model within the ADM class\n\nTo compare the models, we first compute for each observable a ranking of the model versions using their score, computed using the distances between each instance obtained by the genetic tuning and the corresponding refer-ence data set. To then determine the best model among the 14 versions presented above, it is necessary to define a global score for each model version. We consider two possible strategies:\n\u2022 the global score of a model (or data set) is given by its rank averaged over all observables;\n\u2022 the global score of a model (or data set) is given by its score summed over all observables and the global rank is just the rank according to the global score.\nNote that other global ranks could be obtained by attributing different weights to the score or rank for different observables. We choose here however not to favor an observable over another. The resulting rankings are shown on Figure 10. The original ADM performs very low in both rankings, and the two best versions are the baseline version and the version 9, i.e. with \u03b1 ij = \u03b2 ij = \u03b1. We also show in the SM the rankings of ETN autosimilarity for three models (the best V9, the worst V12 and the original ADM V14) and two empirical references. The ETN autosimilarity of a given data set at aggregation level n is the cosine similarity between the motifs observed in this data set at level n and the motifs observed in this data set at level 1. Empirical references are highly autosimilar with respect to this measure. For models, a higher score for the ETN vector corresponds to a higher ETN autosimilarity.\nall model versions for each observable separately: despite a rather large variability between rankings, the baseline version remains within the five first ranks for 6 observables, and the version 9 for 8 observables.\nIn the next subsections, we investigate this global result in more details, to understand in particular how each model performs with respect to each observable, and the impact of the various mechanisms on the models' performances.\n(a) averaging the ranks (b) averaging the scores FIG. 10. Global ranking of the models 1 to 14 according to two possible ranking strategies. In each panel, the y coordinate is arbitrary: empirical data sets have been placed at y = 0 while artificial data sets are aligned at y = 1. The global ranking is obtained by sorting the models by increasing order of the x coordinate (hence \"inverse global score\"). In each panel, the best version as well as the original ADM are highlighted as red dots. The best version in the other ranking strategy is displayed as a yellow dot. Blue vertical dashed lines indicate either the crossover or the gap between empirical and artificial data sets. (a): The x-coordinate is given by the model rank averaged over all observables. The best version is the baseline V1, almost ex aequo with version 9. (b) the x-coordinate is given by the opposite of the averaged score, shifted by the maximum averaged score to take positive values. Version 9 (\u03b1ij = \u03b2ij = \u03b1) is here the best one. In both panels, the original ADM is ranked rather low. There is either an overlap (a) or a small gap (b) between models and empirical data sets. Thus the class of models considered here is able to generate synthetic data sets with statistical properties close from real data sets.\n---\nSimilarity between observables\n\nFirst, we need to investigate the fact that the observables we have chosen to characterize our social temporal networks are not independent. In particular, when modifying a modeling hypothesis or a parameter value, several observables may be modified in a correlated way. Understanding these correlations can help better interpret the effect of varying the modeling hypotheses. We thus define a similarity between two observables as the Kendall tau between the rankings of the models using these ob-servables. The resulting similarity matrix between observables is shown in Fig. 11. We then extract groups of correlated observables by converting this matrix into a weighted undirected network: the nodes of this network are the observables and the weight w OO is the absolute value of the Kendall similarity between rankings of observables O and O . The network is shown in Fig. 11, on which we use the community detection algorithm of the software Gephi [37], based on modularity maximization, to obtain the three following groups:\n\u2022 group I (blue): node activity interduration, edge weight, size of connected components, ETN vector and (2,1)-ETN weight;\n\u2022 group II (orange): node, edge and newborn edge activity duration, degree assortativity and (3,1)-ETN weight;\n\u2022 group III (green): edge activity interduration, events activity duration and clustering coefficient.\n---\nImpact of hypotheses on model performances\n\nIn order to have more precise information about how hypotheses impact each observable, depending on the group it belongs to, we define for each model version its score relative to a group of observables as follows: 3. we sum the differences obtained for each observable in the group.\nFigure 12 shows the resulting group scores for the various versions. We also indicate the relative contribution of each observable inside the group to the group score. Finally, we summarize in Table VI which hypotheses lead to an improvement or a worsening with respect to the baseline version.\nFigure 12(a) indicates that the baseline version seems to be optimal for observables from group I and III, since no version exhibits improvement on either group. However, 8 out of 12 adjacent versions show an improvement for group II. The most common signature is 0, +, 0: 5 versions show no change on groups I and III and an improvement on group II.\nIn terms of mechanisms, updating the social bond graph with a linear Hebbian process with no decay (V2) improves over the exponential Hebbian process of the baseline version, but if we use an exponential Hebbian process with a uniform value \u03b1 ij = \u03b2 ij = \u03b1 (V9), then we get still better results. Thus, in order to recover a more the associated rankings of our model versions (note that the results might be different if considering a different ensemble of models). The observables are quite independent from each other (low absolute values for the similarity for almost all observables pairs). Strong negative couplings are observed only between the point observables and some distribution observables. The ETN vector is either positively or weakly coupled to every other observable, in accordance with the conclusion of III C 2: improving on motifs generally means improving on other observables. (b) The matrix is turned into a weighted network by taking as edge weight the absolute value of the Kendall similarity. A community detection algorithm based on modularity optimization detects three groups of observables (colored according to the group they belong to). The thickness of an edge is proportional to its weight and we filter out small weights for visualization purposes.\nrealistic social system, agents should all update their social ties in the same way, i.e. with the same homogeneous parameter \u03b1. The observation for the intrinsic activity a i is the opposite: imposing a uniform value a i = a (V12) leads to a drastic loss in score for all groups. Heterogeneity in the intrinsic activities seems to be necessary to recover a realistic social system. On the other hand, a uniform number of emitted interactions (V13) leads to an improvement. Actually (see SM), the value for m or m max returned by the genetic tuning is 1 in most cases: a higher value probably causes the nodes to have a too large instantaneous degree, i.  ETN motifs. Figure 12(a) also yields interesting insights concerning the update of the social bond graph and the contextual interactions. A uniform node pruning (V3) leads to poor performance on group III observables quite as equivalent as the gain over group II. Not taking into account the social context, i.e. putting c ij = 1 (V5) also leads to opposite changes: we gain over group II and loose over group III. Regarding contextual interactions, considering them leads to a significant improvement under the condition that they are treated as pure noise (V7). Having no contextual interaction at all (V8) also leads to an improvement, but of smaller amplitude. Thus adding noise in our system makes it more realistic, which can be understood by the fact that many interactions have in fact little social significance and occur only due to context.\n- 0 5 cij(t) = 1 0 + - 6 R = W = I \u222a C 0 0 - 7 R = W = I 0 + 0 8 pc = 0 0 + 0 9 \u03b1 0 + 0 10 \u03b1i, \u03b2i 0 0 0 11 \u03b1ij, \u03b2ij 0 + - 12 a - - - 13 m 0 + 0\nFigure 12(b) gives more detailed information by indicating the relative contribution of each observable to the group score. In particular, some observables always give a negligible contribution to the score of the group they belong to. This is in part due to the fact that some observables are shared across all versions, i.e. their realizations are similar in all versions. This is the case of \"ETN2 weight\" and \"ETN3 weight\", whose distribution always match almost perfectly the empirical case (after genetic tuning).\nOther observables are shared across almost all versions, like \"node activity\" and \"node interactivity\", which are similar for all versions except version 12, characterized by a i = a (however, for this version the loss in score relatively to those two observables is negligible compared to the loss relative to the other ones).\nOverall, the 8 major observables, which are mainly responsible for the observed group scores, are:\n\u2022 in the group I: \"size of connected components\", \"edge weight\", \"ETN3\"\n\u2022 in the group II: \"edge activity\", \"edge newborn activity\"\n\u2022 in the group III: \"edge interactivity\", \"clustering coeff\", \"edge events activity\"\nAll observables relative to edges are major observables. However, the fact that an observable contributes a lot to the score of its group does not mean that it is necessarily relevant: as the point observables are not shared across empirical references, we must be careful when we score a model relatively to them. For instance, if we considered that only relevant observables should be robust over empirical social systems, then the clustering coefficient and the degree assortativity should not be used to score and rank models. Why some observables contribute more than others might also depend on how shared they are between references: if an observable has almost the exact same realizations in every empirical reference, then the associated interquartile range will be almost zero, which can lead to high variations in the score for models (cf. Eq. 15). It is finally important to note that, except for the original ADM (V14), the model versions considered differ from the baseline version by one hypothesis only. The question arising naturally is the following: if we accumulate modifications with respect to the baseline version, do variations in score accumulate accordingly? If so, Table VI could be used to design even more realistic models by combining the hypotheses that lead to improvements: for each mechanism, we can check whether the variation from the baseline leads to an improvement or not, and combine the variations that do. We explore this avenue in the SM for several composite versions. The relation between the score of a composite version and the scores of its adjacent components is however non trivial, and the best version remains V9 even when taking into account the composite versions.\n---\nIV. DISCUSSION\n\nIn this paper, we have presented a general framework allowing to design various models by controlling their qualitative aspects. We have considered a modeling framework based on the idea of a co-evolution of an observed interaction network and an underlying and unobserved social bond network. Within the overall framework of the activity driven model with memory [21,30], we hypothesized that social bonds partially drive the observed interactions, together with an influence of the current social context, and that interactions impact social bonds [22]: the corresponding strengthening and weakening of social bonds take into account the fact that an interaction reinforces a social bond, and that resources (time, energy) are needed to maintain a social bond, so that the absence of interaction weakens it. Instead of the usual exploration of a parameter space for a given set of mechanisms, we have then considered, within this framework, an exploration of a hypotheses space, corresponding to representations of several possible social mechanisms. Parameters corresponding to each hypothesis are then tuned by a genetic algorithm to maximize the similarity between model instances and a given empirical data set. While such similarity can be defined a priori in many ways, we find that using only the ETN vector to quantify it and tune the parameters leads to an improvement for many other observables, indicating that many statistical properties of a social temporal network are related to its ETN motifs [27]. We recall that the ETN vector is given by the list and frequencies of ETN motifs at various levels of aggregation (1 to 10 in our case), which thus encodes several spatiotemporal scales. This procedure allows us to define a score for each model, relative to each observable considered and globally, and to deduce which mechanisms lead to more realistic artificial temporal networks. In particular, many of the model versions considered perform better than the original Activity Driven model with memory. Once tuned, each model version can produce synthetic data sets of arbitrary sizes and durations and with realistic properties, which can be used for instance as support for numerical simulations of dynamical processes on temporal networks.\nOur work entails a number of limitations that are worth discussing. First, the list of observables we consider to rank models is somewhat arbitrary: we investigated observables of different types (point, with multiple realizations, vector) and dealing with various scales, but other observables could be thought of, while some might be removed from the list because of their variability among the empirical references (e.g., clustering coefficient). Second, the scoring mechanism may also be improved. Indeed, a higher score is not always clearly associated with a value of the observable closer to the empirical value. Future work will thus address the issue of building another ad hoc score measure with a clearer interpretation.\nThe use of a series of statistical properties to determine whether a model is producing realistic temporal networks can also be discussed. Indeed, empirical data sets show large activity variations, i.e., in the number of interactions per timestamp. These variations can be driven by changes in population size or in intrinsic activity [38], either due to imposed schedules or to spontaneous bursts. Such patterns cannot be recovered in the class of models we have explored, for which the number of interactions per timestamp is stationary with small fluctuations. Exploring other classes of models would be necessary to account for the large empirical variations. The methodology considered in this paper could however then still be used to cover such extended classes. In particular, our results suggest that the full exploration of the hypotheses space is not necessary, as properties of composite models could be predicted from their adjacent components.\nDespite these limitations, the partial exploration we performed allowed to determine models with a much higher degree of realism than the original ADM, and also to show the interest of modeling several social mech-anisms such as taking into account the social context, considering casual interactions (dynamic triadic closure) and updating the underlying social bond ties through an exponential Hebbian process with both strengthening and weakening mechanisms. The class of models we have considered could also be extended, e.g. by adding group memberships, or by considering various types of Hebbian processes: delayed or anti-Hebbian process, or allowing negative interactions and possibly negative social bonds [34,35].\nThe empirical temporal networks we use represent face-to-face interaction data. Individuals are represented as nodes, and an edge is drawn between two nodes each time the associated individuals are interacting with each other. In practice, interactions are detected by wearable sensors that exchange low-power radio signals [1,2]. This typically allows to detect face-to-face close proximity (\u223c 1 meter) with a temporal resolution of about 20 seconds. The data sets we consider here are publicly available thanks to two independent collaborations. The first is provided by Toth et al. [2] and the others by the SocioPatterns collaboration [1]:\n\u2022 the \"utah\" data set describes the proximity interactions which occurred on November 28 and 29, 2012 in an urban public middle school in Utah (USA) [2];\n\u2022 the \"highschool3\" data set gives the interactions between 327 students of nine classes within a high school in Marseille, during 5 days in December 2013 [3];\n\u2022 the \"conf16\" data set was collected during the 3rd GESIS Computational Social Science Winter Symposium, held on November 30 and December 1, 2016. It is described and called \"WS16\" in [4];\n\u2022 the \"conf17\" data set was collected during the International Conference on Computational Social Science, held from July 10 to 13, 2017. It is described and called \"ICCSS17\" in [4];\n\u2022 the \"work2\" data set contains the temporal network of contacts between individuals recorded in an office building in France during two weeks in 2015 [5].\nIn practice, as timestamps in the data are multiple of the temporal resolution, we divide the times by the temporal resolution in order to relabel them as successive integers, removing moreover timestamps in which no interactions are observed.\n---\nB. Statistical properties\n\nEmpirical temporal networks show broad distributions for many observables [6], which indicates strong fluctuations as well as the absence of any characteristic scale. These distributions are very similar for temporal networks obtained in various social contexts, which indicates that their statistical properties may be emergent properties that do not depend strongly on the microscopic dynamics of human relationships. Instead they may depend on a small number of features of these relationships, which would make it possible to obtain realistic temporal networks without a deep understanding of human behaviour.\nWe compare the statistical properties of the empirical data sets with respect to the observables we have considered in this paper. For the point observables studied, conferences show different properties from the two schools and the workplace. Conferences have a high clustering coefficient (\u223c 0.7) and a negative degree assortativity (\u223c -0.1). The three other data sets have a lower clustering coefficient (0.3-0.5) and a positive degree assortativity (0.02-0.07).\nOn the other hand, distribution and vector observables show similar properties across data sets (as illustrated in Supplemental Figure 1) despite their differences in number of nodes and edge activity (the mean number of interactions per time step). In particular the duration of an interaction follows the same statistics in the conferences data sets, where the edge activity is high, as in the workplace data set, where the edge activity is low. Thus it seems that the duration of a face-to-face interaction is not influenced by how rare those interactions are.\nAs the number of observables is high, it is useful to introduce a global similarity matrix relative to each type of observable. We define the similarity between two data sets D and D relative to a given type T of observable as the product of the similarities between those data sets with respect to each observable O of the chosen type:\nSim T (D, D ) = O,Type(O)=T (1 -D[O] D,D )(1)\nwhere D is the distance tensor introduced in the main text. As we have three types of observables, this gives rise to three similarity matrices visible on Supplemental Figures 1(d), 1(e) and 1(f). Supplemental Figure 1 shows that differences exist between empirical data sets for all types of observables. is an expected exception since it depends strongly on the edge activity, which is not shared across data sets. Panels 1(d) to 1(f) give the similarity matrices between empirical data sets for the various types of observables (1(d): point observables; 1(e): distribution observables; 1(f): ETN vector observable): the element at row x and column y of a matrix gives the similarity between the observables measured in data sets x and y (the similarity for an ensemble of observables is defined as the product of the similarities measured for each observable). For the point observables, the conferences are clearly separated from the other data sets. Panel 1(f) shows instead that the motifs are quite robust: different contexts, like a workplace, conferences or a highschool exhibit similar motifs. Only the primary school \"utah\" seems to distinguish itself by its motifs. A possible explanation is the age of the social agents at play: in the \"utah\" data set, they are mainly children while in all other data sets, social agents are young adults or older.\nCan we identify the observables responsible for the low similarity observed between some data sets? To do so, we first collect every similarity value between each pair of data sets with respect to each observable of each type, and build an histogram from it. This way, we obtain one similarity histogram per type of observable, as displayed on Supplemental Figure 2. We can then choose a threshold and collect all the observables responsible for similarity values below this threshold. As there are only two point observables and one vector observable, we consider this issue only for the distribution observables. It turns out that all similarities below 0.87 are only due to the weights of (2,1) and (3,1)-ETN (\"ETN2 weight\" and \"ETN3 weight\"). However Supplemental Figures 2(d) and 2(f) seem to indicate a robustness of these observables, while the distribution of connected components sizes varies more (Supplemental Fig. 2(e)). We have used a similarity measure based on the well-known Jensen-Shannon divergence (JSD), a well-known distance. In fact, Supplemental Figure panels 2(d), 2(e) and 2(f) in-dicate that the heads of the distributions are similar for the observable \"cc size\" while they are more dissimilar for \"ETN2 weight\" and \"ETN3 weight\". It might thus be interesting to define another measure of similarity than the JSD that would coincide more with the visual inspection of these broad distributions. obtained after tuning: even if the genetic algorithm takes into account only the similarity with the ETN vector, optimizing this feature allows to improve also on other observables, even if the agreement is not always perfect (Note that the worst agreement is obtained for the distributions of connected component sizes; however, this observable also varies substantially from one data set to another).  most of the float parameters are strongly affected by the genetic tuning, since their distribution over instances is far from the uniform case after tuning. Only two parameters are viewed by the KS test as possibly having a uniform distribution: p u and \u03b1. To check whether there are false positives, we display their distribution after tuning on Supplemental Figure 6. The distribution of p u (panel 6(a)) over all instances seems to be compatible with the uniform case. However it is less clear for the distribution for \u03b1 (panel 6(b)), as many values are missing and a value close to 1 seems preferred. Further investigation would be needed to decide whether or not p u and \u03b1 are relevant parameters in the class of models we have considered.\n---\nSupplemental Section 4. THE NODE WEIGHT OBSERVABLE\n\nIn the main text we have considered 13 different observables, in particular the weight of edges in the aggregated network. We did not consider the node weight (the number of timestamps a node is present with degree non-zero), although this observable has a strong link with an important parameter of the activity-driven class: the distribution of the nodes intrinsic activities. Recall that the node intrinsic activity of a node i is the probability a i that this node emits intentional interactions at each time step. The form of the distribution for a i has proven to be relevant, since in this paper we showed that a power-law distribution for node intrinsic activities yielded more realistic statistical properties than a uniform activity a i = a, \u2200i (cf V12). Although it would thus be of fundamental interest to be able to measure the empirical distribution of a i , we do not have direct access to it but only to the node weight. We can however study how the node weight and the node intrinsic activity distributions are related in the models, and on the other hand we can compare the distributions of the node weight in the models and in the empirical data sets.\nWe show in Supplemental Figures 7 and8 the node weight distributions for some empirical and artificial data sets (rescaled to have a support between 0 and 1). These distributions differ from one empirical reference to another and between models. Empirical references tend to have a more peaked distribution than the models (except V12).\nMoreover, Supplemental Figures 7 and8 show that the distributions of intrinsic activity and node weight differ. A shift towards higher values is apparent, as expected since an active node can interact with an inactive node. The shape of the distribution is also different, and the node weight distribution is closer to the intrinsic activity one when the reference contains less interactions.\nTo summarize:\n\u2022 the node weight distribution varies across empirical data sets;\n\u2022 it is different between references and model instances;\n\u2022 for a chosen version, the denser a reference data set, the more different are the node weight and intrinsic activity distributions for the associated model instances.\nWe confirm these observations using Kolmogorov-Smirnov (KS) tests between observations. We define similarity matrices between data sets as follows: A data set is declared similar to another if the p value returned by the KS test of the two-sided hypothesis of identity between their node weight CDF is greater than 0.05. In this case, their similarity is set to 1. Otherwise they are declared dissimilar, and their similarity is set to 0. Results are displayed in Supplemental Figure 9. We also consider rescaled node weight (between 0 and 1 or not) to compare the shapes of the distributions. Even after rescaling, empirical data sets are found to be all dissimilar to each other (cf fig 9(b)). For the models instead, groups with similar rescaled node weight distributions are obtained (cf fig 9(c), 9(d) and 9(e)). It seems that models separate into two main families: models with a Dirac law for the intrinsic activity (models 12, 17 and 1) and models with a power-law. Thus similar distributions for the intrinsic activity are associated to similar distributions for the node weight observable. Supplemental Figure 9. KS test of the two-sided hypothesis of identity between the node weight CDF in models and empirical data sets. For each similarity matrix, a white box refers to a similarity of 1 and a black box refers to a similarity of 0. Panels 9(a) and 9(b): According to the KS test, empirical data sets are all different from each other, in accordance with the observed fact that the node weight is not a universal observable. In panel 9(b), the node weight has been linearly rescaled between 0 and 1 before performing the KS test. Panels 9(c) to 9(e): Before performing the test, the node weight has been linearly rescaled between 0 and 1. Contrary to the case of empirical references, we observe large communities, meaning the node weight observable is shared between many models despite their mechanistic differences. The models for which the intrinsic activity is a Dirac are the versions 12, 17 and 18.\n---\nSupplemental Section 5. COMPOSITE MODELS\n\nTo do better than versions 1 and 9, we need to explore the ADM class further than one hypothesis away from the baseline. However testing all possible combinations of hypotheses would require large computational resources. Instead, we want to check whether the signatures combine when we combine hypotheses, or more precisely, if \u2206s(hyp) denotes the score variation of hypothesis \"hyp\" with respect to the baseline for a given group of observables, whether: \u2200 hyp1, hyp2, \u2206s(hyp1) > 0, \u2206s(hyp2) > 0, \u2206s(hyp1, hyp2) > ??\nmax(\u2206s(hyp1), \u2206s(hyp2))\nAs we cannot check this assumption for all possible combinations of observables, we will test it on some combinations that we expect to perform better and on some combinations that we expect to perform worse than the baseline. We can determine such expectations from what we learned in the previous sub-subsection:\n\u2022 contextual interactions are relevant as pure noise: R = W = I (as V7 has a better score than the baseline V1);\n\u2022 social context as implemented here is relevant: c ij should not be set to one (V1 has a better score than V5);\n\u2022 the social tie rate should be shared by all nodes: \u03b1 ij = \u03b2 ij = \u03b1 (as V9 has a better score than V1);\n\u2022 all active nodes should emit the same number of intentional interactions: m i = m (as V13 has a better score than V1);\n\u2022 the node activity should be drawn from the powerlaw of the baseline, and not be shared by all nodes (as V1 has a better score than V12);\n\u2022 edges have to be pruned, rather than nodes (as V1 has a better score than V3);\n\u2022 the egonet growth rate should be constant (as V1 has a better score than V4).\nThus, by adding to the baseline model only the changes in mechanisms that yield separately an improvement of the performance, we deduce that the composite model 7+9+13 is expected to perform the best. Moreover, we consider the following combinations:\n\u2022 V15: 2+5+8+13 (same as the original ADM but with edge pruning and constant egonet growth rate);\n\u2022 V16: 5+8+11+13 (same as V15 but with a heterogeneous exponential Hebbian process instead of linear);\n\u2022 V17: 3+5+8+9+12+13 (simplest version with an exponential Hebbian process);\n\u2022 V18: 2+3+5+8+12+13 (simplest version with a linear Hebbian process).\nWe denote as V19 the best expected case 7+9+13 and we include V14 in the set of composite versions. Supplemental Figure 10 leads to the conclusions that (i) the sign of the summed scores of the adjacent versions is a good indicator of the sign of the score of the resulting composite version, and (ii) a linear relation with positive slope approximates well the relation between the actual score of a composite version and the summed scores of its adjacent components. Hence the relation between the statistical properties of a composite model and its adjacent components seems to be rather simple.\nHowever this relation is not trivial (Supplemental Fig. 10): for group II in particular, there is a shift between the score of a composite version and the combination of the scores of its components. As a result, V19 is not the best version, as seen in Supplemental Figure 11 which shows the global rankings of adjacent and composite models (see also next section).\n---\nSupplemental Section 6. RANKINGS OF MODELS\n\nIn the main text we have defined a global ranking for models. However, as a different score is computed w.r.t. each observable, we also have one ranking for models per observable. In the main text, we have shown that the Kendall similarities between those rankings are small. However, as the Kendall similarity puts on an equal footing the head and the tail of the rankings, we also give the complete rankings w.r.t. each observable in Supplementary Tables I andII \n---\nSupplemental Section 2. DESCRIPTION AND ILLUSTRATION OF THE GENETIC ALGORITHM\n\nRecall that a model instance has several free parameters, and that we want to select the instance closest to a given reference data set. The tuning of the free parameters is performed using a genetic algorithm. In the language of those algorithms, it can be described as follows:\n1. Decide of a genetic code (sequence of binary digits)\nfor the free parameters. In our case, probabilities or floats were coded on ten digits while integers were coded on four.\n2. Initialize randomly a population of genetic sequences. We took 40 sequences.\n3. Compute the fitness of each sequence. To minimize computation, we stored in a dictionary the fitness of already evaluated sequences. To evaluate the fitness of a new sequence, first we translate the sequence into parameter values, second we generate the artificial data set using those parameters and our model, third we define the fitness of our sequence as the ETN vector similarity between the generated data set and the associated reference.\n4. Take from the population the sequences with the greatest fitness, discard the others and refill the population with combinations between the selected sequences. In practice we took the 7 best sequences, to which we added a sequence generated at random to avoid premature convergence to a local maximum. We combined sequences by pairs, using single-locus and double-loci crossover as well as random mutations. We also imposed the best sequence to survive in the next generation, to ensure the fitness of the best individual is monotonically increasing across generations.\n5. Define a stopping criterion. The algorithm stops after 20 generations and returns the parameters which yield the highest fitness. We have checked that this is enough for the fitness to saturate in all cases.\nIn Supplemental Figure 3, we illustrate the evolution of the fitness across generations of sequences. Panel 3(c) shows the existence of a maximum fitness, around 0.8-0.9 and independent of the initial fitness. This indicates the existence of a limit on the ETN vector similarity, intrinsic either to the algorithm used for tuning, or the class of models considered, or both. Further investigation would be needed to understand the origin of this limit, and how to go beyond.\nSupplemental Figure 4 finally compares several observables measured on random model instances with tuned instances: a clear improvement in the similarity of distributions between the model and the empirical data is For every model version and every reference we collect the best fitness in the population of sequences before and after the genetic tuning. In each case we build the fitness histogram. Before tuning, the most probable value for the fitness is close to zero while after tuning, it lies around 0.8-0.9. Panel 3(c): The fitness improvement, defined as the difference between the fitness after tuning and the fitness before, seems to decrease linearly with the initial fitness, bounding the final fitness below a maximum value of 0.8-0.9, independent of the initial fitness.\n---\nSupplemental Section 3. VARIABILITY OF THE GENETIC TUNING ACROSS REFERENCES\n\nEach model version is tuned successively to the various empirical data sets taken as reference. Here we detail how the tuned parameters of a version change with the reference. In particular, for each parameter, we want to check whether it is distributed at random after tuning. If yes it means that this parameter has no impact on ETN motifs, and thus should be removed from the list of free parameters. If its distribution is a Dirac peaked at a given value, this parameter should also be removed from the list of free parameters, and become a parameter frozen at this value.\n---\nA. Integer parameters\n\nThere are three integer parameters:\n\u2022 c: sets the egonet growth probability when it is variable according to p g (i) = c c+di , where i is a social agent and d i its number of neighbours in the social bond graph;\n\u2022 m: number of intentional interactions emitted by a social agent when it is constant;\n\u2022 m max : maximum number of intentional interactions when it is variable across social agents.\nWe show in Supplemental Figure 5 how c, m and m max are distributed among the tuned instances. The only value selected by the genetic algorithm for c is c = 1. This makes sense because c can be interpreted as the minimum size of the egonet of a social agent. Similarly, nodes tend to interact with a few contacts at each time step so it makes sense that the value m = 1 is preferred by the algorithm (cf panel 5(b)). Only for the instance (V14, \"highschool3\") we observe m = 1. A different behaviour is observed for m max . For most instances it equals 1 but for instances tuned w.r.t. the conference references, it takes higher values depending on the model version. This makes sense because the conference data sets have the highest density, i.e. number of active links per time step.\n---\nB. Float parameters\n\nThere are 9 float parameters:\n\u2022 a: probability that a node emits intentional interactions;\n\u2022 \u03bb: sets the probability of removing a directed tie in the social bond graph, based on the tie's weight;\n\u2022 \u03b1: sets the rate at which the weight of a tie changes in the social bond graph upon activation or inactivation; \u2022 a min : lower bound for the probability of emitting intentional interactions;\n\u2022 a max : upper bound for the probability of emitting intentional interactions;\n\u2022 p c : sets the probability of dynamic triadic closure;\n\u2022 p u : when a node chooses to grow its egonet, probability of growing it by choosing a new partner at random instead of triadic closure;\n\u2022 p g : probability that a node grows its egonet;\n\u2022 p d : probability that a node resets its egonet to the empty set.\nFor each parameter we perform a Kolmogorov-Smirnov test to check whether or not its distribution has been altered by the genetic tuning. As expected, we find that    The best version is now V9 according to both ranking strategies. We highlight the baseline V1, the original ADM V14 and the composite version V19, that was expected to be the best under the hypothesis that the score of a composite version is the sum of the group scores of its adjacent components. As this hypothesis does not fully hold (Supp. Fig. 10), V19 has a good score but is not the best. (a):\nThe x-coordinate is given by the model rank averaged over all observables. (b): The x-coordinate is given by the opposite of the averaged score, shifted by the maximum averaged score to take positive values."
    ],
    "len_text": [
        27707,
        36641,
        26780,
        40459,
        21583,
        29440,
        22705,
        45867,
        32069,
        73865,
        47259,
        14799,
        28699,
        28888,
        20837,
        40768,
        29498,
        45615,
        29890,
        68782,
        32625,
        24749,
        17242,
        22534,
        43825,
        23755,
        23037,
        28052,
        43743,
        23907,
        38835,
        20428,
        34953,
        30927,
        42381,
        57922,
        36673,
        23997,
        132307,
        19769,
        71986,
        24767,
        37175,
        43187,
        72532,
        40597,
        41768,
        31188,
        47895,
        80009
    ],
    "len_abstract": [
        1971,
        1804,
        1600,
        1334,
        507,
        1670,
        1368,
        1345,
        1642,
        1790,
        1313,
        1353,
        1892,
        751,
        157,
        3253,
        100,
        1013,
        1219,
        1278,
        1256,
        35,
        1774,
        1829,
        1340,
        1714,
        2661,
        1758,
        1840,
        1960,
        2305,
        1852,
        1202,
        2303,
        1714,
        2085,
        1683,
        1422,
        2237,
        606,
        2152,
        1583,
        1838,
        1117,
        1655,
        935,
        1691,
        1327,
        2019,
        1373
    ]
}